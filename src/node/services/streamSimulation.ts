/**
 * Stream simulation: synthetic stream event sequences for special early-return conditions.
 *
 * Extracted from `streamMessage()` — these functions simulate the full stream
 * lifecycle (start → delta → end/error) without calling an AI provider.
 *
 * Used for:
 * - `forceContextLimitError`: OpenAI SDK testing of context-exceeded handling
 * - `simulateToolPolicyNoop`: OpenAI SDK testing of tool-policy-disabled handling
 */

import type { LatticeMessage, LatticeTextPart } from "@/common/types/message";
import { createLatticeMessage } from "@/common/types/message";
import type { ThinkingLevel } from "@/common/types/thinking";
import type { StreamDeltaEvent, StreamEndEvent, StreamStartEvent } from "@/common/types/stream";
import type { ToolPolicy } from "@/common/utils/tools/toolPolicy";
import type { HistoryService } from "./historyService";
import { createErrorEvent } from "./utils/sendMessageError";

// ---------------------------------------------------------------------------
// Shared context for both simulation paths
// ---------------------------------------------------------------------------

/** Common parameters shared by all simulated stream scenarios. */
export interface SimulationContext {
  minionId: string;
  assistantMessageId: string;
  canonicalModelString: string;
  historySequence: number;
  systemMessageTokens: number;
  effectiveAgentId: string;
  effectiveMode: "plan" | "exec" | "compact";
  effectiveThinkingLevel: ThinkingLevel;
  /** Emit a typed stream event (stream-start, stream-delta, stream-end, error). */
  emit: (event: string, data: unknown) => void;
}

/** Build the common StreamStartEvent used by both simulation paths. */
function createSimulatedStreamStart(ctx: SimulationContext): StreamStartEvent {
  return {
    type: "stream-start",
    minionId: ctx.minionId,
    messageId: ctx.assistantMessageId,
    model: ctx.canonicalModelString,
    historySequence: ctx.historySequence,
    startTime: Date.now(),
    agentId: ctx.effectiveAgentId,
    mode: ctx.effectiveMode,
    thinkingLevel: ctx.effectiveThinkingLevel,
  };
}

// ---------------------------------------------------------------------------
// forceContextLimitError simulation
// ---------------------------------------------------------------------------

/**
 * Simulate a context-length-exceeded error without hitting the provider.
 *
 * Writes an error partial, emits stream-start + error events, then returns.
 * Used by the OpenAI SDK `forceContextLimitError` provider option.
 */
export async function simulateContextLimitError(
  ctx: SimulationContext,
  historyService: HistoryService
): Promise<void> {
  const errorMessage =
    "Context length exceeded: the conversation is too long to send to this OpenAI model. Please shorten the history and try again.";

  const errorPartialMessage: LatticeMessage = {
    id: ctx.assistantMessageId,
    role: "assistant",
    metadata: {
      historySequence: ctx.historySequence,
      timestamp: Date.now(),
      model: ctx.canonicalModelString,
      systemMessageTokens: ctx.systemMessageTokens,
      agentId: ctx.effectiveAgentId,
      thinkingLevel: ctx.effectiveThinkingLevel,
      partial: true,
      error: errorMessage,
      errorType: "context_exceeded",
    },
    parts: [],
  };

  await historyService.writePartial(ctx.minionId, errorPartialMessage);

  ctx.emit("stream-start", createSimulatedStreamStart(ctx));
  ctx.emit(
    "error",
    createErrorEvent(ctx.minionId, {
      messageId: ctx.assistantMessageId,
      error: errorMessage,
      errorType: "context_exceeded",
    })
  );
}

// ---------------------------------------------------------------------------
// simulateToolPolicyNoop simulation
// ---------------------------------------------------------------------------

/**
 * Simulate a full stream lifecycle for a tool-policy-disabled noop response.
 *
 * Emits stream-start → stream-delta → stream-end, then updates history.
 * Used by the OpenAI SDK `simulateToolPolicyNoop` provider option.
 */
export async function simulateToolPolicyNoop(
  ctx: SimulationContext,
  effectiveToolPolicy: ToolPolicy | undefined,
  historyService: HistoryService
): Promise<void> {
  const noopMessage = createLatticeMessage(ctx.assistantMessageId, "assistant", "", {
    timestamp: Date.now(),
    model: ctx.canonicalModelString,
    systemMessageTokens: ctx.systemMessageTokens,
    agentId: ctx.effectiveAgentId,
    thinkingLevel: ctx.effectiveThinkingLevel,
    toolPolicy: effectiveToolPolicy,
  });

  const parts: StreamEndEvent["parts"] = [
    {
      type: "text",
      text: "Tool execution skipped because the requested tool is disabled by policy.",
    },
  ];

  ctx.emit("stream-start", createSimulatedStreamStart(ctx));

  const textParts = parts.filter((part): part is LatticeTextPart => part.type === "text");
  if (textParts.length === 0) {
    throw new Error("simulateToolPolicyNoop requires at least one text part");
  }

  for (const textPart of textParts) {
    if (textPart.text.length === 0) {
      continue;
    }

    const streamDeltaEvent: StreamDeltaEvent = {
      type: "stream-delta",
      minionId: ctx.minionId,
      messageId: ctx.assistantMessageId,
      delta: textPart.text,
      tokens: 0, // Mock scenario — actual tokenization happens in streamManager
      timestamp: Date.now(),
    };
    ctx.emit("stream-delta", streamDeltaEvent);
  }

  const streamEndEvent: StreamEndEvent = {
    type: "stream-end",
    minionId: ctx.minionId,
    messageId: ctx.assistantMessageId,
    metadata: {
      model: ctx.canonicalModelString,
      agentId: ctx.effectiveAgentId,
      thinkingLevel: ctx.effectiveThinkingLevel,
      systemMessageTokens: ctx.systemMessageTokens,
    },
    parts,
  };
  ctx.emit("stream-end", streamEndEvent);

  const finalAssistantMessage: LatticeMessage = {
    ...noopMessage,
    metadata: {
      ...noopMessage.metadata,
      historySequence: ctx.historySequence,
    },
    parts,
  };

  await historyService.deletePartial(ctx.minionId);
  await historyService.updateHistory(ctx.minionId, finalAssistantMessage);
}
