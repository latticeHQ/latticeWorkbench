// AUTO-GENERATED - DO NOT EDIT
// Run: bun scripts/gen_builtin_skills.ts
// Source: src/node/builtinSkills/*.md, src/node/builtinPlugins/, and docs/

export const BUILTIN_SKILL_FILES: Record<string, Record<string, string>> = {
  "bio-research-instrument-data-to-allotrope": {
    "references/asm_schema_overview.md": [
      "# ASM Schema Overview",
      "",
      "The Allotrope Simple Model (ASM) is a JSON-based standard for representing laboratory instrument data with semantic consistency.",
      "",
      "## Core Concepts",
      "",
      "### Structure",
      "ASM uses a hierarchical document structure:",
      "- **Manifest** - Links to ontologies and schemas",
      "- **Data** - The actual measurement data organized by technique",
      "",
      "### Key Components",
      "",
      "```json",
      "{",
      '  "$asm.manifest": {',
      '    "vocabulary": ["http://purl.allotrope.org/voc/afo/REC/2023/09/"],',
      '    "contexts": ["http://purl.allotrope.org/json-ld/afo-context-REC-2023-09.jsonld"]',
      "  },",
      '  "<technique>-aggregate-document": {',
      '    "device-system-document": { ... },',
      '    "<technique>-document": [',
      "      {",
      '        "measurement-aggregate-document": {',
      '          "measurement-document": [ ... ]',
      "        }",
      "      }",
      "    ]",
      "  }",
      "}",
      "```",
      "",
      "## Required Metadata Documents",
      "",
      "### data system document",
      "Every ASM output MUST include this document with:",
      "- `ASM file identifier`: Output filename",
      '- `data system instance identifier`: System ID or "N/A"',
      "- `file name`: Source input filename",
      "- `UNC path`: Path to source file",
      '- `ASM converter name`: Parser identifier (e.g., "allotropy_beckman_coulter_biomek")',
      "- `ASM converter version`: Version string",
      "- `software name`: Instrument software that generated the source file",
      "",
      "### device system document",
      "Every ASM output MUST include this document with:",
      "- `equipment serial number`: Main instrument serial",
      "- `product manufacturer`: Vendor name",
      "- `device document`: Array of sub-components (probes, pods, etc.)",
      '  - `device type`: Standardized type (e.g., "liquid handler probe head")',
      '  - `device identifier`: Logical name (e.g., "Pod1", not serial number)',
      "  - `equipment serial number`: Component serial",
      "  - `product manufacturer`: Component vendor",
      "",
      "## Available ASM Techniques",
      "",
      "The official ASM repository includes **65 technique schemas**:",
      "",
      "```",
      "absorbance, automated-reactors, balance, bga, binding-affinity, bulk-density,",
      "cell-counting, cell-culture-analyzer, chromatography, code-reader, conductance,",
      "conductivity, disintegration, dsc, dvs, electronic-lab-notebook,",
      "electronic-spectrometry, electrophoresis, flow-cytometry, fluorescence,",
      "foam-height, foam-qualification, fplc, ftir, gas-chromatography, gc-ms, gloss,",
      "hot-tack, impedance, lc-ms, light-obscuration, liquid-chromatography,",
      "loss-on-drying, luminescence, mass-spectrometry, metabolite-analyzer,",
      "multi-analyte-profiling, nephelometry, nmr, optical-imaging, optical-microscopy,",
      "osmolality, oven-kf, pcr, ph, plate-reader, pressure-monitoring, psd, pumping,",
      "raman, rheometry, sem, solution-analyzer, specific-rotation, spectrophotometry,",
      "stirring, surface-area-analysis, tablet-hardness, temperature-monitoring,",
      "tensile-test, thermogravimetric-analysis, titration, ultraviolet-absorbance,",
      "x-ray-powder-diffraction",
      "```",
      "",
      "See: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm",
      "",
      "## Common ASM Schemas by Technique",
      "",
      "Below are details for frequently-used techniques:",
      "",
      "### Cell Counting",
      "Schema: `cell-counting/REC/2024/09/cell-counting.schema.json`",
      "",
      "Key fields:",
      "- `viable-cell-density` (cells/mL)",
      "- `viability` (percentage)",
      "- `total-cell-count`",
      "- `dead-cell-count`",
      "- `cell-diameter-distribution-datum`",
      "",
      "### Spectrophotometry (UV-Vis)",
      "Schema: `spectrophotometry/REC/2024/06/spectrophotometry.schema.json`",
      "",
      "Key fields:",
      "- `absorbance` (dimensionless)",
      "- `wavelength` (nm)",
      "- `transmittance` (percentage)",
      "- `pathlength` (cm)",
      "- `concentration` with units",
      "",
      "### Plate Reader",
      "Schema: `plate-reader/REC/2024/06/plate-reader.schema.json`",
      "",
      "Key fields:",
      "- `absorbance`",
      "- `fluorescence`",
      "- `luminescence`",
      "- `well-location` (A1-H12)",
      "- `plate-identifier`",
      "",
      "### qPCR",
      "Schema: `pcr/REC/2024/06/pcr.schema.json`",
      "",
      "Key fields:",
      "- `cycle-threshold-result`",
      "- `amplification-efficiency`",
      "- `melt-curve-datum`",
      "- `target-DNA-description`",
      "",
      "### Chromatography",
      "Schema: `liquid-chromatography/REC/2023/09/liquid-chromatography.schema.json`",
      "",
      "Key fields:",
      "- `retention-time` (minutes)",
      "- `peak-area`",
      "- `peak-height`",
      "- `peak-width`",
      "- `chromatogram-data-cube`",
      "",
      "## Data Patterns",
      "",
      "### Value Datum",
      "Simple value with unit:",
      "```json",
      "{",
      '  "value": 1.5,',
      '  "unit": "mL"',
      "}",
      "```",
      "",
      "### Aggregate Datum",
      "Collection of related values:",
      "```json",
      "{",
      '  "measurement-aggregate-document": {',
      '    "measurement-document": [',
      '      { "viable-cell-density": {"value": 2.5e6, "unit": "(cell/mL)"} },',
      '      { "viability": {"value": 95.2, "unit": "%"} }',
      "    ]",
      "  }",
      "}",
      "```",
      "",
      "### Data Cube",
      "Multi-dimensional array data:",
      "```json",
      "{",
      '  "cube-structure": {',
      '    "dimensions": [{"@componentDatatype": "double", "concept": "elapsed time"}],',
      '    "measures": [{"@componentDatatype": "double", "concept": "absorbance"}]',
      "  },",
      '  "data": {',
      '    "dimensions": [[0, 1, 2, 3, 4]],',
      '    "measures": [[0.1, 0.2, 0.3, 0.4, 0.5]]',
      "  }",
      "}",
      "```",
      "",
      "## Validation",
      "",
      "Validate ASM output against official schemas:",
      "",
      "```python",
      "import json",
      "import jsonschema",
      "from urllib.request import urlopen",
      "",
      "# Load ASM output",
      'with open("output.json") as f:',
      "    asm = json.load(f)",
      "",
      "# Get schema URL from manifest",
      'schema_url = asm.get("$asm.manifest", {}).get("$ref")',
      "",
      "# Validate (simplified - real validation more complex)",
      "# Note: Full validation requires resolving $ref references",
      "```",
      "",
      "## Schema Repository",
      "",
      "Official schemas: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm",
      "",
      "Schema structure:",
      "```",
      "json-schemas/adm/",
      "├── cell-counting/",
      "│   └── REC/2024/09/",
      "│       └── cell-counting.schema.json",
      "├── spectrophotometry/",
      "│   └── REC/2024/06/",
      "│       └── spectrophotometry.schema.json",
      "├── plate-reader/",
      "│   └── REC/2024/06/",
      "│       └── plate-reader.schema.json",
      "└── ...",
      "```",
      "",
      "## Common Issues",
      "",
      "### Missing Fields",
      "Not all instrument exports contain all ASM fields. Report completeness:",
      "```python",
      "def report_completeness(asm, expected_fields):",
      "    found = set(extract_all_fields(asm))",
      "    missing = expected_fields - found",
      "    return len(found) / len(expected_fields) * 100",
      "```",
      "",
      "### Unit Variations",
      "Instruments may use different unit formats. The allotropy library normalizes these:",
      '- "cells/mL" → "(cell/mL)"',
      '- "%" → "%"',
      '- "nm" → "nm"',
      "",
      "### Date Formats",
      "ASM uses ISO 8601: `2024-01-15T10:30:00Z`",
      "",
    ].join("\n"),
    "references/field_classification_guide.md": [
      "# Field Classification Guide",
      "",
      "This guide helps classify instrument data fields into the correct ASM document locations. Use this when mapping raw instrument output to Allotrope Simple Model structure.",
      "",
      "## ASM Document Hierarchy",
      "",
      "```",
      "<technique>-aggregate-document",
      "├── device-system-document          # Instrument hardware info",
      "├── data-system-document            # Software/conversion info",
      "├── <technique>-document[]          # Per-run/sequence data",
      "│   ├── analyst                     # Who performed the analysis",
      "│   ├── measurement-aggregate-document",
      "│   │   ├── measurement-time",
      "│   │   ├── measurement-document[]  # Individual measurements",
      "│   │   │   ├── sample-document",
      "│   │   │   ├── device-control-aggregate-document",
      "│   │   │   └── [measurement fields]",
      "│   │   └── [aggregate-level metadata]",
      "│   ├── processed-data-aggregate-document",
      "│   │   └── processed-data-document[]",
      "│   │       ├── data-processing-document",
      "│   │       └── [processed results]",
      "│   └── calculated-data-aggregate-document",
      "│       └── calculated-data-document[]",
      "```",
      "",
      "## Field Classification Categories",
      "",
      "### 1. Device/Instrument Information → `device-system-document`",
      "",
      "Hardware and firmware details about the physical instrument.",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      '| Instrument name | `model-number` | "Vi-CELL BLU", "NanoDrop One" |',
      '| Serial number | `equipment-serial-number` | "VCB-12345", "SN001234" |',
      '| Manufacturer | `product-manufacturer` | "Beckman Coulter", "Thermo Fisher" |',
      '| Firmware version | `firmware-version` | "v2.1.3" |',
      '| Device ID | `device-identifier` | "Instrument_01" |',
      '| Brand | `brand-name` | "Beckman Coulter" |',
      "",
      "**Rule:** If the value describes the physical instrument and doesn't change between runs, it goes in `device-system-document`.",
      "",
      "---",
      "",
      "### 2. Software/Data System Information → `data-system-document`",
      "",
      "Information about software used for acquisition, analysis, or conversion.",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      '| Software name | `software-name` | "Chromeleon", "Gen5" |',
      '| Software version | `software-version` | "7.3.2" |',
      '| File name | `file-name` | "experiment_001.xlsx" |',
      '| File path | `file-identifier` | "/data/runs/2024-01-15/" |',
      '| Database ID | `ASM-converter-name` | "allotropy v0.1.55" |',
      "",
      "**Rule:** If the value describes software, file metadata, or data provenance, it goes in `data-system-document`.",
      "",
      "---",
      "",
      "### 3. Sample Information → `sample-document`",
      "",
      "Metadata about the biological/chemical sample being analyzed.",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      '| Sample ID | `sample-identifier` | "Sample_A", "LIMS-001234" |',
      '| Sample name | `written-name` | "CHO Cell Culture Day 5" |',
      '| Sample type/role | `sample-role-type` | "unknown sample role", "control sample role" |',
      '| Batch ID | `batch-identifier` | "Batch-2024-001" |',
      '| Description | `description` | "Protein expression sample" |',
      '| Well position | `location-identifier` | "A1", "B3" |',
      "",
      "**Rule:** If the value identifies or describes what was measured (not how), it goes in `sample-document`.",
      "",
      "---",
      "",
      "### 4. Device Control Settings → `device-control-aggregate-document`",
      "",
      "Instrument settings and parameters used during measurement.",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      "| Injection volume | `sample-volume-setting` | 10 µL |",
      "| Wavelength | `detector-wavelength-setting` | 254 nm |",
      "| Temperature | `compartment-temperature` | 37°C |",
      "| Flow rate | `flow-rate` | 1.0 mL/min |",
      "| Exposure time | `exposure-duration-setting` | 500 ms |",
      "| Detector gain | `detector-gain-setting` | 1.5 |",
      "| Illumination | `illumination-setting` | 80% |",
      "",
      "**Rule:** If the value is a configurable instrument parameter that affects measurement, it goes in `device-control-aggregate-document`.",
      "",
      "---",
      "",
      "### 5. Environmental Conditions → `device-control-document` or technique-specific",
      "",
      "Ambient or controlled environmental parameters during measurement.",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      "| Ambient temperature | `ambient-temperature` | 22.5°C |",
      "| Humidity | `ambient-relative-humidity` | 45% |",
      "| Column temperature | `compartment-temperature` | 30°C |",
      "| Sample temperature | `sample-temperature` | 4°C |",
      "| Electrophoresis temp | (technique-specific) | 26.4°C |",
      "",
      "**Rule:** Environmental conditions that affect measurement quality go with device control or in technique-specific locations.",
      "",
      "---",
      "",
      "### 6. Raw Measurement Data → `measurement-document`",
      "",
      'Direct instrument readings - the "ground truth" data.',
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      "| Absorbance | `absorbance` | 0.523 AU |",
      "| Fluorescence | `fluorescence` | 12500 RFU |",
      "| Cell count | `total-cell-count` | 2.5e6 cells |",
      "| Peak area | `peak-area` | 1234.5 mAU·min |",
      "| Retention time | `retention-time` | 5.67 min |",
      "| Ct value | `cycle-threshold-result` | 24.5 |",
      "| Concentration (measured) | `mass-concentration` | 1.5 mg/mL |",
      "",
      "**Rule:** If the value is a direct instrument reading that wasn't computed from other values in this analysis, it goes in `measurement-document`.",
      "",
      "---",
      "",
      "### 7. Calculated/Derived Data → `calculated-data-aggregate-document`",
      "",
      "Values computed from raw measurements.",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      "| Viability % | `calculated-result` | 95.2% |",
      "| Concentration (from std curve) | `calculated-result` | 125 ng/µL |",
      "| Ratio (260/280) | `calculated-result` | 1.89 |",
      "| Relative quantity | `calculated-result` | 2.5x |",
      "| % Recovery | `calculated-result` | 98.7% |",
      "| CV% | `calculated-result` | 2.3% |",
      "",
      "**Calculated data document structure:**",
      "```json",
      "{",
      '  "calculated-data-name": "viability",',
      '  "calculated-result": {"value": 95.2, "unit": "%"},',
      '  "calculation-description": "viable cells / total cells * 100"',
      "}",
      "```",
      "",
      "**Rule:** If the value was computed from other measurements in this analysis, it goes in `calculated-data-aggregate-document`. Include `calculation-description` when possible.",
      "",
      "---",
      "",
      "### 8. Processed/Analyzed Data → `processed-data-aggregate-document`",
      "",
      "Results from data processing algorithms (peak integration, cell classification, etc.).",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      "| Peak list | `peak-list` | Integrated peak results |",
      "| Cell size distribution | `cell-diameter-distribution` | Histogram data |",
      "| Baseline-corrected data | (in processed-data-document) | Corrected spectra |",
      "| Fitted curve | (in processed-data-document) | Standard curve fit |",
      "",
      "**Associated `data-processing-document`:**",
      "```json",
      "{",
      '  "cell-type-processing-method": "trypan blue exclusion",',
      '  "cell-density-dilution-factor": {"value": 2, "unit": "(unitless)"},',
      '  "minimum-cell-diameter-setting": {"value": 5, "unit": "µm"},',
      '  "maximum-cell-diameter-setting": {"value": 50, "unit": "µm"}',
      "}",
      "```",
      "",
      "**Rule:** If the value results from an algorithm or processing method applied to raw data, it goes in `processed-data-aggregate-document` with its processing parameters in `data-processing-document`.",
      "",
      "---",
      "",
      "### 9. Timing/Timestamps → Various locations",
      "",
      "| Timestamp Type | Location | ASM Field |",
      "|----------------|----------|-----------|",
      "| Measurement time | `measurement-document` | `measurement-time` |",
      "| Run start time | `analysis-sequence-document` | `analysis-sequence-start-time` |",
      "| Run end time | `analysis-sequence-document` | `analysis-sequence-end-time` |",
      "| Data export time | `data-system-document` | (custom) |",
      "",
      "**Rule:** Use ISO 8601 format: `2024-01-15T10:30:00Z`",
      "",
      "---",
      "",
      "### 10. Analyst/Operator Information → `<technique>-document`",
      "",
      "| Field Type | ASM Field | Examples |",
      "|------------|-----------|----------|",
      '| Operator name | `analyst` | "jsmith" |',
      '| Reviewer | (custom or extension) | "Pending" |',
      "",
      "**Rule:** Analyst goes at the technique-document level, not in individual measurements.",
      "",
      "---",
      "",
      "## Decision Tree",
      "",
      "```",
      "Is this field about...",
      "",
      "THE INSTRUMENT ITSELF?",
      "├── Hardware specs → device-system-document",
      "└── Software/files → data-system-document",
      "",
      "THE SAMPLE?",
      "└── Sample ID, name, type, batch → sample-document",
      "",
      "INSTRUMENT SETTINGS?",
      "└── Configurable parameters → device-control-aggregate-document",
      "",
      "ENVIRONMENTAL CONDITIONS?",
      "└── Temp, humidity, etc. → device-control-document",
      "",
      "A DIRECT READING?",
      "└── Raw instrument output → measurement-document",
      "",
      "A COMPUTED VALUE?",
      "├── From other measurements → calculated-data-document",
      "└── From processing algorithm → processed-data-document",
      "",
      "TIMING?",
      "├── When measured → measurement-document.measurement-time",
      "└── When run started/ended → analysis-sequence-document",
      "",
      "WHO DID IT?",
      "└── Operator/analyst → <technique>-document.analyst",
      "```",
      "",
      "## Common Instrument-to-ASM Mappings",
      "",
      "> **Note:** These mappings are derived from the [Benchling allotropy library](https://github.com/Benchling-Open-Source/allotropy/tree/main/src/allotropy/parsers). For authoritative mappings, consult the parser source code for your specific instrument.",
      "",
      "### Cell Counter (Vi-CELL BLU)",
      "*Source: `allotropy/parsers/beckman_vi_cell_blu/vi_cell_blu_structure.py`*",
      "",
      "| Instrument Field | ASM Field |",
      "|-----------------|-----------|",
      "| Sample ID | `sample_identifier` |",
      "| Analysis date/time | `measurement_time` |",
      "| Analysis by | `analyst` |",
      "| Viability (%) | `viability` |",
      "| Viable (x10^6) cells/mL | `viable_cell_density` |",
      "| Total (x10^6) cells/mL | `total_cell_density` |",
      "| Cell count | `total_cell_count` |",
      "| Viable cells | `viable_cell_count` |",
      "| Average diameter (μm) | `average_total_cell_diameter` |",
      "| Average viable diameter (μm) | `average_live_cell_diameter` |",
      "| Average circularity | `average_total_cell_circularity` |",
      "| Cell type | `cell_type_processing_method` (data-processing) |",
      "| Dilution | `cell_density_dilution_factor` (data-processing) |",
      "| Min/Max Diameter | `minimum/maximum_cell_diameter_setting` (data-processing) |",
      "",
      "### Spectrophotometer (NanoDrop)",
      "| Instrument Field | ASM Field |",
      "|-----------------|-----------|",
      "| Sample Name | `sample_identifier` |",
      "| A260, A280 | `absorbance` (with wavelength) |",
      "| Concentration | `mass_concentration` |",
      "| 260/280 ratio | `a260_a280_ratio` |",
      "| Pathlength | `pathlength` |",
      "",
      "### Plate Reader",
      "| Instrument Field | ASM Field |",
      "|-----------------|-----------|",
      "| Well | `location_identifier` |",
      "| Sample Type | `sample_role_type` |",
      "| Absorbance/OD | `absorbance` |",
      "| Fluorescence | `fluorescence` |",
      "| Plate ID | `container_identifier` |",
      "",
      "### Chromatography (HPLC)",
      "| Instrument Field | ASM Field |",
      "|-----------------|-----------|",
      "| Sample ID | `sample_identifier` |",
      "| Injection Volume | `injection_volume` |",
      "| Retention Time | `retention_time` |",
      "| Peak Area | `peak_area` |",
      "| Peak Height | `peak_height` |",
      "| Column Temp | `column_oven_temperature` |",
      "| Flow Rate | `flow_rate` |",
      "",
      "## Unit Handling",
      "",
      "Only use units explicitly present in source data. If a value has no unit specified:",
      "- Use `(unitless)` as the unit value",
      "- Do NOT infer units based on domain knowledge",
      "",
      "## Calculated Data Traceability",
      "",
      "When creating calculated values, always link them to their source data using `data-source-aggregate-document`:",
      "",
      "```json",
      "{",
      '    "calculated-data-name": "DIN",',
      '    "calculated-result": {"value": 5.8, "unit": "(unitless)"},',
      '    "calculated-data-identifier": "TEST_ID_147",',
      '    "data-source-aggregate-document": {',
      '        "data-source-document": [{',
      '            "data-source-identifier": "TEST_ID_145",',
      '            "data-source-feature": "sample"',
      "        }]",
      "    }",
      "}",
      "```",
      "",
      'This declares: "DIN 5.8 was calculated from the sample at `TEST_ID_145`."',
      "",
      "**Why this matters:**",
      "- **Audits**: Prove a value came from specific raw data",
      "- **Debugging**: Trace unexpected results back to their source",
      "- **Reprocessing**: Know which inputs to re-analyze if algorithms change",
      "",
      "**Assign unique IDs to:**",
      "- Measurements, peaks, regions, and calculated values",
      "- Use a consistent naming pattern (e.g., `INSTRUMENT_TYPE_TEST_ID_N`)",
      "",
      "This enables bidirectional traversal: trace from calculated → raw, or raw → all derived values.",
      "",
      "---",
      "",
      "## Nested Document Structure (Critical)",
      "",
      'A common mistake is "flattening" fields directly onto measurement documents when they should be wrapped in nested structures. This breaks schema compliance and loses semantic context.',
      "",
      "### Why Nesting Matters",
      "",
      "ASM uses nested documents for semantic grouping:",
      "",
      "| Document | Purpose | Contains |",
      "|----------|---------|----------|",
      "| `sample document` | What was measured | Sample ID, locations, plate identifiers |",
      "| `device control aggregate document` | How instrument operated | Settings, parameters, techniques |",
      "| `custom information document` | Vendor-specific fields | Non-standard fields that don't map to ASM |",
      "",
      "### Sample Document Fields",
      "",
      "These fields MUST be inside `sample document`, not flattened on measurement:",
      "",
      "```json",
      "// ❌ WRONG - Fields flattened on measurement",
      "{",
      '  "measurement identifier": "TEST_001",',
      '  "sample identifier": "Sample_A",',
      '  "location identifier": "A1",',
      '  "absorbance": {"value": 0.5, "unit": "(unitless)"}',
      "}",
      "",
      "// ✅ CORRECT - Fields nested in sample document",
      "{",
      '  "measurement identifier": "TEST_001",',
      '  "sample document": {',
      '    "sample identifier": "Sample_A",',
      '    "location identifier": "A1",',
      '    "well plate identifier": "96WP001"',
      "  },",
      '  "absorbance": {"value": 0.5, "unit": "(unitless)"}',
      "}",
      "```",
      "",
      "**Fields belonging in sample document:**",
      "- `sample identifier` - Sample ID/name",
      "- `written name` - Descriptive sample name",
      "- `batch identifier` - Batch/lot number",
      "- `sample role type` - Standard, blank, control, unknown",
      "- `location identifier` - Well position (A1, B3, etc.)",
      "- `well plate identifier` - Plate barcode",
      "- `description` - Sample description",
      "",
      "### Device Control Document Fields",
      "",
      "Instrument settings MUST be inside `device control aggregate document`:",
      "",
      "```json",
      "// ❌ WRONG - Device settings flattened",
      "{",
      '  "measurement identifier": "TEST_001",',
      '  "device identifier": "Pod1",',
      '  "technique": "Custom",',
      '  "volume": {"value": 26, "unit": "μL"}',
      "}",
      "",
      "// ✅ CORRECT - Settings nested in device control",
      "{",
      '  "measurement identifier": "TEST_001",',
      '  "device control aggregate document": {',
      '    "device control document": [{',
      '      "device type": "liquid handler",',
      '      "device identifier": "Pod1"',
      "    }]",
      "  },",
      '  "aspiration volume": {"value": 26, "unit": "μL"}',
      "}",
      "```",
      "",
      "**Fields belonging in device control:**",
      "- `device type` - Type of device",
      "- `device identifier` - Device ID",
      "- `detector wavelength setting` - Wavelength for detection",
      "- `compartment temperature` - Temperature setting",
      "- `sample volume setting` - Volume setting",
      "- `flow rate` - Flow rate setting",
      "",
      "### Custom Information Document",
      "",
      "Vendor-specific fields that don't map to standard ASM terms go in `custom information document`:",
      "",
      "```json",
      '"device control document": [{',
      '  "device type": "liquid handler",',
      '  "custom information document": {',
      '    "probe": "2",',
      '    "pod": "Pod1",',
      '    "source labware name": "Inducer",',
      '    "destination labware name": "GRP1"',
      "  }",
      "}]",
      "```",
      "",
      "### Liquid Handler: Transfer Pairing",
      "",
      "For liquid handlers, a measurement represents a complete transfer (aspirate + dispense), not separate operations:",
      "",
      "```json",
      "// ❌ WRONG - Separate records for aspirate and dispense",
      "[",
      '  {"measurement identifier": "OP_001", "transfer type": "Aspirate", "volume": {"value": 26, "unit": "μL"}},',
      '  {"measurement identifier": "OP_002", "transfer type": "Dispense", "volume": {"value": 26, "unit": "μL"}}',
      "]",
      "",
      "// ✅ CORRECT - Single record with source and destination",
      "{",
      '  "measurement identifier": "TRANSFER_001",',
      '  "sample document": {',
      '    "source well location identifier": "1",',
      '    "destination well location identifier": "2",',
      '    "source well plate identifier": "96WP001",',
      '    "destination well plate identifier": "96WP002"',
      "  },",
      '  "aspiration volume": {"value": 26, "unit": "μL"},',
      '  "transfer volume": {"value": 26, "unit": "μL"}',
      "}",
      "```",
      "",
      "**Pairing logic:**",
      "1. Match aspirate and dispense operations by probe number",
      "2. Create one measurement per matched pair",
      "3. Use `source_*` fields for aspirate location",
      "4. Use `destination_*` fields for dispense location",
      "5. Include both `aspiration volume` and `transfer volume`",
      "",
      "### Quick Reference: Nesting Decision",
      "",
      "```",
      "Is this field about...",
      "",
      "THE SAMPLE BEING MEASURED?",
      "├── Sample ID, name, batch → sample document",
      "├── Well position → sample document.location identifier",
      "├── Plate barcode → sample document.well plate identifier",
      "└── Source/destination locations → sample document (with prefixes)",
      "",
      "INSTRUMENT SETTINGS?",
      "├── Standard settings → device control aggregate document",
      "└── Vendor-specific → custom information document",
      "",
      "A MEASUREMENT VALUE?",
      "└── Direct on measurement document (e.g., absorbance, volume)",
      "",
      "TRANSFER OPERATION TYPE?",
      '└── DON\'T use "transfer type" - pair into single measurement',
      "    with source/destination fields instead",
      "```",
      "",
      "### Validation",
      "",
      "Use `validate_asm.py` to check for nesting issues:",
      "```bash",
      "python scripts/validate_asm.py output.json --reference known_good.json",
      "```",
      "",
      "The validator checks for:",
      "- Fields incorrectly flattened on measurements",
      "- Missing `sample document` wrapper",
      "- Missing `device control aggregate document` wrapper",
      "- Missing `custom information document` for vendor fields",
      "- Liquid handler: separate transfer types instead of paired records",
      "",
      "## Sources",
      "",
      "- [Allotrope Simple Model Introduction](https://www.allotrope.org/introduction-to-allotrope-simple-model)",
      "- [Benchling allotropy library](https://github.com/Benchling-Open-Source/allotropy)",
      "- [Allotrope Foundation ASM Overview](https://www.allotrope.org/asm)",
      "",
    ].join("\n"),
    "references/flattening_guide.md": [
      "# Flattening ASM to 2D CSV",
      "",
      "Converting hierarchical ASM JSON to flat 2D tables for LIMS import, spreadsheet analysis, or data engineering pipelines.",
      "",
      "## Why Flatten?",
      "",
      "ASM is semantically rich but hierarchical. Many systems need flat tables:",
      "- LIMS import (Benchling, STARLIMS, LabWare)",
      "- Excel/CSV analysis",
      "- Database loading",
      "- Quick visual inspection",
      "",
      "## Flattening Strategy",
      "",
      "### Core Principle",
      "Each **measurement** becomes one **row**. Metadata is repeated per row.",
      "",
      "### What's Excluded",
      "The flattening intentionally **omits top-level ASM metadata** such as:",
      "- `$asm.manifest` (model version, schema URIs)",
      "- Root-level fields outside the technique aggregate document",
      "",
      "This keeps the output focused on experimental data. If you need schema version tracking for compliance or audit purposes, consider storing the original ASM JSON alongside the flattened CSV, or modify the flattening script to include these fields.",
      "",
      "### Hierarchy to Columns",
      "```",
      "ASM Hierarchy                    → Flat Column",
      "─────────────────────────────────────────────────",
      "device-system-document.",
      "  device-identifier              → instrument_serial_number",
      "  model-number                   → instrument_model",
      "  ",
      "measurement-aggregate-document.",
      "  analyst                        → analyst",
      "  measurement-time               → measurement_datetime",
      "  ",
      "measurement-document[].",
      "  sample-identifier              → sample_id",
      "  viable-cell-density.value      → viable_cell_density",
      "  viable-cell-density.unit       → viable_cell_density_unit",
      "  viability.value                → viability_percent",
      "```",
      "",
      "## Column Naming Convention",
      "",
      "Use snake_case with descriptive suffixes:",
      "",
      "| ASM Field | Flat Column |",
      "|-----------|-------------|",
      "| `viable-cell-density` | `viable_cell_density` |",
      "| `.value` | `_value` (or omit if obvious) |",
      "| `.unit` | `_unit` |",
      "| `measurement-time` | `measurement_datetime` |",
      "",
      "## Example: Cell Counting",
      "",
      "### ASM Input (simplified)",
      "```json",
      "{",
      '  "cell-counting-aggregate-document": {',
      '    "device-system-document": {',
      '      "device-identifier": "VCB001",',
      '      "model-number": "Vi-CELL BLU"',
      "    },",
      '    "cell-counting-document": [{',
      '      "measurement-aggregate-document": {',
      '        "analyst": "jsmith",',
      '        "measurement-time": "2024-01-15T10:30:00Z",',
      '        "measurement-document": [',
      "          {",
      '            "sample-identifier": "Sample_A",',
      '            "viable-cell-density": {"value": 2500000, "unit": "(cell/mL)"},',
      '            "viability": {"value": 95.2, "unit": "%"}',
      "          },",
      "          {",
      '            "sample-identifier": "Sample_B",',
      '            "viable-cell-density": {"value": 1800000, "unit": "(cell/mL)"},',
      '            "viability": {"value": 88.7, "unit": "%"}',
      "          }",
      "        ]",
      "      }",
      "    }]",
      "  }",
      "}",
      "```",
      "",
      "### Flattened Output",
      "```csv",
      "sample_id,viable_cell_density,viable_cell_density_unit,viability_percent,analyst,measurement_datetime,instrument_serial_number,instrument_model",
      "Sample_A,2500000,(cell/mL),95.2,jsmith,2024-01-15T10:30:00Z,VCB001,Vi-CELL BLU",
      "Sample_B,1800000,(cell/mL),88.7,jsmith,2024-01-15T10:30:00Z,VCB001,Vi-CELL BLU",
      "```",
      "",
      "## Example: Plate Reader",
      "",
      "### ASM Input (simplified)",
      "```json",
      "{",
      '  "plate-reader-aggregate-document": {',
      '    "plate-reader-document": [{',
      '      "measurement-aggregate-document": {',
      '        "plate-identifier": "ELISA_001",',
      '        "measurement-document": [',
      '          {"well-location": "A1", "absorbance": {"value": 0.125, "unit": "mAU"}},',
      '          {"well-location": "A2", "absorbance": {"value": 0.892, "unit": "mAU"}},',
      '          {"well-location": "A3", "absorbance": {"value": 1.456, "unit": "mAU"}}',
      "        ]",
      "      }",
      "    }]",
      "  }",
      "}",
      "```",
      "",
      "### Flattened Output",
      "```csv",
      "plate_id,well_position,absorbance,absorbance_unit",
      "ELISA_001,A1,0.125,mAU",
      "ELISA_001,A2,0.892,mAU",
      "ELISA_001,A3,1.456,mAU",
      "```",
      "",
      "## Handling Data Cubes",
      "",
      "Data cubes (time series, spectra) need special handling:",
      "",
      "### Option 1: Expand to rows",
      "Each point becomes a row:",
      "```csv",
      "sample_id,time_seconds,absorbance",
      "Sample_A,0,0.100",
      "Sample_A,60,0.125",
      "Sample_A,120,0.150",
      "```",
      "",
      "### Option 2: Wide format",
      "Measurements as columns:",
      "```csv",
      "sample_id,abs_0s,abs_60s,abs_120s",
      "Sample_A,0.100,0.125,0.150",
      "```",
      "",
      "### Option 3: JSON array in cell",
      "Keep as array (some systems support this):",
      "```csv",
      "sample_id,absorbance_timeseries",
      'Sample_A,"[0.100,0.125,0.150]"',
      "```",
      "",
      "## Standard Column Sets by Technique",
      "",
      "### Cell Counting",
      "```",
      "sample_id, viable_cell_density, viable_cell_density_unit, total_cell_count,",
      "viability_percent, average_cell_diameter, average_cell_diameter_unit,",
      "analyst, measurement_datetime, instrument_serial_number",
      "```",
      "",
      "### Spectrophotometry",
      "```",
      "sample_id, wavelength_nm, absorbance, pathlength_cm, concentration,",
      "concentration_unit, a260_a280_ratio, a260_a230_ratio,",
      "analyst, measurement_datetime, instrument_serial_number",
      "```",
      "",
      "### Plate Reader / ELISA",
      "```",
      "plate_id, well_position, sample_type, sample_id, absorbance, absorbance_unit,",
      "concentration, concentration_unit, dilution_factor, cv_percent,",
      "analyst, measurement_datetime, instrument_serial_number",
      "```",
      "",
      "### qPCR",
      "```",
      "sample_id, target_name, well_position, ct_value, ct_mean, ct_sd,",
      "quantity, quantity_unit, amplification_efficiency,",
      "analyst, measurement_datetime, instrument_serial_number",
      "```",
      "",
      "## Python Implementation",
      "",
      "```python",
      "import json",
      "import pandas as pd",
      "",
      'def flatten_asm(asm_dict, technique="cell-counting"):',
      '    """',
      "    Flatten ASM JSON to pandas DataFrame.",
      "    ",
      "    Args:",
      "        asm_dict: Parsed ASM JSON",
      "        technique: ASM technique type",
      "        ",
      "    Returns:",
      "        pandas DataFrame with one row per measurement",
      '    """',
      "    rows = []",
      "    ",
      "    # Get aggregate document",
      '    agg_key = f"{technique}-aggregate-document"',
      "    agg_doc = asm_dict.get(agg_key, {})",
      "    ",
      "    # Extract device info",
      '    device = agg_doc.get("device-system-document", {})',
      "    device_info = {",
      '        "instrument_serial_number": device.get("device-identifier"),',
      '        "instrument_model": device.get("model-number")',
      "    }",
      "    ",
      "    # Get technique documents",
      '    doc_key = f"{technique}-document"',
      "    for doc in agg_doc.get(doc_key, []):",
      '        meas_agg = doc.get("measurement-aggregate-document", {})',
      "        ",
      "        # Extract common metadata",
      "        common = {",
      '            "analyst": meas_agg.get("analyst"),',
      '            "measurement_datetime": meas_agg.get("measurement-time"),',
      "            **device_info",
      "        }",
      "        ",
      "        # Extract each measurement",
      '        for meas in meas_agg.get("measurement-document", []):',
      "            row = {**common}",
      "            ",
      "            # Flatten measurement fields",
      "            for key, value in meas.items():",
      '                if isinstance(value, dict) and "value" in value:',
      "                    # Value datum pattern",
      '                    col = key.replace("-", "_")',
      '                    row[col] = value["value"]',
      '                    if "unit" in value:',
      '                        row[f"{col}_unit"] = value["unit"]',
      "                else:",
      '                    row[key.replace("-", "_")] = value',
      "            ",
      "            rows.append(row)",
      "    ",
      "    return pd.DataFrame(rows)",
      "",
      "# Usage",
      'with open("asm_output.json") as f:',
      "    asm = json.load(f)",
      "",
      'df = flatten_asm(asm, "cell-counting")',
      'df.to_csv("flattened_output.csv", index=False)',
      "```",
      "",
      "## LIMS Import Considerations",
      "",
      "When importing flattened data into a LIMS:",
      "- Match column names to your LIMS schema field names",
      "- Use ISO 8601 date format for timestamps",
      "- Ensure sample IDs match existing LIMS sample identifiers",
      "- Check if your LIMS expects units in separate columns or embedded in values",
      "",
    ].join("\n"),
    "references/supported_instruments.md": [
      "# Supported Instruments",
      "",
      "## What Can This Skill Convert?",
      "",
      "**Any instrument data that maps to an Allotrope schema can be converted.** The skill uses a tiered parsing approach:",
      "",
      "1. **Native allotropy parsers** (listed below) - Highest fidelity, validated against vendor-specific formats",
      "2. **Flexible fallback parser** - Handles any tabular data (CSV, Excel, TXT) by mapping columns to ASM fields",
      "3. **PDF extraction** - Extracts tables from PDFs, then applies flexible parsing",
      "",
      "If your instrument isn't listed below, the skill can still convert it as long as your data contains recognizable measurement fields (sample IDs, values, units, timestamps, etc.) that map to an ASM technique schema.",
      "",
      "---",
      "",
      "## Instruments with Native Allotropy Parsers",
      "",
      "The following instruments have optimized parsers in the allotropy library with their Vendor enum values.",
      "",
      "## Cell Counting",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Beckman Coulter Vi-CELL BLU | `BECKMAN_VI_CELL_BLU` | .csv |",
      "| Beckman Coulter Vi-CELL XR | `BECKMAN_VI_CELL_XR` | .txt, .xls, .xlsx |",
      "| ChemoMetec NucleoView NC-200 | `CHEMOMETEC_NUCLEOVIEW` | .xlsx |",
      "| ChemoMetec NC-View | `CHEMOMETEC_NC_VIEW` | .xlsx |",
      "| Revvity Matrix | `REVVITY_MATRIX` | .csv |",
      "",
      "## Spectrophotometry (UV-Vis)",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Thermo Fisher NanoDrop One | `THERMO_FISHER_NANODROP_ONE` | .csv, .xlsx |",
      "| Thermo Fisher NanoDrop Eight | `THERMO_FISHER_NANODROP_EIGHT` | .tsv, .txt |",
      "| Thermo Fisher NanoDrop 8000 | `THERMO_FISHER_NANODROP_8000` | .csv |",
      "| Unchained Labs Lunatic | `UNCHAINED_LABS_LUNATIC` | .csv, .xlsx |",
      "| Thermo Fisher Genesys 30 | `THERMO_FISHER_GENESYS30` | .csv |",
      "",
      "## Plate Readers (Multi-mode, Absorbance, Fluorescence)",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Molecular Devices SoftMax Pro | `MOLDEV_SOFTMAX_PRO` | .txt |",
      "| PerkinElmer EnVision | `PERKIN_ELMER_ENVISION` | .csv |",
      "| Agilent Gen5 (BioTek) | `AGILENT_GEN5` | .xlsx |",
      "| Agilent Gen5 Image | `AGILENT_GEN5_IMAGE` | .xlsx |",
      "| BMG MARS (CLARIOstar) | `BMG_MARS` | .csv, .txt |",
      "| BMG LabTech Smart Control | `BMG_LABTECH_SMART_CONTROL` | .csv |",
      "| Thermo SkanIt | `THERMO_SKANIT` | .xlsx |",
      "| Revvity Kaleido | `REVVITY_KALEIDO` | .csv |",
      "| Tecan Magellan | `TECAN_MAGELLAN` | .xlsx |",
      "",
      "## ELISA / Immunoassay",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Molecular Devices SoftMax Pro | `MOLDEV_SOFTMAX_PRO` | .txt |",
      "| MSD Discovery Workbench | `MSD_WORKBENCH` | .txt |",
      "| MSD Methodical Mind | `METHODICAL_MIND` | .xlsx |",
      "| BMG MARS | `BMG_MARS` | .csv, .txt |",
      "",
      "## qPCR / PCR",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Applied Biosystems QuantStudio | `APPBIO_QUANTSTUDIO` | .xlsx |",
      "| Applied Biosystems QuantStudio Design & Analysis | `APPBIO_QUANTSTUDIO_DESIGNANALYSIS` | .xlsx, .csv |",
      "| Bio-Rad CFX Maestro | `BIORAD_CFX_MAESTRO` | .csv, .xlsx |",
      "| Roche LightCycler | `ROCHE_LIGHTCYCLER` | .txt |",
      "",
      "## Chromatography (HPLC, LC)",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Waters Empower | `WATERS_EMPOWER` | .xml |",
      "| Thermo Fisher Chromeleon | `THERMO_FISHER_CHROMELEON` | .xml |",
      "| Agilent ChemStation | `AGILENT_CHEMSTATION` | .csv |",
      "",
      "## Electrophoresis",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Agilent TapeStation | `AGILENT_TAPESTATION` | .csv |",
      "| PerkinElmer LabChip | `PERKIN_ELMER_LABCHIP` | .csv |",
      "",
      "## Flow Cytometry",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| BD Biosciences FACSDiva | `BD_BIOSCIENCES_FACSDIVA` | .xml |",
      "| FlowJo | `FLOWJO` | .wsp |",
      "",
      "## Solution Analysis",
      "",
      "| Instrument | Vendor Enum | File Types |",
      "|------------|-------------|------------|",
      "| Roche Cedex BioHT | `ROCHE_CEDEX_BIOHT` | .xlsx |",
      "| Beckman Coulter Biomek | `BECKMAN_COULTER_BIOMEK` | .csv |",
      "",
      "## Auto-Detection Patterns",
      "",
      "The skill attempts to identify instrument type from file contents using these patterns:",
      "",
      "### Vi-CELL BLU",
      '- Column headers: "Sample ID", "Viable cells (x10^6 cells/mL)", "Viability (%)"',
      "- File structure: CSV with specific column order",
      "",
      "### Vi-CELL XR",
      '- Column headers: "Sample", "Total cells/ml", "Viable cells/ml"',
      "- Multiple export formats supported",
      "",
      "### NanoDrop",
      '- Column headers: "Sample Name", "Nucleic Acid Conc.", "A260", "A280"',
      "- 260/280 and 260/230 ratio columns",
      "",
      "### Plate Readers (General)",
      "- Well identifiers (A1-H12 pattern)",
      '- "Plate", "Well", "Sample" columns',
      "- Block-based structure with metadata headers",
      "",
      "### ELISA",
      "- Standard curve data with concentrations",
      "- OD/absorbance readings",
      "- Sample/blank/standard classification",
      "",
      "## Using Vendor Enums",
      "",
      "```python",
      "from allotropy.parser_factory import Vendor",
      "from allotropy.to_allotrope import allotrope_from_file",
      "",
      "# List all supported vendors",
      "for v in Vendor:",
      '    print(f"{v.name}: {v.value}")',
      "",
      "# Convert file",
      'asm = allotrope_from_file("data.csv", Vendor.BECKMAN_VI_CELL_BLU)',
      "```",
      "",
      "## Checking Supported Status",
      "",
      "```python",
      "from allotropy.parser_factory import get_parser",
      "",
      "# Check if a vendor/file combo is supported",
      "try:",
      "    parser = get_parser(Vendor.BECKMAN_VI_CELL_BLU)",
      '    print("Supported!")',
      "except Exception as e:",
      '    print(f"Not supported: {e}")',
      "```",
      "",
    ].join("\n"),
    "scripts/convert_to_asm.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Instrument Data to ASM Converter",
      "",
      "Converts laboratory instrument output files to Allotrope Simple Model (ASM) JSON format.",
      "Supports auto-detection of instrument types and fallback parsing for unsupported formats.",
      "",
      "Usage:",
      "    python convert_to_asm.py <input_file> [--vendor VENDOR] [--output OUTPUT]",
      '"""',
      "",
      "import json",
      "import sys",
      "import re",
      "import hashlib",
      "import importlib.metadata",
      "from pathlib import Path",
      "from typing import Optional, Tuple, Dict, Any",
      "from datetime import datetime",
      "",
      "",
      "# Lazy imports to avoid errors if not installed",
      "def get_allotropy():",
      "    try:",
      "        from allotropy.parser_factory import Vendor",
      "        from allotropy.to_allotrope import allotrope_from_file, allotrope_from_io",
      "",
      "        return Vendor, allotrope_from_file, allotrope_from_io",
      "    except ImportError:",
      "        return None, None, None",
      "",
      "",
      "def get_pandas():",
      "    try:",
      "        import pandas as pd",
      "",
      "        return pd",
      "    except ImportError:",
      "        return None",
      "",
      "",
      "# Detection patterns for instrument identification",
      "DETECTION_PATTERNS = {",
      '    "BECKMAN_VI_CELL_BLU": {',
      '        "columns": [',
      '            "Sample ID",',
      '            "Viable cells",',
      '            "Viability",',
      '            "Total cells",',
      '            "Average diameter",',
      "        ],",
      '        "keywords": ["Vi-CELL BLU", "Beckman Coulter"],',
      '        "file_patterns": [r".*\\.csv$"],',
      '        "confidence_boost": 20,',
      "    },",
      '    "BECKMAN_VI_CELL_XR": {',
      '        "columns": ["Sample", "Total cells/ml", "Viable cells/ml", "Viability (%)"],',
      '        "keywords": ["Vi-CELL XR", "Cell Viability Analyzer"],',
      '        "file_patterns": [r".*\\.(txt|xls|xlsx)$"],',
      '        "confidence_boost": 20,',
      "    },",
      '    "THERMO_FISHER_NANODROP_EIGHT": {',
      '        "columns": ["Sample Name", "Nucleic Acid Conc.", "A260", "A280", "260/280"],',
      '        "keywords": ["NanoDrop Eight", "NanoDrop 8"],',
      '        "file_patterns": [r".*\\.(tsv|txt)$"],',
      '        "confidence_boost": 15,',
      "    },",
      '    "THERMO_FISHER_NANODROP_ONE": {',
      '        "columns": ["Sample Name", "Nucleic Acid(ng/uL)", "A260", "A280"],',
      '        "keywords": ["NanoDrop One", "NanoDrop"],',
      '        "file_patterns": [r".*\\.(csv|xlsx)$"],',
      '        "confidence_boost": 15,',
      "    },",
      '    "MOLDEV_SOFTMAX_PRO": {',
      '        "columns": ["Well", "Sample", "Values", "Mean", "SD"],',
      '        "keywords": ["SoftMax Pro", "SpectraMax", "Molecular Devices"],',
      '        "file_patterns": [r".*\\.txt$"],',
      '        "confidence_boost": 15,',
      "    },",
      '    "BMG_MARS": {',
      '        "columns": ["Well", "Content", "Conc.", "Mean", "SD", "CV"],',
      '        "keywords": ["BMG LABTECH", "MARS", "CLARIOstar", "PHERAstar"],',
      '        "file_patterns": [r".*\\.(csv|txt)$"],',
      '        "confidence_boost": 15,',
      "    },",
      '    "AGILENT_GEN5": {',
      '        "columns": ["Well", "Read", "Time", "Temperature"],',
      '        "keywords": ["Gen5", "BioTek", "Synergy"],',
      '        "file_patterns": [r".*\\.xlsx$"],',
      '        "confidence_boost": 15,',
      "    },",
      '    "APPBIO_QUANTSTUDIO": {',
      '        "columns": ["Well", "Sample Name", "Target Name", "CT", "Ct Mean"],',
      '        "keywords": ["QuantStudio", "Applied Biosystems", "qPCR"],',
      '        "file_patterns": [r".*\\.xlsx$"],',
      '        "confidence_boost": 15,',
      "    },",
      "}",
      "",
      "",
      "def detect_instrument_type(",
      "    filepath: str, file_content: Optional[str] = None",
      ") -> Tuple[str, float]:",
      '    """',
      "    Auto-detect instrument type from file contents.",
      "",
      "    Returns:",
      "        Tuple of (vendor_name, confidence_score)",
      "        confidence_score is 0-100",
      '    """',
      "    path = Path(filepath)",
      "    filename = path.name.lower()",
      "    extension = path.suffix.lower()",
      "",
      "    # Read file content if not provided",
      "    if file_content is None:",
      "        try:",
      '            if extension in [".xlsx", ".xls"]:',
      "                pd = get_pandas()",
      "                if pd:",
      "                    df = pd.read_excel(filepath, nrows=50)",
      '                    file_content = df.to_string() + "\\n" + "\\n".join(df.columns)',
      "                else:",
      '                    file_content = ""',
      "            else:",
      '                with open(filepath, "r", encoding="utf-8", errors="ignore") as f:',
      "                    file_content = f.read(10000)  # First 10KB",
      "        except Exception as e:",
      '            print(f"Warning: Could not read file for detection: {e}")',
      '            file_content = ""',
      "",
      "    content_lower = file_content.lower()",
      "    scores = {}",
      "",
      "    for vendor, patterns in DETECTION_PATTERNS.items():",
      "        score = 0",
      "",
      "        # Check file extension patterns",
      '        for pattern in patterns.get("file_patterns", []):',
      "            if re.match(pattern, filename, re.IGNORECASE):",
      "                score += 10",
      "                break",
      "",
      "        # Check column headers",
      "        columns_found = 0",
      '        for col in patterns.get("columns", []):',
      "            if col.lower() in content_lower:",
      "                columns_found += 1",
      "        if columns_found > 0:",
      "            score += min(50, columns_found * 15)",
      "",
      "        # Check keywords",
      '        for keyword in patterns.get("keywords", []):',
      "            if keyword.lower() in content_lower:",
      '                score += patterns.get("confidence_boost", 10)',
      "",
      "        scores[vendor] = min(100, score)",
      "",
      "    # Return best match",
      "    if scores:",
      "        best = max(scores.items(), key=lambda x: x[1])",
      "        return best[0], best[1]",
      "",
      '    return "UNKNOWN", 0',
      "",
      "",
      "def convert_with_allotropy(filepath: str, vendor_name: str) -> Optional[Dict[str, Any]]:",
      '    """',
      "    Convert file using allotropy library.",
      "",
      "    Returns:",
      "        ASM dictionary or None if conversion fails",
      '    """',
      "    Vendor, allotrope_from_file, _ = get_allotropy()",
      "",
      "    if Vendor is None:",
      "        print(",
      '            "Warning: allotropy not installed. Run: pip install allotropy --break-system-packages"',
      "        )",
      "        return None",
      "",
      "    try:",
      "        vendor = getattr(Vendor, vendor_name, None)",
      "        if vendor is None:",
      '            print(f"Warning: Vendor {vendor_name} not found in allotropy")',
      "            return None",
      "",
      "        asm = allotrope_from_file(filepath, vendor)",
      "        return asm",
      "    except Exception as e:",
      '        print(f"Allotropy conversion failed: {e}")',
      "        return None",
      "",
      "",
      "def get_deterministic_timestamp(filepath: str) -> str:",
      '    """',
      "    Get deterministic timestamp for file.",
      "    Uses file modification time for reproducibility.",
      "",
      "    Returns:",
      "        ISO format timestamp string",
      '    """',
      "    try:",
      "        path = Path(filepath)",
      "        mtime = path.stat().st_mtime",
      "        return datetime.fromtimestamp(mtime).isoformat()",
      "    except Exception:",
      '        return "TIMESTAMP_NOT_AVAILABLE"',
      "",
      "",
      "def calculate_file_hash(filepath: str) -> str:",
      '    """Calculate SHA256 hash of file for provenance tracking."""',
      "    try:",
      '        with open(filepath, "rb") as f:',
      "            return hashlib.sha256(f.read()).hexdigest()",
      "    except Exception:",
      '        return "HASH_NOT_AVAILABLE"',
      "",
      "",
      "def get_library_version(library: str) -> str:",
      '    """Get version of installed library."""',
      "    try:",
      "        return importlib.metadata.version(library)",
      "    except Exception:",
      '        return "VERSION_NOT_AVAILABLE"',
      "",
      "",
      "def add_provenance_metadata(",
      "    asm: Dict[str, Any],",
      "    filepath: str,",
      "    vendor: str,",
      "    confidence: float,",
      "    used_fallback: bool,",
      "    warnings: list = None,",
      ") -> Dict[str, Any]:",
      '    """',
      "    Add provenance metadata to ASM for reproducibility and audit trail.",
      "",
      "    This metadata enables:",
      "    - Reproducing conversions months later",
      "    - Determining which version generated data",
      "    - Auditing data lineage for regulatory compliance",
      '    """',
      "    pd = get_pandas()",
      "",
      '    asm["$conversion_metadata"] = {',
      '        "skill_version": "1.0.0",',
      '        "allotropy_version": get_library_version("allotropy"),',
      '        "pandas_version": pd.__version__ if pd else "NOT_INSTALLED",',
      '        "conversion_timestamp_utc": datetime.utcnow().isoformat(),',
      '        "input_file_sha256": calculate_file_hash(filepath),',
      '        "input_file_size_bytes": Path(filepath).stat().st_size,',
      '        "input_file_name": Path(filepath).name,',
      '        "parser_used": "fallback" if used_fallback else "allotropy",',
      '        "detection_confidence": confidence,',
      '        "vendor_detected": vendor,',
      '        "warnings": warnings or [],',
      "    }",
      "",
      "    return asm",
      "",
      "",
      "def flexible_parse(filepath: str, detected_type: str) -> Optional[Dict[str, Any]]:",
      '    """',
      "    Flexible fallback parser when allotropy fails.",
      "    Creates ASM-like structure from parsed data.",
      "",
      "    **WARNING:** This parser creates simplified ASM that:",
      "    - Does NOT distinguish raw vs. calculated data",
      "    - LACKS instrument control parameters (temperature, wavelengths, etc.)",
      "    - MAY NOT be compatible with regulatory requirements (GxP)",
      "    - Should be used for exploratory analysis only, not production LIMS import",
      '    """',
      "    pd = get_pandas()",
      "    if pd is None:",
      '        print("Warning: pandas not installed for flexible parsing")',
      "        return None",
      "",
      "    path = Path(filepath)",
      "    extension = path.suffix.lower()",
      "",
      "    try:",
      "        # Read file based on extension",
      '        if extension in [".xlsx", ".xls"]:',
      '            df = pd.read_excel(filepath, engine="openpyxl")',
      '        elif extension == ".tsv":',
      '            df = pd.read_csv(filepath, sep="\\t")',
      '        elif extension == ".csv":',
      "            df = pd.read_csv(filepath)",
      "        else:",
      '            df = pd.read_csv(filepath, sep=None, engine="python")',
      "",
      "        # Build ASM-like structure",
      "        asm = build_flexible_asm(df, detected_type, filepath)",
      "        return asm",
      "",
      "    except Exception as e:",
      '        print(f"Flexible parsing failed: {e}")',
      "        return None",
      "",
      "",
      "def build_flexible_asm(df, detected_type: str, filepath: str) -> Dict[str, Any]:",
      '    """',
      "    Build ASM-like JSON structure from parsed DataFrame.",
      '    """',
      "    timestamp = get_deterministic_timestamp(filepath)",
      "",
      "    # Determine technique from detected type",
      '    technique = "generic"',
      '    if "VI_CELL" in detected_type:',
      '        technique = "cell-counting"',
      '    elif "NANODROP" in detected_type:',
      '        technique = "spectrophotometry"',
      '    elif detected_type in ["MOLDEV_SOFTMAX_PRO", "BMG_MARS", "AGILENT_GEN5"]:',
      '        technique = "plate-reader"',
      '    elif "QUANTSTUDIO" in detected_type:',
      '        technique = "pcr"',
      "",
      "    # Build base structure",
      "    asm = {",
      '        "$asm.manifest": {',
      '            "vocabulary": ["http://purl.allotrope.org/voc/afo/REC/2023/09/"],',
      '            "contexts": [',
      '                "http://purl.allotrope.org/json-ld/afo-context-REC-2023-09.jsonld"',
      "            ],",
      "        },",
      '        f"{technique}-aggregate-document": {',
      '            "device-system-document": {',
      '                "device-identifier": "FLEXIBLE_PARSER",',
      '                "product-manufacturer": (',
      '                    detected_type.split("_")[0] if "_" in detected_type else "Unknown"',
      "                ),",
      "            },",
      '            f"{technique}-document": [',
      "                {",
      '                    "measurement-aggregate-document": {',
      '                        "measurement-time": timestamp,',
      '                        "measurement-document": [],',
      "                    }",
      "                }",
      "            ],",
      "        },",
      "    }",
      "",
      "    # Add measurements from DataFrame",
      '    measurements = asm[f"{technique}-aggregate-document"][f"{technique}-document"][0][',
      '        "measurement-aggregate-document"',
      '    ]["measurement-document"]',
      "",
      "    for _, row in df.iterrows():",
      "        meas = {}",
      "        for col in df.columns:",
      "            value = row[col]",
      "            if pd.notna(value):",
      "                # Clean column name",
      '                clean_col = str(col).lower().replace(" ", "-").replace("_", "-")',
      '                clean_col = re.sub(r"[^a-z0-9-]", "", clean_col)',
      "",
      "                # Handle numeric values",
      "                if isinstance(value, (int, float)):",
      '                    meas[clean_col] = {"value": value, "unit": "(unitless)"}',
      "                else:",
      "                    meas[clean_col] = str(value)",
      "",
      "        if meas:",
      "            measurements.append(meas)",
      "",
      "    return asm",
      "",
      "",
      "def main():",
      '    """Main entry point."""',
      "    import argparse",
      "",
      "    parser = argparse.ArgumentParser(",
      '        description="Convert instrument data to ASM format"',
      "    )",
      '    parser.add_argument("input", help="Input file path")',
      "    parser.add_argument(",
      '        "--vendor", help="Vendor enum name (auto-detected if not provided)"',
      "    )",
      "    parser.add_argument(",
      '        "--output", "-o", help="Output file path (default: input_asm.json)"',
      "    )",
      "    parser.add_argument(",
      '        "--flatten", action="store_true", help="Also generate flattened CSV"',
      "    )",
      "    parser.add_argument(",
      '        "--allow-fallback",',
      '        action="store_true",',
      '        help="Allow fallback to simplified parser (reduced metadata)",',
      "    )",
      "    parser.add_argument(",
      '        "--skip-validation",',
      '        action="store_true",',
      '        help="Skip automatic validation (not recommended)",',
      "    )",
      "    parser.add_argument(",
      '        "--force",',
      '        action="store_true",',
      '        help="Force conversion even with low confidence detection",',
      "    )",
      "",
      "    args = parser.parse_args()",
      "",
      "    input_path = Path(args.input)",
      "    if not input_path.exists():",
      '        print(f"Error: File not found: {args.input}")',
      "        sys.exit(1)",
      "",
      "    warnings = []",
      "",
      "    # Detect or use provided vendor",
      "    if args.vendor:",
      "        vendor = args.vendor.upper()",
      "        confidence = 100",
      '        print(f"Using specified vendor: {vendor}")',
      "    else:",
      "        vendor, confidence = detect_instrument_type(str(input_path))",
      '        print(f"Detected instrument: {vendor} (confidence: {confidence}%)")',
      "",
      "        # Enforce confidence thresholds",
      "        if confidence < 30:",
      "            print(",
      '                f"ERROR: Detection confidence too low ({confidence}%). Cannot proceed."',
      "            )",
      '            print("Please specify --vendor explicitly.")',
      "            sys.exit(1)",
      "        elif confidence < 60:",
      '            warning_msg = f"WARNING: Low confidence detection ({confidence}%)."',
      "            print(warning_msg)",
      "            warnings.append(warning_msg)",
      "            if not args.force:",
      '                print("Use --force to proceed anyway (not recommended).")',
      "                sys.exit(1)",
      "",
      "    # Try allotropy first",
      "    asm = convert_with_allotropy(str(input_path), vendor)",
      "    used_fallback = False",
      "",
      "    # Fall back to flexible parser",
      "    if asm is None:",
      '        print("\\n" + "=" * 60)',
      '        print("ALLOTROPY PARSING FAILED - USING REDUCED METADATA PARSER")',
      '        print("=" * 60)',
      '        print("Output will lack:")',
      '        print("  - Calculated data traceability")',
      '        print("  - Device control settings")',
      '        print("  - Data processing metadata")',
      '        print("\\nNot suitable for:")',
      '        print("  - Regulatory submissions")',
      '        print("  - LIMS import with validation")',
      '        print("=" * 60 + "\\n")',
      "",
      "        if not args.allow_fallback:",
      "            print(",
      '                "ERROR: Allotropy parsing failed. Use --allow-fallback to continue with"',
      "            )",
      '            print("simplified parser, but note that output will lack required metadata")',
      '            print("for GxP compliance.")',
      "            sys.exit(1)",
      "",
      "        asm = flexible_parse(str(input_path), vendor)",
      "        used_fallback = True",
      '        warnings.append("Used fallback parser - reduced metadata")',
      "",
      "    if asm is None:",
      '        print("Error: Could not convert file")',
      "        sys.exit(1)",
      "",
      "    # Add provenance metadata",
      "    asm = add_provenance_metadata(",
      "        asm, str(input_path), vendor, confidence, used_fallback, warnings",
      "    )",
      "",
      "    # Determine output path",
      "    if args.output:",
      "        output_path = Path(args.output)",
      "    else:",
      '        output_path = input_path.with_suffix(".asm.json")',
      "",
      "    # Write to temporary file first",
      '    temp_path = output_path.with_suffix(".tmp")',
      "",
      "    try:",
      '        with open(temp_path, "w") as f:',
      "            json.dump(asm, f, indent=2, default=str)",
      "",
      "        # Validate unless skipped",
      "        if not args.skip_validation:",
      '            print("Running validation...")',
      "            try:",
      "                from validate_asm import validate_asm",
      "",
      "                result = validate_asm(str(temp_path))",
      "",
      "                if not result.is_valid():",
      '                    print("\\n" + "=" * 60)',
      '                    print("VALIDATION FAILED")',
      '                    print("=" * 60)',
      "                    for error in result.errors:",
      '                        print(f"ERROR: {error}")',
      "                    for warning in result.warnings:",
      '                        print(f"WARNING: {warning}")',
      '                    print("=" * 60)',
      "",
      "                    # Remove temp file",
      "                    temp_path.unlink()",
      '                    print("\\nValidation failed. Output file not created.")',
      "                    sys.exit(1)",
      "                else:",
      "                    if result.warnings:",
      '                        print("\\nValidation warnings:")',
      "                        for warning in result.warnings:",
      '                            print(f"  WARNING: {warning}")',
      '                    print("Validation passed.")',
      "            except ImportError:",
      "                print(",
      '                    "Warning: validate_asm.py not found. Skipping validation. "',
      '                    "Consider adding validation script."',
      "                )",
      "",
      "        # Move temp file to final location",
      "        temp_path.replace(output_path)",
      '        print(f"ASM output written to: {output_path}")',
      "",
      "    except Exception as e:",
      "        # Clean up temp file on error",
      "        if temp_path.exists():",
      "            temp_path.unlink()",
      "        raise e",
      "",
      "    # Optionally flatten",
      "    if args.flatten:",
      "        from flatten_asm import flatten_asm_to_csv",
      "",
      '        flat_path = input_path.with_suffix(".flat.csv")',
      "        flatten_asm_to_csv(asm, str(flat_path))",
      '        print(f"Flattened CSV written to: {flat_path}")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/export_parser.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Export Parser Code",
      "",
      "Generates standalone Python scripts that can be handed off to data engineers",
      "or run in Jupyter notebooks. The exported code is self-contained and",
      "production-ready.",
      "",
      "Usage:",
      "    python export_parser.py --vendor VI_CELL_BLU --output vicell_parser.py",
      "    python export_parser.py --vendor NANODROP_EIGHT --format notebook --output nanodrop_parser.ipynb",
      '"""',
      "",
      "import sys",
      "from pathlib import Path",
      "from datetime import datetime",
      "from typing import Optional",
      "",
      "",
      "# Template for standalone Python script",
      "SCRIPT_TEMPLATE = '''#!/usr/bin/env python3",
      '"""',
      "{instrument_name} to Allotrope Simple Model (ASM) Parser",
      "",
      "Auto-generated by Claude instrument-data-to-allotrope skill",
      "Generated: {timestamp}",
      "Vendor: {vendor}",
      "",
      "This script converts {instrument_name} output files to Allotrope Simple Model (ASM)",
      "JSON format for LIMS import, data lakes, or downstream analysis.",
      "",
      "Requirements:",
      "    pip install allotropy pandas openpyxl",
      "",
      "Usage:",
      "    python {script_name} input_file.csv --output output_asm.json",
      "    python {script_name} input_file.csv --flatten  # Also generate CSV",
      "",
      "Input file format:",
      "    {file_format_description}",
      '"""',
      "",
      "import json",
      "import argparse",
      "from pathlib import Path",
      "from typing import Dict, Any, Optional",
      "",
      "try:",
      "    from allotropy.parser_factory import Vendor",
      "    from allotropy.to_allotrope import allotrope_from_file",
      "    ALLOTROPY_AVAILABLE = True",
      "except ImportError:",
      "    ALLOTROPY_AVAILABLE = False",
      '    print("Warning: allotropy not installed. Install with: pip install allotropy")',
      "",
      "try:",
      "    import pandas as pd",
      "    PANDAS_AVAILABLE = True",
      "except ImportError:",
      "    PANDAS_AVAILABLE = False",
      "",
      "",
      "def convert_to_asm(filepath: str) -> Optional[Dict[str, Any]]:",
      '    """',
      "    Convert {instrument_name} file to ASM format.",
      "    ",
      "    Args:",
      "        filepath: Path to input file",
      "        ",
      "    Returns:",
      "        ASM dictionary or None if conversion fails",
      '    """',
      "    if not ALLOTROPY_AVAILABLE:",
      '        raise ImportError("allotropy library required. Install with: pip install allotropy")',
      "    ",
      "    try:",
      "        asm = allotrope_from_file(filepath, Vendor.{vendor})",
      "        return asm",
      "    except Exception as e:",
      '        print(f"Conversion error: {{e}}")',
      "        return None",
      "",
      "",
      "def flatten_asm(asm: Dict[str, Any]) -> list:",
      '    """',
      "    Flatten ASM to list of row dictionaries for CSV export.",
      "    ",
      "    Args:",
      "        asm: ASM dictionary",
      "        ",
      "    Returns:",
      "        List of flattened row dictionaries",
      '    """',
      '    technique = "{technique}"',
      "    rows = []",
      "    ",
      '    agg_key = f"{{technique}}-aggregate-document"',
      "    agg_doc = asm.get(agg_key, {{}})",
      "    ",
      "    # Extract device info",
      '    device = agg_doc.get("device-system-document", {{}})',
      "    device_info = {{",
      '        "instrument_serial_number": device.get("device-identifier"),',
      '        "instrument_model": device.get("model-number"),',
      "    }}",
      "    ",
      '    doc_key = f"{{technique}}-document"',
      "    for doc in agg_doc.get(doc_key, []):",
      '        meas_agg = doc.get("measurement-aggregate-document", {{}})',
      "        ",
      "        common = {{",
      '            "analyst": meas_agg.get("analyst"),',
      '            "measurement_time": meas_agg.get("measurement-time"),',
      "            **device_info",
      "        }}",
      "        ",
      '        for meas in meas_agg.get("measurement-document", []):',
      "            row = {{**common}}",
      "            for key, value in meas.items():",
      '                clean_key = key.replace("-", "_")',
      '                if isinstance(value, dict) and "value" in value:',
      '                    row[clean_key] = value["value"]',
      '                    if "unit" in value:',
      '                        row[f"{{clean_key}}_unit"] = value["unit"]',
      "                else:",
      "                    row[clean_key] = value",
      "            rows.append(row)",
      "    ",
      "    return rows",
      "",
      "",
      "def main():",
      '    parser = argparse.ArgumentParser(description="Convert {instrument_name} to ASM")',
      '    parser.add_argument("input", help="Input file path")',
      '    parser.add_argument("--output", "-o", help="Output JSON path")',
      '    parser.add_argument("--flatten", action="store_true", help="Also generate CSV")',
      "    ",
      "    args = parser.parse_args()",
      "    ",
      "    input_path = Path(args.input)",
      "    if not input_path.exists():",
      '        print(f"Error: File not found: {{args.input}}")',
      "        return 1",
      "    ",
      "    # Convert to ASM",
      '    print(f"Converting {{args.input}}...")',
      "    asm = convert_to_asm(str(input_path))",
      "    ",
      "    if asm is None:",
      '        print("Conversion failed")',
      "        return 1",
      "    ",
      "    # Write ASM JSON",
      "    output_path = args.output or str(input_path.with_suffix('.asm.json'))",
      "    with open(output_path, 'w') as f:",
      "        json.dump(asm, f, indent=2, default=str)",
      '    print(f"ASM written to: {{output_path}}")',
      "    ",
      "    # Optionally flatten",
      "    if args.flatten and PANDAS_AVAILABLE:",
      "        rows = flatten_asm(asm)",
      "        df = pd.DataFrame(rows)",
      "        flat_path = str(input_path.with_suffix('.flat.csv'))",
      "        df.to_csv(flat_path, index=False)",
      '        print(f"CSV written to: {{flat_path}}")',
      "    ",
      "    return 0",
      "",
      "",
      'if __name__ == "__main__":',
      "    sys.exit(main())",
      "'''",
      "",
      "",
      "# Template for Jupyter notebook",
      'NOTEBOOK_TEMPLATE = """{{',
      ' "cells": [',
      "  {{",
      '   "cell_type": "markdown",',
      '   "metadata": {{}},',
      '   "source": [',
      '    "# {instrument_name} to Allotrope Simple Model (ASM) Parser\\\\n",',
      '    "\\\\n",',
      '    "Auto-generated by Claude instrument-data-to-allotrope skill\\\\n",',
      '    "Generated: {timestamp}\\\\n",',
      '    "Vendor: {vendor}\\\\n",',
      '    "\\\\n",',
      '    "This notebook converts {instrument_name} output files to Allotrope Simple Model (ASM) JSON format."',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "# Install requirements (uncomment if needed)\\\\n",',
      '    "# !pip install allotropy pandas openpyxl"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "import json\\\\n",',
      '    "from pathlib import Path\\\\n",',
      '    "import pandas as pd\\\\n",',
      '    "\\\\n",',
      '    "from allotropy.parser_factory import Vendor\\\\n",',
      '    "from allotropy.to_allotrope import allotrope_from_file"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "markdown",',
      '   "metadata": {{}},',
      '   "source": [',
      '    "## Configuration\\\\n",',
      '    "\\\\n",',
      '    "Set your input file path here:"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "# Configure input/output paths\\\\n",',
      '    "INPUT_FILE = \\\\"your_data_file.csv\\\\"  # <-- Change this\\\\n",',
      '    "OUTPUT_ASM = \\\\"output_asm.json\\\\"\\\\n",',
      '    "OUTPUT_CSV = \\\\"output_flat.csv\\\\""',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "markdown",',
      '   "metadata": {{}},',
      '   "source": [',
      '    "## Convert to ASM"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "# Convert file to ASM\\\\n",',
      '    "asm = allotrope_from_file(INPUT_FILE, Vendor.{vendor})\\\\n",',
      '    "\\\\n",',
      '    "# Save ASM JSON\\\\n",',
      "    \"with open(OUTPUT_ASM, 'w') as f:\\\\n\",",
      '    "    json.dump(asm, f, indent=2, default=str)\\\\n",',
      '    "\\\\n",',
      '    "print(f\\\\"ASM saved to: {{OUTPUT_ASM}}\\\\")"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "markdown",',
      '   "metadata": {{}},',
      '   "source": [',
      '    "## Preview ASM Structure"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "# Show ASM structure\\\\n",',
      '    "print(json.dumps(asm, indent=2, default=str)[:2000])"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "markdown",',
      '   "metadata": {{}},',
      '   "source": [',
      '    "## Flatten to CSV"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "def flatten_asm(asm, technique=\\\\"{technique}\\\\"):\\\\n",',
      '    "    rows = []\\\\n",',
      '    "    agg_key = f\\\\"{{technique}}-aggregate-document\\\\"\\\\n",',
      '    "    agg_doc = asm.get(agg_key, {{}})\\\\n",',
      '    "    \\\\n",',
      '    "    device = agg_doc.get(\\\\"device-system-document\\\\", {{}})\\\\n",',
      '    "    device_info = {{\\\\n",',
      '    "        \\\\"instrument_serial_number\\\\": device.get(\\\\"device-identifier\\\\"),\\\\n",',
      '    "        \\\\"instrument_model\\\\": device.get(\\\\"model-number\\\\"),\\\\n",',
      '    "    }}\\\\n",',
      '    "    \\\\n",',
      '    "    doc_key = f\\\\"{{technique}}-document\\\\"\\\\n",',
      '    "    for doc in agg_doc.get(doc_key, []):\\\\n",',
      '    "        meas_agg = doc.get(\\\\"measurement-aggregate-document\\\\", {{}})\\\\n",',
      '    "        common = {{\\\\n",',
      '    "            \\\\"analyst\\\\": meas_agg.get(\\\\"analyst\\\\"),\\\\n",',
      '    "            \\\\"measurement_time\\\\": meas_agg.get(\\\\"measurement-time\\\\"),\\\\n",',
      '    "            **device_info\\\\n",',
      '    "        }}\\\\n",',
      '    "        \\\\n",',
      '    "        for meas in meas_agg.get(\\\\"measurement-document\\\\", []):\\\\n",',
      '    "            row = {{**common}}\\\\n",',
      '    "            for key, value in meas.items():\\\\n",',
      '    "                clean_key = key.replace(\\\\"-\\\\", \\\\"_\\\\")\\\\n",',
      '    "                if isinstance(value, dict) and \\\\"value\\\\" in value:\\\\n",',
      '    "                    row[clean_key] = value[\\\\"value\\\\"]\\\\n",',
      '    "                    if \\\\"unit\\\\" in value:\\\\n",',
      '    "                        row[f\\\\"{{clean_key}}_unit\\\\"] = value[\\\\"unit\\\\"]\\\\n",',
      '    "                else:\\\\n",',
      '    "                    row[clean_key] = value\\\\n",',
      '    "            rows.append(row)\\\\n",',
      '    "    return rows\\\\n",',
      '    "\\\\n",',
      '    "# Flatten and save\\\\n",',
      '    "rows = flatten_asm(asm)\\\\n",',
      '    "df = pd.DataFrame(rows)\\\\n",',
      '    "df.to_csv(OUTPUT_CSV, index=False)\\\\n",',
      '    "print(f\\\\"CSV saved to: {{OUTPUT_CSV}}\\\\")"',
      "   ]",
      "  }},",
      "  {{",
      '   "cell_type": "code",',
      '   "execution_count": null,',
      '   "metadata": {{}},',
      '   "source": [',
      '    "# Preview flattened data\\\\n",',
      '    "df.head()"',
      "   ]",
      "  }}",
      " ],",
      ' "metadata": {{',
      '  "kernelspec": {{',
      '   "display_name": "Python 3",',
      '   "language": "python",',
      '   "name": "python3"',
      "  }},",
      '  "language_info": {{',
      '   "name": "python",',
      '   "version": "3.10.0"',
      "  }}",
      " }},",
      ' "nbformat": 4,',
      ' "nbformat_minor": 4',
      '}}"""',
      "",
      "",
      "# Instrument metadata for templates",
      "INSTRUMENT_INFO = {",
      '    "BECKMAN_VI_CELL_BLU": {',
      '        "name": "Beckman Coulter Vi-CELL BLU",',
      '        "technique": "cell-counting",',
      '        "file_format": "CSV export from Vi-CELL BLU software with columns: Sample ID, Viable cells, Viability, Total cells, etc.",',
      "    },",
      '    "BECKMAN_VI_CELL_XR": {',
      '        "name": "Beckman Coulter Vi-CELL XR",',
      '        "technique": "cell-counting",',
      '        "file_format": "TXT or XLS/XLSX export from Vi-CELL XR with sample and measurement data",',
      "    },",
      '    "THERMO_FISHER_NANODROP_EIGHT": {',
      '        "name": "Thermo Fisher NanoDrop Eight",',
      '        "technique": "spectrophotometry",',
      '        "file_format": "TSV or TXT export with Sample Name, Nucleic Acid Conc., A260, A280, 260/280 ratio",',
      "    },",
      '    "THERMO_FISHER_NANODROP_ONE": {',
      '        "name": "Thermo Fisher NanoDrop One",',
      '        "technique": "spectrophotometry",',
      '        "file_format": "CSV or XLSX export with spectrophotometry measurements",',
      "    },",
      '    "MOLDEV_SOFTMAX_PRO": {',
      '        "name": "Molecular Devices SoftMax Pro",',
      '        "technique": "plate-reader",',
      '        "file_format": "TXT export from SoftMax Pro with plate reader data",',
      "    },",
      '    "BMG_MARS": {',
      '        "name": "BMG MARS (CLARIOstar)",',
      '        "technique": "plate-reader",',
      '        "file_format": "CSV or TXT export from BMG MARS with Well, Content, Conc., Mean, SD, CV columns",',
      "    },",
      '    "AGILENT_GEN5": {',
      '        "name": "Agilent Gen5 (BioTek)",',
      '        "technique": "plate-reader",',
      '        "file_format": "XLSX export from Gen5 software",',
      "    },",
      '    "APPBIO_QUANTSTUDIO": {',
      '        "name": "Applied Biosystems QuantStudio",',
      '        "technique": "pcr",',
      '        "file_format": "XLSX export with qPCR data including Well, Sample Name, Target Name, CT values",',
      "    },",
      "}",
      "",
      "",
      "def generate_script(vendor: str, output_path: str) -> None:",
      '    """Generate standalone Python script for given vendor."""',
      "    info = INSTRUMENT_INFO.get(",
      "        vendor,",
      "        {",
      '            "name": vendor.replace("_", " ").title(),',
      '            "technique": "generic",',
      '            "file_format": "Instrument output file",',
      "        },",
      "    )",
      "",
      "    script = SCRIPT_TEMPLATE.format(",
      '        instrument_name=info["name"],',
      "        timestamp=datetime.now().isoformat(),",
      "        vendor=vendor,",
      "        script_name=Path(output_path).name,",
      '        file_format_description=info["file_format"],',
      '        technique=info["technique"],',
      "    )",
      "",
      '    with open(output_path, "w") as f:',
      "        f.write(script)",
      "",
      "",
      "def generate_notebook(vendor: str, output_path: str) -> None:",
      '    """Generate Jupyter notebook for given vendor."""',
      "    info = INSTRUMENT_INFO.get(",
      "        vendor,",
      "        {",
      '            "name": vendor.replace("_", " ").title(),',
      '            "technique": "generic",',
      '            "file_format": "Instrument output file",',
      "        },",
      "    )",
      "",
      "    notebook = NOTEBOOK_TEMPLATE.format(",
      '        instrument_name=info["name"],',
      "        timestamp=datetime.now().isoformat(),",
      "        vendor=vendor,",
      '        technique=info["technique"],',
      "    )",
      "",
      '    with open(output_path, "w") as f:',
      "        f.write(notebook)",
      "",
      "",
      "def main():",
      "    import argparse",
      "",
      "    parser = argparse.ArgumentParser(",
      '        description="Export parser code for data engineers"',
      "    )",
      '    parser.add_argument("--vendor", help="Vendor enum name (e.g., VI_CELL_BLU)")',
      '    parser.add_argument("--output", "-o", help="Output file path")',
      "    parser.add_argument(",
      '        "--format",',
      '        choices=["script", "notebook"],',
      '        default="script",',
      '        help="Output format (default: script)",',
      "    )",
      "    parser.add_argument(",
      '        "--list-vendors", action="store_true", help="List supported vendors"',
      "    )",
      "",
      "    args = parser.parse_args()",
      "",
      "    if args.list_vendors:",
      '        print("Supported vendors:")',
      "        for vendor in INSTRUMENT_INFO.keys():",
      '            print(f"  {vendor}")',
      "        return 0",
      "",
      "    if not args.vendor or not args.output:",
      '        parser.error("--vendor and --output are required when not using --list-vendors")',
      "",
      "    vendor = args.vendor.upper()",
      "",
      '    if args.format == "notebook":',
      "        generate_notebook(vendor, args.output)",
      "    else:",
      "        generate_script(vendor, args.output)",
      "",
      '    print(f"Parser code exported to: {args.output}")',
      "    return 0",
      "",
      "",
      'if __name__ == "__main__":',
      "    sys.exit(main())",
      "",
    ].join("\n"),
    "scripts/flatten_asm.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Flatten ASM JSON to 2D CSV",
      "",
      "Converts hierarchical Allotrope Simple Model (ASM) JSON to flat tabular format",
      "suitable for LIMS import, spreadsheet analysis, or database loading.",
      "",
      "Usage:",
      "    python flatten_asm.py <input_asm.json> [--output OUTPUT.csv]",
      '"""',
      "",
      "import json",
      "import sys",
      "import re",
      "from pathlib import Path",
      "from typing import Dict, Any, List, Optional",
      "from datetime import datetime",
      "",
      "try:",
      "    import pandas as pd",
      "",
      "    PANDAS_AVAILABLE = True",
      "except ImportError:",
      "    PANDAS_AVAILABLE = False",
      "",
      "",
      "def detect_technique(asm: Dict[str, Any]) -> str:",
      '    """Detect the ASM technique type from document structure."""',
      "    for key in asm.keys():",
      '        if key.endswith("-aggregate-document"):',
      '            return key.replace("-aggregate-document", "")',
      '    return "generic"',
      "",
      "",
      'def flatten_value(value: Any, prefix: str = "") -> Dict[str, Any]:',
      '    """',
      "    Flatten a single ASM value, handling value datum patterns.",
      "",
      "    Returns dict of {column_name: value}",
      '    """',
      "    result = {}",
      "",
      "    if isinstance(value, dict):",
      '        if "value" in value:',
      "            # Value datum pattern",
      '            result[prefix] = value["value"]',
      '            if "unit" in value:',
      '                result[f"{prefix}_unit"] = value["unit"]',
      "        else:",
      "            # Nested dict - recurse",
      "            for k, v in value.items():",
      '                clean_key = k.replace("-", "_")',
      '                nested_prefix = f"{prefix}_{clean_key}" if prefix else clean_key',
      "                result.update(flatten_value(v, nested_prefix))",
      "    elif isinstance(value, list):",
      "        # Array - could be data cube or list of items",
      "        if len(value) > 0 and isinstance(value[0], dict):",
      "            # List of objects - this shouldn't happen at leaf level",
      "            result[prefix] = json.dumps(value)",
      "        else:",
      "            # Simple array - store as JSON string",
      "            result[prefix] = json.dumps(value)",
      "    else:",
      "        # Scalar value",
      "        result[prefix] = value",
      "",
      "    return result",
      "",
      "",
      "def extract_device_info(asm: Dict[str, Any], technique: str) -> Dict[str, Any]:",
      '    """Extract device/instrument information from ASM."""',
      '    agg_key = f"{technique}-aggregate-document"',
      "    agg_doc = asm.get(agg_key, {})",
      "",
      '    device = agg_doc.get("device-system-document", {})',
      "",
      "    return {",
      '        "instrument_serial_number": device.get("device-identifier"),',
      '        "instrument_model": device.get("model-number"),',
      '        "instrument_manufacturer": device.get("product-manufacturer"),',
      '        "software_name": device.get("software-name"),',
      '        "software_version": device.get("software-version"),',
      "    }",
      "",
      "",
      "def flatten_asm(asm: Dict[str, Any]) -> List[Dict[str, Any]]:",
      '    """',
      "    Flatten ASM JSON to list of row dictionaries.",
      "",
      "    Each measurement becomes one row with metadata repeated.",
      '    """',
      "    technique = detect_technique(asm)",
      "    rows = []",
      "",
      "    # Extract device info (shared across all rows)",
      "    device_info = extract_device_info(asm, technique)",
      "    device_info = {k: v for k, v in device_info.items() if v is not None}",
      "",
      "    # Navigate to measurements",
      '    agg_key = f"{technique}-aggregate-document"',
      "    agg_doc = asm.get(agg_key, {})",
      "",
      '    doc_key = f"{technique}-document"',
      "    technique_docs = agg_doc.get(doc_key, [])",
      "",
      "    for doc in technique_docs:",
      "        # Get measurement aggregate",
      '        meas_agg = doc.get("measurement-aggregate-document", {})',
      "",
      "        # Extract common measurement metadata",
      "        common_meta = {}",
      "        for key, value in meas_agg.items():",
      '            if key == "measurement-document":',
      "                continue",
      '            clean_key = key.replace("-", "_")',
      "            if isinstance(value, (str, int, float, bool)):",
      "                common_meta[clean_key] = value",
      '            elif isinstance(value, dict) and "value" in value:',
      '                common_meta[clean_key] = value["value"]',
      '                if "unit" in value:',
      '                    common_meta[f"{clean_key}_unit"] = value["unit"]',
      "",
      "        # Extract each measurement as a row",
      '        measurements = meas_agg.get("measurement-document", [])',
      "        for meas in measurements:",
      "            row = {**device_info, **common_meta}",
      "",
      "            for key, value in meas.items():",
      '                clean_key = key.replace("-", "_")',
      "                flattened = flatten_value(value, clean_key)",
      "                row.update(flattened)",
      "",
      "            rows.append(row)",
      "",
      "    return rows",
      "",
      "",
      "def flatten_asm_to_csv(asm: Dict[str, Any], output_path: str) -> None:",
      '    """',
      "    Flatten ASM and write to CSV file.",
      "",
      "    Args:",
      "        asm: Parsed ASM JSON dictionary",
      "        output_path: Path for output CSV",
      '    """',
      "    if not PANDAS_AVAILABLE:",
      "        raise ImportError(",
      '            "pandas is required for CSV output. Install with: pip install pandas"',
      "        )",
      "",
      "    rows = flatten_asm(asm)",
      "",
      "    if not rows:",
      '        print("Warning: No measurements found to flatten")',
      "        # Create empty CSV with header",
      '        with open(output_path, "w") as f:',
      '            f.write("# No measurements found in ASM\\n")',
      "        return",
      "",
      "    df = pd.DataFrame(rows)",
      "",
      "    # Reorder columns for readability",
      "    priority_cols = [",
      '        "sample_identifier",',
      '        "sample_id",',
      '        "well_location",',
      '        "well_position",',
      '        "measurement_time",',
      '        "measurement_datetime",',
      '        "analyst",',
      "    ]",
      "",
      "    ordered_cols = []",
      "    for col in priority_cols:",
      "        if col in df.columns:",
      "            ordered_cols.append(col)",
      "",
      "    remaining = [c for c in df.columns if c not in ordered_cols]",
      "    df = df[ordered_cols + remaining]",
      "",
      "    df.to_csv(output_path, index=False)",
      "",
      "",
      "def flatten_asm_to_dict(asm: Dict[str, Any]) -> Dict[str, Any]:",
      '    """',
      "    Flatten ASM and return as dictionary with rows and columns.",
      "",
      "    Useful for non-CSV outputs or further processing.",
      '    """',
      "    rows = flatten_asm(asm)",
      "",
      "    if not rows:",
      '        return {"columns": [], "rows": []}',
      "",
      "    columns = list(rows[0].keys())",
      "    return {",
      '        "columns": columns,',
      '        "rows": [[row.get(col) for col in columns] for row in rows],',
      "    }",
      "",
      "",
      "def main():",
      '    """Main entry point."""',
      "    import argparse",
      "",
      '    parser = argparse.ArgumentParser(description="Flatten ASM JSON to CSV")',
      '    parser.add_argument("input", help="Input ASM JSON file")',
      "    parser.add_argument(",
      '        "--output", "-o", help="Output CSV path (default: input_flat.csv)"',
      "    )",
      "    parser.add_argument(",
      '        "--format",',
      '        choices=["csv", "json"],',
      '        default="csv",',
      '        help="Output format (default: csv)",',
      "    )",
      "",
      "    args = parser.parse_args()",
      "",
      "    input_path = Path(args.input)",
      "    if not input_path.exists():",
      '        print(f"Error: File not found: {args.input}")',
      "        sys.exit(1)",
      "",
      "    # Load ASM",
      "    with open(input_path) as f:",
      "        asm = json.load(f)",
      "",
      "    # Determine output path",
      "    if args.output:",
      "        output_path = args.output",
      "    else:",
      '        suffix = ".flat.csv" if args.format == "csv" else ".flat.json"',
      '        output_path = str(input_path.with_suffix("")) + suffix',
      "",
      "    # Flatten and write",
      '    if args.format == "csv":',
      "        flatten_asm_to_csv(asm, output_path)",
      "    else:",
      "        result = flatten_asm_to_dict(asm)",
      '        with open(output_path, "w") as f:',
      "            json.dump(result, f, indent=2)",
      "",
      '    print(f"Flattened output written to: {output_path}")',
      "",
      "    # Report stats",
      "    rows = flatten_asm(asm)",
      '    print(f"  Rows: {len(rows)}")',
      "    if rows:",
      '        print(f"  Columns: {len(rows[0])}")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/validate_asm.py": [
      "#!/usr/bin/env python3",
      '"""',
      "ASM Output Validation Script",
      "",
      "Validates ASM JSON output against common issues:",
      "- Wrong technique selection",
      "- Hyphenated field names (should be space-separated)",
      "- Missing statistics documents",
      "- Incorrect units",
      "- Missing required fields",
      "- Missing calculated data traceability",
      "- Improperly flattened nested documents (sample document, device control, etc.)",
      "",
      "Validation Rules:",
      "    Based on: Allotrope ASM specification (December 2024)",
      "    Last Updated: 2026-01-07",
      "    Source: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm",
      "",
      "Note: Unknown techniques/units generate WARNINGS (not errors) to allow for new",
      "additions to the Allotrope specification. This prevents blocking valid data",
      "when the Allotrope foundation adds new techniques or units.",
      "",
      "Usage:",
      "    python validate_asm.py output.json",
      "    python validate_asm.py output.json --reference reference.json",
      "    python validate_asm.py output.json --strict",
      '"""',
      "",
      "import json",
      "import re",
      "import sys",
      "import argparse",
      "from typing import Dict, List, Tuple, Any, Optional",
      "",
      "# Validation metadata",
      'ASM_SPEC_VERSION = "2024-12"',
      'VALIDATION_RULES_DATE = "2026-01-07"',
      'SCHEMA_SOURCE = "https://gitlab.com/allotrope-public/asm"',
      "",
      "",
      "# All valid ASM techniques from https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm",
      "VALID_TECHNIQUES = [",
      '    "absorbance",',
      '    "automated-reactors",',
      '    "balance",',
      '    "bga",',
      '    "binding-affinity",',
      '    "bulk-density",',
      '    "cell-counting",',
      '    "cell-culture-analyzer",',
      '    "chromatography",',
      '    "code-reader",',
      '    "conductance",',
      '    "conductivity",',
      '    "disintegration",',
      '    "dsc",',
      '    "dvs",',
      '    "electronic-lab-notebook",',
      '    "electronic-spectrometry",',
      '    "electrophoresis",',
      '    "flow-cytometry",',
      '    "fluorescence",',
      '    "foam-height",',
      '    "foam-qualification",',
      '    "fplc",',
      '    "ftir",',
      '    "gas-chromatography",',
      '    "gc-ms",',
      '    "gloss",',
      '    "hot-tack",',
      '    "impedance",',
      '    "lc-ms",',
      '    "light-obscuration",',
      '    "liquid-chromatography",',
      '    "liquid-handler",  # Added for liquid handler support',
      '    "loss-on-drying",',
      '    "luminescence",',
      '    "mass-spectrometry",',
      '    "metabolite-analyzer",',
      '    "multi-analyte-profiling",',
      '    "nephelometry",',
      '    "nmr",',
      '    "optical-imaging",',
      '    "optical-microscopy",',
      '    "osmolality",',
      '    "oven-kf",',
      '    "pcr",',
      '    "ph",',
      '    "plate-reader",',
      '    "pressure-monitoring",',
      '    "psd",',
      '    "pumping",',
      '    "raman",',
      '    "rheometry",',
      '    "sem",',
      '    "solution-analyzer",',
      '    "specific-rotation",',
      '    "spectrophotometry",',
      '    "stirring",',
      '    "surface-area-analysis",',
      '    "tablet-hardness",',
      '    "temperature-monitoring",',
      '    "tensile-test",',
      '    "thermogravimetric-analysis",',
      '    "titration",',
      '    "ultraviolet-absorbance",',
      '    "x-ray-powder-diffraction",',
      "]",
      "",
      "# Instrument keywords that indicate specific techniques",
      "TECHNIQUE_INDICATORS = {",
      '    "multi-analyte-profiling": [',
      '        "bead",',
      '        "luminex",',
      '        "bio-plex",',
      '        "bioplex",',
      '        "multiplex",',
      '        "plex",',
      '        "msd",',
      '        "region",',
      "    ],",
      '    "electrophoresis": [',
      '        "tapestation",',
      '        "bioanalyzer",',
      '        "labchip",',
      '        "fragment",',
      '        "din",',
      '        "rin",',
      '        "gel",',
      '        "capillary",',
      "    ],",
      '    "spectrophotometry": ["nanodrop", "lunatic", "a260", "a280", "wavelength"],',
      '    "cell-counting": [',
      '        "viability",',
      '        "viable cell",',
      '        "cell count",',
      '        "vi-cell",',
      '        "vicell",',
      '        "nucleocounter",',
      '        "cell density",',
      "    ],",
      '    "pcr": [',
      '        "ct",',
      '        "quantstudio",',
      '        "cfx",',
      '        "amplification",',
      '        "melt curve",',
      '        "qpcr",',
      '        "cycle threshold",',
      "    ],",
      '    "plate-reader": [',
      '        "microplate",',
      '        "96-well",',
      '        "384-well",',
      '        "plate reader",',
      '        "envision",',
      '        "spectramax",',
      "    ],",
      '    "liquid-chromatography": [',
      '        "hplc",',
      '        "uplc",',
      '        "retention time",',
      '        "chromatogram",',
      '        "empower",',
      '        "chromeleon",',
      "    ],",
      '    "flow-cytometry": ["facs", "flow cytometry", "scatter", "gating", "cytometer"],',
      '    "mass-spectrometry": ["m/z", "mass spec", "ms/ms", "lcms", "maldi"],',
      '    "fluorescence": ["fluorescence", "excitation", "emission", "fluorimeter"],',
      '    "luminescence": ["luminescence", "bioluminescence", "chemiluminescence"],',
      '    "absorbance": ["absorbance", "optical density", "od600"],',
      '    "ph": ["ph meter", "ph measurement"],',
      '    "osmolality": ["osmolality", "osmometer"],',
      '    "conductivity": ["conductivity", "conductance"],',
      '    "balance": ["balance", "weight", "mass measurement"],',
      '    "nmr": ["nmr", "nuclear magnetic resonance"],',
      '    "ftir": ["ftir", "infrared", "ir spectrum"],',
      '    "raman": ["raman", "raman spectroscopy"],',
      '    "liquid-handler": [',
      '        "biomek",',
      '        "liquid handler",',
      '        "aspirate",',
      '        "dispense",',
      '        "transfer volume",',
      '        "liquid handling",',
      "    ],",
      "}",
      "",
      "# Fields that should typically be in calculated-data-document, not measurement-document",
      "SHOULD_BE_CALCULATED = [",
      '    "dna integrity number",',
      '    "rna integrity number",',
      '    "din",',
      '    "rin",',
      '    "viability",',
      '    "260/280",',
      '    "a260/a280",',
      '    "concentration",  # When derived from standard curve',
      '    "percent of total",',
      '    "average size",',
      '    "molarity",  # When calculated from concentration',
      '    "relative quantity",',
      '    "fold change",',
      '    "coefficient of variation",',
      "]",
      "",
      "# =============================================================================",
      "# NESTED DOCUMENT STRUCTURE DEFINITIONS",
      "# =============================================================================",
      "",
      "# Fields that MUST be inside 'sample document' (space or hyphen separated)",
      "SAMPLE_DOCUMENT_FIELDS = {",
      "    # Core sample identification",
      '    "sample identifier",',
      '    "sample-identifier",',
      '    "written name",',
      '    "written-name",',
      '    "batch identifier",',
      '    "batch-identifier",',
      '    "sample role type",',
      '    "sample-role-type",',
      '    "description",',
      "    # Location fields (should be in sample document for most techniques)",
      '    "location identifier",',
      '    "location-identifier",',
      '    "well location identifier",',
      '    "well-location-identifier",',
      '    "well plate identifier",',
      '    "well-plate-identifier",',
      "    # Liquid handler specific - source/destination pairs",
      '    "source location identifier",',
      '    "source-location-identifier",',
      '    "destination location identifier",',
      '    "destination-location-identifier",',
      '    "source well plate identifier",',
      '    "source-well-plate-identifier",',
      '    "destination well plate identifier",',
      '    "destination-well-plate-identifier",',
      '    "source well location identifier",',
      '    "source-well-location-identifier",',
      '    "destination well location identifier",',
      '    "destination-well-location-identifier",',
      "}",
      "",
      "# Fields that MUST be inside 'device control aggregate document' -> 'device control document'",
      "DEVICE_CONTROL_FIELDS = {",
      "    # General device control",
      '    "device type",',
      '    "device-type",',
      '    "detector wavelength setting",',
      '    "detector-wavelength-setting",',
      '    "compartment temperature",',
      '    "compartment-temperature",',
      '    "sample volume setting",',
      '    "sample-volume-setting",',
      '    "flow rate",',
      '    "flow-rate",',
      '    "exposure duration setting",',
      '    "exposure-duration-setting",',
      '    "detector gain setting",',
      '    "detector-gain-setting",',
      '    "illumination setting",',
      '    "illumination-setting",',
      "    # Liquid handler specific",
      '    "liquid handling technique",',
      '    "liquid-handling-technique",',
      '    "source liquid handling technique",',
      '    "source-liquid-handling-technique",',
      '    "destination liquid handling technique",',
      '    "destination-liquid-handling-technique",',
      "}",
      "",
      "# Fields that should be in 'custom information document' (vendor-specific)",
      "CUSTOM_INFO_FIELDS = {",
      "    # Liquid handler specific",
      '    "probe",',
      '    "pod",',
      '    "source labware name",',
      '    "source-labware-name",',
      '    "destination labware name",',
      '    "destination-labware-name",',
      '    "deck position",',
      '    "deck-position",',
      "}",
      "",
      "# Fields that commonly get incorrectly flattened (superset for general checking)",
      "COMMONLY_FLATTENED_FIELDS = {",
      "    # Sample-related (often incorrectly put directly on measurement)",
      '    "sample identifier",',
      '    "sample-identifier",',
      '    "sample barcode",',
      '    "sample-barcode",',
      '    "well index",',
      '    "well-index",',
      '    "location identifier",',
      '    "location-identifier",',
      "    # Device control related (often incorrectly put directly on measurement)",
      '    "probe identifier",',
      '    "probe-identifier",',
      '    "device identifier",  # When it should be in device control doc, not measurement',
      '    "device-identifier",',
      '    "technique",  # Should be "liquid handling technique" in device control',
      '    "transfer type",  # Should be structured differently',
      '    "transfer-type",',
      "}",
      "",
      "# Standard ASM units",
      "VALID_UNITS = {",
      '    "fluorescence": ["RFU", "MFI", "(unitless)"],',
      '    "counts": ["#"],',
      '    "volume": ["μL", "mL", "L", "µL"],',
      '    "concentration": [',
      '        "ng/μL",',
      '        "ng/mL",',
      '        "pg/mL",',
      '        "mg/mL",',
      '        "μg/mL",',
      '        "M",',
      '        "mM",',
      '        "μM",',
      '        "nM",',
      "    ],",
      '    "temperature": ["degC"],',
      '    "unitless": ["(unitless)", "%"],',
      '    "molecular_weight": ["bp", "Da", "kDa"],',
      '    "time": ["s", "min", "h"],',
      "}",
      "",
      "# Standard sample role types",
      "VALID_SAMPLE_ROLES = [",
      '    "standard sample role",',
      '    "blank role",',
      '    "control sample role",',
      '    "unknown sample role",',
      '    "reference sample role",',
      '    "calibration sample role",',
      "]",
      "",
      "# Standard statistic datum roles",
      "VALID_STATISTIC_ROLES = [",
      '    "median role",',
      '    "arithmetic mean role",',
      '    "coefficient of variation role",',
      '    "standard deviation role",',
      '    "standard error role",',
      '    "trimmed arithmetic mean role",',
      '    "trimmed standard deviation role",',
      '    "minimum value role",',
      '    "maximum value role",',
      "]",
      "",
      "",
      "class ValidationResult:",
      '    """Container for validation results."""',
      "",
      "    def __init__(self):",
      "        self.errors: List[str] = []",
      "        self.warnings: List[str] = []",
      "        self.info: List[str] = []",
      "        self.metrics: Dict[str, Any] = {}",
      "",
      "    def add_error(self, msg: str):",
      '        self.errors.append(f"ERROR: {msg}")',
      "",
      "    def add_warning(self, msg: str):",
      '        self.warnings.append(f"WARNING: {msg}")',
      "",
      "    def add_info(self, msg: str):",
      '        self.info.append(f"INFO: {msg}")',
      "",
      "    def is_valid(self) -> bool:",
      "        return len(self.errors) == 0",
      "",
      "    def print_report(self):",
      '        print("\\n" + "=" * 60)',
      '        print("ASM VALIDATION REPORT")',
      '        print("=" * 60)',
      "",
      "        # Print metrics",
      "        if self.metrics:",
      '            print("\\nMetrics:")',
      "            for key, value in self.metrics.items():",
      '                print(f"   {key}: {value}")',
      "",
      "        # Print info",
      "        if self.info:",
      '            print("\\n" + "\\n".join(self.info))',
      "",
      "        # Print warnings",
      "        if self.warnings:",
      '            print("\\n" + "\\n".join(self.warnings))',
      "",
      "        # Print errors",
      "        if self.errors:",
      '            print("\\n" + "\\n".join(self.errors))',
      "",
      "        # Summary",
      '        print("\\n" + "-" * 60)',
      "        if self.is_valid():",
      "            if self.warnings:",
      '                print(f"PASSED with {len(self.warnings)} warning(s)")',
      "            else:",
      '                print("PASSED - No issues found")',
      "        else:",
      "            print(",
      '                f"FAILED - {len(self.errors)} error(s), {len(self.warnings)} warning(s)"',
      "            )",
      '        print("=" * 60 + "\\n")',
      "",
      "",
      "def validate_manifest(asm: Dict, result: ValidationResult):",
      '    """Check for valid manifest."""',
      '    if "$asm.manifest" not in asm:',
      '        result.add_error("Missing $asm.manifest")',
      "        return",
      "",
      '    manifest = asm["$asm.manifest"]',
      "    if isinstance(manifest, str):",
      '        if "allotrope.org" in manifest:',
      '            result.add_info(f"Manifest: {manifest}")',
      "        else:",
      '            result.add_warning(f"Non-standard manifest URL: {manifest}")',
      "    elif isinstance(manifest, dict):",
      '        if "vocabulary" in manifest or "contexts" in manifest:',
      '            result.add_info("Manifest: Object format with vocabulary/contexts")',
      "        else:",
      '            result.add_warning("Manifest object missing vocabulary or contexts")',
      "",
      "",
      "def detect_technique(asm: Dict) -> Tuple[str, float]:",
      '    """Detect technique from ASM structure."""',
      "    # Check for technique in top-level keys",
      "    for key in asm.keys():",
      '        if key == "$asm.manifest":',
      "            continue",
      "        # Extract technique name from aggregate document key",
      '        # Handle both "liquid handler aggregate document" and "liquid-handler-aggregate-document"',
      '        key_normalized = key.lower().replace("-", " ")',
      '        if "aggregate document" in key_normalized:',
      '            technique = key_normalized.replace(" aggregate document", "").strip()',
      "            return technique, 100.0",
      "",
      '    return "unknown", 0.0',
      "",
      "",
      "def validate_technique(asm: Dict, result: ValidationResult, content_str: str):",
      '    """Validate technique selection."""',
      "    technique, confidence = detect_technique(asm)",
      '    result.metrics["technique"] = technique',
      '    result.metrics["technique_confidence"] = confidence',
      "",
      '    if technique == "unknown":',
      '        result.add_warning("Could not detect technique from ASM structure")',
      "        return",
      "",
      '    result.add_info(f"Detected technique: {technique}")',
      "",
      "    # Check if technique is in known list (soft validation)",
      '    technique_normalized = technique.replace(" ", "-")',
      "    if technique_normalized not in VALID_TECHNIQUES:",
      "        result.add_warning(",
      "            f\"Unknown technique '{technique}' not in known list (as of {VALIDATION_RULES_DATE}). \"",
      '            f"This may be a new Allotrope addition. Verify at: {SCHEMA_SOURCE}"',
      "        )",
      "",
      "    # Check if technique seems appropriate for content",
      "    content_lower = content_str.lower()",
      "    suggested_technique = None",
      "",
      "    for tech, keywords in TECHNIQUE_INDICATORS.items():",
      "        matches = sum(1 for kw in keywords if kw in content_lower)",
      "        if matches >= 2:  # Multiple keyword matches",
      '            if tech != technique.replace(" ", "-"):',
      "                suggested_technique = tech",
      "                break",
      "",
      "    if suggested_technique:",
      "        result.add_warning(",
      "            f\"Content suggests '{suggested_technique}' but ASM uses '{technique}' - \"",
      '            "verify correct technique selection"',
      "        )",
      "",
      "",
      "def validate_naming_conventions(content_str: str, result: ValidationResult):",
      '    """Check for proper space-separated naming (not hyphens)."""',
      "    # Find all keys that look like ASM field names",
      "    # Hyphenated keys in ASM are typically wrong (should be space-separated)",
      "    hyphenated_keys = re.findall(r'\"([a-z]+-[a-z]+-?[a-z]*-?[a-z]*)\":', content_str)",
      "",
      "    # Filter to likely ASM fields (not URLs, not manifest)",
      "    asm_hyphenated = []",
      "    for key in hyphenated_keys:",
      '        if "http" in key or "manifest" in key:',
      "            continue",
      "        # Known hyphenated keys that are OK",
      '        if key in ["data-source-identifier", "data-source-feature"]:',
      "            continue",
      "        asm_hyphenated.append(key)",
      "",
      "    if asm_hyphenated:",
      "        unique = list(set(asm_hyphenated))[:10]",
      "        result.add_warning(",
      '            f"Found hyphenated field names (ASM uses spaces): {unique}"',
      '            + (" ... and more" if len(set(asm_hyphenated)) > 10 else "")',
      "        )",
      "        result.add_info(\"Tip: Use 'sample identifier' not 'sample-identifier'\")",
      "",
      "",
      "def count_measurements(content_str: str) -> int:",
      '    """Count measurement documents in ASM."""',
      "    # Count occurrences of measurement document patterns",
      "    count = len(re.findall(r'\"measurement identifier\":', content_str))",
      "    if count == 0:",
      "        count = len(re.findall(r'\"measurement-identifier\":', content_str))",
      "    return count",
      "",
      "",
      "def validate_measurements(content_str: str, result: ValidationResult):",
      '    """Validate measurement documents."""',
      "    count = count_measurements(content_str)",
      '    result.metrics["measurement_count"] = count',
      "",
      "    if count == 0:",
      '        result.add_warning("No measurement documents found")',
      "    else:",
      '        result.add_info(f"Measurement count: {count}")',
      "",
      "",
      "def validate_sample_roles(content_str: str, result: ValidationResult):",
      '    """Check for valid sample roles."""',
      '    roles = re.findall(r\'"sample.role.type":\\s*"([^"]+)"\', content_str)',
      "    if not roles:",
      '        roles = re.findall(r\'"sample role type":\\s*"([^"]+)"\', content_str)',
      "",
      "    if roles:",
      "        unknown_roles = [r for r in set(roles) if r not in VALID_SAMPLE_ROLES]",
      "        if unknown_roles:",
      "            result.add_warning(",
      '                f"Unknown sample roles not in known list (as of {VALIDATION_RULES_DATE}): {unknown_roles}. "',
      '                f"These may be valid Allotrope roles added after spec version {ASM_SPEC_VERSION}. "',
      '                f"Verify at: {SCHEMA_SOURCE}"',
      "            )",
      "",
      "",
      "def validate_statistics(asm: Dict, content_str: str, result: ValidationResult):",
      '    """Check for statistics documents where expected."""',
      "    technique, _ = detect_technique(asm)",
      "",
      "    has_stats = (",
      '        "statistics aggregate document" in content_str.lower()',
      '        or "statistics-aggregate-document" in content_str',
      "    )",
      "",
      '    result.metrics["has_statistics"] = has_stats',
      "",
      "    # Statistics are required for multi-analyte profiling",
      '    if "multi analyte" in technique or "multiplex" in content_str.lower():',
      "        if not has_stats:",
      "            result.add_warning(",
      '                "No statistics aggregate document found - bead-based assays should include "',
      '                "median, mean, CV, std dev per analyte"',
      "            )",
      "        else:",
      '            result.add_info("Statistics document: Present")',
      "",
      "",
      "def validate_units(content_str: str, result: ValidationResult):",
      '    """Check for valid units."""',
      "    # Find all unit values",
      '    units = re.findall(r\'"unit":\\s*"([^"]+)"\', content_str)',
      "",
      "    # Check for common case-sensitivity issues",
      "    case_issues = []",
      "    for unit in set(units):",
      '        if unit.lower() in ["rfu", "mfi"] and unit not in ["RFU", "MFI"]:',
      '            case_issues.append(f"{unit} (should be uppercase)")',
      '        elif unit in ["ul", "uL", "µl"] and unit != "μL":',
      '            case_issues.append(f"{unit} (should be μL)")',
      "",
      "    if case_issues:",
      '        result.add_warning(f"Non-standard unit capitalization: {case_issues}")',
      "",
      "    # Soft validation: check against known units list",
      "    all_known_units = set()",
      "    for unit_list in VALID_UNITS.values():",
      "        all_known_units.update(unit_list)",
      "",
      "    unknown_units = []",
      "    for unit in set(units):",
      "        # Skip units that have case issues (already reported above)",
      "        if unit not in all_known_units and unit not in [u.lower() for u in case_issues]:",
      "            unknown_units.append(unit)",
      "",
      "    if unknown_units:",
      "        result.add_warning(",
      '            f"Unknown units not in known list (as of {VALIDATION_RULES_DATE}): {unknown_units}. "',
      '            f"These may be valid Allotrope units added after spec version {ASM_SPEC_VERSION}. "',
      '            f"Verify at: {SCHEMA_SOURCE}"',
      "        )",
      "",
      "",
      "def validate_metadata(content_str: str, result: ValidationResult):",
      '    """Check for required metadata fields."""',
      "    required_fields = [",
      '        ("device system document", "equipment serial number"),',
      '        ("data system document", "software name"),',
      '        ("data system document", "software version"),',
      "    ]",
      "",
      "    missing = []",
      "    for _, field in required_fields:",
      "        if (",
      "            field not in content_str.lower()",
      '            and field.replace(" ", "-") not in content_str',
      "        ):",
      "            missing.append(field)",
      "",
      "    if missing:",
      '        result.add_warning(f"Missing recommended metadata: {missing}")',
      "",
      "",
      "def validate_calculated_data(content_str: str, result: ValidationResult):",
      '    """Check calculated data has proper traceability."""',
      "    content_lower = content_str.lower()",
      "",
      "    has_calculated = (",
      '        "calculated data document" in content_lower',
      '        or "calculated-data-document" in content_str',
      "    )",
      "    has_data_source = (",
      '        "data source aggregate document" in content_lower',
      '        or "data-source-aggregate-document" in content_str',
      "    )",
      "",
      '    result.metrics["has_calculated_data"] = has_calculated',
      '    result.metrics["has_data_source_traceability"] = has_data_source',
      "",
      "    if has_calculated:",
      '        result.add_info("Calculated data document: Present")',
      "        if not has_data_source:",
      "            result.add_error(",
      '                "Calculated data found without data-source-aggregate-document - "',
      '                "traceability is required for audit/regulatory compliance"',
      "            )",
      "        else:",
      '            result.add_info("Data source traceability: Present")',
      "",
      "    # Check for calculated fields that might be incorrectly placed in measurement-document",
      "    misplaced = []",
      "    for field in SHOULD_BE_CALCULATED:",
      "        # Check if field appears in measurement document context but not in calculated data",
      '        field_pattern = field.replace("/", ".")',
      "        if field_pattern in content_lower:",
      "            # If we have the field but no calculated-data-document, it's misplaced",
      "            if not has_calculated:",
      "                misplaced.append(field)",
      "",
      "    if misplaced:",
      "        result.add_warning(",
      '            f"Fields that should likely be in calculated-data-document: {misplaced[:5]}"',
      '            + (f" ... and {len(misplaced)-5} more" if len(misplaced) > 5 else "")',
      "        )",
      "",
      "",
      "def validate_unique_identifiers(content_str: str, result: ValidationResult):",
      '    """Validate that entities have unique identifiers for traceability."""',
      "    # Count different identifier types",
      "    measurement_ids = len(",
      '        re.findall(r\'"measurement identifier":\\s*"[^"]+"\', content_str)',
      "    )",
      "    if measurement_ids == 0:",
      "        measurement_ids = len(",
      '            re.findall(r\'"measurement-identifier":\\s*"[^"]+"\', content_str)',
      "        )",
      "",
      "    calculated_ids = len(",
      '        re.findall(r\'"calculated data identifier":\\s*"[^"]+"\', content_str)',
      "    )",
      "    if calculated_ids == 0:",
      "        calculated_ids = len(",
      '            re.findall(r\'"calculated-data-identifier":\\s*"[^"]+"\', content_str)',
      "        )",
      "",
      "    data_source_ids = len(",
      '        re.findall(r\'"data source identifier":\\s*"[^"]+"\', content_str)',
      "    )",
      "    if data_source_ids == 0:",
      "        data_source_ids = len(",
      '            re.findall(r\'"data-source-identifier":\\s*"[^"]+"\', content_str)',
      "        )",
      "",
      '    result.metrics["measurement_identifiers"] = measurement_ids',
      '    result.metrics["calculated_data_identifiers"] = calculated_ids',
      '    result.metrics["data_source_identifiers"] = data_source_ids',
      "",
      "    if measurement_ids == 0:",
      "        result.add_warning(",
      '            "No measurement identifiers found - required for traceability"',
      "        )",
      "",
      "    # If we have calculated data but no data source identifiers, that's a problem",
      "    if calculated_ids > 0 and data_source_ids == 0:",
      "        result.add_error(",
      '            f"Found {calculated_ids} calculated data entries but no data source identifiers - "',
      '            "each calculated value should reference its source"',
      "        )",
      "",
      "",
      "# =============================================================================",
      "# NEW: NESTED DOCUMENT STRUCTURE VALIDATION",
      "# =============================================================================",
      "",
      "",
      "def validate_nested_document_structure(",
      "    asm: Dict, content_str: str, result: ValidationResult",
      "):",
      '    """',
      "    Validate that fields are properly nested in their correct documents.",
      "",
      "    This checks for common mistakes like:",
      "    - Sample fields flattened directly onto measurement instead of in 'sample document'",
      "    - Device control fields flattened instead of in 'device control aggregate document'",
      "    - Custom/vendor fields not wrapped in 'custom information document'",
      '    """',
      "    content_lower = content_str.lower()",
      "",
      "    # Check if proper nested documents exist",
      "    has_sample_doc = (",
      "        '\"sample document\"' in content_lower or '\"sample-document\"' in content_str",
      "    )",
      "    has_device_control_doc = (",
      "        '\"device control aggregate document\"' in content_lower",
      "        or '\"device-control-aggregate-document\"' in content_str",
      "    )",
      "    has_custom_info_doc = (",
      "        '\"custom information document\"' in content_lower",
      "        or '\"custom-information-document\"' in content_str",
      "    )",
      "",
      '    result.metrics["has_sample_document"] = has_sample_doc',
      '    result.metrics["has_device_control_document"] = has_device_control_doc',
      '    result.metrics["has_custom_information_document"] = has_custom_info_doc',
      "",
      "    # Parse ASM to check field locations",
      '    def find_flattened_fields_in_measurements(obj, path=""):',
      '        """Recursively find fields that appear directly on measurement documents."""',
      '        issues = {"sample": [], "device_control": [], "custom": []}',
      "",
      "        if isinstance(obj, dict):",
      "            # Check if we're inside a measurement document",
      "            in_measurement = (",
      '                "measurement document" in path.lower() or "measurement-document" in path',
      "            )",
      "            in_sample_doc = (",
      '                "sample document" in path.lower() or "sample-document" in path',
      "            )",
      "            in_device_control = (",
      '                "device control" in path.lower() or "device-control" in path',
      "            )",
      "            in_custom_info = (",
      '                "custom information" in path.lower() or "custom-information" in path',
      "            )",
      "",
      "            for key, value in obj.items():",
      '                key_normalized = key.lower().replace("-", " ")',
      '                new_path = f"{path}.{key}"',
      "",
      "                # Check if this key should be nested but isn't",
      "                if in_measurement and not in_sample_doc:",
      "                    if key_normalized in [",
      '                        f.lower().replace("-", " ") for f in SAMPLE_DOCUMENT_FIELDS',
      "                    ]:",
      '                        issues["sample"].append(key)',
      "",
      "                if in_measurement and not in_device_control:",
      "                    if key_normalized in [",
      '                        f.lower().replace("-", " ") for f in DEVICE_CONTROL_FIELDS',
      "                    ]:",
      '                        issues["device_control"].append(key)',
      "",
      "                if in_measurement and not in_custom_info:",
      "                    if key_normalized in [",
      '                        f.lower().replace("-", " ") for f in CUSTOM_INFO_FIELDS',
      "                    ]:",
      '                        issues["custom"].append(key)',
      "",
      "                # Recurse",
      "                child_issues = find_flattened_fields_in_measurements(value, new_path)",
      "                for k in issues:",
      "                    issues[k].extend(child_issues[k])",
      "",
      "        elif isinstance(obj, list):",
      "            for i, item in enumerate(obj):",
      "                child_issues = find_flattened_fields_in_measurements(",
      '                    item, f"{path}[{i}]"',
      "                )",
      "                for k in issues:",
      "                    issues[k].extend(child_issues[k])",
      "",
      "        return issues",
      "",
      "    issues = find_flattened_fields_in_measurements(asm)",
      '    flattened_sample_fields = list(set(issues["sample"]))',
      '    flattened_device_control_fields = list(set(issues["device_control"]))',
      '    flattened_custom_fields = list(set(issues["custom"]))',
      "",
      "    # Report issues",
      "    if flattened_sample_fields:",
      "        result.add_error(",
      "            f\"Fields that should be nested in 'sample document' are flattened on measurement: \"",
      '            f"{flattened_sample_fields[:5]}"',
      "            + (",
      '                f" ... and {len(flattened_sample_fields)-5} more"',
      "                if len(flattened_sample_fields) > 5",
      '                else ""',
      "            )",
      "        )",
      "        result.add_info(",
      "            \"Tip: Wrap sample fields in a 'sample document' object inside each measurement\"",
      "        )",
      "",
      "    if flattened_device_control_fields:",
      "        result.add_error(",
      "            f\"Fields that should be nested in 'device control aggregate document' are flattened: \"",
      '            f"{flattened_device_control_fields[:5]}"',
      "            + (",
      '                f" ... and {len(flattened_device_control_fields)-5} more"',
      "                if len(flattened_device_control_fields) > 5",
      '                else ""',
      "            )",
      "        )",
      "        result.add_info(",
      "            \"Tip: Wrap device control fields in 'device control aggregate document' → 'device control document'\"",
      "        )",
      "",
      "    if flattened_custom_fields:",
      "        result.add_warning(",
      "            f\"Vendor-specific fields that should be in 'custom information document': \"",
      '            f"{flattened_custom_fields[:5]}"',
      "            + (",
      '                f" ... and {len(flattened_custom_fields)-5} more"',
      "                if len(flattened_custom_fields) > 5",
      '                else ""',
      "            )",
      "        )",
      "",
      "",
      "def validate_liquid_handler_structure(",
      "    asm: Dict, content_str: str, result: ValidationResult",
      "):",
      '    """',
      "    Specific validation for liquid handler ASM documents.",
      "",
      "    Checks for:",
      "    - Proper transfer pairing (aspirate + dispense = 1 measurement)",
      "    - Source/destination field pairs",
      "    - Aspiration volume + transfer volume instead of single volume",
      '    """',
      "    technique, _ = detect_technique(asm)",
      "",
      "    # Only run for liquid handler techniques",
      '    if "liquid" not in technique.lower() and "handler" not in technique.lower():',
      "        # Also check content for liquid handler indicators",
      "        content_lower = content_str.lower()",
      "        if not any(",
      "            kw in content_lower",
      '            for kw in ["aspirate", "dispense", "liquid handler", "biomek"]',
      "        ):",
      "            return",
      "",
      '    result.add_info("Liquid handler specific validation...")',
      "",
      "    content_lower = content_str.lower()",
      "",
      "    # Check for proper volume field structure",
      "    has_aspiration_volume = (",
      '        "aspiration volume" in content_lower or "aspiration-volume" in content_str',
      "    )",
      "    has_transfer_volume = (",
      '        "transfer volume" in content_lower or "transfer-volume" in content_str',
      "    )",
      "    has_single_volume = (",
      "        '\"volume\"' in content_str",
      "        and not has_aspiration_volume",
      "        and not has_transfer_volume",
      "    )",
      "",
      "    if has_single_volume and not has_aspiration_volume:",
      "        result.add_warning(",
      "            \"Liquid handler ASM uses single 'volume' field - \"",
      "            \"consider using 'aspiration volume' and 'transfer volume' for full transfer semantics\"",
      "        )",
      "",
      "    if has_aspiration_volume and has_transfer_volume:",
      '        result.add_info("Volume fields: Proper aspiration/transfer volume structure")',
      "",
      "    # Check for source/destination pairing",
      "    has_source_dest = (",
      '        "source location" in content_lower or "source-location" in content_str',
      "    ) and (",
      '        "destination location" in content_lower or "destination-location" in content_str',
      "    )",
      "",
      "    has_separate_transfer_type = (",
      '        "transfer type" in content_lower or "transfer-type" in content_str',
      "    )",
      "",
      "    if has_separate_transfer_type and not has_source_dest:",
      "        result.add_warning(",
      "            \"Found 'transfer type' field (Aspirate/Dispense as separate records) - \"",
      "            \"proper ASM pairs source→destination in single measurement with 'source location identifier' \"",
      "            \"and 'destination location identifier'\"",
      "        )",
      "        result.add_info(",
      '            "Tip: Pair aspirate+dispense operations by probe number into single transfer measurements"',
      "        )",
      "",
      "    if has_source_dest:",
      '        result.add_info("Source/destination: Proper paired transfer structure")',
      "",
      "    # Check for labware name fields in custom information document",
      "    has_labware_names = (",
      '        "source labware name" in content_lower',
      '        or "destination labware name" in content_lower',
      "    )",
      "",
      "    if has_labware_names:",
      "        result.add_info(",
      '            "Labware names: Present (should be in custom information document)"',
      "        )",
      "",
      "",
      "def compare_to_reference(",
      "    asm: Dict,",
      "    reference: Dict,",
      "    content_str: str,",
      "    ref_content: str,",
      "    result: ValidationResult,",
      "):",
      '    """Compare generated ASM to reference ASM."""',
      '    result.add_info("Comparing to reference ASM...")',
      "",
      "    # Compare techniques",
      "    gen_tech, _ = detect_technique(asm)",
      "    ref_tech, _ = detect_technique(reference)",
      "",
      '    if gen_tech.replace("-", " ") != ref_tech.replace("-", " "):',
      "        result.add_error(",
      "            f\"Technique mismatch: generated '{gen_tech}' vs reference '{ref_tech}'\"",
      "        )",
      "",
      "    # Compare measurement counts",
      "    gen_count = count_measurements(content_str)",
      "    ref_count = count_measurements(ref_content)",
      "",
      '    result.metrics["reference_measurement_count"] = ref_count',
      "",
      "    if gen_count != ref_count:",
      "        diff = ref_count - gen_count",
      "        if diff > 0:",
      "            result.add_error(",
      '                f"Missing {diff} measurements: generated {gen_count} vs reference {ref_count}"',
      "            )",
      "        else:",
      "            result.add_warning(",
      '                f"Extra {-diff} measurements: generated {gen_count} vs reference {ref_count}"',
      "            )",
      "",
      "    # Compare sample roles",
      '    gen_roles = set(re.findall(r\'"sample.role.type":\\s*"([^"]+)"\', content_str))',
      '    ref_roles = set(re.findall(r\'"sample role type":\\s*"([^"]+)"\', ref_content))',
      "",
      "    missing_roles = ref_roles - gen_roles",
      "    if missing_roles:",
      '        result.add_warning(f"Missing sample roles from reference: {missing_roles}")',
      "",
      "    # Compare nested document presence",
      "    ref_has_sample_doc = '\"sample document\"' in ref_content.lower()",
      "    gen_has_sample_doc = (",
      "        '\"sample document\"' in content_str.lower() or '\"sample-document\"' in content_str",
      "    )",
      "",
      "    if ref_has_sample_doc and not gen_has_sample_doc:",
      "        result.add_error(",
      "            \"Reference has 'sample document' but generated ASM does not - fields may be incorrectly flattened\"",
      "        )",
      "",
      "    ref_has_device_control = (",
      "        '\"device control aggregate document\"' in ref_content.lower()",
      "    )",
      "    gen_has_device_control = (",
      "        '\"device control aggregate document\"' in content_str.lower()",
      "        or '\"device-control-aggregate-document\"' in content_str",
      "    )",
      "",
      "    if ref_has_device_control and not gen_has_device_control:",
      "        result.add_error(",
      "            \"Reference has 'device control aggregate document' but generated ASM does not\"",
      "        )",
      "",
      "    ref_has_custom_info = '\"custom information document\"' in ref_content.lower()",
      "    gen_has_custom_info = (",
      "        '\"custom information document\"' in content_str.lower()",
      "        or '\"custom-information-document\"' in content_str",
      "    )",
      "",
      "    if ref_has_custom_info and not gen_has_custom_info:",
      "        result.add_warning(",
      "            \"Reference has 'custom information document' for vendor fields but generated ASM does not\"",
      "        )",
      "",
      "",
      "def validate_asm(",
      "    filepath: str, reference_path: Optional[str] = None, strict: bool = False",
      ") -> ValidationResult:",
      '    """',
      "    Validate ASM JSON file.",
      "",
      "    Args:",
      "        filepath: Path to ASM JSON file",
      "        reference_path: Optional path to reference ASM for comparison",
      "        strict: If True, treat warnings as errors",
      "",
      "    Returns:",
      "        ValidationResult with errors, warnings, and metrics",
      '    """',
      "    result = ValidationResult()",
      "",
      "    # Load ASM file",
      "    try:",
      '        with open(filepath, "r", encoding="utf-8") as f:',
      "            content_str = f.read()",
      "            asm = json.loads(content_str)",
      "    except json.JSONDecodeError as e:",
      '        result.add_error(f"Invalid JSON: {e}")',
      "        return result",
      "    except FileNotFoundError:",
      '        result.add_error(f"File not found: {filepath}")',
      "        return result",
      "",
      '    result.add_info(f"Validating: {filepath}")',
      "",
      "    # Run validations",
      "    validate_manifest(asm, result)",
      "    validate_technique(asm, result, content_str)",
      "    validate_naming_conventions(content_str, result)",
      "    validate_measurements(content_str, result)",
      "    validate_sample_roles(content_str, result)",
      "    validate_statistics(asm, content_str, result)",
      "    validate_units(content_str, result)",
      "    validate_metadata(content_str, result)",
      "    validate_calculated_data(content_str, result)",
      "    validate_unique_identifiers(content_str, result)",
      "",
      "    # NEW: Nested document structure validation",
      "    validate_nested_document_structure(asm, content_str, result)",
      "    validate_liquid_handler_structure(asm, content_str, result)",
      "",
      "    # Compare to reference if provided",
      "    if reference_path:",
      "        try:",
      '            with open(reference_path, "r", encoding="utf-8") as f:',
      "                ref_content = f.read()",
      "                reference = json.loads(ref_content)",
      "            compare_to_reference(asm, reference, content_str, ref_content, result)",
      "        except Exception as e:",
      '            result.add_warning(f"Could not load reference file: {e}")',
      "",
      "    # In strict mode, convert warnings to errors",
      "    if strict:",
      '        result.errors.extend([w.replace("WARNING", "ERROR") for w in result.warnings])',
      "        result.warnings = []",
      "",
      "    return result",
      "",
      "",
      "def main():",
      '    parser = argparse.ArgumentParser(description="Validate ASM JSON output")',
      '    parser.add_argument("input", help="ASM JSON file to validate")',
      '    parser.add_argument("--reference", "-r", help="Reference ASM file for comparison")',
      "    parser.add_argument(",
      '        "--strict", "-s", action="store_true", help="Treat warnings as errors"',
      "    )",
      '    parser.add_argument("--quiet", "-q", action="store_true", help="Only show errors")',
      "",
      "    args = parser.parse_args()",
      "",
      "    result = validate_asm(args.input, args.reference, args.strict)",
      "",
      "    if args.quiet:",
      "        if result.errors:",
      "            for error in result.errors:",
      "                print(error)",
      "            sys.exit(1)",
      "        sys.exit(0)",
      "",
      "    result.print_report()",
      "    sys.exit(0 if result.is_valid() else 1)",
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: bio-research-instrument-data-to-allotrope",
      "description: Convert laboratory instrument output files (PDF, CSV, Excel, TXT) to Allotrope Simple Model (ASM) JSON format or flattened 2D CSV. Use this skill when scientists need to standardize instrument data for LIMS systems, data lakes, or downstream analysis. Supports auto-detection of instrument types. Outputs include full ASM JSON, flattened CSV for easy import, and exportable Python code for data engineers. Common triggers include converting instrument files, standardizing lab data, preparing data for upload to LIMS/ELN systems, or generating parser code for production pipelines.",
      "metadata:",
      "  plugin: bio-research",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Instrument Data to Allotrope Converter",
      "",
      "Convert instrument files into standardized Allotrope Simple Model (ASM) format for LIMS upload, data lakes, or handoff to data engineering teams.",
      "",
      "> **Note: This is an Example Skill**",
      ">",
      "> This skill demonstrates how skills can support your data engineering tasks—automating schema transformations, parsing instrument outputs, and generating production-ready code.",
      ">",
      "> **To customize for your organization:**",
      "> - Modify the `references/` files to include your company's specific schemas or ontology mappings",
      "> - Use an MCP server to connect to systems that define your schemas (e.g., your LIMS, data catalog, or schema registry)",
      "> - Extend the `scripts/` to handle proprietary instrument formats or internal data standards",
      ">",
      "> This pattern can be adapted for any data transformation workflow where you need to convert between formats or validate against organizational standards.",
      "",
      "## Workflow Overview",
      "",
      "1. **Detect instrument type** from file contents (auto-detect or user-specified)",
      "2. **Parse file** using allotropy library (native) or flexible fallback parser",
      "3. **Generate outputs**:",
      "   - ASM JSON (full semantic structure)",
      "   - Flattened CSV (2D tabular format)",
      "   - Python parser code (for data engineer handoff)",
      "4. **Deliver** files with summary and usage instructions",
      "",
      "> **When Uncertain:** If you're unsure how to map a field to ASM (e.g., is this raw data or calculated? device setting or environmental condition?), ask the user for clarification. Refer to `references/field_classification_guide.md` for guidance, but when ambiguity remains, confirm with the user rather than guessing.",
      "",
      "## Quick Start",
      "",
      "```python",
      "# Install requirements first",
      "pip install allotropy pandas openpyxl pdfplumber --break-system-packages",
      "",
      "# Core conversion",
      "from allotropy.parser_factory import Vendor",
      "from allotropy.to_allotrope import allotrope_from_file",
      "",
      "# Convert with allotropy",
      'asm = allotrope_from_file("instrument_data.csv", Vendor.BECKMAN_VI_CELL_BLU)',
      "```",
      "",
      "## Output Format Selection",
      "",
      "**ASM JSON (default)** - Full semantic structure with ontology URIs",
      "- Best for: LIMS systems expecting ASM, data lakes, long-term archival",
      "- Validates against Allotrope schemas",
      "",
      "**Flattened CSV** - 2D tabular representation",
      "- Best for: Quick analysis, Excel users, systems without JSON support",
      "- Each measurement becomes one row with metadata repeated",
      "",
      "**Both** - Generate both formats for maximum flexibility",
      "",
      "## Calculated Data Handling",
      "",
      "**IMPORTANT:** Separate raw measurements from calculated/derived values.",
      "",
      "- **Raw data** → `measurement-document` (direct instrument readings)",
      "- **Calculated data** → `calculated-data-aggregate-document` (derived values)",
      "",
      "Calculated values MUST include traceability via `data-source-aggregate-document`:",
      "",
      "```json",
      '"calculated-data-aggregate-document": {',
      '  "calculated-data-document": [{',
      '    "calculated-data-identifier": "SAMPLE_B1_DIN_001",',
      '    "calculated-data-name": "DNA integrity number",',
      '    "calculated-result": {"value": 9.5, "unit": "(unitless)"},',
      '    "data-source-aggregate-document": {',
      '      "data-source-document": [{',
      '        "data-source-identifier": "SAMPLE_B1_MEASUREMENT",',
      '        "data-source-feature": "electrophoresis trace"',
      "      }]",
      "    }",
      "  }]",
      "}",
      "```",
      "",
      "**Common calculated fields by instrument type:**",
      "| Instrument | Calculated Fields |",
      "|------------|-------------------|",
      "| Cell counter | Viability %, cell density dilution-adjusted values |",
      "| Spectrophotometer | Concentration (from absorbance), 260/280 ratio |",
      "| Plate reader | Concentrations from standard curve, %CV |",
      "| Electrophoresis | DIN/RIN, region concentrations, average sizes |",
      "| qPCR | Relative quantities, fold change |",
      "",
      "See `references/field_classification_guide.md` for detailed guidance on raw vs. calculated classification.",
      "",
      "## Validation",
      "",
      "Always validate ASM output before delivering to the user:",
      "",
      "```bash",
      "python scripts/validate_asm.py output.json",
      "python scripts/validate_asm.py output.json --reference known_good.json  # Compare to reference",
      "python scripts/validate_asm.py output.json --strict  # Treat warnings as errors",
      "```",
      "",
      "**Validation Rules:**",
      "- Based on Allotrope ASM specification (December 2024)",
      "- Last updated: 2026-01-07",
      "- Source: https://gitlab.com/allotrope-public/asm",
      "",
      "**Soft Validation Approach:**",
      "Unknown techniques, units, or sample roles generate **warnings** (not errors) to allow for forward compatibility. If Allotrope adds new values after December 2024, the validator won't block them—it will flag them for manual verification. Use `--strict` mode to treat warnings as errors if you need stricter validation.",
      "",
      "**What it checks:**",
      "- Correct technique selection (e.g., multi-analyte profiling vs plate reader)",
      "- Field naming conventions (space-separated, not hyphenated)",
      "- Calculated data has traceability (`data-source-aggregate-document`)",
      "- Unique identifiers exist for measurements and calculated values",
      "- Required metadata present",
      "- Valid units and sample roles (with soft validation for unknown values)",
      "",
      "## Supported Instruments",
      "",
      "See `references/supported_instruments.md` for complete list. Key instruments:",
      "",
      "| Category | Instruments |",
      "|----------|-------------|",
      "| Cell Counting | Vi-CELL BLU, Vi-CELL XR, NucleoCounter |",
      "| Spectrophotometry | NanoDrop One/Eight/8000, Lunatic |",
      "| Plate Readers | SoftMax Pro, EnVision, Gen5, CLARIOstar |",
      "| ELISA | SoftMax Pro, BMG MARS, MSD Workbench |",
      "| qPCR | QuantStudio, Bio-Rad CFX |",
      "| Chromatography | Empower, Chromeleon |",
      "",
      "## Detection & Parsing Strategy",
      "",
      "### Tier 1: Native allotropy parsing (PREFERRED)",
      "**Always try allotropy first.** Check available vendors directly:",
      "",
      "```python",
      "from allotropy.parser_factory import Vendor",
      "",
      "# List all supported vendors",
      "for v in Vendor:",
      '    print(f"{v.name}")',
      "",
      "# Common vendors:",
      "# AGILENT_TAPESTATION_ANALYSIS  (for TapeStation XML)",
      "# BECKMAN_VI_CELL_BLU",
      "# THERMO_FISHER_NANODROP_EIGHT",
      "# MOLDEV_SOFTMAX_PRO",
      "# APPBIO_QUANTSTUDIO",
      "# ... many more",
      "```",
      "",
      "**When the user provides a file, check if allotropy supports it before falling back to manual parsing.** The `scripts/convert_to_asm.py` auto-detection only covers a subset of allotropy vendors.",
      "",
      "### Tier 2: Flexible fallback parsing",
      "**Only use if allotropy doesn't support the instrument.** This fallback:",
      "- Does NOT generate `calculated-data-aggregate-document`",
      "- Does NOT include full traceability",
      "- Produces simplified ASM structure",
      "",
      "Use flexible parser with:",
      "- Column name fuzzy matching",
      "- Unit extraction from headers",
      "- Metadata extraction from file structure",
      "",
      "### Tier 3: PDF extraction",
      "For PDF-only files, extract tables using pdfplumber, then apply Tier 2 parsing.",
      "",
      "## Pre-Parsing Checklist",
      "",
      "Before writing a custom parser, ALWAYS:",
      "",
      "1. **Check if allotropy supports it** - Use native parser if available",
      "2. **Find a reference ASM file** - Check `references/examples/` or ask user",
      "3. **Review instrument-specific guide** - Check `references/instrument_guides/`",
      "4. **Validate against reference** - Run `validate_asm.py --reference <file>`",
      "",
      "## Common Mistakes to Avoid",
      "",
      "| Mistake | Correct Approach |",
      "|---------|------------------|",
      "| Manifest as object | Use URL string |",
      '| Lowercase detection types | Use "Absorbance" not "absorbance" |',
      '| "emission wavelength setting" | Use "detector wavelength setting" for emission |',
      "| All measurements in one document | Group by well/sample location |",
      "| Missing procedure metadata | Extract ALL device settings per measurement |",
      "",
      "## Code Export for Data Engineers",
      "",
      "Generate standalone Python scripts that scientists can hand off:",
      "",
      "```python",
      "# Export parser code",
      'python scripts/export_parser.py --input "data.csv" --vendor "VI_CELL_BLU" --output "parser_script.py"',
      "```",
      "",
      "The exported script:",
      "- Has no external dependencies beyond pandas/allotropy",
      "- Includes inline documentation",
      "- Can run in Jupyter notebooks",
      "- Is production-ready for data pipelines",
      "",
      "## File Structure",
      "",
      "```",
      "instrument-data-to-allotrope/",
      "├── SKILL.md                          # This file",
      "├── scripts/",
      "│   ├── convert_to_asm.py            # Main conversion script",
      "│   ├── flatten_asm.py               # ASM → 2D CSV conversion",
      "│   ├── export_parser.py             # Generate standalone parser code",
      "│   └── validate_asm.py              # Validate ASM output quality",
      "└── references/",
      "    ├── supported_instruments.md     # Full instrument list with Vendor enums",
      "    ├── asm_schema_overview.md       # ASM structure reference",
      "    ├── field_classification_guide.md # Where to put different field types",
      "    └── flattening_guide.md          # How flattening works",
      "```",
      "",
      "## Usage Examples",
      "",
      "### Example 1: Vi-CELL BLU file",
      "```",
      'User: "Convert this cell counting data to Allotrope format"',
      "[uploads viCell_Results.xlsx]",
      "",
      "Claude:",
      "1. Detects Vi-CELL BLU (95% confidence)",
      "2. Converts using allotropy native parser",
      "3. Outputs:",
      "   - viCell_Results_asm.json (full ASM)",
      "   - viCell_Results_flat.csv (2D format)",
      "   - viCell_parser.py (exportable code)",
      "```",
      "",
      "### Example 2: Request for code handoff",
      "```",
      'User: "I need to give our data engineer code to parse NanoDrop files"',
      "",
      "Claude:",
      "1. Generates self-contained Python script",
      "2. Includes sample input/output",
      "3. Documents all assumptions",
      "4. Provides Jupyter notebook version",
      "```",
      "",
      "### Example 3: LIMS-ready flattened output",
      "```",
      'User: "Convert this ELISA data to a CSV I can upload to our LIMS"',
      "",
      "Claude:",
      "1. Parses plate reader data",
      "2. Generates flattened CSV with columns:",
      "   - sample_identifier, well_position, measurement_value, measurement_unit",
      "   - instrument_serial_number, analysis_datetime, assay_type",
      "3. Validates against common LIMS import requirements",
      "```",
      "",
      "## Implementation Notes",
      "",
      "### Installing allotropy",
      "```bash",
      "pip install allotropy --break-system-packages",
      "```",
      "",
      "### Handling parse failures",
      "If allotropy native parsing fails:",
      "1. Log the error for debugging",
      "2. Fall back to flexible parser",
      "3. Report reduced metadata completeness to user",
      "4. Suggest exporting different format from instrument",
      "",
      "### ASM Schema Validation",
      "Validate output against Allotrope schemas when available:",
      "```python",
      "import jsonschema",
      "# Schema URLs in references/asm_schema_overview.md",
      "```",
      "",
    ].join("\n"),
  },
  "bio-research-nextflow-development": {
    "references/geo-sra-acquisition.md": [
      "# GEO/SRA Data Acquisition",
      "",
      "Download raw sequencing data from NCBI GEO/SRA and prepare it for nf-core pipelines.",
      "",
      "**Use this when:** Reanalyzing published datasets, validating findings, or comparing results against public cohorts.",
      "",
      "## Table of Contents",
      "",
      "- [Workflow Overview](#workflow-overview)",
      "- [Step 1: Fetch Study Information](#step-1-fetch-study-information)",
      "- [Step 2: Review Sample Groups](#step-2-review-sample-groups)",
      "- [Step 3: Download FASTQ Files](#step-3-download-fastq-files)",
      "- [Step 4: Generate Samplesheet](#step-4-generate-samplesheet)",
      "- [Step 5: Run nf-core Pipeline](#step-5-run-nf-core-pipeline)",
      "- [Supported Pipelines](#supported-pipelines)",
      "- [Supported Organisms](#supported-organisms)",
      "- [Complete Example](#complete-example)",
      "- [Troubleshooting](#troubleshooting)",
      "",
      "---",
      "",
      "## Workflow Overview",
      "",
      'Example: "Find differentially expressed genes in GSE309891 (drug-treated vs control)"',
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                    GEO/SRA DATA ACQUISITION                     │",
      "└─────────────────────────────────────────────────────────────────┘",
      "                              │",
      "                              ▼",
      "                 ┌────────────────────────┐",
      "                 │   Fetch study info     │",
      "                 │   • Query NCBI/SRA     │",
      "                 │   • Get metadata       │",
      "                 │   • Detect organism    │",
      "                 │   • Identify data type │",
      "                 └────────────────────────┘",
      "                              │",
      "                              ▼",
      "                 ┌────────────────────────┐",
      "                 │   Present summary      │",
      "                 │   • Organism: Human    │",
      "                 │   • Genome: GRCh38     │",
      "                 │   • Type: RNA-Seq      │",
      "                 │   • Pipeline: rnaseq   │",
      "                 │   • Samples: 12        │",
      "                 │     (6 treated,        │",
      "                 │      6 control)        │",
      "                 │   • Size: ~24 GB       │",
      "                 └────────────────────────┘",
      "                              │",
      "                              ▼",
      "                    ┌─────────────────┐",
      "                    │  USER CONFIRMS  │◄──── Decision point",
      "                    │  genome/pipeline│",
      "                    └─────────────────┘",
      "                              │",
      "                              ▼",
      "                 ┌────────────────────────┐",
      "                 │   Select samples       │",
      "                 │   • Group by condition │",
      "                 │   • Show treated/ctrl  │",
      "                 └────────────────────────┘",
      "                              │",
      "                              ▼",
      "                    ┌─────────────────┐",
      "                    │  USER SELECTS   │◄──── Decision point",
      "                    │  sample subset  │",
      "                    └─────────────────┘",
      "                              │",
      "                              ▼",
      "                 ┌────────────────────────┐",
      "                 │   Download FASTQs      │",
      "                 │   • 24 files (R1+R2)   │",
      "                 │   • Parallel transfers │",
      "                 │   • Auto-resume        │",
      "                 └────────────────────────┘",
      "                              │",
      "                              ▼",
      "                 ┌────────────────────────┐",
      "                 │   Generate samplesheet │",
      "                 │   • Map SRR to files   │",
      "                 │   • Pair R1/R2         │",
      "                 │   • Assign conditions  │",
      "                 └────────────────────────┘",
      "                              │",
      "                              ▼",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                    NF-CORE PIPELINE EXECUTION                   │",
      "│              (Continue with Step 1 of main workflow)            │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## Instructions for Claude",
      "",
      "When assisting users with GEO/SRA data acquisition:",
      "",
      "1. **Always fetch study info first** to show the user what data is available",
      "2. **Ask for confirmation before downloading** - Present the sample groups and sizes, then ask which subset to download using AskUserQuestion",
      "3. **Suggest appropriate genome and pipeline** based on the organism and data type",
      "4. **Return to main SKILL.md workflow** after data preparation is complete",
      "",
      "Example confirmation question:",
      "```",
      'Question: "Which sample group would you like to download?"',
      "Options:",
      '  - "RNA-Seq:PAIRED (42 samples, ~87 GB)"',
      '  - "RNA-Seq:SINGLE (7 samples, ~4.5 GB)"',
      '  - "All samples (49 samples, ~92 GB)"',
      "```",
      "",
      "---",
      "",
      "## Step 1: Fetch Study Information",
      "",
      "Get metadata about a GEO study before downloading.",
      "",
      "```bash",
      "python scripts/sra_geo_fetch.py info <GEO_ID>",
      "```",
      "",
      "**Example:**",
      "```bash",
      "python scripts/sra_geo_fetch.py info GSE110004",
      "```",
      "",
      "**Output includes:**",
      "- Study title and summary",
      "- Organism (with auto-suggested genome)",
      "- Number of samples and runs",
      "- Data types (RNA-Seq, ATAC-seq, etc.)",
      "- Estimated download size",
      "- Suggested nf-core pipeline",
      "",
      "**Save info to JSON:**",
      "```bash",
      "python scripts/sra_geo_fetch.py info GSE110004 -o study_info.json",
      "```",
      "",
      "---",
      "",
      "## Step 2: Review Sample Groups",
      "",
      "View sample groups organized by data type and layout. This is useful for studies with mixed data types.",
      "",
      "```bash",
      "python scripts/sra_geo_fetch.py groups <GEO_ID>",
      "```",
      "",
      "**Example output:**",
      "```",
      "Sample Group          Count Layout     GSM Range                    Est. Size",
      "--------------------------------------------------------------------------------",
      "RNA-Seq                  42 PAIRED     GSM2879618...(42 samples)      87.4 GB",
      "RNA-Seq                   7 SINGLE     GSM2976181-GSM2976187           4.5 GB",
      "--------------------------------------------------------------------------------",
      "TOTAL                    49                                           91.9 GB",
      "",
      "Available groups for --subset option:",
      '  1. "RNA-Seq:PAIRED" - 42 samples (~87.4 GB)',
      '  2. "RNA-Seq:SINGLE" - 7 samples (~4.5 GB)',
      "```",
      "",
      "**List individual runs:**",
      "```bash",
      "python scripts/sra_geo_fetch.py list <GEO_ID>",
      "",
      "# Filter by data type",
      'python scripts/sra_geo_fetch.py list GSE110004 --filter "RNA-Seq:PAIRED"',
      "```",
      "",
      "**DECISION POINT:** Review the sample groups. Decide which subset to download if the study has multiple data types.",
      "",
      "---",
      "",
      "## Step 3: Download FASTQ Files",
      "",
      "Download FASTQ files from ENA (faster than SRA).",
      "",
      "```bash",
      "python scripts/sra_geo_fetch.py download <GEO_ID> -o <OUTPUT_DIR>",
      "```",
      "",
      "**Options:**",
      "- `-o, --output`: Output directory (required)",
      "- `-i, --interactive`: Interactively select sample group to download",
      '- `-s, --subset`: Filter by data type (e.g., "RNA-Seq:PAIRED")',
      "- `-p, --parallel`: Parallel downloads (default: 4)",
      "- `-t, --timeout`: Download timeout in seconds (default: 600)",
      "",
      "### Interactive Mode (Recommended)",
      "",
      "Use `-i` flag for interactive sample selection when the study has multiple data types:",
      "",
      "```bash",
      "python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i",
      "```",
      "",
      "**Interactive output:**",
      "```",
      "============================================================",
      "  SELECT SAMPLE GROUP TO DOWNLOAD",
      "============================================================",
      "",
      "  [1] RNA-Seq (paired)",
      "      Samples: 42",
      "      GSM: GSM2879618...(42 samples)",
      "      Size: ~87.4 GB",
      "",
      "  [2] RNA-Seq (single)",
      "      Samples: 7",
      "      GSM: GSM2976181-GSM2976187",
      "      Size: ~4.5 GB",
      "",
      "  [0] Download ALL (49 samples)",
      "------------------------------------------------------------",
      "",
      "Enter selection (0-2):",
      "```",
      "",
      "### Direct Subset Selection",
      "",
      "Alternatively, specify the subset directly:",
      "",
      "```bash",
      "# Download only RNA-Seq paired-end data",
      "python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq \\",
      '    --subset "RNA-Seq:PAIRED" --parallel 6',
      "```",
      "",
      "**Note:** Downloads automatically skip existing files. Resume interrupted downloads by re-running the command.",
      "",
      "---",
      "",
      "## Step 4: Generate Samplesheet",
      "",
      "Create a samplesheet compatible with nf-core pipelines.",
      "",
      "```bash",
      "python scripts/sra_geo_fetch.py samplesheet <GEO_ID> \\",
      "    --fastq-dir <FASTQ_DIR> \\",
      "    -o samplesheet.csv",
      "```",
      "",
      "**Options:**",
      "- `-f, --fastq-dir`: Directory containing downloaded FASTQ files (required)",
      "- `-o, --output`: Output samplesheet path (default: samplesheet.csv)",
      "- `-p, --pipeline`: Target pipeline (auto-detected if not specified)",
      "",
      "**Example:**",
      "```bash",
      "python scripts/sra_geo_fetch.py samplesheet GSE110004 \\",
      "    --fastq-dir ./fastq \\",
      "    -o samplesheet.csv",
      "```",
      "",
      "**Output:** The script will:",
      "1. Create samplesheet in the format required by the target pipeline",
      "2. Display suggested genome reference",
      "3. Show suggested nf-core command",
      "",
      "---",
      "",
      "## Step 5: Run nf-core Pipeline",
      "",
      "After generating the samplesheet, the script provides a suggested command.",
      "",
      "**Example output:**",
      "```",
      "Suggested command:",
      "   nextflow run nf-core/rnaseq \\",
      "       --input samplesheet.csv \\",
      "       --outdir results \\",
      "       --genome R64-1-1 \\",
      "       -profile docker",
      "```",
      "",
      "**DECISION POINT:** Review and confirm:",
      "1. Is the suggested pipeline correct?",
      "2. Is the genome reference correct for your organism?",
      "3. Do you need additional pipeline options?",
      "",
      "Then return to the main SKILL.md workflow (Step 1: Environment Check) to proceed with pipeline execution.",
      "",
      "---",
      "",
      "## Supported Pipelines",
      "",
      "The skill auto-detects appropriate pipelines based on library strategy. Pipelines marked with ★ are fully supported with configs, samplesheet generation, and documentation. Others are suggested but require manual setup following nf-core documentation.",
      "",
      "| Library Strategy | Suggested Pipeline | Support |",
      "|------------------|--------------------|---------|",
      "| RNA-Seq          | nf-core/rnaseq     | ★ Full  |",
      "| ATAC-seq         | nf-core/atacseq    | ★ Full  |",
      "| WGS/WXS          | nf-core/sarek      | ★ Full  |",
      "| ChIP-seq         | nf-core/chipseq    | Manual  |",
      "| Bisulfite-Seq    | nf-core/methylseq  | Manual  |",
      "| miRNA-Seq        | nf-core/smrnaseq   | Manual  |",
      "| Amplicon         | nf-core/ampliseq   | Manual  |",
      "",
      "---",
      "",
      "## Supported Organisms",
      "",
      "Common organisms with auto-suggested genomes:",
      "",
      "| Organism | Genome | Notes |",
      "|----------|--------|-------|",
      "| Homo sapiens | GRCh38 | Human reference |",
      "| Mus musculus | GRCm39 | Mouse reference |",
      "| Saccharomyces cerevisiae | R64-1-1 | Yeast S288C |",
      "| Drosophila melanogaster | BDGP6 | Fruit fly |",
      "| Caenorhabditis elegans | WBcel235 | C. elegans |",
      "| Danio rerio | GRCz11 | Zebrafish |",
      "| Arabidopsis thaliana | TAIR10 | Arabidopsis |",
      "| Rattus norvegicus | Rnor_6.0 | Rat |",
      "",
      "See `scripts/config/genomes.yaml` for the full list.",
      "",
      "---",
      "",
      "## Complete Example",
      "",
      "Reanalyze GSE110004 (yeast RNA-seq):",
      "",
      "```bash",
      "# 1. Get study info and sample groups",
      "python scripts/sra_geo_fetch.py info GSE110004",
      "",
      "# 2. Download with interactive selection",
      "python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i",
      "# Select option [1] for RNA-Seq paired-end samples",
      "",
      "# 3. Generate samplesheet",
      "python scripts/sra_geo_fetch.py samplesheet GSE110004 \\",
      "    --fastq-dir ./fastq \\",
      "    -o samplesheet.csv",
      "",
      "# 4. Run nf-core/rnaseq (continue with main SKILL.md workflow)",
      "nextflow run nf-core/rnaseq \\",
      "    --input samplesheet.csv \\",
      "    --outdir results \\",
      "    --genome R64-1-1 \\",
      "    -profile docker",
      "```",
      "",
      "### Alternative: Non-interactive Download",
      "",
      "```bash",
      "# Review sample groups first",
      "python scripts/sra_geo_fetch.py groups GSE110004",
      "",
      "# Download specific subset directly",
      "python scripts/sra_geo_fetch.py download GSE110004 \\",
      '    --subset "RNA-Seq:PAIRED" \\',
      "    -o ./fastq \\",
      "    --parallel 4",
      "```",
      "",
      "---",
      "",
      "## Troubleshooting",
      "",
      "### ENA Download Fails",
      "If ENA downloads fail, the data may need to be fetched directly from SRA:",
      "",
      "```bash",
      "# Create SRA tools environment",
      "conda create -n sra_tools -c bioconda sra-tools",
      "",
      "# Download with prefetch + fasterq-dump",
      "conda run -n sra_tools prefetch SRR6357070",
      "conda run -n sra_tools fasterq-dump SRR6357070 -O ./fastq",
      "```",
      "",
      "### No SRA Runs Found",
      "Some GEO datasets only have processed data, not raw sequencing reads. Check:",
      "```bash",
      "python scripts/sra_geo_fetch.py info <GEO_ID>",
      "```",
      'If "Runs: 0", the dataset may not have raw data in SRA.',
      "",
      "### SuperSeries Support",
      "GEO SuperSeries (which contain multiple SubSeries) are automatically handled. The tool will:",
      "1. Detect that a GEO ID is a SuperSeries",
      "2. Find the linked BioProject accession",
      "3. Fetch all SRA runs from the BioProject",
      "",
      "Example: GSE110004 is a SuperSeries that links to BioProject PRJNA432544.",
      "",
      "### Genome Not Recognized",
      "If the organism is not in the genome mapping, manually specify the genome:",
      "```bash",
      "# Check available iGenomes",
      "python scripts/manage_genomes.py list",
      "",
      "# Or provide custom reference files to nf-core",
      "nextflow run nf-core/rnaseq --fasta /path/to/genome.fa --gtf /path/to/genes.gtf",
      "```",
      "",
      "---",
      "",
      "## Requirements",
      "",
      "- Python 3.8+",
      "- `requests` library (optional but recommended)",
      "- `pyyaml` library (optional, for genome config)",
      "- Network access to NCBI and ENA",
      "",
      "Install optional dependencies:",
      "```bash",
      "pip install requests pyyaml",
      "```",
      "",
    ].join("\n"),
    "references/installation.md": [
      "# Installation",
      "",
      "## Contents",
      "- [Quick install](#quick-install)",
      "- [Docker setup](#docker-setup)",
      "- [Singularity setup (HPC)](#singularity-setup-hpc)",
      "- [nf-core tools (optional)](#nf-core-tools-optional)",
      "- [Verify installation](#verify-installation)",
      "- [Common issues](#common-issues)",
      "",
      "## Quick install",
      "",
      "```bash",
      "# Nextflow",
      "curl -s https://get.nextflow.io | bash",
      "mv nextflow ~/bin/",
      'export PATH="$HOME/bin:$PATH"',
      "",
      "# Verify",
      "nextflow -version",
      "java -version  # Requires 11+",
      "```",
      "",
      "## Docker setup",
      "",
      "### Linux",
      "```bash",
      "sudo apt-get update && sudo apt-get install docker.io",
      "sudo systemctl enable --now docker",
      "sudo usermod -aG docker $USER",
      "# Log out and back in",
      "```",
      "",
      "### macOS",
      "Download Docker Desktop: https://docker.com/products/docker-desktop",
      "",
      "### Verify",
      "```bash",
      "docker run hello-world",
      "```",
      "",
      "## Singularity setup (HPC)",
      "",
      "```bash",
      "# Ubuntu/Debian",
      "sudo apt-get install singularity-container",
      "",
      "# Or via conda",
      "conda install -c conda-forge singularity",
      "```",
      "",
      "### Configure cache",
      "```bash",
      'export NXF_SINGULARITY_CACHEDIR="$HOME/.singularity/cache"',
      "mkdir -p $NXF_SINGULARITY_CACHEDIR",
      "echo 'export NXF_SINGULARITY_CACHEDIR=\"$HOME/.singularity/cache\"' >> ~/.bashrc",
      "```",
      "",
      "## nf-core tools (optional)",
      "",
      "```bash",
      "pip install nf-core",
      "```",
      "",
      "Useful commands:",
      "```bash",
      "nf-core list                    # Available pipelines",
      "nf-core launch rnaseq           # Interactive parameter selection",
      "nf-core download rnaseq -r 3.14.0  # Download for offline use",
      "```",
      "",
      "## Verify installation",
      "",
      "```bash",
      "nextflow run nf-core/demo -profile test,docker --outdir test_demo",
      "ls test_demo/",
      "```",
      "",
      "## Common issues",
      "",
      "**Java version wrong:**",
      "```bash",
      "export JAVA_HOME=/path/to/java11",
      "```",
      "",
      "**Docker permission denied:**",
      "```bash",
      "sudo usermod -aG docker $USER",
      "# Log out and back in",
      "```",
      "",
      "**Nextflow not found:**",
      "```bash",
      "echo 'export PATH=\"$HOME/bin:$PATH\"' >> ~/.bashrc",
      "source ~/.bashrc",
      "```",
      "",
    ].join("\n"),
    "references/pipelines/atacseq.md": [
      "# nf-core/atacseq",
      "",
      "**Version:** 2.1.2",
      "",
      "**Official Documentation:** https://nf-co.re/atacseq/2.1.2/",
      "**GitHub:** https://github.com/nf-core/atacseq",
      "",
      "> **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/atacseq/releases) for breaking changes and update the version in commands below.",
      "",
      "## Contents",
      "- [Test command](#test-command)",
      "- [Samplesheet format](#samplesheet-format)",
      "- [Parameters](#parameters)",
      "- [Output files](#output-files)",
      "- [Quality metrics](#quality-metrics)",
      "",
      "## Test command",
      "",
      "```bash",
      "nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq",
      "```",
      "",
      "Expected: ~15 min, creates peaks and BigWig tracks.",
      "",
      "## Samplesheet format",
      "",
      "```csv",
      "sample,fastq_1,fastq_2,replicate",
      "CONTROL,/path/to/ctrl_rep1_R1.fq.gz,/path/to/ctrl_rep1_R2.fq.gz,1",
      "CONTROL,/path/to/ctrl_rep2_R1.fq.gz,/path/to/ctrl_rep2_R2.fq.gz,2",
      "TREATMENT,/path/to/treat_rep1_R1.fq.gz,/path/to/treat_rep1_R2.fq.gz,1",
      "TREATMENT,/path/to/treat_rep2_R1.fq.gz,/path/to/treat_rep2_R2.fq.gz,2",
      "```",
      "",
      "| Column | Required | Description |",
      "|--------|----------|-------------|",
      "| sample | Yes | Condition/group identifier |",
      "| fastq_1 | Yes | Absolute path to R1 |",
      "| fastq_2 | Yes | Absolute path to R2 (paired-end required) |",
      "| replicate | Yes | Replicate number (integer) |",
      "",
      "### Design file for differential analysis",
      "```csv",
      "sample,condition",
      "CONTROL,control",
      "TREATMENT,treatment",
      "```",
      "",
      "Use with `--deseq2_design design.csv`.",
      "",
      "## Parameters",
      "",
      "### Minimal run",
      "```bash",
      "nextflow run nf-core/atacseq -r 2.1.2 -profile docker \\",
      "    --input samplesheet.csv --outdir results --genome GRCh38 --read_length 50",
      "```",
      "",
      "### Common parameters",
      "",
      "| Parameter | Default | Description |",
      "|-----------|---------|-------------|",
      "| `--genome` | - | `GRCh38`, `GRCh37`, `mm10` |",
      "| `--read_length` | 50 | Read length for MACS2 optimization |",
      "| `--narrow_peak` | true | Narrow peaks (false for broad) |",
      "| `--mito_name` | chrM | Mitochondrial chromosome name |",
      "| `--keep_mito` | false | Keep mitochondrial reads |",
      "| `--min_reps_consensus` | 1 | Min replicates for consensus peaks |",
      "",
      "### Differential accessibility",
      "```bash",
      "--deseq2_design design.csv",
      "```",
      "",
      "## Output files",
      "",
      "```",
      "results/",
      "├── bwa/mergedLibrary/",
      "│   ├── *.mLb.mkD.sorted.bam     # Filtered, deduplicated alignments",
      "│   └── bigwig/",
      "│       └── *.bigWig             # Coverage tracks",
      "├── macs2/narrowPeak/",
      "│   ├── *.narrowPeak             # Peak calls",
      "│   └── consensus/",
      "│       └── consensus_peaks.bed  # Merged peaks across replicates",
      "├── deeptools/",
      "│   ├── plotFingerprint/         # Library complexity",
      "│   └── plotProfile/             # TSS enrichment",
      "├── deseq2/                      # If --deseq2_design provided",
      "└── multiqc/",
      "```",
      "",
      "**Key outputs:**",
      "- `*.mLb.mkD.sorted.bam`: Analysis-ready alignments",
      "- `*.narrowPeak`: MACS2 peak calls (BED format)",
      "- `consensus_peaks.bed`: Consensus peaks across replicates",
      "- `*.bigWig`: Genome browser tracks",
      "",
      "## Quality metrics",
      "",
      "| Metric | Good | Acceptable | Poor |",
      "|--------|------|------------|------|",
      "| Mapped reads | >80% | 60-80% | <60% |",
      "| Mitochondrial | <20% | 20-40% | >40% |",
      "| Duplicates | <30% | 30-50% | >50% |",
      "| FRiP | >30% | 15-30% | <15% |",
      "| TSS enrichment | >6 | 4-6 | <4 |",
      "",
      "**Fragment size**: Should show nucleosomal periodicity (~50bp nucleosome-free, ~200bp mono-nucleosome).",
      "",
      "## Downstream analysis",
      "",
      "```r",
      "library(ChIPseeker)",
      "library(GenomicRanges)",
      'peaks <- import("consensus_peaks.bed")',
      "peakAnno <- annotatePeak(peaks, TxDb = TxDb.Hsapiens.UCSC.hg38.knownGene)",
      "```",
      "",
      "**Motif analysis:**",
      "```bash",
      "findMotifsGenome.pl consensus_peaks.bed hg38 motifs/ -size 200",
      "```",
      "",
      "## Troubleshooting",
      "",
      "**Low FRiP**: Check library complexity in `plotFingerprint/`. May indicate over-transposition.",
      "",
      "**Few peaks**: Lower threshold with `--macs_qvalue 0.1` or use `--narrow_peak false` for broader peaks.",
      "",
      "**High duplicates**: Normal for low-input; pipeline removes by default.",
      "",
      "## More Information",
      "",
      "- **Full parameter list:** https://nf-co.re/atacseq/2.1.2/parameters/",
      "- **Output documentation:** https://nf-co.re/atacseq/2.1.2/docs/output/",
      "- **Usage documentation:** https://nf-co.re/atacseq/2.1.2/docs/usage/",
      "",
    ].join("\n"),
    "references/pipelines/rnaseq.md": [
      "# nf-core/rnaseq",
      "",
      "**Version:** 3.22.2",
      "",
      "**Official Documentation:** https://nf-co.re/rnaseq/3.22.2/",
      "**GitHub:** https://github.com/nf-core/rnaseq",
      "",
      "> **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/rnaseq/releases) for breaking changes and update the version in commands below.",
      "",
      "## Contents",
      "- [Test command](#test-command)",
      "- [Samplesheet format](#samplesheet-format)",
      "- [Parameters](#parameters)",
      "- [Output files](#output-files)",
      "- [Downstream analysis](#downstream-analysis)",
      "",
      "## Test command",
      "",
      "```bash",
      "nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq",
      "```",
      "",
      "Expected: ~15 min, creates `multiqc/multiqc_report.html`.",
      "",
      "## Samplesheet format",
      "",
      "```csv",
      "sample,fastq_1,fastq_2,strandedness",
      "CONTROL_REP1,/path/to/ctrl1_R1.fq.gz,/path/to/ctrl1_R2.fq.gz,auto",
      "CONTROL_REP2,/path/to/ctrl2_R1.fq.gz,/path/to/ctrl2_R2.fq.gz,auto",
      "TREATMENT_REP1,/path/to/treat1_R1.fq.gz,/path/to/treat1_R2.fq.gz,auto",
      "```",
      "",
      "| Column | Required | Values |",
      "|--------|----------|--------|",
      "| sample | Yes | Alphanumeric, underscores allowed |",
      "| fastq_1 | Yes | Absolute path to R1 |",
      "| fastq_2 | No | Absolute path to R2 (empty for single-end) |",
      "| strandedness | Yes | `auto`, `forward`, `reverse`, `unstranded` |",
      "",
      "**Strandedness guide:**",
      "- `auto`: Inferred from data (recommended)",
      "- `forward`: TruSeq Stranded, dUTP protocols",
      "- `reverse`: Ligation-based protocols",
      "- `unstranded`: Non-stranded protocols",
      "",
      "## Parameters",
      "",
      "### Minimal run",
      "```bash",
      "nextflow run nf-core/rnaseq -r 3.22.2 -profile docker \\",
      "    --input samplesheet.csv --outdir results --genome GRCh38",
      "```",
      "",
      "### Common parameters",
      "",
      "| Parameter | Default | Description |",
      "|-----------|---------|-------------|",
      "| `--aligner` | `star_salmon` | Options: `star_salmon`, `star_rsem`, `hisat2` |",
      "| `--genome` | - | `GRCh38`, `GRCh37`, `mm10`, `BDGP6` |",
      "| `--pseudo_aligner` | - | Set to `salmon` for pseudo-alignment only |",
      "| `--skip_trimming` | false | Skip adapter trimming |",
      "| `--skip_alignment` | false | Pseudo-alignment only |",
      "",
      "### Custom reference",
      "```bash",
      "--fasta /path/to/genome.fa \\",
      "--gtf /path/to/annotation.gtf \\",
      "--star_index /path/to/star/  # Optional, builds if absent",
      "```",
      "",
      "## Output files",
      "",
      "```",
      "results/",
      "├── star_salmon/",
      "│   ├── salmon.merged.gene_counts.tsv    # Raw counts for DESeq2",
      "│   ├── salmon.merged.gene_tpm.tsv       # TPM values",
      "│   └── *.bam                            # Alignments",
      "├── multiqc/",
      "│   └── multiqc_report.html              # QC summary",
      "└── pipeline_info/",
      "```",
      "",
      "**Key outputs:**",
      "- `salmon.merged.gene_counts.tsv`: Input for DESeq2/edgeR",
      "- `salmon.merged.gene_tpm.tsv`: Normalized expression",
      "",
      "## Downstream analysis",
      "",
      "```r",
      "library(DESeq2)",
      'counts <- read.delim("salmon.merged.gene_counts.tsv", row.names=1)',
      "coldata <- data.frame(",
      '    condition = factor(c("control", "control", "treatment", "treatment"))',
      ")",
      "dds <- DESeqDataSetFromMatrix(",
      "    countData = round(counts),",
      "    colData = coldata,",
      "    design = ~ condition",
      ")",
      "dds <- DESeq(dds)",
      'res <- results(dds, contrast = c("condition", "treatment", "control"))',
      "```",
      "",
      "## Troubleshooting",
      "",
      "**STAR index fails**: Increase memory with `--max_memory '64.GB'` or provide pre-built `--star_index`.",
      "",
      "**Low alignment rate**: Verify genome matches species; check FastQC for adapter contamination.",
      "",
      "**Strandedness detection fails**: Specify explicitly with `--strandedness reverse`.",
      "",
      "## More Information",
      "",
      "- **Full parameter list:** https://nf-co.re/rnaseq/3.22.2/parameters/",
      "- **Output documentation:** https://nf-co.re/rnaseq/3.22.2/docs/output/",
      "- **Usage documentation:** https://nf-co.re/rnaseq/3.22.2/docs/usage/",
      "",
    ].join("\n"),
    "references/pipelines/sarek.md": [
      "# nf-core/sarek",
      "",
      "**Version:** 3.7.1",
      "",
      "**Official Documentation:** https://nf-co.re/sarek/3.7.1/",
      "**GitHub:** https://github.com/nf-core/sarek",
      "",
      "> **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/sarek/releases) for breaking changes and update the version in commands below.",
      "",
      "## Contents",
      "- [Test command](#test-command)",
      "- [Samplesheet format](#samplesheet-format)",
      "- [Variant calling modes](#variant-calling-modes)",
      "- [Parameters](#parameters)",
      "- [Output files](#output-files)",
      "",
      "## Test command",
      "",
      "```bash",
      "nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek",
      "```",
      "",
      "Expected: ~20 min, creates aligned BAMs and variant calls.",
      "",
      "## Samplesheet format",
      "",
      "### From FASTQ",
      "```csv",
      "patient,sample,lane,fastq_1,fastq_2",
      "patient1,tumor,L001,/path/to/tumor_L001_R1.fq.gz,/path/to/tumor_L001_R2.fq.gz",
      "patient1,tumor,L002,/path/to/tumor_L002_R1.fq.gz,/path/to/tumor_L002_R2.fq.gz",
      "patient1,normal,L001,/path/to/normal_R1.fq.gz,/path/to/normal_R2.fq.gz",
      "```",
      "",
      "### From BAM/CRAM",
      "```csv",
      "patient,sample,bam,bai",
      "patient1,tumor,/path/to/tumor.bam,/path/to/tumor.bam.bai",
      "patient1,normal,/path/to/normal.bam,/path/to/normal.bam.bai",
      "```",
      "",
      "### With tumor/normal status",
      "```csv",
      "patient,sample,lane,fastq_1,fastq_2,status",
      "patient1,tumor,L001,tumor_R1.fq.gz,tumor_R2.fq.gz,1",
      "patient1,normal,L001,normal_R1.fq.gz,normal_R2.fq.gz,0",
      "```",
      "",
      "`status`: 0 = normal, 1 = tumor",
      "",
      "## Variant calling modes",
      "",
      "### Germline (single sample)",
      "```bash",
      "nextflow run nf-core/sarek -r 3.7.1 -profile docker \\",
      "    --input samplesheet.csv --outdir results --genome GRCh38 \\",
      "    --tools haplotypecaller,snpeff",
      "```",
      "",
      "### Somatic (tumor-normal pair)",
      "```bash",
      "nextflow run nf-core/sarek -r 3.7.1 -profile docker \\",
      "    --input samplesheet.csv --outdir results --genome GRCh38 \\",
      "    --tools mutect2,strelka,snpeff",
      "```",
      "",
      "### WES (exome)",
      "```bash",
      "nextflow run nf-core/sarek -r 3.7.1 -profile docker \\",
      "    --input samplesheet.csv --outdir results --genome GRCh38 \\",
      "    --wes --intervals /path/to/targets.bed \\",
      "    --tools haplotypecaller,snpeff",
      "```",
      "",
      "### Joint germline (cohort)",
      "```bash",
      "--tools haplotypecaller --joint_germline",
      "```",
      "",
      "## Parameters",
      "",
      "### Available tools",
      "",
      "**Germline callers:**",
      "- `haplotypecaller`: GATK HaplotypeCaller",
      "- `freebayes`: FreeBayes",
      "- `deepvariant`: DeepVariant (GPU optional)",
      "- `strelka`: Strelka2 germline",
      "",
      "**Somatic callers:**",
      "- `mutect2`: GATK Mutect2",
      "- `strelka`: Strelka2 somatic",
      "- `manta`: Structural variants",
      "",
      "**CNV callers:**",
      "- `ascat`: Copy number",
      "- `controlfreec`: CNV detection",
      "- `tiddit`: SV calling",
      "",
      "**Annotation:**",
      "- `snpeff`: Functional annotation",
      "- `vep`: Variant Effect Predictor",
      "",
      "### Key parameters",
      "",
      "| Parameter | Default | Description |",
      "|-----------|---------|-------------|",
      "| `--tools` | - | Comma-separated list of tools |",
      "| `--genome` | - | `GRCh38`, `GRCh37` |",
      "| `--wes` | false | Exome mode (requires `--intervals`) |",
      "| `--intervals` | - | BED file for targeted regions |",
      "| `--joint_germline` | false | Joint calling for cohorts |",
      "| `--skip_bqsr` | false | Skip base quality recalibration |",
      "",
      "## Output files",
      "",
      "```",
      "results/",
      "├── preprocessing/",
      "│   └── recalibrated/           # Analysis-ready BAMs",
      "│       └── *.recal.bam",
      "├── variant_calling/",
      "│   ├── haplotypecaller/        # Germline VCFs",
      "│   ├── mutect2/                # Somatic VCFs (filtered)",
      "│   └── strelka/",
      "├── annotation/",
      "│   └── snpeff/                 # Annotated VCFs",
      "└── multiqc/",
      "```",
      "",
      "## Troubleshooting",
      "",
      "**BQSR fails**: Check known sites available for genome. Skip with `--skip_bqsr` for non-standard references.",
      "",
      "**Mutect2 no variants**: Verify tumor/normal pairing in samplesheet (check `status` column).",
      "",
      "**Out of memory**: `--max_memory '128.GB'` for WGS.",
      "",
      "**DeepVariant GPU**: Ensure NVIDIA Docker runtime configured.",
      "",
      "## More Information",
      "",
      "- **Full parameter list:** https://nf-co.re/sarek/3.7.1/parameters/",
      "- **Output documentation:** https://nf-co.re/sarek/3.7.1/docs/output/",
      "- **Usage documentation:** https://nf-co.re/sarek/3.7.1/docs/usage/",
      "",
    ].join("\n"),
    "references/troubleshooting.md": [
      "# Troubleshooting",
      "",
      "Quick fixes for common nf-core pipeline issues.",
      "",
      "## Contents",
      "- [Exit Codes](#exit-codes)",
      "- [HPC/Singularity Issues](#hpcsingularity-issues)",
      "- [Pipeline Failures](#pipeline-failures)",
      "- [RNA-seq Specific](#rna-seq-specific)",
      "- [Sarek Specific](#sarek-specific)",
      "- [ATAC-seq Specific](#atac-seq-specific)",
      "- [Resource Management](#resource-management)",
      "- [Getting Help](#getting-help)",
      "",
      "## Exit Codes",
      "",
      "Common exit codes indicating resource issues (per [nf-core docs](https://nf-co.re/docs/usage/troubleshooting/crash_halfway)):",
      "",
      "| Code | Cause | Fix |",
      "|------|-------|-----|",
      "| 137 | Out of memory | `--max_memory '32.GB'` or `'64.GB'` for WGS |",
      "| 143 | Out of memory | `--max_memory '32.GB'` or `'64.GB'` for WGS |",
      "| 104, 134, 139, 247 | Out of memory | Increase `--max_memory` |",
      "| 1 | General error | Check `.nextflow.log` for details |",
      "",
      "Most pipelines auto-retry with 2x then 3x resources before failing.",
      "",
      "## HPC/Singularity Issues",
      "",
      "### Singularity cache issues",
      "```bash",
      'export NXF_SINGULARITY_CACHEDIR="$HOME/.singularity/cache"',
      "mkdir -p $NXF_SINGULARITY_CACHEDIR",
      "```",
      "",
      "### Using Singularity instead of Docker",
      "On HPC systems without Docker, use Singularity:",
      "```bash",
      "nextflow run nf-core/<pipeline> -profile singularity ...",
      "```",
      "",
      "> **Note**: For basic environment setup (Docker, Nextflow, Java installation), see the inline instructions in Step 1 of SKILL.md.",
      "",
      "## Pipeline Failures",
      "",
      "### Container pull failed",
      "- Check network connectivity",
      "- Try: `-profile singularity` instead of docker",
      "- For offline: `nf-core download <pipeline> -r <version>`",
      "",
      '### "No such file" errors',
      "- Use **absolute paths** in samplesheet",
      "- Verify files exist: `ls /path/to/file`",
      "",
      "### Resume not working",
      "```bash",
      "# Check work directory exists",
      "ls -la work/",
      "",
      "# Force clean restart (loses cache)",
      "rm -rf work/ .nextflow*",
      "nextflow run nf-core/<pipeline> ...",
      "```",
      "",
      "## RNA-seq Specific",
      "",
      "### STAR index fails",
      "- Increase memory: `--max_memory '64.GB'`",
      "- Or provide pre-built: `--star_index /path/to/star/`",
      "",
      "### Low alignment rate",
      "- Verify genome matches species",
      "- Check FastQC for adapter contamination",
      "- Try different aligner: `--aligner hisat2`",
      "",
      "### Strandedness detection fails",
      "- Specify explicitly: `--strandedness reverse`",
      "- Common values: `forward`, `reverse`, `unstranded`",
      "",
      "## Sarek Specific",
      "",
      "### BQSR fails",
      "- Check known sites for genome",
      "- Skip for non-standard references: `--skip_bqsr`",
      "",
      "### Mutect2 no variants",
      "- Verify tumor/normal pairing",
      "- Check samplesheet `status` column: 0=normal, 1=tumor",
      "",
      "### Out of memory for WGS",
      "```bash",
      "--max_memory '128.GB' --max_cpus 16",
      "```",
      "",
      "### DeepVariant GPU issues",
      "- Ensure NVIDIA Docker runtime configured",
      "- Or use CPU mode (slower)",
      "",
      "## ATAC-seq Specific",
      "",
      "### Low FRiP score",
      "- Check library complexity in `plotFingerprint/`",
      "- May indicate over-transposition",
      "",
      "### Few peaks called",
      "- Lower threshold: `--macs_qvalue 0.1`",
      "- Use broad peaks: `--narrow_peak false`",
      "",
      "### High duplicates",
      "- Normal for low-input samples",
      "- Pipeline removes by default",
      "- Consider deeper sequencing",
      "",
      "## Resource Management",
      "",
      "### Set resource limits",
      "```bash",
      "--max_cpus 8 --max_memory '32.GB' --max_time '24.h'",
      "```",
      "",
      "### Check available resources",
      "```bash",
      "# CPUs",
      "nproc",
      "",
      "# Memory",
      "free -h",
      "",
      "# Disk",
      "df -h .",
      "```",
      "",
      "## Getting Help",
      "",
      "1. Check `.nextflow.log` for error details",
      "2. Search nf-core Slack: https://nf-co.re/join",
      "3. Open issue on GitHub: https://github.com/nf-core/<pipeline>/issues",
      "",
    ].join("\n"),
    "scripts/check_environment.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Pre-flight environment validation for nf-core pipelines.",
      "",
      "Checks Docker, Nextflow, Java, system resources, and network connectivity.",
      "Run this BEFORE attempting any pipeline execution.",
      "",
      "Usage:",
      "    python check_environment.py",
      "    python check_environment.py --json",
      '"""',
      "",
      "import json",
      "import os",
      "import shutil",
      "import subprocess",
      "import sys",
      "from dataclasses import dataclass, field, asdict",
      "from typing import List, Optional",
      "",
      "",
      "@dataclass",
      "class CheckResult:",
      '    """Result of a single environment check."""',
      "    name: str",
      "    passed: bool",
      "    message: str",
      "    details: Optional[str] = None",
      "    fix: Optional[str] = None",
      "",
      "",
      "@dataclass",
      "class EnvironmentReport:",
      '    """Complete environment validation report."""',
      "    ready: bool",
      "    checks: List[CheckResult] = field(default_factory=list)",
      "    recommendations: List[str] = field(default_factory=list)",
      "",
      "    def to_dict(self):",
      "        return {",
      '            "ready": self.ready,',
      '            "checks": [asdict(c) for c in self.checks],',
      '            "recommendations": self.recommendations',
      "        }",
      "",
      "",
      "def check_docker() -> CheckResult:",
      '    """Check Docker availability, daemon status, and permissions."""',
      '    if not shutil.which("docker"):',
      "        return CheckResult(",
      '            name="Docker",',
      "            passed=False,",
      '            message="Docker not found in PATH",',
      '            fix="Install Docker: https://docs.docker.com/get-docker/"',
      "        )",
      "",
      "    try:",
      "        result = subprocess.run(",
      '            ["docker", "info"],',
      "            capture_output=True,",
      "            text=True,",
      "            timeout=15",
      "        )",
      "",
      "        if result.returncode != 0:",
      "            stderr_lower = result.stderr.lower()",
      '            if "permission denied" in stderr_lower:',
      "                return CheckResult(",
      '                    name="Docker",',
      "                    passed=False,",
      '                    message="Docker permission denied",',
      '                    details="Cannot connect to Docker daemon",',
      '                    fix="sudo usermod -aG docker $USER && newgrp docker"',
      "                )",
      '            elif "cannot connect" in stderr_lower or "is the docker daemon running" in stderr_lower:',
      "                return CheckResult(",
      '                    name="Docker",',
      "                    passed=False,",
      '                    message="Docker daemon not running",',
      "                    details=result.stderr[:200] if result.stderr else None,",
      '                    fix="sudo systemctl start docker"',
      "                )",
      "            else:",
      "                return CheckResult(",
      '                    name="Docker",',
      "                    passed=False,",
      '                    message="Docker error",',
      "                    details=result.stderr[:200] if result.stderr else None,",
      '                    fix="Check Docker installation and daemon status"',
      "                )",
      "",
      "        return CheckResult(",
      '            name="Docker",',
      "            passed=True,",
      '            message="Docker is available and running"',
      "        )",
      "",
      "    except subprocess.TimeoutExpired:",
      "        return CheckResult(",
      '            name="Docker",',
      "            passed=False,",
      '            message="Docker command timed out",',
      '            fix="Check Docker daemon status: sudo systemctl status docker"',
      "        )",
      "    except Exception as e:",
      "        return CheckResult(",
      '            name="Docker",',
      "            passed=False,",
      '            message=f"Docker check failed: {str(e)}"',
      "        )",
      "",
      "",
      "def check_nextflow() -> CheckResult:",
      '    """Check Nextflow installation and version (requires >= 23.04)."""',
      '    if not shutil.which("nextflow"):',
      "        return CheckResult(",
      '            name="Nextflow",',
      "            passed=False,",
      '            message="Nextflow not found in PATH",',
      '            fix="curl -s https://get.nextflow.io | bash && mv nextflow ~/bin/ && export PATH=$HOME/bin:$PATH"',
      "        )",
      "",
      "    try:",
      "        result = subprocess.run(",
      '            ["nextflow", "-version"],',
      "            capture_output=True,",
      "            text=True,",
      "            timeout=30",
      "        )",
      "",
      "        output = result.stdout + result.stderr",
      "        version_line = output.strip().split('\\n')[0] if output else \"\"",
      "",
      "        import re",
      "        match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', version_line)",
      "",
      "        if match:",
      "            major, minor, patch = int(match.group(1)), int(match.group(2)), int(match.group(3))",
      '            version_str = f"{major}.{minor}.{patch}"',
      "",
      "            # Require version >= 23.04",
      "            if major > 23 or (major == 23 and minor >= 4):",
      "                return CheckResult(",
      '                    name="Nextflow",',
      "                    passed=True,",
      '                    message=f"Nextflow {version_str} installed",',
      "                    details=version_line",
      "                )",
      "            else:",
      "                return CheckResult(",
      '                    name="Nextflow",',
      "                    passed=False,",
      '                    message=f"Nextflow {version_str} is outdated (requires >= 23.04)",',
      "                    details=version_line,",
      '                    fix="nextflow self-update"',
      "                )",
      "",
      "        return CheckResult(",
      '            name="Nextflow",',
      "            passed=True,",
      '            message="Nextflow installed (version unknown)",',
      "            details=version_line",
      "        )",
      "",
      "    except subprocess.TimeoutExpired:",
      "        return CheckResult(",
      '            name="Nextflow",',
      "            passed=False,",
      '            message="Nextflow command timed out",',
      '            fix="Check Nextflow installation"',
      "        )",
      "    except Exception as e:",
      "        return CheckResult(",
      '            name="Nextflow",',
      "            passed=False,",
      '            message=f"Nextflow check failed: {str(e)}"',
      "        )",
      "",
      "",
      "def check_java() -> CheckResult:",
      '    """Check Java version (requires >= 11)."""',
      '    if not shutil.which("java"):',
      "        return CheckResult(",
      '            name="Java",',
      "            passed=False,",
      '            message="Java not found in PATH",',
      '            fix="Install Java 11+: sudo apt install openjdk-11-jdk"',
      "        )",
      "",
      "    try:",
      "        result = subprocess.run(",
      '            ["java", "-version"],',
      "            capture_output=True,",
      "            text=True,",
      "            timeout=10",
      "        )",
      "",
      "        # Java version is typically in stderr",
      "        output = result.stderr or result.stdout",
      "        import re",
      "        match = re.search(r'version \"(\\d+)', output)",
      "",
      "        if match:",
      "            version = int(match.group(1))",
      "            version_line = output.strip().split('\\n')[0]",
      "",
      "            if version >= 11:",
      "                return CheckResult(",
      '                    name="Java",',
      "                    passed=True,",
      '                    message=f"Java {version} installed",',
      "                    details=version_line",
      "                )",
      "            else:",
      "                return CheckResult(",
      '                    name="Java",',
      "                    passed=False,",
      '                    message=f"Java {version} is too old (requires >= 11)",',
      "                    details=version_line,",
      '                    fix="Install Java 11+: sudo apt install openjdk-11-jdk"',
      "                )",
      "",
      "        return CheckResult(",
      '            name="Java",',
      "            passed=True,",
      '            message="Java installed",',
      "            details=output.strip().split('\\n')[0] if output else None",
      "        )",
      "",
      "    except Exception as e:",
      "        return CheckResult(",
      '            name="Java",',
      "            passed=False,",
      '            message=f"Java check failed: {str(e)}"',
      "        )",
      "",
      "",
      "def check_resources() -> CheckResult:",
      '    """Check system resources (CPU, memory, disk)."""',
      "    try:",
      "        # CPU cores",
      "        cpu_count = os.cpu_count() or 1",
      "",
      "        # Memory",
      "        mem_gb = 0",
      "        try:",
      "            # Linux: read from /proc/meminfo",
      "            with open('/proc/meminfo', 'r') as f:",
      "                for line in f:",
      "                    if line.startswith('MemTotal:'):",
      "                        mem_kb = int(line.split()[1])",
      "                        mem_gb = mem_kb / (1024 * 1024)",
      "                        break",
      "        except (FileNotFoundError, PermissionError):",
      "            # macOS: use sysctl",
      "            try:",
      "                result = subprocess.run(",
      "                    ['sysctl', '-n', 'hw.memsize'],",
      "                    capture_output=True, text=True, timeout=5",
      "                )",
      "                if result.returncode == 0:",
      "                    mem_gb = int(result.stdout.strip()) / (1024**3)",
      "            except:",
      "                pass",
      "",
      "        # Disk space (current directory)",
      "        disk_gb = 0",
      "        try:",
      "            statvfs = os.statvfs('.')",
      "            disk_gb = (statvfs.f_frsize * statvfs.f_bavail) / (1024**3)",
      "        except:",
      "            pass",
      "",
      '        details = f"CPUs: {cpu_count}, Memory: {mem_gb:.1f}GB, Disk: {disk_gb:.1f}GB available"',
      "",
      "        # Check minimums",
      "        warnings = []",
      "        if cpu_count < 4:",
      '            warnings.append(f"Low CPU count ({cpu_count}). Consider --max_cpus {cpu_count}")',
      "        if 0 < mem_gb < 8:",
      "            warnings.append(f\"Low memory ({mem_gb:.1f}GB). Use --max_memory '{int(mem_gb)}GB'\")",
      "        if 0 < disk_gb < 50:",
      '            warnings.append(f"Low disk space ({disk_gb:.1f}GB). Pipelines need ~100GB for human data")',
      "",
      "        if warnings:",
      "            return CheckResult(",
      '                name="Resources",',
      "                passed=True,",
      '                message="Resources available (with warnings)",',
      "                details=details,",
      '                fix="; ".join(warnings)',
      "            )",
      "",
      "        return CheckResult(",
      '            name="Resources",',
      "            passed=True,",
      '            message="Sufficient resources available",',
      "            details=details",
      "        )",
      "",
      "    except Exception as e:",
      "        return CheckResult(",
      '            name="Resources",',
      "            passed=True,  # Don't fail on resource check errors",
      '            message=f"Could not fully check resources: {str(e)}"',
      "        )",
      "",
      "",
      "def check_network() -> CheckResult:",
      '    """Check network connectivity to Docker Hub and nf-core."""',
      "    try:",
      "        import urllib.request",
      "",
      "        # User-Agent header to avoid 403 from sites that block default Python agent",
      "        headers = {'User-Agent': 'nf-core-helper/1.0'}",
      "",
      "        # Try Docker Hub",
      "        try:",
      '            req = urllib.request.Request("https://hub.docker.com", headers=headers)',
      "            urllib.request.urlopen(req, timeout=10)",
      "            docker_hub_ok = True",
      "        except:",
      "            docker_hub_ok = False",
      "",
      "        # Try nf-core (for pipeline downloads)",
      "        try:",
      '            req = urllib.request.Request("https://nf-co.re", headers=headers)',
      "            urllib.request.urlopen(req, timeout=10)",
      "            nfcore_ok = True",
      "        except:",
      "            nfcore_ok = False",
      "",
      "        if docker_hub_ok and nfcore_ok:",
      "            return CheckResult(",
      '                name="Network",',
      "                passed=True,",
      '                message="Network connectivity OK (Docker Hub & nf-core reachable)"',
      "            )",
      "        elif docker_hub_ok:",
      "            return CheckResult(",
      '                name="Network",',
      "                passed=True,",
      '                message="Docker Hub reachable (nf-core.re not reachable)",',
      '                details="Pipeline downloads may still work via GitHub"',
      "            )",
      "        else:",
      "            return CheckResult(",
      '                name="Network",',
      "                passed=False,",
      '                message="Cannot reach Docker Hub",',
      '                fix="Check network connection. Containers require Docker Hub access."',
      "            )",
      "",
      "    except Exception as e:",
      "        return CheckResult(",
      '            name="Network",',
      "            passed=False,",
      '            message=f"Network check failed: {str(e)}",',
      '            fix="Check network connection and proxy settings"',
      "        )",
      "",
      "",
      "def run_all_checks() -> EnvironmentReport:",
      '    """Run all environment checks and return comprehensive report."""',
      "    checks = [",
      "        check_docker(),",
      "        check_nextflow(),",
      "        check_java(),",
      "        check_resources(),",
      "        check_network(),",
      "    ]",
      "",
      "    # Critical checks that must pass",
      '    critical_checks = ["Docker", "Nextflow", "Java"]',
      "    ready = all(c.passed for c in checks if c.name in critical_checks)",
      "",
      "    # Build recommendations",
      "    recommendations = []",
      "    for check in checks:",
      "        if not check.passed and check.fix:",
      '            recommendations.append(f"{check.name}: {check.fix}")',
      "        elif check.passed and check.fix:  # Warnings",
      '            recommendations.append(f"{check.name} (warning): {check.fix}")',
      "",
      "    return EnvironmentReport(",
      "        ready=ready,",
      "        checks=checks,",
      "        recommendations=recommendations",
      "    )",
      "",
      "",
      "def print_report(report: EnvironmentReport):",
      '    """Print human-readable report to stdout."""',
      '    print("\\n" + "=" * 50)',
      '    print("  nf-core Environment Check")',
      '    print("=" * 50 + "\\n")',
      "",
      "    for check in report.checks:",
      '        status = "\\033[92m[PASS]\\033[0m" if check.passed else "\\033[91m[FAIL]\\033[0m"',
      '        print(f"{status} {check.name}: {check.message}")',
      "",
      "        if check.details:",
      '            print(f"       {check.details}")',
      "",
      "        if not check.passed and check.fix:",
      '            print(f"       \\033[93mFix:\\033[0m {check.fix}")',
      "        elif check.passed and check.fix:  # Warning",
      '            print(f"       \\033[93mWarning:\\033[0m {check.fix}")',
      "",
      "    print()",
      "    if report.ready:",
      '        print("\\033[92m✓ Environment is READY for nf-core pipelines.\\033[0m")',
      "    else:",
      '        print("\\033[91m✗ Environment is NOT READY. Please address the issues above.\\033[0m")',
      "",
      "    if report.recommendations:",
      '        print("\\n--- Recommendations ---")',
      "        for i, rec in enumerate(report.recommendations, 1):",
      '            print(f"  {i}. {rec}")',
      "",
      "    print()",
      "",
      "",
      "def main():",
      "    import argparse",
      "",
      "    parser = argparse.ArgumentParser(",
      '        description="Check environment for nf-core pipeline execution",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    python check_environment.py           # Human-readable output",
      "    python check_environment.py --json    # JSON output for parsing",
      '        """',
      "    )",
      '    parser.add_argument("--json", action="store_true",',
      '                        help="Output results as JSON")',
      "",
      "    args = parser.parse_args()",
      "",
      "    report = run_all_checks()",
      "",
      "    if args.json:",
      "        print(json.dumps(report.to_dict(), indent=2))",
      "    else:",
      "        print_report(report)",
      "",
      "    sys.exit(0 if report.ready else 1)",
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/config/genomes.yaml": [
      "# Organism to Genome Mapping for nf-core Pipelines",
      "# Maps organism names (as they appear in GEO/SRA) to iGenomes keys",
      "",
      "organisms:",
      "  # Human",
      '  "Homo sapiens":',
      '    genome: "GRCh38"',
      "    taxid: 9606",
      '    aliases: ["human", "hg38", "GRCh38"]',
      '    notes: "Primary human reference genome"',
      "",
      '  "Homo sapiens (legacy)":',
      '    genome: "GRCh37"',
      "    taxid: 9606",
      '    aliases: ["hg19", "GRCh37"]',
      '    notes: "Legacy human reference, still used for some clinical data"',
      "",
      "  # Mouse",
      '  "Mus musculus":',
      '    genome: "GRCm39"',
      "    taxid: 10090",
      '    aliases: ["mouse", "mm39", "GRCm39"]',
      '    notes: "Current mouse reference genome"',
      "",
      '  "Mus musculus (legacy)":',
      '    genome: "GRCm38"',
      "    taxid: 10090",
      '    aliases: ["mm10", "GRCm38"]',
      '    notes: "Legacy mouse reference"',
      "",
      "  # Yeast",
      '  "Saccharomyces cerevisiae":',
      '    genome: "R64-1-1"',
      "    taxid: 4932",
      '    aliases: ["yeast", "sacCer3", "S288C", "budding yeast"]',
      '    notes: "S288C reference strain"',
      "",
      "  # Fruit fly",
      '  "Drosophila melanogaster":',
      '    genome: "BDGP6"',
      "    taxid: 7227",
      '    aliases: ["fly", "dm6", "fruit fly", "Dmel"]',
      '    notes: "Berkeley Drosophila Genome Headquarter release 6"',
      "",
      "  # Worm",
      '  "Caenorhabditis elegans":',
      '    genome: "WBcel235"',
      "    taxid: 6239",
      '    aliases: ["worm", "ce11", "C. elegans", "Cele"]',
      '    notes: "WormBase reference"',
      "",
      "  # Zebrafish",
      '  "Danio rerio":',
      '    genome: "GRCz11"',
      "    taxid: 7955",
      '    aliases: ["zebrafish", "danRer11", "Drer"]',
      '    notes: "Genome Reference Consortium Zebrafish Build 11"',
      "",
      "  # Arabidopsis",
      '  "Arabidopsis thaliana":',
      '    genome: "TAIR10"',
      "    taxid: 3702",
      '    aliases: ["arabidopsis", "thale cress", "Atha"]',
      '    notes: "The Arabidopsis Information Resource v10"',
      "",
      "  # Rat",
      '  "Rattus norvegicus":',
      '    genome: "Rnor_6.0"',
      "    taxid: 10116",
      '    aliases: ["rat", "rn6", "Rnor"]',
      '    notes: "Rnor 6.0 reference"',
      "",
      "  # Chicken",
      '  "Gallus gallus":',
      '    genome: "GRCg6a"',
      "    taxid: 9031",
      '    aliases: ["chicken", "galGal6", "Ggal"]',
      '    notes: "Genome Reference Consortium Chicken Build 6a"',
      "",
      "  # Pig",
      '  "Sus scrofa":',
      '    genome: "Sscrofa11.1"',
      "    taxid: 9823",
      '    aliases: ["pig", "susScr11", "Sscr"]',
      '    notes: "Swine genome assembly 11.1"',
      "",
      "  # Cow",
      '  "Bos taurus":',
      '    genome: "ARS-UCD1.2"',
      "    taxid: 9913",
      '    aliases: ["cow", "bosTau9", "cattle", "Btau"]',
      '    notes: "USDA ARS assembly"',
      "",
      "  # Dog",
      '  "Canis lupus familiaris":',
      '    genome: "CanFam3.1"',
      "    taxid: 9615",
      '    aliases: ["dog", "canFam3", "Clup"]',
      '    notes: "Broad Institute CanFam3.1"',
      "",
      "  # Frog",
      '  "Xenopus tropicalis":',
      '    genome: "JGI_4.2"',
      "    taxid: 8364",
      '    aliases: ["frog", "xenTro9", "Xtro"]',
      '    notes: "JGI assembly version 4.2"',
      "",
      "  # Maize/Corn",
      '  "Zea mays":',
      '    genome: "Zm-B73-REFERENCE-NAM-5.0"',
      "    taxid: 4577",
      '    aliases: ["maize", "corn", "Zmay"]',
      '    notes: "B73 reference genome v5"',
      "",
      "  # Rice",
      '  "Oryza sativa":',
      '    genome: "IRGSP-1.0"',
      "    taxid: 39947",
      '    aliases: ["rice", "Osat"]',
      '    notes: "International Rice Genome Sequencing Headquarter"',
      "",
      "  # E. coli (common bacterial model)",
      '  "Escherichia coli":',
      "    genome: null",
      "    taxid: 562",
      '    aliases: ["E. coli", "Ecol"]',
      '    notes: "Use specific strain reference; K-12 MG1655 common"',
      "",
      "  # Fission yeast",
      '  "Schizosaccharomyces pombe":',
      '    genome: "ASM294v2"',
      "    taxid: 4896",
      '    aliases: ["fission yeast", "S. pombe", "Spom"]',
      '    notes: "PomBase reference"',
      "",
      "# Pipeline mapping based on library strategy",
      "pipeline_suggestions:",
      '  "RNA-SEQ": "rnaseq"',
      '  "ATAC-SEQ": "atacseq"',
      '  "CHIP-SEQ": "chipseq"',
      '  "WGS": "sarek"',
      '  "WXS": "sarek"',
      '  "EXOME": "sarek"',
      '  "AMPLICON": "ampliseq"',
      '  "BISULFITE-SEQ": "methylseq"',
      '  "HI-C": "hic"',
      '  "MIRNA-SEQ": "smrnaseq"',
      '  "RRBS": "methylseq"',
      "",
    ].join("\n"),
    "scripts/config/pipelines/atacseq.yaml": [
      "name: atacseq",
      'version: "2.1.2"',
      'description: "Chromatin accessibility analysis and peak calling"',
      "",
      "# Documentation and source - NOTE: Update version in URLs when upgrading pipeline",
      "urls:",
      '  documentation: "https://nf-co.re/atacseq/{version}/"',
      '  parameters: "https://nf-co.re/atacseq/{version}/parameters/"',
      '  output_docs: "https://nf-co.re/atacseq/{version}/docs/output/"',
      '  github: "https://github.com/nf-core/atacseq"',
      '  releases: "https://github.com/nf-core/atacseq/releases"',
      "",
      "data_types:",
      "  - ATAC-seq",
      "  - chromatin accessibility",
      "  - open chromatin",
      "",
      "detection_hints:",
      "  filename:",
      "    - atac",
      "    - atacseq",
      "    - chromatin",
      "    - accessibility",
      "  directory:",
      "    - atac",
      "    - atacseq",
      "    - chromatin",
      "    - epigenome",
      "    - epigenetics",
      "",
      "samplesheet:",
      "  input_types:",
      "    - fastq",
      "",
      "  columns:",
      "    - name: sample",
      "      required: true",
      "      type: string",
      "      inference: filename",
      '      description: "Condition/group identifier (replicates share same name)"',
      "",
      "    - name: fastq_1",
      "      required: true",
      "      type: path",
      "      inference: auto",
      '      description: "Absolute path to R1 FASTQ"',
      "",
      "    - name: fastq_2",
      "      required: true",
      "      type: path",
      "      inference: auto",
      '      description: "Absolute path to R2 FASTQ (paired-end required)"',
      "",
      "    - name: replicate",
      "      required: true",
      "      type: integer",
      "      default: 1",
      "      inference: filename",
      '      description: "Replicate number (integer)"',
      "",
      "decision_points:",
      "  - parameter: genome",
      '    prompt: "Which reference genome matches your organism?"',
      "    options:",
      "      - value: GRCh38",
      '        label: "Human GRCh38/hg38 (recommended)"',
      '        description: "Latest human reference"',
      "      - value: GRCh37",
      '        label: "Human GRCh37/hg19 (legacy)"',
      '        description: "Older human reference"',
      "      - value: mm10",
      '        label: "Mouse mm10"',
      '        description: "Mouse reference genome"',
      "    default: GRCh38",
      '    recommendation: "Default to GRCh38 for human samples"',
      "",
      "  - parameter: read_length",
      '    prompt: "What is the read length of your sequencing data?"',
      "    options:",
      "      - value: 50",
      '        label: "50 bp"',
      '        description: "Short reads"',
      "      - value: 75",
      '        label: "75 bp"',
      '        description: "Standard length"',
      "      - value: 100",
      '        label: "100 bp"',
      '        description: "Common for modern sequencers"',
      "      - value: 150",
      '        label: "150 bp"',
      '        description: "Long reads"',
      "    default: 50",
      '    recommendation: "Check FASTQ files or sequencing report for exact length"',
      "",
      "  - parameter: narrow_peak",
      '    prompt: "What type of peaks are you expecting?"',
      "    options:",
      '      - value: "true"',
      '        label: "Narrow peaks (default for ATAC-seq)"',
      '        description: "Standard ATAC-seq open chromatin regions"',
      '      - value: "false"',
      '        label: "Broad peaks"',
      '        description: "For histone marks or broader accessibility regions"',
      '    default: "true"',
      '    recommendation: "Use narrow peaks for standard ATAC-seq"',
      "",
      "test_profile:",
      '  command: "nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq"',
      '  duration: "15 minutes"',
      "  success_indicators:",
      '    - "test_atacseq/multiqc/multiqc_report.html"',
      '  log_pattern: "Pipeline completed successfully"',
      "",
      "run_command:",
      "  template: |",
      "    nextflow run nf-core/atacseq \\",
      "        -r 2.1.2 \\",
      "        -profile docker \\",
      "        --input {samplesheet} \\",
      "        --outdir {outdir} \\",
      "        --genome {genome} \\",
      "        --read_length {read_length} \\",
      "        -resume",
      "",
      "outputs:",
      "  primary:",
      '    - path: "bwa/mergedLibrary/*.mLb.mkD.sorted.bam"',
      '      description: "Filtered, deduplicated alignments"',
      '    - path: "bwa/mergedLibrary/bigwig/*.bigWig"',
      '      description: "Coverage tracks for genome browsers"',
      '    - path: "macs2/narrowPeak/*.narrowPeak"',
      '      description: "Peak calls (BED format)"',
      '    - path: "macs2/narrowPeak/consensus/consensus_peaks.bed"',
      '      description: "Consensus peaks across replicates"',
      "",
      "  validation:",
      '    - file: "multiqc/multiqc_report.html"',
      "      check: exists",
      '      description: "QC report must exist"',
      '    - file: "macs2/narrowPeak"',
      "      check: exists",
      '      description: "Peak calls directory"',
      "",
      "quality_metrics:",
      "  - name: mapped_reads",
      '    good: ">80%"',
      '    acceptable: "60-80%"',
      '    poor: "<60%"',
      "  - name: mitochondrial",
      '    good: "<20%"',
      '    acceptable: "20-40%"',
      '    poor: ">40%"',
      "  - name: duplicates",
      '    good: "<30%"',
      '    acceptable: "30-50%"',
      '    poor: ">50%"',
      "  - name: frip",
      '    good: ">30%"',
      '    acceptable: "15-30%"',
      '    poor: "<15%"',
      "  - name: tss_enrichment",
      '    good: ">6"',
      '    acceptable: "4-6"',
      '    poor: "<4"',
      "",
      "resources:",
      '  min_memory: "8.GB"',
      '  recommended_memory: "32.GB"',
      "  min_cpus: 4",
      "  recommended_cpus: 8",
      '  disk_space: "100.GB"',
      "",
      "troubleshooting:",
      '  - error: "Low FRiP score"',
      '    fix: "Check library complexity in plotFingerprint. May indicate over-transposition or low quality"',
      '  - error: "Few peaks called"',
      '    fix: "Lower threshold with --macs_qvalue 0.1 or use --narrow_peak false for broader peaks"',
      '  - error: "High duplicates"',
      '    fix: "Normal for low-input samples. Pipeline removes by default. Consider deeper sequencing"',
      '  - error: "High mitochondrial reads"',
      '    fix: "Sample quality issue. Pipeline filters mito by default (--keep_mito false)"',
      "",
      "replicate_patterns:",
      '  - "_rep(\\\\d+)"',
      '  - "_R(\\\\d+)_"',
      '  - "_(\\\\d+)$"',
      '  - "_replicate(\\\\d+)"',
      "",
    ].join("\n"),
    "scripts/config/pipelines/rnaseq.yaml": [
      "name: rnaseq",
      'version: "3.22.2"',
      'description: "Gene expression quantification and differential expression analysis"',
      "",
      "# Documentation and source - NOTE: Update version in URLs when upgrading pipeline",
      "urls:",
      '  documentation: "https://nf-co.re/rnaseq/{version}/"',
      '  parameters: "https://nf-co.re/rnaseq/{version}/parameters/"',
      '  output_docs: "https://nf-co.re/rnaseq/{version}/docs/output/"',
      '  github: "https://github.com/nf-core/rnaseq"',
      '  releases: "https://github.com/nf-core/rnaseq/releases"',
      "",
      "data_types:",
      "  - RNA-seq",
      "  - mRNA-seq",
      "  - bulk RNA-seq",
      "",
      "detection_hints:",
      "  filename:",
      "    - rna",
      "    - rnaseq",
      "    - mrna",
      "    - expression",
      "  directory:",
      "    - rnaseq",
      "    - rna",
      "    - expression",
      "    - transcriptome",
      "",
      "samplesheet:",
      "  input_types:",
      "    - fastq",
      "",
      "  columns:",
      "    - name: sample",
      "      required: true",
      "      type: string",
      "      inference: filename",
      '      description: "Sample identifier"',
      "",
      "    - name: fastq_1",
      "      required: true",
      "      type: path",
      "      inference: auto",
      '      description: "Absolute path to R1 FASTQ"',
      "",
      "    - name: fastq_2",
      "      required: false",
      "      type: path",
      "      inference: auto",
      '      description: "Absolute path to R2 FASTQ (empty for single-end)"',
      "",
      "    - name: strandedness",
      "      required: true",
      "      type: enum",
      "      allowed:",
      "        - auto",
      "        - forward",
      "        - reverse",
      "        - unstranded",
      '      default: "auto"',
      "      inference: default",
      '      description: "Library strandedness (auto recommended)"',
      "",
      "decision_points:",
      "  - parameter: genome",
      '    prompt: "Which reference genome matches your organism?"',
      "    options:",
      "      - value: GRCh38",
      '        label: "Human GRCh38/hg38 (recommended for human)"',
      '        description: "Latest human reference assembly"',
      "      - value: GRCh37",
      '        label: "Human GRCh37/hg19 (legacy)"',
      '        description: "Older human reference for compatibility"',
      "      - value: mm10",
      '        label: "Mouse mm10/GRCm38"',
      '        description: "Mouse reference genome"',
      "      - value: BDGP6",
      '        label: "Drosophila BDGP6"',
      '        description: "Fruit fly reference"',
      "    default: GRCh38",
      '    recommendation: "Default to GRCh38 for human samples"',
      "",
      "  - parameter: aligner",
      '    prompt: "Which alignment strategy would you prefer?"',
      "    options:",
      "      - value: star_salmon",
      '        label: "STAR + Salmon (recommended)"',
      '        description: "Most accurate, standard for differential expression"',
      "      - value: star_rsem",
      '        label: "STAR + RSEM"',
      '        description: "Better for isoform-level quantification"',
      "      - value: hisat2",
      '        label: "HISAT2"',
      '        description: "Lower memory requirements, faster"',
      "    default: star_salmon",
      '    recommendation: "Use star_salmon unless memory-constrained or need isoforms"',
      "",
      "test_profile:",
      '  command: "nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq"',
      '  duration: "15 minutes"',
      "  success_indicators:",
      '    - "test_rnaseq/multiqc/multiqc_report.html"',
      '  log_pattern: "Pipeline completed successfully"',
      "",
      "run_command:",
      "  template: |",
      "    nextflow run nf-core/rnaseq \\",
      "        -r 3.22.2 \\",
      "        -profile docker \\",
      "        --input {samplesheet} \\",
      "        --outdir {outdir} \\",
      "        --genome {genome} \\",
      "        --aligner {aligner} \\",
      "        -resume",
      "",
      "outputs:",
      "  primary:",
      '    - path: "star_salmon/salmon.merged.gene_counts.tsv"',
      '      description: "Raw gene counts for DESeq2/edgeR"',
      '    - path: "star_salmon/salmon.merged.gene_tpm.tsv"',
      '      description: "TPM normalized expression values"',
      '    - path: "star_salmon/*.bam"',
      '      description: "Aligned reads"',
      "",
      "  validation:",
      '    - file: "multiqc/multiqc_report.html"',
      "      check: exists",
      '      description: "QC report must exist"',
      '    - file: "star_salmon/salmon.merged.gene_counts.tsv"',
      "      check: non_empty",
      '      description: "Count matrix must have data"',
      "",
      "resources:",
      '  min_memory: "8.GB"',
      '  recommended_memory: "32.GB"',
      "  min_cpus: 4",
      "  recommended_cpus: 8",
      '  disk_space: "100.GB"',
      "",
      "troubleshooting:",
      '  - error: "STAR index fails"',
      "    fix: \"Increase memory with --max_memory '64.GB' or provide pre-built --star_index\"",
      '  - error: "Low alignment rate"',
      '    fix: "Verify genome matches species; check FastQC for adapter contamination"',
      '  - error: "Strandedness detection fails"',
      '    fix: "Specify explicitly with --strandedness reverse (or forward/unstranded)"',
      "",
    ].join("\n"),
    "scripts/config/pipelines/sarek.yaml": [
      "name: sarek",
      'version: "3.7.1"',
      'description: "Variant calling for WGS/WES data (germline and somatic)"',
      "",
      "# Documentation and source - NOTE: Update version in URLs when upgrading pipeline",
      "urls:",
      '  documentation: "https://nf-co.re/sarek/{version}/"',
      '  parameters: "https://nf-co.re/sarek/{version}/parameters/"',
      '  output_docs: "https://nf-co.re/sarek/{version}/docs/output/"',
      '  github: "https://github.com/nf-core/sarek"',
      '  releases: "https://github.com/nf-core/sarek/releases"',
      "",
      "data_types:",
      "  - WGS",
      "  - WES",
      "  - whole genome sequencing",
      "  - whole exome sequencing",
      "  - tumor-normal",
      "  - germline",
      "  - somatic",
      "",
      "detection_hints:",
      "  filename:",
      "    - tumor",
      "    - normal",
      "    - germline",
      "    - wgs",
      "    - wes",
      "    - exome",
      "    - dna",
      "    - variant",
      "  directory:",
      "    - variant",
      "    - wgs",
      "    - wes",
      "    - exome",
      "    - germline",
      "    - somatic",
      "",
      "samplesheet:",
      "  input_types:",
      "    - fastq",
      "    - bam",
      "    - cram",
      "",
      "  columns:",
      "    - name: patient",
      "      required: true",
      "      type: string",
      "      inference: filename",
      '      description: "Patient/subject identifier for grouping samples"',
      "",
      "    - name: sample",
      "      required: true",
      "      type: string",
      "      inference: filename",
      '      description: "Sample identifier (e.g., tumor, normal)"',
      "",
      "    - name: lane",
      "      required: false",
      "      type: string",
      '      default: "L001"',
      "      inference: filename",
      '      description: "Sequencing lane"',
      "",
      "    - name: fastq_1",
      "      required: true",
      "      type: path",
      "      inference: auto",
      "      condition: \"input_type == 'fastq'\"",
      '      description: "Absolute path to R1 FASTQ"',
      "",
      "    - name: fastq_2",
      "      required: false",
      "      type: path",
      "      inference: auto",
      "      condition: \"input_type == 'fastq'\"",
      '      description: "Absolute path to R2 FASTQ"',
      "",
      "    - name: bam",
      "      required: true",
      "      type: path",
      "      inference: auto",
      "      condition: \"input_type in ['bam', 'cram']\"",
      '      description: "Absolute path to BAM/CRAM file"',
      "",
      "    - name: bai",
      "      required: true",
      "      type: path",
      "      inference: auto",
      "      condition: \"input_type in ['bam', 'cram']\"",
      '      description: "Absolute path to BAM/CRAM index"',
      "",
      "    - name: status",
      "      required: false",
      "      type: integer",
      "      allowed:",
      "        - 0",
      "        - 1",
      "      default: 0",
      "      inference: filename",
      '      description: "0=normal, 1=tumor (critical for somatic calling)"',
      "",
      "decision_points:",
      "  - parameter: genome",
      '    prompt: "Which reference genome should be used?"',
      "    options:",
      "      - value: GRCh38",
      '        label: "Human GRCh38/hg38 (recommended)"',
      '        description: "Latest human reference with most annotation support"',
      "      - value: GRCh37",
      '        label: "Human GRCh37/hg19 (legacy)"',
      '        description: "For compatibility with older datasets"',
      "      - value: mm10",
      '        label: "Mouse mm10"',
      '        description: "Mouse reference genome"',
      "    default: GRCh38",
      '    recommendation: "Default to GRCh38 for human data"',
      "",
      "  - parameter: tools",
      '    prompt: "What type of variant calling do you need?"',
      "    options:",
      '      - value: "haplotypecaller,snpeff"',
      '        label: "Germline variants (single samples)"',
      '        description: "For finding inherited variants in normal samples"',
      '        condition: "no tumor samples detected"',
      '      - value: "mutect2,strelka,snpeff"',
      '        label: "Somatic variants (tumor-normal pairs)"',
      '        description: "For finding cancer mutations with matched normal"',
      '        condition: "tumor-normal pairs detected"',
      '      - value: "haplotypecaller,deepvariant,snpeff"',
      '        label: "Germline with DeepVariant"',
      '        description: "Higher accuracy germline calling (requires GPU)"',
      '      - value: "mutect2,manta,snpeff"',
      '        label: "Somatic with structural variants"',
      '        description: "Comprehensive tumor analysis including SVs"',
      '    default: "haplotypecaller,snpeff"',
      '    recommendation: "Use somatic tools if tumor/normal pairs detected, otherwise germline"',
      "",
      "  - parameter: wes",
      '    prompt: "Is this whole exome sequencing (WES) data?"',
      "    options:",
      '      - value: "false"',
      '        label: "No - Whole Genome Sequencing (WGS)"',
      '        description: "Full genome coverage"',
      '      - value: "true"',
      '        label: "Yes - Whole Exome Sequencing (WES)"',
      '        description: "Requires --intervals BED file"',
      '    default: "false"',
      '    recommendation: "If WES, user must provide intervals BED file"',
      "",
      "test_profile:",
      '  command: "nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek"',
      '  duration: "20 minutes"',
      "  success_indicators:",
      '    - "test_sarek/multiqc/multiqc_report.html"',
      '  log_pattern: "Pipeline completed successfully"',
      "",
      "run_command:",
      "  template: |",
      "    nextflow run nf-core/sarek \\",
      "        -r 3.7.1 \\",
      "        -profile docker \\",
      "        --input {samplesheet} \\",
      "        --outdir {outdir} \\",
      "        --genome {genome} \\",
      "        --tools {tools} \\",
      "        -resume",
      "",
      "  wes_template: |",
      "    nextflow run nf-core/sarek \\",
      "        -r 3.7.1 \\",
      "        -profile docker \\",
      "        --input {samplesheet} \\",
      "        --outdir {outdir} \\",
      "        --genome {genome} \\",
      "        --tools {tools} \\",
      "        --wes \\",
      "        --intervals {intervals} \\",
      "        -resume",
      "",
      "outputs:",
      "  primary:",
      '    - path: "preprocessing/recalibrated/*.recal.bam"',
      '      description: "Analysis-ready BAM files"',
      '    - path: "variant_calling/*/*.vcf.gz"',
      '      description: "Variant call files"',
      '    - path: "annotation/snpeff/*.ann.vcf.gz"',
      '      description: "Annotated variants"',
      "",
      "  validation:",
      '    - file: "multiqc/multiqc_report.html"',
      "      check: exists",
      '      description: "QC report must exist"',
      '    - file: "preprocessing/recalibrated"',
      "      check: exists",
      '      description: "Recalibrated BAMs directory"',
      "",
      "resources:",
      '  min_memory: "16.GB"',
      '  recommended_memory: "64.GB"',
      '  wgs_memory: "128.GB"',
      "  min_cpus: 4",
      "  recommended_cpus: 16",
      '  disk_space: "500.GB"',
      "",
      "troubleshooting:",
      '  - error: "BQSR fails"',
      '    fix: "Check known sites available for genome. Skip with --skip_bqsr for non-standard references"',
      '  - error: "Mutect2 no variants"',
      '    fix: "Verify tumor/normal pairing in samplesheet (check status column: 0=normal, 1=tumor)"',
      '  - error: "Out of memory"',
      "    fix: \"--max_memory '128.GB' for WGS data\"",
      '  - error: "DeepVariant GPU issues"',
      '    fix: "Ensure NVIDIA Docker runtime configured, or use CPU mode"',
      "",
      "tumor_normal_keywords:",
      "  tumor:",
      "    - tumor",
      "    - tumour",
      "    - met",
      "    - metastasis",
      "    - primary",
      "    - cancer",
      "    - malignant",
      "  normal:",
      "    - normal",
      "    - germline",
      "    - blood",
      "    - pbmc",
      "    - control",
      "    - healthy",
      "    - matched",
      "",
    ].join("\n"),
    "scripts/detect_data_type.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Auto-detect appropriate nf-core pipeline from data directory.",
      "",
      "Analyzes filenames, directory structure, and file content hints to suggest",
      "the most appropriate pipeline for the data.",
      "",
      "Usage:",
      "    python detect_data_type.py /path/to/data",
      "    python detect_data_type.py /path/to/data --json",
      '"""',
      "",
      "import argparse",
      "import json",
      "import os",
      "import sys",
      "from pathlib import Path",
      "from typing import Dict, List, Tuple",
      "",
      "import yaml",
      "",
      "",
      "def load_all_pipeline_configs() -> Dict[str, Dict]:",
      '    """Load all pipeline configurations."""',
      '    config_dir = Path(__file__).parent / "config" / "pipelines"',
      "    configs = {}",
      "",
      '    for config_file in config_dir.glob("*.yaml"):',
      '        if config_file.stem.startswith("_"):',
      "            continue",
      "        with open(config_file) as f:",
      "            configs[config_file.stem] = yaml.safe_load(f)",
      "",
      "    return configs",
      "",
      "",
      "def scan_directory(directory: str) -> Dict:",
      '    """Scan directory and collect file information."""',
      "    info = {",
      "        'fastq_count': 0,",
      "        'bam_count': 0,",
      "        'cram_count': 0,",
      "        'filenames': [],",
      "        'directories': [],",
      "        'total_size_gb': 0,",
      "    }",
      "",
      "    directory = os.path.abspath(directory)",
      "",
      "    for root, dirs, files in os.walk(directory):",
      "        # Collect directory names",
      "        rel_root = os.path.relpath(root, directory)",
      "        if rel_root != '.':",
      "            info['directories'].append(rel_root.lower())",
      "",
      "        for filename in files:",
      "            filename_lower = filename.lower()",
      "",
      "            # Count file types",
      "            if any(filename_lower.endswith(ext) for ext in ['.fastq.gz', '.fq.gz', '.fastq', '.fq']):",
      "                info['fastq_count'] += 1",
      "            elif filename_lower.endswith('.bam'):",
      "                info['bam_count'] += 1",
      "            elif filename_lower.endswith('.cram'):",
      "                info['cram_count'] += 1",
      "",
      "            # Collect filenames for pattern matching",
      "            info['filenames'].append(filename_lower)",
      "",
      "            # Sum file sizes",
      "            try:",
      "                size = os.path.getsize(os.path.join(root, filename))",
      "                info['total_size_gb'] += size / (1024**3)",
      "            except:",
      "                pass",
      "",
      "    return info",
      "",
      "",
      "def calculate_pipeline_scores(scan_info: Dict, configs: Dict) -> Dict[str, Dict]:",
      '    """Calculate confidence scores for each pipeline."""',
      "    scores = {}",
      "",
      "    for pipeline_name, config in configs.items():",
      "        score = 0",
      "        matches = []",
      "",
      "        # Check detection hints",
      "        hints = config.get('detection_hints', {})",
      "",
      "        # Filename hints",
      "        filename_hints = hints.get('filename', [])",
      "        for hint in filename_hints:",
      "            hint_lower = hint.lower()",
      "            for filename in scan_info['filenames']:",
      "                if hint_lower in filename:",
      "                    score += 10",
      "                    matches.append(f\"Filename contains '{hint}'\")",
      "                    break",
      "",
      "        # Directory hints",
      "        directory_hints = hints.get('directory', [])",
      "        for hint in directory_hints:",
      "            hint_lower = hint.lower()",
      "            for dirname in scan_info['directories']:",
      "                if hint_lower in dirname:",
      "                    score += 15",
      "                    matches.append(f\"Directory contains '{hint}'\")",
      "                    break",
      "",
      "        # Check data type compatibility",
      "        data_types = config.get('data_types', [])",
      "        input_types = config.get('samplesheet', {}).get('input_types', ['fastq'])",
      "",
      "        # Prefer pipelines that support the available file types",
      "        if 'fastq' in input_types and scan_info['fastq_count'] > 0:",
      "            score += 5",
      "        if 'bam' in input_types and scan_info['bam_count'] > 0:",
      "            score += 5",
      "        if 'cram' in input_types and scan_info['cram_count'] > 0:",
      "            score += 5",
      "",
      "        # Pipeline-specific boosts",
      "        if pipeline_name == 'sarek':",
      "            # Check for tumor/normal indicators",
      "            tumor_indicators = ['tumor', 'tumour', 'cancer', 'met', 'primary']",
      "            normal_indicators = ['normal', 'germline', 'blood', 'control']",
      "",
      "            has_tumor = any(ind in ' '.join(scan_info['filenames']) for ind in tumor_indicators)",
      "            has_normal = any(ind in ' '.join(scan_info['filenames']) for ind in normal_indicators)",
      "",
      "            if has_tumor or has_normal:",
      "                score += 20",
      "                if has_tumor:",
      '                    matches.append("Found tumor sample indicators")',
      "                if has_normal:",
      '                    matches.append("Found normal sample indicators")',
      "",
      "            # DNA-related hints",
      "            dna_hints = ['wgs', 'wes', 'exome', 'dna', 'variant', 'snp', 'indel']",
      "            for hint in dna_hints:",
      "                if hint in ' '.join(scan_info['filenames'] + scan_info['directories']):",
      "                    score += 10",
      "                    matches.append(f\"Found DNA/variant indicator: '{hint}'\")",
      "                    break",
      "",
      "        elif pipeline_name == 'rnaseq':",
      "            # RNA-related hints",
      "            rna_hints = ['rna', 'rnaseq', 'mrna', 'expression', 'transcript', 'counts']",
      "            for hint in rna_hints:",
      "                if hint in ' '.join(scan_info['filenames'] + scan_info['directories']):",
      "                    score += 15",
      "                    matches.append(f\"Found RNA indicator: '{hint}'\")",
      "                    break",
      "",
      "        elif pipeline_name == 'atacseq':",
      "            # ATAC-related hints",
      "            atac_hints = ['atac', 'atacseq', 'chromatin', 'accessibility', 'peak', 'macs']",
      "            for hint in atac_hints:",
      "                if hint in ' '.join(scan_info['filenames'] + scan_info['directories']):",
      "                    score += 20",
      "                    matches.append(f\"Found ATAC-seq indicator: '{hint}'\")",
      "                    break",
      "",
      "        scores[pipeline_name] = {",
      "            'score': score,",
      "            'matches': matches,",
      "            'description': config.get('description', ''),",
      "            'version': config.get('version', 'unknown'),",
      "        }",
      "",
      "    return scores",
      "",
      "",
      "def detect_pipeline(directory: str) -> Tuple[str, Dict]:",
      '    """',
      "    Detect the most appropriate pipeline for the data.",
      "",
      "    Args:",
      "        directory: Path to data directory",
      "",
      "    Returns:",
      "        Tuple of (recommended_pipeline, all_scores)",
      '    """',
      "    if not os.path.isdir(directory):",
      '        raise ValueError(f"Not a directory: {directory}")',
      "",
      "    configs = load_all_pipeline_configs()",
      "    scan_info = scan_directory(directory)",
      "",
      "    # Check if any sequencing files found",
      "    total_files = scan_info['fastq_count'] + scan_info['bam_count'] + scan_info['cram_count']",
      "    if total_files == 0:",
      '        raise ValueError(f"No sequencing files (FASTQ/BAM/CRAM) found in {directory}")',
      "",
      "    scores = calculate_pipeline_scores(scan_info, configs)",
      "",
      "    # Find highest scoring pipeline",
      "    best_pipeline = max(scores.keys(), key=lambda k: scores[k]['score'])",
      "",
      "    return best_pipeline, scores",
      "",
      "",
      "def print_results(",
      "    directory: str,",
      "    recommended: str,",
      "    scores: Dict,",
      "    scan_info: Dict,",
      "    output_json: bool = False",
      "):",
      '    """Print detection results."""',
      "    if output_json:",
      "        result = {",
      "            'recommended': recommended,",
      "            'scores': scores,",
      "            'scan_info': {",
      "                'fastq_count': scan_info['fastq_count'],",
      "                'bam_count': scan_info['bam_count'],",
      "                'cram_count': scan_info['cram_count'],",
      "                'total_size_gb': round(scan_info['total_size_gb'], 2),",
      "            }",
      "        }",
      "        print(json.dumps(result, indent=2))",
      "        return",
      "",
      '    print("\\n" + "=" * 50)',
      '    print("  nf-core Pipeline Detection")',
      '    print("=" * 50)',
      '    print(f"\\nDirectory: {directory}")',
      "    print(f\"Files found: {scan_info['fastq_count']} FASTQ, \"",
      "          f\"{scan_info['bam_count']} BAM, {scan_info['cram_count']} CRAM\")",
      "    print(f\"Total size: {scan_info['total_size_gb']:.1f} GB\")",
      "",
      '    print("\\n--- Pipeline Scores ---")',
      "    sorted_pipelines = sorted(scores.keys(), key=lambda k: scores[k]['score'], reverse=True)",
      "",
      "    for pipeline in sorted_pipelines:",
      "        info = scores[pipeline]",
      '        indicator = "→" if pipeline == recommended else " "',
      "        print(f\"\\n{indicator} {pipeline} (score: {info['score']})\")",
      "        print(f\"  {info['description']}\")",
      "        if info['matches']:",
      "            print(f\"  Matches: {', '.join(info['matches'][:3])}\")",
      "",
      "    print(f\"\\n{'=' * 50}\")",
      '    print(f"\\n\\033[92mRecommended: {recommended}\\033[0m")',
      "    print(f\"Version: {scores[recommended]['version']}\")",
      "",
      "    # Print suggested next steps",
      '    print(f"\\n--- Next Steps ---")',
      '    print(f"1. Run environment check:")',
      '    print(f"   python scripts/check_environment.py")',
      '    print(f"\\n2. Run test profile:")',
      "    config = load_all_pipeline_configs().get(recommended, {})",
      "    test_cmd = config.get('test_profile', {}).get('command', '')",
      "    if test_cmd:",
      '        print(f"   {test_cmd}")',
      '    print(f"\\n3. Generate samplesheet:")',
      '    print(f"   python scripts/generate_samplesheet.py {directory} {recommended}")',
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      "        description='Detect appropriate nf-core pipeline for data',",
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    %(prog)s ./data",
      "    %(prog)s ./fastqs --json",
      '        """',
      "    )",
      "",
      "    parser.add_argument('directory', help='Directory containing sequencing data')",
      "    parser.add_argument('--json', action='store_true', help='Output as JSON')",
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        scan_info = scan_directory(args.directory)",
      "        recommended, scores = detect_pipeline(args.directory)",
      "        print_results(args.directory, recommended, scores, scan_info, args.json)",
      "        sys.exit(0)",
      "",
      "    except ValueError as e:",
      "        if args.json:",
      "            print(json.dumps({'error': str(e)}))",
      "        else:",
      '            print(f"Error: {e}")',
      "        sys.exit(1)",
      "",
      "    except Exception as e:",
      "        if args.json:",
      "            print(json.dumps({'error': str(e)}))",
      "        else:",
      '            print(f"Error: {e}")',
      "        sys.exit(1)",
      "",
      "",
      "if __name__ == '__main__':",
      "    main()",
      "",
    ].join("\n"),
    "scripts/generate_samplesheet.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Enhanced nf-core samplesheet generator.",
      "",
      "Features:",
      "- FASTQ, BAM, and CRAM support",
      "- Tumor/normal status inference for sarek",
      "- Robust R1/R2 matching with scoring",
      "- Pre-write validation with clear error messages",
      "- Pipeline config-driven column generation",
      "",
      "Usage:",
      "    python generate_samplesheet.py /path/to/data rnaseq -o samplesheet.csv",
      "    python generate_samplesheet.py /path/to/bams sarek --input-type bam",
      "    python generate_samplesheet.py --validate samplesheet.csv rnaseq",
      '"""',
      "",
      "import argparse",
      "import os",
      "import sys",
      "from pathlib import Path",
      "from typing import Dict, List, Optional, Tuple",
      "",
      "import yaml",
      "",
      "# Add parent directory to path for utils import",
      "sys.path.insert(0, str(Path(__file__).parent))",
      "",
      "from utils.file_discovery import discover_files, detect_input_type, find_index_file",
      "from utils.sample_inference import (",
      "    extract_sample_info,",
      "    infer_tumor_normal_status,",
      "    match_read_pairs,",
      "    extract_replicate_number",
      ")",
      "from utils.validators import validate_samplesheet, ValidationResult",
      "",
      "",
      "def load_pipeline_config(pipeline: str) -> Dict:",
      '    """Load pipeline configuration from YAML."""',
      '    config_dir = Path(__file__).parent / "config" / "pipelines"',
      '    config_file = config_dir / f"{pipeline}.yaml"',
      "",
      "    if not config_file.exists():",
      '        available = [f.stem for f in config_dir.glob("*.yaml") if not f.stem.startswith("_")]',
      "        raise ValueError(f\"Unknown pipeline '{pipeline}'. Available: {', '.join(available)}\")",
      "",
      "    with open(config_file) as f:",
      "        return yaml.safe_load(f)",
      "",
      "",
      "def generate_samplesheet(",
      "    input_dir: str,",
      "    pipeline: str,",
      "    output_file: Optional[str] = None,",
      '    input_type: str = "auto",',
      "    single_end: bool = False,",
      "    interactive: bool = True",
      ") -> Tuple[Optional[str], ValidationResult]:",
      '    """',
      "    Generate samplesheet for specified pipeline.",
      "",
      "    Args:",
      "        input_dir: Directory containing sequencing files",
      "        pipeline: Pipeline name (rnaseq, sarek, atacseq)",
      "        output_file: Output CSV path (default: samplesheet_{pipeline}.csv)",
      "        input_type: File type (auto, fastq, bam, cram)",
      "        single_end: Suppress pairing warnings for single-end data",
      "        interactive: Prompt for missing info",
      "",
      "    Returns:",
      "        Tuple of (output_path, validation_result)",
      '    """',
      "    config = load_pipeline_config(pipeline)",
      '    samplesheet_config = config.get("samplesheet", {})',
      '    supported_types = samplesheet_config.get("input_types", ["fastq"])',
      "",
      "    # Determine input type",
      '    if input_type == "auto":',
      "        input_type = detect_input_type(input_dir)",
      '        print(f"Auto-detected input type: {input_type.upper()}")',
      "",
      "    if input_type not in supported_types:",
      "        return None, ValidationResult(",
      "            valid=False,",
      "            errors=[f\"Pipeline '{pipeline}' does not support {input_type.upper()} input. \"",
      '                    f"Supported: {supported_types}"]',
      "        )",
      "",
      "    # Discover files",
      "    try:",
      "        files = discover_files(input_dir, input_type)",
      "    except ValueError as e:",
      "        return None, ValidationResult(valid=False, errors=[str(e)])",
      "",
      "    if not files:",
      "        return None, ValidationResult(",
      "            valid=False,",
      '            errors=[f"No {input_type.upper()} files found in {input_dir}"],',
      "            suggestions=[",
      '                "Check directory path is correct",',
      '                "Verify file extensions (.fastq.gz, .fq.gz, .bam, .cram)",',
      '                f"Run: ls {input_dir}"',
      "            ]",
      "        )",
      "",
      '    print(f"Found {len(files)} {input_type.upper()} files")',
      "",
      "    # Process based on input type",
      '    if input_type == "fastq":',
      "        rows = _process_fastq_files(files, config, single_end)",
      "    else:",
      "        rows = _process_alignment_files(files, config, input_type)",
      "",
      "    if not rows:",
      "        return None, ValidationResult(",
      "            valid=False,",
      '            errors=["Could not generate any samplesheet rows from files"]',
      "        )",
      "",
      '    print(f"Generated {len(rows)} samplesheet rows")',
      "",
      "    # Pipeline-specific processing",
      '    if pipeline == "sarek":',
      "        rows = _process_sarek_samples(rows, interactive)",
      '    elif pipeline == "atacseq":',
      "        rows = _process_atacseq_samples(rows)",
      "",
      "    # Validate before writing",
      "    validation = validate_samplesheet(rows, pipeline, config)",
      "",
      "    if not validation.valid:",
      '        print("\\nValidation errors:")',
      "        for error in validation.errors:",
      '            print(f"  - {error}")',
      "",
      "        if interactive:",
      '            response = input("\\nProceed anyway? [y/N]: ").strip().lower()',
      "            if response != 'y':",
      "                return None, validation",
      "    elif validation.warnings:",
      '        print("\\nWarnings:")',
      "        for warning in validation.warnings:",
      '            print(f"  - {warning}")',
      "",
      "    # Determine output path",
      '    output_path = output_file or f"samplesheet_{pipeline}.csv"',
      "",
      "    # Write samplesheet",
      "    _write_samplesheet(rows, config, output_path)",
      "",
      '    print(f"\\nGenerated: {output_path}")',
      "    print(f\"  Pipeline: {pipeline} v{config.get('version', 'unknown')}\")",
      "    print(f\"  Samples: {len(set(r.get('sample', r.get('patient', '')) for r in rows))}\")",
      '    print(f"  Rows: {len(rows)}")',
      "",
      "    # Preview",
      "    _print_preview(rows, config)",
      "",
      "    return output_path, validation",
      "",
      "",
      "def _process_fastq_files(files, config: Dict, single_end: bool) -> List[Dict]:",
      '    """Process FASTQ files into samplesheet rows."""',
      "    pairs = match_read_pairs(files)",
      "",
      "    if not pairs:",
      "        return []",
      "",
      "    # Check for unpaired files",
      "    unpaired = [k for k, v in pairs.items() if v.get('r1') and not v.get('r2')]",
      "    if unpaired and not single_end:",
      '        print(f"\\nNote: {len(unpaired)} samples appear to be single-end (no R2)")',
      "",
      "    rows = []",
      '    columns = config.get("samplesheet", {}).get("columns", [])',
      "",
      "    for sample_key, pair_info in sorted(pairs.items()):",
      "        if not pair_info.get('r1'):",
      "            continue  # Skip entries with only R2",
      "",
      "        info = pair_info.get('info', {})",
      "",
      "        row = {",
      "            'sample': info.get('sample', sample_key),",
      "            'fastq_1': str(Path(pair_info['r1']).absolute()),",
      "            'fastq_2': str(Path(pair_info['r2']).absolute()) if pair_info.get('r2') else '',",
      "        }",
      "",
      "        # Add additional info from filename",
      "        if 'patient' in [c['name'] for c in columns]:",
      "            row['patient'] = info.get('patient', info.get('sample', sample_key))",
      "",
      "        if 'lane' in [c['name'] for c in columns]:",
      "            row['lane'] = info.get('lane', 'L001')",
      "",
      "        # Apply defaults from config",
      "        for col in columns:",
      "            if col['name'] not in row and 'default' in col:",
      "                row[col['name']] = col['default']",
      "",
      "        rows.append(row)",
      "",
      "    return rows",
      "",
      "",
      "def _process_alignment_files(files, config: Dict, input_type: str) -> List[Dict]:",
      '    """Process BAM/CRAM files into samplesheet rows."""',
      "    rows = []",
      '    columns = config.get("samplesheet", {}).get("columns", [])',
      "",
      "    for file_info in files:",
      "        # Find index file",
      "        index_path = find_index_file(file_info.path)",
      "",
      "        info = extract_sample_info(file_info.path)",
      "",
      "        row = {",
      "            'sample': info.get('sample', file_info.stem),",
      "            'bam': str(Path(file_info.path).absolute()),",
      "            'bai': str(Path(index_path).absolute()) if index_path else '',",
      "        }",
      "",
      "        # Add patient for sarek",
      "        if 'patient' in [c['name'] for c in columns]:",
      "            row['patient'] = info.get('patient', info.get('sample', file_info.stem))",
      "",
      "        # Apply defaults",
      "        for col in columns:",
      "            if col['name'] not in row and 'default' in col:",
      "                row[col['name']] = col['default']",
      "",
      "        # Warn if no index found",
      "        if not index_path:",
      '            print(f"  Warning: No index found for {file_info.name}")',
      "",
      "        rows.append(row)",
      "",
      "    return rows",
      "",
      "",
      "def _process_sarek_samples(rows: List[Dict], interactive: bool) -> List[Dict]:",
      '    """Process sarek samples: infer and confirm tumor/normal status."""',
      "    # Auto-infer status from sample names",
      "    for row in rows:",
      "        sample_name = row.get('sample', '')",
      "        inferred = infer_tumor_normal_status(sample_name)",
      "        if inferred is not None:",
      "            row['status'] = inferred",
      "",
      "    # Report inference results",
      "    inferred_tumor = [r for r in rows if r.get('status') == 1]",
      "    inferred_normal = [r for r in rows if r.get('status') == 0]",
      "    unknown = [r for r in rows if 'status' not in r]",
      "",
      "    if inferred_tumor or inferred_normal:",
      '        print(f"\\nTumor/normal inference:")',
      '        print(f"  Tumor samples: {len(inferred_tumor)}")',
      '        print(f"  Normal samples: {len(inferred_normal)}")',
      "",
      "    # Handle unknown samples",
      "    if unknown and interactive:",
      '        print(f"\\n{len(unknown)} sample(s) with unknown status:")',
      "        for r in unknown:",
      "            print(f\"  - {r.get('sample')}\")",
      "",
      '        print("\\nSpecify status for each (0=normal, 1=tumor, Enter=skip):")',
      "        for r in unknown:",
      "            response = input(f\"  {r.get('sample')} [0/1/Enter]: \").strip()",
      "            if response in ['0', '1']:",
      "                r['status'] = int(response)",
      "            else:",
      "                r['status'] = 0  # Default to normal",
      '                print(f"    Defaulting to normal (0)")',
      "    elif unknown:",
      "        # Non-interactive: default to normal",
      "        for r in unknown:",
      "            r['status'] = 0",
      "",
      "    return rows",
      "",
      "",
      "def _process_atacseq_samples(rows: List[Dict]) -> List[Dict]:",
      '    """Process ATAC-seq samples: ensure replicate numbers."""',
      "    # Group by sample name",
      "    sample_counts = {}",
      "    for row in rows:",
      "        sample = row.get('sample', '')",
      "        if sample not in sample_counts:",
      "            sample_counts[sample] = 0",
      "        sample_counts[sample] += 1",
      "",
      "    # Assign replicate numbers if not present",
      "    sample_rep = {}",
      "    for row in rows:",
      "        sample = row.get('sample', '')",
      "",
      "        if 'replicate' not in row or not row['replicate']:",
      "            # Try to extract from filename",
      "            extracted = extract_replicate_number(row.get('fastq_1', ''))",
      "            if extracted:",
      "                row['replicate'] = extracted",
      "            else:",
      "                # Auto-assign sequential",
      "                if sample not in sample_rep:",
      "                    sample_rep[sample] = 0",
      "                sample_rep[sample] += 1",
      "                row['replicate'] = sample_rep[sample]",
      "",
      "    return rows",
      "",
      "",
      "def _write_samplesheet(rows: List[Dict], config: Dict, output_path: str):",
      '    """Write samplesheet to CSV file."""',
      '    columns = config.get("samplesheet", {}).get("columns", [])',
      "    column_names = [c['name'] for c in columns]",
      "",
      "    # Filter to columns that have data",
      "    active_columns = [c for c in column_names if any(c in row and row[c] for row in rows)]",
      "",
      "    # Ensure fastq_1/fastq_2 or bam/bai are included",
      "    for required in ['fastq_1', 'bam']:",
      "        if required in column_names and required not in active_columns:",
      "            if any(required in row for row in rows):",
      "                active_columns.append(required)",
      "",
      "    # Maintain original column order",
      "    active_columns = [c for c in column_names if c in active_columns]",
      "",
      "    with open(output_path, 'w') as f:",
      "        f.write(','.join(active_columns) + '\\n')",
      "        for row in rows:",
      "            values = [str(row.get(col, '')) for col in active_columns]",
      "            f.write(','.join(values) + '\\n')",
      "",
      "",
      "def _print_preview(rows: List[Dict], config: Dict):",
      '    """Print preview of generated samplesheet."""',
      '    columns = config.get("samplesheet", {}).get("columns", [])',
      "    column_names = [c['name'] for c in columns]",
      "    active_columns = [c for c in column_names if any(c in row for row in rows)]",
      "",
      '    print(f"\\nPreview (first 3 rows):")',
      "    print(','.join(active_columns))",
      "    for row in rows[:3]:",
      "        values = [str(row.get(col, ''))[:40] for col in active_columns]  # Truncate long paths",
      "        print(','.join(values))",
      "    if len(rows) > 3:",
      '        print(f"... ({len(rows) - 3} more rows)")',
      "",
      "",
      "def validate_existing_samplesheet(csv_path: str, pipeline: str) -> ValidationResult:",
      '    """Validate an existing samplesheet file."""',
      "    import csv",
      "",
      "    if not os.path.exists(csv_path):",
      '        return ValidationResult(valid=False, errors=[f"File not found: {csv_path}"])',
      "",
      "    try:",
      "        with open(csv_path, 'r') as f:",
      "            reader = csv.DictReader(f)",
      "            rows = list(reader)",
      "    except Exception as e:",
      '        return ValidationResult(valid=False, errors=[f"Failed to read CSV: {e}"])',
      "",
      "    if not rows:",
      '        return ValidationResult(valid=False, errors=["Samplesheet is empty"])',
      "",
      "    config = load_pipeline_config(pipeline)",
      "    return validate_samplesheet(rows, pipeline, config)",
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      "        description='Generate nf-core samplesheet from data directory',",
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # Generate samplesheet for RNA-seq",
      "    %(prog)s ./fastqs rnaseq -o samples.csv",
      "",
      "    # Generate samplesheet for sarek from BAM files",
      "    %(prog)s ./bams sarek --input-type bam",
      "",
      "    # Validate existing samplesheet",
      "    %(prog)s --validate samplesheet.csv rnaseq",
      "",
      "Supported pipelines: rnaseq, sarek, atacseq",
      '        """',
      "    )",
      "",
      "    parser.add_argument('input', help='Directory with data files, or CSV path for --validate')",
      "    parser.add_argument('pipeline', help='Pipeline name (rnaseq, sarek, atacseq)')",
      "    parser.add_argument('-o', '--output', help='Output CSV filename')",
      "    parser.add_argument('--input-type', choices=['auto', 'fastq', 'bam', 'cram'],",
      "                        default='auto', help='Input file type (default: auto-detect)')",
      "    parser.add_argument('--single-end', action='store_true',",
      "                        help='Treat as single-end data (suppress pairing warnings)')",
      "    parser.add_argument('--validate', action='store_true',",
      "                        help='Validate existing samplesheet instead of generating')",
      "    parser.add_argument('--no-interactive', action='store_true',",
      "                        help='Non-interactive mode (use defaults)')",
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        if args.validate:",
      "            # Validate existing samplesheet",
      "            result = validate_existing_samplesheet(args.input, args.pipeline)",
      "            if result.valid:",
      '                print(f"✓ Samplesheet is valid for {args.pipeline}")',
      "                if result.warnings:",
      '                    print("\\nWarnings:")',
      "                    for w in result.warnings:",
      '                        print(f"  - {w}")',
      "                sys.exit(0)",
      "            else:",
      '                print(f"✗ Samplesheet validation failed")',
      "                print(result.summary())",
      "                sys.exit(1)",
      "        else:",
      "            # Generate new samplesheet",
      "            if not os.path.isdir(args.input):",
      '                print(f"Error: Not a directory: {args.input}")',
      "                sys.exit(1)",
      "",
      "            output_path, result = generate_samplesheet(",
      "                args.input,",
      "                args.pipeline,",
      "                args.output,",
      "                args.input_type,",
      "                args.single_end,",
      "                interactive=not args.no_interactive",
      "            )",
      "",
      "            if output_path is None:",
      '                print("\\nFailed to generate samplesheet.")',
      "                if result.suggestions:",
      '                    print("\\nSuggestions:")',
      "                    for s in result.suggestions:",
      '                        print(f"  - {s}")',
      "                sys.exit(1)",
      "",
      "            sys.exit(0)",
      "",
      "    except ValueError as e:",
      '        print(f"Error: {e}")',
      "        sys.exit(1)",
      "    except KeyboardInterrupt:",
      '        print("\\nAborted.")',
      "        sys.exit(1)",
      "",
      "",
      "if __name__ == '__main__':",
      "    main()",
      "",
    ].join("\n"),
    "scripts/manage_genomes.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Genome reference management for nf-core pipelines.",
      "",
      "Manages downloading, caching, and accessing genome references from iGenomes.",
      "Supports auto-download when references aren't available locally.",
      "",
      "Usage:",
      "    python manage_genomes.py list",
      "    python manage_genomes.py check GRCh38",
      "    python manage_genomes.py download GRCh38",
      "    python manage_genomes.py params GRCh38",
      '"""',
      "",
      "import argparse",
      "import json",
      "import os",
      "import subprocess",
      "import sys",
      "from pathlib import Path",
      "from typing import Dict, List, Optional",
      "",
      "",
      "# iGenomes reference configuration",
      "IGENOMES = {",
      "    # Human",
      "    'GRCh38': {",
      "        'display_name': 'Human GRCh38/hg38',",
      "        'species': 'Homo sapiens',",
      "        'aliases': ['hg38', 'GRCh38.p14'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Homo_sapiens/NCBI/GRCh38',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    'GRCh37': {",
      "        'display_name': 'Human GRCh37/hg19',",
      "        'species': 'Homo sapiens',",
      "        'aliases': ['hg19', 'GRCh37.p13'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Homo_sapiens/NCBI/GRCh37',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    # Mouse",
      "    'GRCm39': {",
      "        'display_name': 'Mouse GRCm39/mm39',",
      "        'species': 'Mus musculus',",
      "        'aliases': ['mm39', 'GRCm39'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Mus_musculus/Ensembl/GRCm39',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    'GRCm38': {",
      "        'display_name': 'Mouse GRCm38/mm10',",
      "        'species': 'Mus musculus',",
      "        'aliases': ['mm10', 'GRCm38'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Mus_musculus/NCBI/GRCm38',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    # Yeast",
      "    'R64-1-1': {",
      "        'display_name': 'Yeast R64-1-1/sacCer3',",
      "        'species': 'Saccharomyces cerevisiae',",
      "        'aliases': ['sacCer3', 'S288C', 'yeast'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Saccharomyces_cerevisiae/Ensembl/R64-1-1',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    # Fruit fly",
      "    'BDGP6': {",
      "        'display_name': 'Drosophila BDGP6/dm6',",
      "        'species': 'Drosophila melanogaster',",
      "        'aliases': ['dm6', 'BDGP6', 'fly'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Drosophila_melanogaster/Ensembl/BDGP6',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "        }",
      "    },",
      "    # C. elegans",
      "    'WBcel235': {",
      "        'display_name': 'C. elegans WBcel235/ce11',",
      "        'species': 'Caenorhabditis elegans',",
      "        'aliases': ['ce11', 'worm'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Caenorhabditis_elegans/Ensembl/WBcel235',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    # Zebrafish",
      "    'GRCz11': {",
      "        'display_name': 'Zebrafish GRCz11/danRer11',",
      "        'species': 'Danio rerio',",
      "        'aliases': ['danRer11', 'zebrafish'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Danio_rerio/Ensembl/GRCz11',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    'GRCz10': {",
      "        'display_name': 'Zebrafish GRCz10/danRer10',",
      "        'species': 'Danio rerio',",
      "        'aliases': ['danRer10'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Danio_rerio/Ensembl/GRCz10',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "        }",
      "    },",
      "    # Rat",
      "    'Rnor_6.0': {",
      "        'display_name': 'Rat Rnor_6.0/rn6',",
      "        'species': 'Rattus norvegicus',",
      "        'aliases': ['rn6', 'Rnor6', 'rat'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Rattus_norvegicus/Ensembl/Rnor_6.0',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    # Arabidopsis",
      "    'TAIR10': {",
      "        'display_name': 'Arabidopsis TAIR10',",
      "        'species': 'Arabidopsis thaliana',",
      "        'aliases': ['arabidopsis'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Arabidopsis_thaliana/Ensembl/TAIR10',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "            'bwa_index': 'Sequence/BWAIndex/',",
      "            'star_index': 'Sequence/STARIndex/',",
      "        }",
      "    },",
      "    # Chicken",
      "    'GRCg6a': {",
      "        'display_name': 'Chicken GRCg6a/galGal6',",
      "        'species': 'Gallus gallus',",
      "        'aliases': ['galGal6', 'chicken'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Gallus_gallus/Ensembl/GRCg6a',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "        }",
      "    },",
      "    # Dog",
      "    'CanFam3.1': {",
      "        'display_name': 'Dog CanFam3.1/canFam3',",
      "        'species': 'Canis lupus familiaris',",
      "        'aliases': ['canFam3', 'dog'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Canis_familiaris/Ensembl/CanFam3.1',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "        }",
      "    },",
      "    # Pig",
      "    'Sscrofa11.1': {",
      "        'display_name': 'Pig Sscrofa11.1/susScr11',",
      "        'species': 'Sus scrofa',",
      "        'aliases': ['susScr11', 'pig'],",
      "        's3_base': 's3://ngi-igenomes/igenomes/Sus_scrofa/Ensembl/Sscrofa11.1',",
      "        'files': {",
      "            'fasta': 'Sequence/WholeGenomeFasta/genome.fa',",
      "            'gtf': 'Annotation/Genes/genes.gtf',",
      "        }",
      "    },",
      "}",
      "",
      "",
      "def get_cache_dir() -> Path:",
      '    """Get genome cache directory."""',
      "    cache_dir = os.environ.get(",
      "        'NF_CORE_GENOME_CACHE',",
      "        os.path.expanduser('~/.nf-core/genomes')",
      "    )",
      "    return Path(cache_dir)",
      "",
      "",
      "def resolve_genome_id(genome: str) -> Optional[str]:",
      '    """Resolve genome ID from name or alias."""',
      "    # Direct match",
      "    if genome in IGENOMES:",
      "        return genome",
      "",
      "    # Check aliases",
      "    genome_lower = genome.lower()",
      "    for gid, info in IGENOMES.items():",
      "        if genome_lower in [a.lower() for a in info.get('aliases', [])]:",
      "            return gid",
      "",
      "    return None",
      "",
      "",
      "def is_genome_installed(genome_id: str) -> bool:",
      '    """Check if genome is installed locally."""',
      "    cache_dir = get_cache_dir()",
      "    genome_dir = cache_dir / genome_id",
      "",
      "    # Check for fasta as minimum requirement",
      "    fasta_path = genome_dir / 'genome.fa'",
      "    return fasta_path.exists()",
      "",
      "",
      "def get_genome_path(genome_id: str) -> Optional[Path]:",
      '    """Get local path to genome if installed."""',
      "    if not is_genome_installed(genome_id):",
      "        return None",
      "    return get_cache_dir() / genome_id",
      "",
      "",
      "def list_genomes(installed_only: bool = False) -> List[Dict]:",
      '    """List available genomes."""',
      "    result = []",
      "",
      "    for genome_id, info in IGENOMES.items():",
      "        installed = is_genome_installed(genome_id)",
      "",
      "        if installed_only and not installed:",
      "            continue",
      "",
      "        genome_path = get_genome_path(genome_id) if installed else None",
      "",
      "        result.append({",
      "            'id': genome_id,",
      "            'display_name': info['display_name'],",
      "            'species': info['species'],",
      "            'aliases': info.get('aliases', []),",
      "            'installed': installed,",
      "            'path': str(genome_path) if genome_path else None,",
      "        })",
      "",
      "    return result",
      "",
      "",
      "def download_genome(",
      "    genome_id: str,",
      "    components: Optional[List[str]] = None,",
      "    force: bool = False",
      ") -> bool:",
      '    """',
      "    Download genome reference files from iGenomes.",
      "",
      "    Args:",
      "        genome_id: Genome identifier (e.g., GRCh38)",
      "        components: Specific components to download (fasta, gtf, etc.)",
      "        force: Overwrite existing files",
      "",
      "    Returns:",
      "        True if successful",
      '    """',
      "    # Resolve genome ID",
      "    resolved = resolve_genome_id(genome_id)",
      "    if not resolved:",
      '        print(f"Unknown genome: {genome_id}")',
      "        print(f\"Available: {', '.join(IGENOMES.keys())}\")",
      "        return False",
      "",
      "    genome_id = resolved",
      "    info = IGENOMES[genome_id]",
      "",
      "    # Check for AWS CLI",
      "    aws_available = subprocess.run(",
      "        ['which', 'aws'],",
      "        capture_output=True",
      "    ).returncode == 0",
      "",
      "    if not aws_available:",
      '        print("AWS CLI not found. Required for iGenomes download.")',
      '        print("Install with: pip install awscli")',
      '        print("\\nAlternative: Use --genome flag with nf-core pipelines")',
      '        print("which will auto-download references (slower, per-run).")',
      "        return False",
      "",
      "    # Create cache directory",
      "    cache_dir = get_cache_dir()",
      "    genome_dir = cache_dir / genome_id",
      "    genome_dir.mkdir(parents=True, exist_ok=True)",
      "",
      "    # Determine components to download",
      "    if components is None:",
      "        components = ['fasta', 'gtf']  # Minimum required",
      "",
      "    print(f\"Downloading {info['display_name']} to {genome_dir}\")",
      "    print(f\"Components: {', '.join(components)}\")",
      "",
      "    success = True",
      "    for component in components:",
      "        if component not in info.get('files', {}):",
      '            print(f"  Skipping {component}: not available for {genome_id}")',
      "            continue",
      "",
      "        remote_path = info['files'][component]",
      "        s3_path = f\"{info['s3_base']}/{remote_path}\"",
      "",
      "        # Determine local path",
      "        if remote_path.endswith('/'):",
      "            # Directory (e.g., index)",
      "            local_path = genome_dir / component",
      "        else:",
      "            # File",
      "            filename = Path(remote_path).name",
      "            local_path = genome_dir / filename",
      "",
      "        if local_path.exists() and not force:",
      '            print(f"  {component}: Already exists (use --force to overwrite)")',
      "            continue",
      "",
      '        print(f"  Downloading {component}...")',
      "",
      "        # Build AWS command",
      "        cmd = ['aws', 's3', 'cp', '--no-sign-request']",
      "",
      "        if remote_path.endswith('/'):",
      "            cmd.extend(['--recursive', s3_path, str(local_path)])",
      "        else:",
      "            cmd.extend([s3_path, str(local_path)])",
      "",
      "        result = subprocess.run(cmd, capture_output=True, text=True)",
      "",
      "        if result.returncode != 0:",
      '            print(f"  ERROR downloading {component}:")',
      '            print(f"    {result.stderr[:200]}")',
      "            success = False",
      "        else:",
      '            print(f"  {component}: Downloaded successfully")',
      "",
      "    if success:",
      '        print(f"\\nGenome {genome_id} ready at: {genome_dir}")',
      "    else:",
      '        print(f"\\nSome components failed to download.")',
      "",
      "    return success",
      "",
      "",
      "def get_nextflow_params(genome_id: str) -> Dict[str, str]:",
      '    """',
      "    Get Nextflow parameters for a genome.",
      "",
      "    Returns dict with --fasta, --gtf if local,",
      "    or just --genome if using iGenomes key.",
      '    """',
      "    resolved = resolve_genome_id(genome_id)",
      "    if not resolved:",
      "        return {'error': f'Unknown genome: {genome_id}'}",
      "",
      "    genome_id = resolved",
      "",
      "    # Check if installed locally",
      "    genome_path = get_genome_path(genome_id)",
      "",
      "    if genome_path:",
      "        params = {}",
      "",
      "        # Check for local files",
      "        fasta = genome_path / 'genome.fa'",
      "        if fasta.exists():",
      "            params['fasta'] = str(fasta)",
      "",
      "        gtf = genome_path / 'genes.gtf'",
      "        if gtf.exists():",
      "            params['gtf'] = str(gtf)",
      "",
      "        if params:",
      "            return params",
      "",
      "    # Fall back to iGenomes key",
      "    return {'genome': genome_id}",
      "",
      "",
      "def print_genome_list(genomes: List[Dict], output_json: bool = False):",
      '    """Print genome list."""',
      "    if output_json:",
      "        print(json.dumps(genomes, indent=2))",
      "        return",
      "",
      '    print("\\n" + "=" * 50)',
      '    print("  Available Genomes")',
      '    print("=" * 50 + "\\n")',
      "",
      "    for g in genomes:",
      '        status = "\\033[92m[installed]\\033[0m" if g[\'installed\'] else ""',
      "        print(f\"  {g['id']}: {g['display_name']} {status}\")",
      "        print(f\"      Species: {g['species']}\")",
      "        print(f\"      Aliases: {', '.join(g['aliases'])}\")",
      "        if g['path']:",
      "            print(f\"      Path: {g['path']}\")",
      "        print()",
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      "        description='Manage genome references for nf-core pipelines',",
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Commands:",
      "    list              List available genomes",
      "    check <genome>    Check if genome is installed",
      "    download <genome> Download genome from iGenomes",
      "    params <genome>   Get Nextflow parameters for genome",
      "",
      "Examples:",
      "    %(prog)s list",
      "    %(prog)s list --installed",
      "    %(prog)s check GRCh38",
      "    %(prog)s download GRCh38",
      "    %(prog)s download GRCh38 --components fasta gtf star_index",
      "    %(prog)s params GRCh38",
      '        """',
      "    )",
      "",
      "    subparsers = parser.add_subparsers(dest='command', help='Commands')",
      "",
      "    # List command",
      "    list_parser = subparsers.add_parser('list', help='List available genomes')",
      "    list_parser.add_argument('--installed', action='store_true',",
      "                             help='Show only installed genomes')",
      "    list_parser.add_argument('--json', action='store_true',",
      "                             help='Output as JSON')",
      "",
      "    # Check command",
      "    check_parser = subparsers.add_parser('check', help='Check if genome is installed')",
      "    check_parser.add_argument('genome', help='Genome ID (e.g., GRCh38)')",
      "    check_parser.add_argument('--json', action='store_true',",
      "                              help='Output as JSON')",
      "",
      "    # Download command",
      "    dl_parser = subparsers.add_parser('download', help='Download genome from iGenomes')",
      "    dl_parser.add_argument('genome', help='Genome ID (e.g., GRCh38)')",
      "    dl_parser.add_argument('--components', nargs='+',",
      "                           help='Specific components (fasta, gtf, bwa_index, star_index)')",
      "    dl_parser.add_argument('--force', action='store_true',",
      "                           help='Overwrite existing files')",
      "",
      "    # Params command",
      "    params_parser = subparsers.add_parser('params', help='Get Nextflow params for genome')",
      "    params_parser.add_argument('genome', help='Genome ID')",
      "    params_parser.add_argument('--json', action='store_true',",
      "                               help='Output as JSON')",
      "",
      "    args = parser.parse_args()",
      "",
      "    if args.command == 'list':",
      "        genomes = list_genomes(installed_only=args.installed)",
      "        print_genome_list(genomes, args.json)",
      "",
      "    elif args.command == 'check':",
      "        resolved = resolve_genome_id(args.genome)",
      "        if not resolved:",
      '            print(f"Unknown genome: {args.genome}")',
      "            sys.exit(1)",
      "",
      "        installed = is_genome_installed(resolved)",
      "        path = get_genome_path(resolved) if installed else None",
      "",
      "        if args.json:",
      "            print(json.dumps({",
      "                'genome': resolved,",
      "                'installed': installed,",
      "                'path': str(path) if path else None",
      "            }))",
      "        else:",
      "            if installed:",
      '                print(f"✓ Genome {resolved} is installed at: {path}")',
      "            else:",
      '                print(f"✗ Genome {resolved} is not installed locally")',
      '                print(f"  Download with: python {sys.argv[0]} download {resolved}")',
      "",
      "        sys.exit(0 if installed else 1)",
      "",
      "    elif args.command == 'download':",
      "        success = download_genome(args.genome, args.components, args.force)",
      "        sys.exit(0 if success else 1)",
      "",
      "    elif args.command == 'params':",
      "        params = get_nextflow_params(args.genome)",
      "",
      "        if args.json:",
      "            print(json.dumps(params))",
      "        else:",
      "            if 'error' in params:",
      "                print(f\"Error: {params['error']}\")",
      "                sys.exit(1)",
      "",
      "            for key, value in params.items():",
      '                print(f"--{key} {value}")',
      "",
      "    else:",
      "        parser.print_help()",
      "        sys.exit(1)",
      "",
      "",
      "if __name__ == '__main__':",
      "    main()",
      "",
    ].join("\n"),
    "scripts/sra_geo_fetch.py": [
      "#!/usr/bin/env python3",
      '"""',
      "GEO/SRA Data Fetcher",
      "====================",
      "Download raw sequencing data from NCBI GEO/SRA and prepare for nf-core pipelines.",
      "",
      "Usage:",
      "    python sra_geo_fetch.py info <GEO_ID>              # Get study information",
      "    python sra_geo_fetch.py list <GEO_ID>              # List all samples/runs",
      "    python sra_geo_fetch.py download <GEO_ID> -o DIR   # Download FASTQ files",
      "    python sra_geo_fetch.py samplesheet <GEO_ID> ...   # Generate samplesheet",
      "",
      "Examples:",
      "    python sra_geo_fetch.py info GSE110004",
      '    python sra_geo_fetch.py list GSE110004 --filter "RNA-Seq:PAIRED"',
      "    python sra_geo_fetch.py download GSE110004 -o ./fastq --parallel 4",
      "    python sra_geo_fetch.py samplesheet GSE110004 --fastq-dir ./fastq -o samplesheet.csv",
      '"""',
      "",
      "import argparse",
      "import json",
      "import logging",
      "import os",
      "import re",
      "import subprocess",
      "import sys",
      "from concurrent.futures import ThreadPoolExecutor, as_completed",
      "from dataclasses import dataclass, asdict",
      "from pathlib import Path",
      "from typing import Dict, List, Optional, Tuple",
      "",
      "# Add utils to path",
      "sys.path.insert(0, str(Path(__file__).parent))",
      "from utils.ncbi_utils import (",
      "    check_network_access,",
      "    fetch_geo_metadata,",
      "    fetch_sra_study_accession,",
      "    fetch_sra_run_info,",
      "    fetch_sra_run_info_detailed,",
      "    fetch_ena_fastq_urls,",
      "    download_file,",
      "    format_file_size,",
      "    estimate_download_size,",
      "    group_samples_by_type,",
      "    format_sample_groups_table,",
      ")",
      "",
      "# Set up logging",
      "logging.basicConfig(",
      "    level=logging.INFO,",
      "    format='%(message)s'",
      ")",
      "logger = logging.getLogger(__name__)",
      "",
      "# Load genome mapping",
      "SCRIPT_DIR = Path(__file__).parent",
      'GENOMES_FILE = SCRIPT_DIR / "config" / "genomes.yaml"',
      "",
      "",
      "@dataclass",
      "class StudyInfo:",
      '    """Information about a GEO study."""',
      "    geo_id: str",
      "    title: str",
      "    organism: str",
      "    n_samples: int",
      "    summary: str",
      "    sra_study: Optional[str]",
      "    suggested_genome: Optional[str]",
      "    suggested_pipeline: Optional[str]",
      "",
      "",
      "def load_genome_mapping() -> Dict:",
      '    """Load organism to genome mapping from config."""',
      "    if not GENOMES_FILE.exists():",
      "        return {}",
      "",
      "    try:",
      "        import yaml",
      "        with open(GENOMES_FILE) as f:",
      "            config = yaml.safe_load(f)",
      "        return config.get('organisms', {})",
      "    except ImportError:",
      "        # Fallback: parse YAML manually for simple cases",
      "        mapping = {}",
      "        try:",
      "            with open(GENOMES_FILE) as f:",
      "                content = f.read()",
      "            # Simple regex parsing for organism blocks",
      '            pattern = r\'"([^"]+)":\\s*\\n\\s*genome:\\s*"([^"]+)"\'',
      "            for match in re.finditer(pattern, content):",
      "                mapping[match.group(1)] = {'genome': match.group(2)}",
      "        except Exception:",
      "            pass",
      "        return mapping",
      "",
      "",
      "def suggest_genome(organism: str) -> Optional[str]:",
      '    """Suggest a genome based on organism name."""',
      "    genome_map = load_genome_mapping()",
      "",
      "    # Direct match",
      "    if organism in genome_map:",
      "        return genome_map[organism].get('genome')",
      "",
      "    # Case-insensitive search",
      "    organism_lower = organism.lower()",
      "    for org_name, info in genome_map.items():",
      "        if org_name.lower() == organism_lower:",
      "            return info.get('genome')",
      "        # Check aliases",
      "        aliases = info.get('aliases', [])",
      "        if any(alias.lower() == organism_lower for alias in aliases):",
      "            return info.get('genome')",
      "",
      "    # Common fallbacks",
      "    fallbacks = {",
      "        'homo sapiens': 'GRCh38',",
      "        'human': 'GRCh38',",
      "        'mus musculus': 'GRCm39',",
      "        'mouse': 'GRCm39',",
      "        'saccharomyces cerevisiae': 'R64-1-1',",
      "        'yeast': 'R64-1-1',",
      "        'drosophila melanogaster': 'BDGP6',",
      "        'caenorhabditis elegans': 'WBcel235',",
      "        'danio rerio': 'GRCz11',",
      "        'arabidopsis thaliana': 'TAIR10',",
      "        'rattus norvegicus': 'Rnor_6.0',",
      "    }",
      "",
      "    return fallbacks.get(organism_lower)",
      "",
      "",
      "def suggest_pipeline(library_strategy: str, library_source: str = '') -> str:",
      '    """Suggest nf-core pipeline based on library strategy."""',
      "    strategy = library_strategy.upper()",
      "",
      "    pipeline_map = {",
      "        'RNA-SEQ': 'rnaseq',",
      "        'ATAC-SEQ': 'atacseq',",
      "        'CHIP-SEQ': 'chipseq',",
      "        'WGS': 'sarek',",
      "        'WXS': 'sarek',",
      "        'AMPLICON': 'ampliseq',",
      "        'BISULFITE-SEQ': 'methylseq',",
      "        'HI-C': 'hic',",
      "    }",
      "",
      "    return pipeline_map.get(strategy, 'rnaseq')",
      "",
      "",
      "def cmd_info(args):",
      '    """Display study information."""',
      "    geo_id = args.geo_id.upper()",
      "",
      '    print(f"\\nFetching information for {geo_id}...")',
      "",
      "    # Check network",
      "    network_ok, network_msg = check_network_access()",
      "    if not network_ok:",
      '        print(f"\\n⚠️  Network issues detected:\\n{network_msg}")',
      "",
      "    # Get GEO metadata",
      "    metadata = fetch_geo_metadata(geo_id)",
      "    if not metadata:",
      '        print(f"\\n❌ Could not fetch metadata for {geo_id}")',
      "        return 1",
      "",
      "    # Get SRA study accession",
      "    sra_study = fetch_sra_study_accession(geo_id)",
      "",
      "    # Get detailed run info",
      '    print("Fetching SRA run information...")',
      "    runs = fetch_sra_run_info_detailed(geo_id)",
      "    if not runs:",
      "        # Fallback to basic method",
      "        runs = fetch_sra_run_info(geo_id)",
      "",
      "    # Group samples by type",
      "    groups = group_samples_by_type(runs) if runs else {}",
      "",
      "    # Suggest genome and pipeline",
      "    organism = metadata.get('organism', 'Unknown')",
      "    genome = suggest_genome(organism)",
      "",
      "    # Determine primary data type",
      "    primary_strategy = 'RNA-SEQ'",
      "    if groups:",
      "        primary_group = max(groups.items(), key=lambda x: x[1]['count'])",
      "        primary_strategy = primary_group[1]['strategy']",
      "    pipeline = suggest_pipeline(primary_strategy)",
      "",
      "    # Estimate download size",
      "    est_size = estimate_download_size(runs)",
      "",
      "    # Display info",
      '    print("\\n" + "━" * 70)',
      "    print(f\"{geo_id}: {metadata.get('title', 'N/A')}\")",
      '    print("━" * 70)',
      '    print(f"Organism:     {organism}")',
      "    print(f\"Samples:      {metadata.get('n_samples', 'N/A')}\")",
      "    print(f\"SRA Study:    {sra_study or 'Not found'}\")",
      '    print(f"Runs:         {len(runs)}")',
      '    print(f"Est. Size:    ~{format_file_size(est_size)}")',
      "    print(f\"Genome:       {genome or 'Unknown (manual selection required)'}\")",
      '    print(f"Pipeline:     nf-core/{pipeline} (suggested)")',
      "",
      "    # Show sample groups table",
      "    if groups:",
      "        print(format_sample_groups_table(groups))",
      "",
      "    if metadata.get('summary'):",
      "        summary = metadata['summary']",
      "        if len(summary) > 300:",
      '            summary = summary[:297] + "..."',
      '        print(f"\\nSummary:\\n  {summary}")',
      "",
      '    print("━" * 70)',
      "",
      "    # Show download hints",
      "    if len(groups) > 1:",
      '        print("\\n💡 To download a specific subset, use:")',
      "        for key in sorted(groups.keys()):",
      '            print(f"   --subset \\"{key}\\"")',
      "",
      "    # Save study info JSON",
      "    if args.output_json:",
      "        info = {",
      "            'geo_id': geo_id,",
      "            'title': metadata.get('title'),",
      "            'organism': organism,",
      "            'n_samples': metadata.get('n_samples'),",
      "            'sra_study': sra_study,",
      "            'n_runs': len(runs),",
      "            'groups': {k: {**v, 'runs': None, 'gsm_ids': list(v.get('gsm_ids', []))} for k, v in groups.items()},",
      "            'suggested_genome': genome,",
      "            'suggested_pipeline': pipeline,",
      "            'summary': metadata.get('summary'),",
      "        }",
      "        output_path = Path(args.output_json)",
      "        with open(output_path, 'w') as f:",
      "            json.dump(info, f, indent=2)",
      '        print(f"\\n📄 Study info saved to: {output_path}")',
      "",
      "    return 0",
      "",
      "",
      "def cmd_groups(args):",
      '    """Display sample groups in a study for interactive selection."""',
      "    geo_id = args.geo_id.upper()",
      "",
      '    print(f"\\nFetching sample groups for {geo_id}...")',
      "",
      "    # Get detailed run info",
      "    runs = fetch_sra_run_info_detailed(geo_id)",
      "    if not runs:",
      "        runs = fetch_sra_run_info(geo_id)",
      "",
      "    if not runs:",
      '        print(f"\\n❌ No runs found for {geo_id}")',
      "        return 1",
      "",
      "    # Group samples",
      "    groups = group_samples_by_type(runs)",
      "",
      "    print(format_sample_groups_table(groups))",
      "",
      "    # Output for interactive selection",
      '    print("\\n📋 Available groups for --subset option:")',
      "    for i, (key, info) in enumerate(sorted(groups.items(), key=lambda x: -x[1]['count']), 1):",
      "        size_str = format_file_size(info['size_estimate'])",
      '        print(f"  {i}. \\"{key}\\" - {info[\'count\']} samples (~{size_str})")',
      "",
      "    # Save to JSON if requested",
      "    if args.output:",
      "        output_path = Path(args.output)",
      "        output_data = {",
      "            'geo_id': geo_id,",
      "            'groups': {}",
      "        }",
      "        for key, info in groups.items():",
      "            output_data['groups'][key] = {",
      "                'count': info['count'],",
      "                'gsm_range': info['gsm_range'],",
      "                'gsm_ids': info.get('gsm_ids', []),",
      "                'size_estimate': info['size_estimate'],",
      "                'strategy': info['strategy'],",
      "                'layout': info['layout'],",
      "                'srr_ids': [r['srr'] for r in info['runs']],",
      "            }",
      "        with open(output_path, 'w') as f:",
      "            json.dump(output_data, f, indent=2)",
      '        print(f"\\n📄 Groups saved to: {output_path}")',
      "",
      "    return 0",
      "",
      "",
      "def cmd_list(args):",
      '    """List all samples and runs in a study."""',
      "    geo_id = args.geo_id.upper()",
      "",
      '    print(f"\\nFetching run list for {geo_id}...")',
      "",
      "    runs = fetch_sra_run_info(geo_id)",
      "    if not runs:",
      '        print(f"\\n❌ No runs found for {geo_id}")',
      "        return 1",
      "",
      "    # Apply filter if specified",
      "    if args.filter:",
      "        filter_parts = args.filter.split(':')",
      "        strategy_filter = filter_parts[0].upper() if filter_parts else None",
      "        layout_filter = filter_parts[1].upper() if len(filter_parts) > 1 else None",
      "",
      "        filtered = []",
      "        for run in runs:",
      "            if strategy_filter and run.get('library_strategy', '').upper() != strategy_filter:",
      "                continue",
      "            if layout_filter and run.get('layout', '').upper() != layout_filter:",
      "                continue",
      "            filtered.append(run)",
      "        runs = filtered",
      "",
      "    print(f\"\\n{'SRR':<15} {'GSM':<12} {'Layout':<8} {'Strategy':<12} {'Size':>10}\")",
      '    print("-" * 60)',
      "",
      "    for run in runs:",
      "        size = format_file_size(run.get('bases', 0) // 4)",
      "        print(f\"{run['srr']:<15} {run.get('gsm', 'N/A'):<12} {run.get('layout', 'N/A'):<8} \"",
      "              f\"{run.get('library_strategy', 'N/A'):<12} {size:>10}\")",
      "",
      '    print(f"\\nTotal: {len(runs)} runs")',
      "",
      "    # Output as TSV if requested",
      "    if args.output:",
      "        output_path = Path(args.output)",
      "        with open(output_path, 'w') as f:",
      '            f.write("run_accession\\tgsm\\tlayout\\tlibrary_strategy\\tbases\\n")',
      "            for run in runs:",
      "                f.write(f\"{run['srr']}\\t{run.get('gsm', '')}\\t{run.get('layout', '')}\\t\"",
      "                        f\"{run.get('library_strategy', '')}\\t{run.get('bases', 0)}\\n\")",
      '        print(f"\\n📄 Run list saved to: {output_path}")',
      "",
      "    return 0",
      "",
      "",
      "def download_fastq_file(url: str, output_path: Path, timeout: int = 600) -> Tuple[str, bool]:",
      '    """Download a single FASTQ file."""',
      "    filename = output_path.name",
      "    if output_path.exists():",
      "        return filename, True  # Already exists",
      "",
      "    success = download_file(url, output_path, timeout=timeout, show_progress=False)",
      "    return filename, success",
      "",
      "",
      "def interactive_select_group(groups: Dict[str, Dict]) -> Optional[str]:",
      '    """Interactively select a sample group."""',
      "    if len(groups) <= 1:",
      "        return None  # No selection needed",
      "",
      '    print("\\n" + "=" * 60)',
      '    print("  SELECT SAMPLE GROUP TO DOWNLOAD")',
      '    print("=" * 60)',
      "",
      "    sorted_groups = sorted(groups.items(), key=lambda x: -x[1]['count'])",
      "",
      "    for i, (key, info) in enumerate(sorted_groups, 1):",
      "        size_str = format_file_size(info['size_estimate'])",
      "        print(f\"\\n  [{i}] {info['strategy']} ({info['layout'].lower()})\")",
      "        print(f\"      Samples: {info['count']}\")",
      "        print(f\"      GSM: {info['gsm_range']}\")",
      '        print(f"      Size: ~{size_str}")',
      "",
      "    print(f\"\\n  [0] Download ALL ({sum(g['count'] for g in groups.values())} samples)\")",
      '    print("-" * 60)',
      "",
      "    try:",
      '        choice = input("\\nEnter selection (0-{}): ".format(len(sorted_groups))).strip()',
      "        choice_num = int(choice)",
      "",
      "        if choice_num == 0:",
      "            return None  # Download all",
      "        elif 1 <= choice_num <= len(sorted_groups):",
      "            selected_key = sorted_groups[choice_num - 1][0]",
      '            print(f"\\n✓ Selected: {selected_key}")',
      "            return selected_key",
      "        else:",
      '            print("Invalid selection, downloading all.")',
      "            return None",
      "    except (ValueError, EOFError, KeyboardInterrupt):",
      '        print("\\nInvalid input, downloading all.")',
      "        return None",
      "",
      "",
      "def cmd_download(args):",
      '    """Download FASTQ files from ENA."""',
      "    geo_id = args.geo_id.upper()",
      "    output_dir = Path(args.output)",
      "    output_dir.mkdir(parents=True, exist_ok=True)",
      "",
      '    print(f"\\nPreparing download for {geo_id}...")',
      "",
      "    # Get detailed run info (includes BioProject fallback for SuperSeries)",
      '    print("Fetching SRA run information...")',
      "    runs = fetch_sra_run_info_detailed(geo_id)",
      "    if not runs:",
      "        runs = fetch_sra_run_info(geo_id)",
      "",
      "    if not runs:",
      '        print(f"❌ No runs found for {geo_id}")',
      "        return 1",
      "",
      "    # Collect all unique SRA studies from runs (SuperSeries may have multiple)",
      "    sra_studies = set(r.get('sra_study', '') for r in runs if r.get('sra_study'))",
      "    if not sra_studies:",
      '        print(f"❌ Could not find any SRA studies for {geo_id}")',
      "        return 1",
      "",
      "    if len(sra_studies) > 1:",
      "        print(f\"SuperSeries detected with {len(sra_studies)} SRA studies: {', '.join(sorted(sra_studies))}\")",
      "    else:",
      '        print(f"SRA Study: {list(sra_studies)[0]}")',
      "",
      "    # Group samples",
      "    groups = group_samples_by_type(runs)",
      "",
      "    # Show sample groups if multiple types exist",
      "    if len(groups) > 1:",
      "        print(format_sample_groups_table(groups))",
      "",
      "    # Handle subset selection",
      "    selected_subset = args.subset",
      "",
      "    # Interactive mode if multiple groups and no subset specified",
      "    if args.interactive and len(groups) > 1 and not selected_subset:",
      "        selected_subset = interactive_select_group(groups)",
      "",
      "    # Get ENA FASTQ URLs from all SRA studies",
      '    print("\\nFetching FASTQ URLs from ENA...")',
      "    fastq_urls = {}",
      "    for sra_study in sorted(sra_studies):",
      "        study_urls = fetch_ena_fastq_urls(sra_study)",
      "        if study_urls:",
      '            print(f"  {sra_study}: {len(study_urls)} runs")',
      "            fastq_urls.update(study_urls)",
      "",
      "    if not fastq_urls:",
      '        print("❌ No FASTQ URLs found in ENA")',
      '        print("Tip: Try using SRA toolkit directly with prefetch + fasterq-dump")',
      "        return 1",
      "",
      "    # Apply filter if specified",
      "    if selected_subset:",
      "        filter_parts = selected_subset.split(':')",
      "        strategy_filter = filter_parts[0].upper() if filter_parts else None",
      "        layout_filter = filter_parts[1].upper() if len(filter_parts) > 1 else None",
      "",
      "        filtered_srrs = set()",
      "        for run in runs:",
      "            if strategy_filter and run.get('library_strategy', '').upper() != strategy_filter:",
      "                continue",
      "            if layout_filter and run.get('layout', '').upper() != layout_filter:",
      "                continue",
      "            filtered_srrs.add(run['srr'])",
      "",
      "        fastq_urls = {srr: urls for srr, urls in fastq_urls.items() if srr in filtered_srrs}",
      '        print(f"\\n📦 Filtered to {len(fastq_urls)} runs matching \\"{selected_subset}\\"")',
      "",
      "    # Count files to download",
      "    total_files = sum(len(urls) for urls in fastq_urls.values())",
      '    print(f"\\n📦 Found {len(fastq_urls)} runs, {total_files} FASTQ files to download")',
      "",
      "    # Check for existing files",
      "    existing = 0",
      "    downloads_needed = []",
      "    for srr, urls in fastq_urls.items():",
      "        for url in urls:",
      "            filename = url.split('/')[-1]",
      "            filepath = output_dir / filename",
      "            if filepath.exists():",
      "                existing += 1",
      "            else:",
      "                downloads_needed.append((url, filepath))",
      "",
      "    if existing:",
      '        print(f"  ✓ {existing} files already exist, skipping")',
      "",
      "    if not downloads_needed:",
      '        print("\\n✅ All files already downloaded!")',
      "        return 0",
      "",
      '    print(f"  ↓ {len(downloads_needed)} files to download")',
      "    print()",
      "",
      "    # Download files",
      "    successful = 0",
      "    failed = []",
      "",
      "    if args.parallel > 1:",
      "        # Parallel download",
      "        with ThreadPoolExecutor(max_workers=args.parallel) as executor:",
      "            futures = {",
      "                executor.submit(download_fastq_file, url, filepath): filepath",
      "                for url, filepath in downloads_needed",
      "            }",
      "",
      "            for i, future in enumerate(as_completed(futures), 1):",
      "                filepath = futures[future]",
      "                filename, success = future.result()",
      '                status = "✓" if success else "✗"',
      '                print(f"  [{i}/{len(downloads_needed)}] {status} {filename}")',
      "                if success:",
      "                    successful += 1",
      "                else:",
      "                    failed.append(filename)",
      "    else:",
      "        # Sequential download",
      "        for i, (url, filepath) in enumerate(downloads_needed, 1):",
      "            filename = filepath.name",
      '            print(f"  [{i}/{len(downloads_needed)}] Downloading {filename}...")',
      "            success = download_file(url, filepath, timeout=args.timeout)",
      "            if success:",
      "                successful += 1",
      '                print(f"    ✓ Done")',
      "            else:",
      "                failed.append(filename)",
      '                print(f"    ✗ Failed")',
      "",
      '    print(f"\\n📊 Download summary:")',
      '    print(f"  ✓ Successful: {successful + existing}")',
      '    print(f"  ✗ Failed: {len(failed)}")',
      "",
      "    if failed:",
      '        print(f"\\nFailed downloads:")',
      "        for f in failed:",
      '            print(f"  - {f}")',
      "        return 1",
      "",
      '    print(f"\\n✅ All files downloaded to: {output_dir}")',
      "",
      "    # Save metadata",
      '    metadata_path = output_dir / "download_metadata.json"',
      "    metadata = {",
      "        'geo_id': geo_id,",
      "        'sra_studies': sorted(sra_studies),",
      "        'n_runs': len(fastq_urls),",
      "        'n_files': total_files,",
      "        'output_dir': str(output_dir.absolute()),",
      "    }",
      "    with open(metadata_path, 'w') as f:",
      "        json.dump(metadata, f, indent=2)",
      "",
      "    return 0",
      "",
      "",
      "def cmd_samplesheet(args):",
      '    """Generate samplesheet for nf-core pipeline."""',
      "    geo_id = args.geo_id.upper()",
      "    fastq_dir = Path(args.fastq_dir)",
      "    output_path = Path(args.output)",
      "",
      '    print(f"\\nGenerating samplesheet for {geo_id}...")',
      "",
      "    # Get run info",
      "    runs = fetch_sra_run_info(geo_id)",
      "    if not runs:",
      '        print(f"❌ No runs found for {geo_id}")',
      "        return 1",
      "",
      "    # Get GEO metadata for sample naming",
      "    metadata = fetch_geo_metadata(geo_id)",
      "    organism = metadata.get('organism', 'Unknown') if metadata else 'Unknown'",
      "    genome = suggest_genome(organism)",
      "",
      "    # Detect pipeline from data",
      "    strategies = set(r.get('library_strategy', 'RNA-SEQ') for r in runs)",
      "    primary_strategy = list(strategies)[0] if strategies else 'RNA-SEQ'",
      "    pipeline = args.pipeline or suggest_pipeline(primary_strategy)",
      "",
      "    # Map SRR to local FASTQ files",
      "    samples = []",
      "    for run in runs:",
      "        srr = run['srr']",
      "        layout = run.get('layout', 'PAIRED')",
      "",
      "        # Find FASTQ files",
      "        if layout == 'PAIRED':",
      '            r1 = fastq_dir / f"{srr}_1.fastq.gz"',
      '            r2 = fastq_dir / f"{srr}_2.fastq.gz"',
      "            if not r1.exists() or not r2.exists():",
      '                logger.warning(f"FASTQ files not found for {srr}")',
      "                continue",
      "            samples.append({",
      "                'srr': srr,",
      "                'gsm': run.get('gsm', ''),",
      "                'fastq_1': str(r1.absolute()),",
      "                'fastq_2': str(r2.absolute()),",
      "                'layout': 'PAIRED',",
      "            })",
      "        else:",
      '            r1 = fastq_dir / f"{srr}.fastq.gz"',
      "            if not r1.exists():",
      '                r1 = fastq_dir / f"{srr}_1.fastq.gz"',
      "            if not r1.exists():",
      '                logger.warning(f"FASTQ file not found for {srr}")',
      "                continue",
      "            samples.append({",
      "                'srr': srr,",
      "                'gsm': run.get('gsm', ''),",
      "                'fastq_1': str(r1.absolute()),",
      "                'fastq_2': '',",
      "                'layout': 'SINGLE',",
      "            })",
      "",
      "    if not samples:",
      '        print(f"❌ No FASTQ files found in {fastq_dir}")',
      "        return 1",
      "",
      "    # Generate sample names",
      "    # Try to infer meaningful names from GSM IDs or use SRR",
      "    sample_names = {}",
      "    for sample in samples:",
      "        # Default to SRR accession",
      "        sample_names[sample['srr']] = sample['srr']",
      "",
      "    # Write samplesheet",
      "    with open(output_path, 'w') as f:",
      "        if pipeline == 'rnaseq':",
      '            f.write("sample,fastq_1,fastq_2,strandedness\\n")',
      "            for sample in samples:",
      "                name = sample_names[sample['srr']]",
      "                f.write(f\"{name},{sample['fastq_1']},{sample['fastq_2']},auto\\n\")",
      "        elif pipeline == 'atacseq':",
      '            f.write("sample,fastq_1,fastq_2,replicate\\n")',
      "            for i, sample in enumerate(samples, 1):",
      "                name = sample_names[sample['srr']]",
      "                f.write(f\"{name},{sample['fastq_1']},{sample['fastq_2']},1\\n\")",
      "        else:",
      "            # Generic format",
      '            f.write("sample,fastq_1,fastq_2\\n")',
      "            for sample in samples:",
      "                name = sample_names[sample['srr']]",
      "                f.write(f\"{name},{sample['fastq_1']},{sample['fastq_2']}\\n\")",
      "",
      '    print(f"\\n✅ Generated samplesheet: {output_path}")',
      '    print(f"   Samples: {len(samples)}")',
      '    print(f"   Pipeline: nf-core/{pipeline}")',
      "    if genome:",
      '        print(f"   Genome: {genome}")',
      "",
      '    print(f"\\n💡 Suggested command:")',
      '    print(f"   nextflow run nf-core/{pipeline} \\\\")',
      '    print(f"       --input {output_path} \\\\")',
      '    print(f"       --outdir results \\\\")',
      "    if genome:",
      '        print(f"       --genome {genome} \\\\")',
      '    print(f"       -profile docker")',
      "",
      "    return 0",
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Download GEO/SRA data and prepare for nf-core pipelines",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "  %(prog)s info GSE110004                    # Get study info with sample groups",
      "  %(prog)s groups GSE110004                  # Show sample groups for selection",
      "  %(prog)s list GSE110004 --filter RNA-Seq   # List RNA-seq runs",
      "  %(prog)s download GSE110004 -o ./fastq -i  # Download with interactive selection",
      '  %(prog)s download GSE110004 -o ./fastq --subset "RNA-Seq:PAIRED"',
      "  %(prog)s samplesheet GSE110004 \\\\",
      "      --fastq-dir ./fastq -o samplesheet.csv # Generate samplesheet",
      '        """',
      "    )",
      "",
      "    subparsers = parser.add_subparsers(dest='command', help='Commands')",
      "",
      "    # info command",
      "    info_parser = subparsers.add_parser('info', help='Display study information with sample groups')",
      "    info_parser.add_argument('geo_id', help='GEO accession (e.g., GSE110004)')",
      "    info_parser.add_argument('--output-json', '-o', help='Save info to JSON file')",
      "",
      "    # groups command",
      "    groups_parser = subparsers.add_parser('groups', help='Show sample groups for interactive selection')",
      "    groups_parser.add_argument('geo_id', help='GEO accession')",
      "    groups_parser.add_argument('--output', '-o', help='Save groups to JSON file')",
      "",
      "    # list command",
      "    list_parser = subparsers.add_parser('list', help='List samples and runs')",
      "    list_parser.add_argument('geo_id', help='GEO accession')",
      "    list_parser.add_argument('--filter', '-f', help='Filter by strategy:layout (e.g., RNA-Seq:PAIRED)')",
      "    list_parser.add_argument('--output', '-o', help='Save to TSV file')",
      "",
      "    # download command",
      "    dl_parser = subparsers.add_parser('download', help='Download FASTQ files')",
      "    dl_parser.add_argument('geo_id', help='GEO accession')",
      "    dl_parser.add_argument('--output', '-o', required=True, help='Output directory')",
      "    dl_parser.add_argument('--subset', '-s', help='Filter subset (e.g., RNA-Seq:PAIRED)')",
      "    dl_parser.add_argument('--interactive', '-i', action='store_true',",
      "                           help='Interactively select sample group to download')",
      "    dl_parser.add_argument('--parallel', '-p', type=int, default=4, help='Parallel downloads')",
      "    dl_parser.add_argument('--timeout', '-t', type=int, default=600, help='Download timeout (sec)')",
      "",
      "    # samplesheet command",
      "    ss_parser = subparsers.add_parser('samplesheet', help='Generate samplesheet')",
      "    ss_parser.add_argument('geo_id', help='GEO accession')",
      "    ss_parser.add_argument('--fastq-dir', '-f', required=True, help='Directory with FASTQ files')",
      "    ss_parser.add_argument('--output', '-o', default='samplesheet.csv', help='Output samplesheet')",
      "    ss_parser.add_argument('--pipeline', '-p', help='Target pipeline (auto-detected if not specified)')",
      "",
      "    args = parser.parse_args()",
      "",
      "    if not args.command:",
      "        parser.print_help()",
      "        return 1",
      "",
      "    commands = {",
      "        'info': cmd_info,",
      "        'groups': cmd_groups,",
      "        'list': cmd_list,",
      "        'download': cmd_download,",
      "        'samplesheet': cmd_samplesheet,",
      "    }",
      "",
      "    return commands[args.command](args)",
      "",
      "",
      "if __name__ == '__main__':",
      "    sys.exit(main())",
      "",
    ].join("\n"),
    "scripts/utils/__init__.py": [
      '"""',
      "Utility modules for nf-core pipeline deployment.",
      "",
      "Modules:",
      "    ncbi_utils: NCBI/GEO/SRA data fetching and download utilities",
      "    file_discovery: Find FASTQ, BAM, and CRAM files",
      "    sample_inference: Extract sample info, detect tumor/normal",
      "    validators: Validate samplesheets before writing",
      '"""',
      "",
      "# NCBI utilities for GEO/SRA data acquisition",
      "from .ncbi_utils import (",
      "    check_network_access,",
      "    fetch_geo_metadata,",
      "    fetch_sra_study_accession,",
      "    fetch_sra_run_info,",
      "    fetch_sra_run_info_detailed,",
      "    fetch_bioproject_from_geo,",
      "    fetch_ena_fastq_urls,",
      "    download_file,",
      "    fetch_pubmed_metadata,",
      "    format_file_size,",
      "    estimate_download_size,",
      "    group_samples_by_type,",
      "    format_sample_groups_table,",
      ")",
      "",
      "# File discovery utilities",
      "from .file_discovery import discover_files, FileInfo, count_files_by_type",
      "",
      "# Sample inference utilities",
      "from .sample_inference import (",
      "    extract_sample_info,",
      "    infer_tumor_normal_status,",
      "    match_read_pairs,",
      "    extract_replicate_number",
      ")",
      "",
      "# Validation utilities",
      "from .validators import validate_samplesheet, ValidationResult",
      "",
      "__all__ = [",
      "    # ncbi_utils",
      "    'check_network_access',",
      "    'fetch_geo_metadata',",
      "    'fetch_sra_study_accession',",
      "    'fetch_sra_run_info',",
      "    'fetch_sra_run_info_detailed',",
      "    'fetch_bioproject_from_geo',",
      "    'fetch_ena_fastq_urls',",
      "    'download_file',",
      "    'fetch_pubmed_metadata',",
      "    'format_file_size',",
      "    'estimate_download_size',",
      "    'group_samples_by_type',",
      "    'format_sample_groups_table',",
      "    # file_discovery",
      "    'discover_files',",
      "    'FileInfo',",
      "    'count_files_by_type',",
      "    # sample_inference",
      "    'extract_sample_info',",
      "    'infer_tumor_normal_status',",
      "    'match_read_pairs',",
      "    'extract_replicate_number',",
      "    # validators",
      "    'validate_samplesheet',",
      "    'ValidationResult',",
      "]",
      "",
    ].join("\n"),
    "scripts/utils/file_discovery.py": [
      '"""',
      "File discovery utilities for FASTQ, BAM, and CRAM files.",
      "",
      "This module provides functions to recursively discover sequencing data files",
      "in a directory structure.",
      '"""',
      "",
      "import os",
      "from dataclasses import dataclass",
      "from pathlib import Path",
      "from typing import Dict, List, Optional",
      "",
      "",
      "@dataclass",
      "class FileInfo:",
      '    """Information about a discovered file."""',
      "    path: str",
      "    name: str",
      "    stem: str",
      "    extension: str",
      "    size: int",
      "    file_type: str  # fastq, bam, cram",
      "",
      "    def __repr__(self):",
      '        return f"FileInfo({self.name}, type={self.file_type})"',
      "",
      "",
      "# Supported file extensions by type",
      "EXTENSIONS = {",
      '    "fastq": [".fastq.gz", ".fq.gz", ".fastq", ".fq"],',
      '    "bam": [".bam"],',
      '    "cram": [".cram"],',
      "}",
      "",
      "# Index file extensions",
      "INDEX_EXTENSIONS = {",
      '    "bam": [".bam.bai", ".bai"],',
      '    "cram": [".cram.crai", ".crai"],',
      "}",
      "",
      "",
      "def discover_files(",
      "    directory: str,",
      '    file_type: str = "fastq",',
      "    follow_symlinks: bool = True",
      ") -> List[FileInfo]:",
      '    """',
      "    Recursively discover files of specified type.",
      "",
      "    Args:",
      "        directory: Root directory to search",
      "        file_type: One of 'fastq', 'bam', 'cram'",
      "        follow_symlinks: Whether to follow symbolic links",
      "",
      "    Returns:",
      "        List of FileInfo objects sorted by path",
      '    """',
      "    if file_type not in EXTENSIONS:",
      '        raise ValueError(f"Unknown file type: {file_type}. Supported: {list(EXTENSIONS.keys())}")',
      "",
      "    directory = os.path.abspath(directory)",
      "    if not os.path.isdir(directory):",
      '        raise ValueError(f"Not a directory: {directory}")',
      "",
      "    extensions = EXTENSIONS[file_type]",
      "    files = []",
      "    seen_paths = set()  # Avoid duplicates from symlinks",
      "",
      "    for root, _, filenames in os.walk(directory, followlinks=follow_symlinks):",
      "        for filename in filenames:",
      "            # Check each extension",
      "            for ext in extensions:",
      "                if filename.lower().endswith(ext.lower()):",
      "                    full_path = os.path.join(root, filename)",
      "",
      "                    # Resolve to handle symlinks",
      "                    try:",
      "                        real_path = os.path.realpath(full_path)",
      "                    except OSError:",
      "                        real_path = full_path",
      "",
      "                    if real_path in seen_paths:",
      "                        continue",
      "                    seen_paths.add(real_path)",
      "",
      "                    try:",
      "                        size = os.path.getsize(full_path)",
      "                    except OSError:",
      "                        size = 0",
      "",
      "                    # Extract stem (remove extension)",
      "                    stem = filename",
      "                    for e in extensions:",
      "                        if stem.lower().endswith(e.lower()):",
      "                            stem = stem[:-len(e)]",
      "                            break",
      "",
      "                    files.append(FileInfo(",
      "                        path=full_path,",
      "                        name=filename,",
      "                        stem=stem,",
      "                        extension=ext,",
      "                        size=size,",
      "                        file_type=file_type",
      "                    ))",
      "                    break  # Found matching extension, no need to check others",
      "",
      "    return sorted(files, key=lambda f: f.path)",
      "",
      "",
      "def count_files_by_type(directory: str) -> Dict[str, int]:",
      '    """',
      "    Count files by type in directory.",
      "",
      "    Args:",
      "        directory: Directory to scan",
      "",
      "    Returns:",
      "        Dict mapping file_type to count",
      '    """',
      "    counts = {}",
      "    for file_type in EXTENSIONS:",
      "        try:",
      "            files = discover_files(directory, file_type)",
      "            counts[file_type] = len(files)",
      "        except (ValueError, PermissionError):",
      "            counts[file_type] = 0",
      "    return counts",
      "",
      "",
      "def find_index_file(alignment_file: str) -> Optional[str]:",
      '    """',
      "    Find index file for a BAM or CRAM file.",
      "",
      "    Args:",
      "        alignment_file: Path to BAM or CRAM file",
      "",
      "    Returns:",
      "        Path to index file if found, None otherwise",
      '    """',
      "    path = Path(alignment_file)",
      "",
      "    # Determine file type",
      '    if path.suffix.lower() == ".bam":',
      '        index_exts = INDEX_EXTENSIONS["bam"]',
      '    elif path.suffix.lower() == ".cram":',
      '        index_exts = INDEX_EXTENSIONS["cram"]',
      "    else:",
      "        return None",
      "",
      "    # Try common index file patterns",
      "    for ext in index_exts:",
      "        # Pattern: file.bam.bai or file.bai",
      '        if ext.startswith(".bam") or ext.startswith(".cram"):',
      '            candidate = Path(str(path) + ext.split(".")[-1])',
      "        else:",
      "            candidate = path.with_suffix(ext)",
      "",
      "        if candidate.exists():",
      "            return str(candidate)",
      "",
      "        # Also try: file.bam -> file.bam.bai",
      '        candidate = Path(str(path) + "." + ext.lstrip("."))',
      "        if candidate.exists():",
      "            return str(candidate)",
      "",
      "    return None",
      "",
      "",
      "def detect_input_type(directory: str) -> str:",
      '    """',
      "    Auto-detect predominant input file type in directory.",
      "",
      "    Prioritizes: FASTQ > BAM > CRAM",
      "",
      "    Args:",
      "        directory: Directory to scan",
      "",
      "    Returns:",
      "        Detected file type ('fastq', 'bam', or 'cram')",
      '    """',
      "    counts = count_files_by_type(directory)",
      "",
      "    # Prioritize by preference",
      '    for file_type in ["fastq", "bam", "cram"]:',
      "        if counts.get(file_type, 0) > 0:",
      "            return file_type",
      "",
      '    return "fastq"  # Default',
      "",
    ].join("\n"),
    "scripts/utils/ncbi_utils.py": [
      "#!/usr/bin/env python3",
      '"""',
      "NCBI Utilities for GEO/SRA Data Access",
      "======================================",
      "Shared utilities for fetching metadata and downloading data from NCBI services.",
      '"""',
      "",
      "import json",
      "import logging",
      "import re",
      "import shutil",
      "import time",
      "from pathlib import Path",
      "from typing import Dict, List, Optional, Tuple",
      "from urllib.request import Request, urlopen",
      "from urllib.error import URLError, HTTPError",
      "",
      "# Set up logging",
      "logging.basicConfig(",
      "    level=logging.INFO,",
      "    format='%(asctime)s - %(levelname)s - %(message)s'",
      ")",
      "logger = logging.getLogger(__name__)",
      "",
      "# NCBI rate limiting - track last request time",
      "_last_ncbi_request_time = 0.0",
      "_NCBI_MIN_DELAY = 0.34  # 3 requests per second max without API key",
      "",
      "",
      "def _rate_limit_ncbi():",
      '    """Enforce NCBI rate limit of 3 requests/second."""',
      "    global _last_ncbi_request_time",
      "    current_time = time.time()",
      "    elapsed = current_time - _last_ncbi_request_time",
      "    if elapsed < _NCBI_MIN_DELAY:",
      "        time.sleep(_NCBI_MIN_DELAY - elapsed)",
      "    _last_ncbi_request_time = time.time()",
      "",
      "",
      "# Try to import requests for better HTTP handling",
      "try:",
      "    import requests",
      "    HAS_REQUESTS = True",
      "except ImportError:",
      "    HAS_REQUESTS = False",
      '    logger.debug("requests not installed - using urllib fallback")',
      "",
      "",
      "def check_network_access() -> Tuple[bool, str]:",
      '    """',
      "    Check if NCBI/ENA servers are accessible.",
      "",
      "    Returns:",
      "        Tuple of (success, message)",
      '    """',
      "    test_urls = [",
      '        ("NCBI Entrez", "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi"),',
      '        ("NCBI FTP", "https://ftp.ncbi.nlm.nih.gov/"),',
      '        ("ENA API", "https://www.ebi.ac.uk/ena/portal/api/"),',
      "    ]",
      "",
      "    results = []",
      "    for name, url in test_urls:",
      "        try:",
      "            if HAS_REQUESTS:",
      "                # Use GET instead of HEAD - NCBI Entrez returns 405 for HEAD",
      "                response = requests.get(url, timeout=10)",
      "                success = response.status_code < 400",
      "            else:",
      "                req = Request(url, headers={'User-Agent': 'geo-sra-skill/1.0'})",
      "                with urlopen(req, timeout=10) as response:",
      "                    success = True",
      "            results.append((name, success, None))",
      "        except Exception as e:",
      "            results.append((name, False, str(e)))",
      "",
      "    all_success = all(r[1] for r in results)",
      "",
      "    msg_parts = []",
      "    for name, success, error in results:",
      '        status = "✓" if success else "✗"',
      "        msg_parts.append(f\"  {status} {name}: {'OK' if success else error or 'Failed'}\")",
      "",
      '    return all_success, "\\n".join(msg_parts)',
      "",
      "",
      "def fetch_geo_metadata(geo_id: str) -> Optional[Dict]:",
      '    """',
      "    Fetch GEO study metadata using NCBI Entrez E-utilities.",
      "",
      "    Args:",
      "        geo_id: GEO accession (e.g., 'GSE110004')",
      "",
      "    Returns:",
      "        Dict with study metadata or None if failed",
      '    """',
      "    try:",
      "        # Use esearch to get GEO UID",
      '        search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=gds&term={geo_id}[Accession]&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(search_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(search_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        id_list = data.get('esearchresult', {}).get('idlist', [])",
      "        if not id_list:",
      '            logger.warning(f"No GEO entry found for {geo_id}")',
      "            return None",
      "",
      "        # Use esummary to get metadata",
      "        uid = id_list[0]",
      '        summary_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=gds&id={uid}&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(summary_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(summary_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        result = data.get('result', {}).get(uid, {})",
      "",
      "        return {",
      "            'geo_id': geo_id,",
      "            'title': result.get('title', 'N/A'),",
      "            'summary': result.get('summary', 'N/A'),",
      "            'organism': result.get('taxon', 'N/A'),",
      "            'n_samples': result.get('n_samples', 0),",
      "            'gpl': result.get('gpl', 'N/A'),",
      "            'entrytype': result.get('entrytype', 'N/A'),",
      "            'pubmed_ids': result.get('pubmedids', []),",
      "        }",
      "",
      "    except Exception as e:",
      '        logger.error(f"Error fetching GEO metadata for {geo_id}: {e}")',
      "        return None",
      "",
      "",
      "def fetch_sra_study_accession(geo_id: str) -> Optional[str]:",
      '    """',
      "    Get the SRA study accession (SRPxxxxxx) for a GEO accession.",
      "",
      "    Args:",
      "        geo_id: GEO accession (e.g., 'GSE110004')",
      "",
      "    Returns:",
      "        SRA study accession (e.g., 'SRP126328') or None",
      '    """',
      "    try:",
      "        # Search for SRA study linked to GEO",
      '        search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=sra&term={geo_id}[GEO]&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(search_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(search_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        id_list = data.get('esearchresult', {}).get('idlist', [])",
      "        if not id_list:",
      "            return None",
      "",
      "        # Get summary to extract SRP accession",
      "        uid = id_list[0]",
      '        summary_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=sra&id={uid}&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(summary_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(summary_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        result = data.get('result', {}).get(uid, {})",
      "        exp_xml = result.get('expxml', '')",
      "",
      "        # Extract SRP from the XML",
      "        srp_match = re.search(r'<Study acc=\"(SRP\\d+)\"', exp_xml)",
      "        if srp_match:",
      "            return srp_match.group(1)",
      "",
      "        return None",
      "",
      "    except Exception as e:",
      '        logger.debug(f"Error fetching SRA study for {geo_id}: {e}")',
      "        return None",
      "",
      "",
      "def fetch_sra_run_info(geo_id: str, bioproject: Optional[str] = None) -> List[Dict]:",
      '    """',
      "    Fetch SRA run information for all samples in a GEO study.",
      "",
      "    Args:",
      "        geo_id: GEO accession (e.g., 'GSE110004')",
      "        bioproject: Optional BioProject accession for fallback search",
      "",
      "    Returns:",
      "        List of dicts with run info (srr, gsm, layout, library_strategy, etc.)",
      '    """',
      "    runs = []",
      "",
      "    try:",
      "        # First get the BioProject accession",
      '        search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=sra&term={geo_id}[GEO]&retmax=1000&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(search_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(search_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        id_list = data.get('esearchresult', {}).get('idlist', [])",
      "",
      "        # If no results, try BioProject fallback",
      "        if not id_list:",
      "            if not bioproject:",
      "                bioproject = fetch_bioproject_from_geo(geo_id)",
      "",
      "            if bioproject:",
      '                logger.info(f"Using BioProject {bioproject} for {geo_id}")',
      '                search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=sra&term={bioproject}&retmax=1000&retmode=json"',
      "",
      "                _rate_limit_ncbi()",
      "                if HAS_REQUESTS:",
      "                    response = requests.get(search_url, timeout=30)",
      "                    data = response.json()",
      "                else:",
      "                    with urlopen(search_url, timeout=30) as response:",
      "                        data = json.loads(response.read().decode())",
      "",
      "                id_list = data.get('esearchresult', {}).get('idlist', [])",
      "",
      "        if not id_list:",
      '            logger.warning(f"No SRA entries found for {geo_id}")',
      "            return runs",
      "",
      "        # Batch fetch summaries",
      "        ids_str = ','.join(id_list)",
      '        summary_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=sra&id={ids_str}&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(summary_url, timeout=60)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(summary_url, timeout=60) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        result = data.get('result', {})",
      "",
      "        for uid in id_list:",
      "            entry = result.get(uid, {})",
      "            if not entry:",
      "                continue",
      "",
      "            exp_xml = entry.get('expxml', '')",
      "            runs_xml = entry.get('runs', '')",
      "",
      "            # Extract metadata from XML",
      "            layout_match = re.search(r'<LIBRARY_LAYOUT>\\s*<(\\w+)', exp_xml)",
      "            strategy_match = re.search(r'<LIBRARY_STRATEGY>(\\w+)', exp_xml)",
      "            source_match = re.search(r'<LIBRARY_SOURCE>(\\w+)', exp_xml)",
      "            gsm_match = re.search(r'<Sample acc=\"(GSM\\d+)\"', exp_xml)",
      "            srx_match = re.search(r'<Experiment acc=\"(SRX\\d+)\"', exp_xml)",
      "",
      "            # Extract run accessions",
      '            srr_matches = re.findall(r\'<Run acc="(SRR\\d+)"[^>]*total_spots="(\\d+)"[^>]*total_bases="(\\d+)"\', runs_xml)',
      "",
      "            for srr, spots, bases in srr_matches:",
      "                runs.append({",
      "                    'srr': srr,",
      "                    'srx': srx_match.group(1) if srx_match else '',",
      "                    'gsm': gsm_match.group(1) if gsm_match else '',",
      "                    'layout': layout_match.group(1).upper() if layout_match else 'UNKNOWN',",
      "                    'library_strategy': strategy_match.group(1) if strategy_match else 'UNKNOWN',",
      "                    'library_source': source_match.group(1) if source_match else 'UNKNOWN',",
      "                    'spots': int(spots),",
      "                    'bases': int(bases),",
      "                })",
      "",
      "        return runs",
      "",
      "    except Exception as e:",
      '        logger.error(f"Error fetching SRA run info for {geo_id}: {e}")',
      "        return runs",
      "",
      "",
      "def fetch_ena_fastq_urls(study_accession: str) -> Dict[str, List[str]]:",
      '    """',
      "    Get FASTQ download URLs from ENA for an SRA study.",
      "",
      "    ENA provides faster downloads than SRA with pre-split paired files.",
      "",
      "    Args:",
      "        study_accession: SRA study accession (e.g., 'SRP126328')",
      "",
      "    Returns:",
      "        Dict mapping SRR accession to list of FASTQ URLs",
      '    """',
      "    fastq_urls = {}",
      "",
      "    try:",
      "        # Query ENA API",
      '        ena_url = f"https://www.ebi.ac.uk/ena/portal/api/filereport?accession={study_accession}&result=read_run&fields=run_accession,sample_alias,fastq_ftp&format=tsv"',
      "",
      "        if HAS_REQUESTS:",
      "            response = requests.get(ena_url, timeout=60)",
      "            content = response.text",
      "        else:",
      "            with urlopen(ena_url, timeout=60) as response:",
      "                content = response.read().decode()",
      "",
      "        lines = content.strip().split('\\n')",
      "        if len(lines) < 2:",
      '            logger.warning(f"No FASTQ URLs found in ENA for {study_accession}")',
      "            return fastq_urls",
      "",
      "        # Parse TSV",
      "        header = lines[0].split('\\t')",
      "        run_idx = header.index('run_accession') if 'run_accession' in header else 0",
      "        ftp_idx = header.index('fastq_ftp') if 'fastq_ftp' in header else 2",
      "",
      "        for line in lines[1:]:",
      "            if not line.strip():",
      "                continue",
      "            fields = line.split('\\t')",
      "            if len(fields) > max(run_idx, ftp_idx):",
      "                srr = fields[run_idx]",
      "                ftp_urls = fields[ftp_idx]",
      "                if ftp_urls:",
      "                    # URLs are semicolon-separated, convert to HTTP URLs",
      "                    # ENA supports both FTP and HTTP, HTTP is easier with requests",
      "                    urls = [f\"http://{url}\" for url in ftp_urls.split(';') if url]",
      "                    fastq_urls[srr] = urls",
      "",
      "        return fastq_urls",
      "",
      "    except Exception as e:",
      '        logger.error(f"Error fetching ENA URLs for {study_accession}: {e}")',
      "        return fastq_urls",
      "",
      "",
      "def download_file(url: str, output_path: Path, timeout: int = 300, show_progress: bool = True) -> bool:",
      '    """',
      "    Download a file with progress indication.",
      "",
      "    Args:",
      "        url: URL to download",
      "        output_path: Path to save file",
      "        timeout: Download timeout in seconds",
      "        show_progress: Show progress bar",
      "",
      "    Returns:",
      "        True if successful, False otherwise",
      '    """',
      "    try:",
      "        output_path.parent.mkdir(parents=True, exist_ok=True)",
      "",
      "        if HAS_REQUESTS:",
      "            response = requests.get(url, stream=True, timeout=timeout)",
      "            response.raise_for_status()",
      "",
      "            total_size = int(response.headers.get('content-length', 0))",
      "",
      "            with open(output_path, 'wb') as f:",
      "                downloaded = 0",
      "                for chunk in response.iter_content(chunk_size=8192):",
      "                    f.write(chunk)",
      "                    downloaded += len(chunk)",
      "                    if show_progress and total_size > 0:",
      "                        pct = (downloaded / total_size) * 100",
      "                        print(f\"\\r  Progress: {pct:.1f}%\", end='', flush=True)",
      "                if show_progress:",
      "                    print()  # New line after progress",
      "            return True",
      "        else:",
      "            # Fallback to urllib",
      "            req = Request(url, headers={'User-Agent': 'geo-sra-skill/1.0'})",
      "            with urlopen(req, timeout=timeout) as response:",
      "                with open(output_path, 'wb') as f:",
      "                    shutil.copyfileobj(response, f)",
      "            return True",
      "",
      "    except Exception as e:",
      '        logger.error(f"Download error for {url}: {e}")',
      "        return False",
      "",
      "",
      "def fetch_pubmed_metadata(pmid: str, max_retries: int = 3) -> Optional[Dict]:",
      '    """',
      "    Fetch paper metadata from PubMed.",
      "",
      "    Args:",
      "        pmid: PubMed ID",
      "        max_retries: Number of retries on failure",
      "",
      "    Returns:",
      "        Dict with 'authors', 'year', 'journal', 'doi' or None",
      '    """',
      '    url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pmid}&retmode=json"',
      "",
      "    for attempt in range(max_retries):",
      "        try:",
      "            _rate_limit_ncbi()",
      "            if HAS_REQUESTS:",
      "                response = requests.get(url, timeout=30)",
      "                data = response.json()",
      "            else:",
      "                with urlopen(url, timeout=30) as response:",
      "                    data = json.loads(response.read().decode())",
      "",
      "            result = data.get('result', {}).get(pmid, {})",
      "",
      "            if not result or 'error' in result:",
      "                if attempt < max_retries - 1:",
      "                    time.sleep(1 * (attempt + 1))",
      "                    continue",
      "                return None",
      "",
      "            # Extract authors",
      "            authors_list = result.get('authors', [])",
      "            if not authors_list:",
      "                if attempt < max_retries - 1:",
      "                    time.sleep(1 * (attempt + 1))",
      "                    continue",
      "                return None",
      "",
      "            author_names = [f\"{a.get('name', '')}\" for a in authors_list[:3]]",
      "            authors = ', '.join(author_names)",
      "            if len(authors_list) > 3:",
      "                authors += ', et al.'",
      "",
      "            # Extract year",
      "            pubdate = result.get('pubdate', '')",
      "            year_match = re.search(r'\\b(20\\d{2})\\b', pubdate)",
      '            year = year_match.group(1) if year_match else "Unknown"',
      "",
      "            # Extract journal",
      "            journal = result.get('source', 'Unknown')",
      "",
      "            # Extract DOI",
      '            doi = ""',
      "            for aid in result.get('articleids', []):",
      "                if aid.get('idtype') == 'doi':",
      "                    doi = aid.get('value', '')",
      "                    break",
      "",
      "            return {",
      "                'authors': authors,",
      "                'year': year,",
      "                'journal': journal,",
      "                'doi': doi,",
      "                'title': result.get('title', '')",
      "            }",
      "",
      "        except Exception as e:",
      '            logger.debug(f"PubMed fetch error for PMID {pmid} (attempt {attempt + 1}): {e}")',
      "            if attempt < max_retries - 1:",
      "                time.sleep(1 * (attempt + 1))",
      "            continue",
      "",
      "    return None",
      "",
      "",
      "def format_file_size(size_bytes: int) -> str:",
      '    """Format file size in human-readable format."""',
      "    if size_bytes < 1024:",
      '        return f"{size_bytes} B"',
      "    elif size_bytes < 1024 * 1024:",
      '        return f"{size_bytes / 1024:.1f} KB"',
      "    elif size_bytes < 1024 * 1024 * 1024:",
      '        return f"{size_bytes / (1024 * 1024):.1f} MB"',
      "    else:",
      '        return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"',
      "",
      "",
      "def estimate_download_size(runs: List[Dict]) -> int:",
      '    """',
      "    Estimate total download size from SRA run info.",
      "",
      "    Args:",
      "        runs: List of run info dicts with 'bases' field",
      "",
      "    Returns:",
      "        Estimated size in bytes (rough estimate based on bases)",
      '    """',
      "    total_bases = sum(r.get('bases', 0) for r in runs)",
      "    # FASTQ is roughly 1 byte per base when compressed",
      "    return total_bases // 4  # Rough compression ratio",
      "",
      "",
      "def fetch_bioproject_from_geo(geo_id: str) -> Optional[str]:",
      '    """',
      "    Fetch BioProject accession linked to a GEO study.",
      "",
      "    Args:",
      "        geo_id: GEO accession (e.g., 'GSE110004')",
      "",
      "    Returns:",
      "        BioProject accession (e.g., 'PRJNA432544') or None",
      '    """',
      "    try:",
      "        # First get GDS UID",
      '        search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=gds&term={geo_id}[Accession]&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(search_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(search_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        gds_ids = data.get('esearchresult', {}).get('idlist', [])",
      "        if not gds_ids:",
      "            return None",
      "",
      "        # Get linked BioProject",
      '        elink_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=gds&db=bioproject&id={gds_ids[0]}&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(elink_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(elink_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        linksets = data.get('linksets', [])",
      "        if linksets and linksets[0].get('linksetdbs'):",
      "            for linksetdb in linksets[0]['linksetdbs']:",
      "                if linksetdb.get('dbto') == 'bioproject':",
      "                    bp_ids = linksetdb.get('links', [])",
      "                    if bp_ids:",
      "                        # Get BioProject accession",
      '                        summary_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=bioproject&id={bp_ids[0]}&retmode=json"',
      "                        _rate_limit_ncbi()",
      "                        if HAS_REQUESTS:",
      "                            response = requests.get(summary_url, timeout=30)",
      "                            data = response.json()",
      "                        else:",
      "                            with urlopen(summary_url, timeout=30) as response:",
      "                                data = json.loads(response.read().decode())",
      "",
      "                        result = data.get('result', {}).get(str(bp_ids[0]), {})",
      "                        return result.get('project_acc')",
      "",
      "        return None",
      "",
      "    except Exception as e:",
      '        logger.debug(f"Error fetching BioProject for {geo_id}: {e}")',
      "        return None",
      "",
      "",
      "def fetch_sra_run_info_detailed(geo_id: str, bioproject: Optional[str] = None) -> List[Dict]:",
      '    """',
      "    Fetch detailed SRA run information using efetch CSV format.",
      "",
      "    This provides richer metadata than esummary, including sample names.",
      "",
      "    Args:",
      "        geo_id: GEO accession (e.g., 'GSE110004')",
      "        bioproject: Optional BioProject accession for fallback search",
      "",
      "    Returns:",
      "        List of dicts with detailed run info",
      '    """',
      "    runs = []",
      "",
      "    try:",
      "        # First get SRA UIDs using GEO search",
      '        search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=sra&term={geo_id}[GEO]&retmax=1000&retmode=json"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(search_url, timeout=30)",
      "            data = response.json()",
      "        else:",
      "            with urlopen(search_url, timeout=30) as response:",
      "                data = json.loads(response.read().decode())",
      "",
      "        id_list = data.get('esearchresult', {}).get('idlist', [])",
      "",
      "        # If no results with GEO search, try BioProject",
      "        if not id_list:",
      "            # Try to find BioProject if not provided",
      "            if not bioproject:",
      '                logger.info(f"No direct SRA link for {geo_id}, searching for BioProject...")',
      "                bioproject = fetch_bioproject_from_geo(geo_id)",
      "",
      "            if bioproject:",
      '                logger.info(f"Found BioProject: {bioproject}")',
      '                search_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=sra&term={bioproject}&retmax=1000&retmode=json"',
      "",
      "                _rate_limit_ncbi()",
      "                if HAS_REQUESTS:",
      "                    response = requests.get(search_url, timeout=30)",
      "                    data = response.json()",
      "                else:",
      "                    with urlopen(search_url, timeout=30) as response:",
      "                        data = json.loads(response.read().decode())",
      "",
      "                id_list = data.get('esearchresult', {}).get('idlist', [])",
      "",
      "        if not id_list:",
      '            logger.warning(f"No SRA entries found for {geo_id}")',
      "            return runs",
      "",
      "        # Fetch run info in CSV format using efetch",
      "        ids_str = ','.join(id_list)",
      '        efetch_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=sra&id={ids_str}&rettype=runinfo&retmode=csv"',
      "",
      "        _rate_limit_ncbi()",
      "        if HAS_REQUESTS:",
      "            response = requests.get(efetch_url, timeout=60)",
      "            content = response.text",
      "        else:",
      "            with urlopen(efetch_url, timeout=60) as response:",
      "                content = response.read().decode()",
      "",
      "        lines = content.strip().split('\\n')",
      "        if len(lines) < 1:",
      "            return runs",
      "",
      "        # NCBI efetch runinfo CSV doesn't include headers",
      "        # Define the fixed column order for SRA runinfo format",
      "        header = [",
      "            'Run', 'ReleaseDate', 'LoadDate', 'spots', 'bases', 'spots_with_mates',",
      "            'avgLength', 'size_MB', 'AssemblyName', 'download_path', 'Experiment',",
      "            'LibraryName', 'LibraryStrategy', 'LibrarySelection', 'LibrarySource',",
      "            'LibraryLayout', 'InsertSize', 'InsertDev', 'Platform', 'Model',",
      "            'SRAStudy', 'BioProject', 'Study_Pubmed_id', 'ProjectID', 'Sample',",
      "            'BioSample', 'SampleType', 'TaxID', 'ScientificName', 'SampleName',",
      "            'g1k_pop_code', 'source', 'g1k_analysis_group', 'Subject_ID', 'Sex',",
      "            'Disease', 'Tumor', 'Affection_Status', 'Analyte_Type', 'Histological_Type',",
      "            'Body_Site', 'CenterName', 'Submission', 'dbgap_study_accession', 'Consent',",
      "            'RunHash', 'ReadHash'",
      "        ]",
      "",
      "        # Map column names to indices",
      "        col_map = {col: idx for idx, col in enumerate(header)}",
      "",
      "        for line in lines:",
      "            if not line.strip():",
      "                continue",
      "",
      "            # Handle CSV fields (some may contain commas in quotes)",
      "            fields = _parse_csv_line(line)",
      "            if len(fields) < len(header):",
      "                continue",
      "",
      "            def get_field(name, default=''):",
      "                idx = col_map.get(name, -1)",
      "                return fields[idx] if idx >= 0 and idx < len(fields) else default",
      "",
      "            run = {",
      "                'srr': get_field('Run'),",
      "                'srx': get_field('Experiment'),",
      "                'gsm': get_field('SampleName'),  # Often GSM ID",
      "                'sample_name': get_field('SampleName'),",
      "                'library_name': get_field('LibraryName'),",
      "                'layout': get_field('LibraryLayout', 'UNKNOWN').upper(),",
      "                'library_strategy': get_field('LibraryStrategy', 'UNKNOWN'),",
      "                'library_source': get_field('LibrarySource', 'UNKNOWN'),",
      "                'library_selection': get_field('LibrarySelection', ''),",
      "                'platform': get_field('Platform'),",
      "                'model': get_field('Model'),",
      "                'organism': get_field('ScientificName', ''),",
      "                'spots': int(get_field('spots', 0) or 0),",
      "                'bases': int(get_field('bases', 0) or 0),",
      "                'size_mb': float(get_field('size_MB', 0) or 0),",
      "                'bioproject': get_field('BioProject'),",
      "                'biosample': get_field('BioSample'),",
      "                'sra_study': get_field('SRAStudy'),",
      "            }",
      "",
      "            # Only add if we have a valid SRR",
      "            if run['srr'].startswith('SRR'):",
      "                runs.append(run)",
      "",
      "        return runs",
      "",
      "    except Exception as e:",
      '        logger.error(f"Error fetching detailed SRA run info for {geo_id}: {e}")',
      "        return runs",
      "",
      "",
      "def _parse_csv_line(line: str) -> List[str]:",
      '    """Parse a CSV line handling quoted fields."""',
      "    import csv",
      "    import io",
      "    reader = csv.reader(io.StringIO(line))",
      "    for row in reader:",
      "        return row",
      "    return []",
      "",
      "",
      "def group_samples_by_type(runs: List[Dict]) -> Dict[str, Dict]:",
      '    """',
      "    Group SRA runs by library type and layout.",
      "",
      "    Returns dict with group names as keys and info dicts as values:",
      "    {",
      "        'RNA-Seq:PAIRED': {",
      "            'runs': [...],",
      "            'count': 18,",
      "            'gsm_range': 'GSM2879618-GSM2879635',",
      "            'size_estimate': 50000000000,",
      "            'description': 'RNA-Seq paired-end'",
      "        },",
      "        ...",
      "    }",
      '    """',
      "    groups = {}",
      "",
      "    for run in runs:",
      "        strategy = run.get('library_strategy', 'UNKNOWN')",
      "        layout = run.get('layout', 'UNKNOWN')",
      '        key = f"{strategy}:{layout}"',
      "",
      "        if key not in groups:",
      "            groups[key] = {",
      "                'runs': [],",
      "                'gsm_ids': set(),",
      "                'total_bases': 0,",
      "                'strategy': strategy,",
      "                'layout': layout,",
      "            }",
      "",
      "        groups[key]['runs'].append(run)",
      "        gsm = run.get('gsm', '')",
      "        if gsm.startswith('GSM'):",
      "            groups[key]['gsm_ids'].add(gsm)",
      "        groups[key]['total_bases'] += run.get('bases', 0)",
      "",
      "    # Post-process groups",
      "    result = {}",
      "    for key, info in groups.items():",
      "        gsm_list = sorted(info['gsm_ids'])",
      "        gsm_range = _format_gsm_range(gsm_list) if gsm_list else 'N/A'",
      "",
      "        result[key] = {",
      "            'runs': info['runs'],",
      "            'count': len(info['runs']),",
      "            'gsm_range': gsm_range,",
      "            'gsm_ids': gsm_list,",
      "            'size_estimate': info['total_bases'] // 4,  # Rough compressed size",
      "            'strategy': info['strategy'],",
      "            'layout': info['layout'],",
      "            'description': f\"{info['strategy']} {info['layout'].lower()}\",",
      "        }",
      "",
      "    return result",
      "",
      "",
      "def _format_gsm_range(gsm_list: List[str]) -> str:",
      '    """Format list of GSM IDs as a range if consecutive."""',
      "    if not gsm_list:",
      "        return 'N/A'",
      "",
      "    if len(gsm_list) == 1:",
      "        return gsm_list[0]",
      "",
      "    # Extract numbers and check if consecutive",
      "    try:",
      "        numbers = [int(gsm.replace('GSM', '')) for gsm in gsm_list]",
      "        numbers.sort()",
      "",
      "        if numbers[-1] - numbers[0] == len(numbers) - 1:",
      "            # Consecutive",
      '            return f"GSM{numbers[0]}-GSM{numbers[-1]}"',
      "        else:",
      "            # Not consecutive, show count",
      '            return f"{gsm_list[0]}...({len(gsm_list)} samples)"',
      "    except ValueError:",
      '        return f"{len(gsm_list)} samples"',
      "",
      "",
      "def format_sample_groups_table(groups: Dict[str, Dict]) -> str:",
      '    """Format sample groups as a readable table."""',
      "    lines = []",
      '    lines.append("")',
      "    lines.append(f\"{'Sample Group':<20} {'Count':>6} {'Layout':<10} {'GSM Range':<25} {'Est. Size':>12}\")",
      '    lines.append("-" * 80)',
      "",
      "    for key, info in sorted(groups.items(), key=lambda x: -x[1]['count']):",
      "        size_str = format_file_size(info['size_estimate'])",
      "        lines.append(",
      "            f\"{info['strategy']:<20} {info['count']:>6} {info['layout']:<10} \"",
      "            f\"{info['gsm_range']:<25} {size_str:>12}\"",
      "        )",
      "",
      '    lines.append("-" * 80)',
      "    total_runs = sum(g['count'] for g in groups.values())",
      "    total_size = sum(g['size_estimate'] for g in groups.values())",
      "    lines.append(f\"{'TOTAL':<20} {total_runs:>6} {'':<10} {'':<25} {format_file_size(total_size):>12}\")",
      "",
      "    return '\\n'.join(lines)",
      "",
    ].join("\n"),
    "scripts/utils/sample_inference.py": [
      '"""',
      "Sample name and metadata inference from filenames.",
      "",
      "This module extracts sample information, detects tumor/normal status,",
      "and matches R1/R2 read pairs from sequencing file names.",
      '"""',
      "",
      "import os",
      "import re",
      "from typing import Dict, List, Optional, Tuple",
      "",
      "",
      "# R1/R2 patterns with priority scores (higher = more confident)",
      "R1_PATTERNS = [",
      "    (r'_R1_\\d{3}', 10),      # _R1_001 (Illumina standard)",
      "    (r'_R1[_.]', 8),         # _R1. or _R1_",
      "    (r'\\.R1[_.]', 8),        # .R1. or .R1_",
      "    (r'_1[_.]', 5),          # _1. or _1_",
      "    (r'_R1\\.f', 6),          # _R1.fastq",
      "    (r'_1\\.f', 4),           # _1.fastq",
      "]",
      "",
      "R2_PATTERNS = [",
      "    (r'_R2_\\d{3}', 10),      # _R2_001 (Illumina standard)",
      "    (r'_R2[_.]', 8),         # _R2. or _R2_",
      "    (r'\\.R2[_.]', 8),        # .R2. or .R2_",
      "    (r'_2[_.]', 5),          # _2. or _2_",
      "    (r'_R2\\.f', 6),          # _R2.fastq",
      "    (r'_2\\.f', 4),           # _2.fastq",
      "]",
      "",
      "# Tumor/normal keywords",
      "TUMOR_KEYWORDS = [",
      "    r'\\btumou?r\\b',",
      "    r'\\bmetastasis\\b',",
      "    r'\\bmet\\b',",
      "    r'\\bprimary\\b',",
      "    r'\\bcancer\\b',",
      "    r'\\bmalignant\\b',",
      "    r'[-_]T[-_]',",
      "    r'[-_]T\\d*$',",
      "    r'^T\\d*[-_]',",
      "]",
      "",
      "NORMAL_KEYWORDS = [",
      "    r'\\bnormal\\b',",
      "    r'\\bgermline\\b',",
      "    r'\\bblood\\b',",
      "    r'\\bpbmc\\b',",
      "    r'\\bcontrol\\b',",
      "    r'\\bhealthy\\b',",
      "    r'\\bmatched\\b',",
      "    r'[-_]N[-_]',",
      "    r'[-_]N\\d*$',",
      "    r'^N\\d*[-_]',",
      "]",
      "",
      "# Lane pattern",
      "LANE_PATTERN = r'[_.]L(\\d{3})[_.]'",
      "",
      "# Patient/sample extraction patterns",
      "PATIENT_PATTERNS = [",
      "    r'^(P\\d+)[-_]',           # P001_sample",
      "    r'^(patient\\d+)[-_]',     # patient1_sample",
      "    r'^(TCGA-\\w+-\\w+)',       # TCGA format",
      "    r'^([A-Z]{2,3}\\d{3,})[-_]',  # AB123_sample",
      "]",
      "",
      "# Replicate patterns",
      "REPLICATE_PATTERNS = [",
      "    r'[_.]rep(\\d+)',          # _rep1, .rep2",
      "    r'[_.]replicate(\\d+)',    # _replicate1",
      "    r'[_.]R(\\d+)[_.]',        # _R1_ (but not R1/R2 for reads!)",
      "    r'[-_](\\d+)$',            # sample_1 (last resort)",
      "]",
      "",
      "",
      "def extract_sample_info(filepath: str) -> Dict[str, str]:",
      '    """',
      "    Extract sample metadata from filepath.",
      "",
      "    Args:",
      "        filepath: Path to sequencing file",
      "",
      "    Returns:",
      "        Dict with: sample, patient, lane (if detectable)",
      '    """',
      "    filename = os.path.basename(filepath)",
      "",
      "    # Remove extensions",
      "    stem = filename",
      "    for ext in ['.fastq.gz', '.fq.gz', '.fastq', '.fq', '.bam', '.cram', '.bai', '.crai']:",
      "        if stem.lower().endswith(ext):",
      "            stem = stem[:-len(ext)]",
      "            break",
      "",
      "    info = {}",
      "",
      "    # Extract lane",
      "    lane_match = re.search(LANE_PATTERN, stem)",
      '    info[\'lane\'] = f"L{lane_match.group(1)}" if lane_match else "L001"',
      "",
      "    # Remove lane from stem",
      "    clean_stem = re.sub(LANE_PATTERN, '_', stem)",
      "",
      "    # Remove R1/R2 indicators and everything after",
      "    for pattern, _ in R1_PATTERNS + R2_PATTERNS:",
      "        clean_stem = re.sub(pattern + r'.*', '', clean_stem, flags=re.IGNORECASE)",
      "",
      "    # Clean up trailing/multiple underscores and dots",
      "    clean_stem = re.sub(r'[_.-]+$', '', clean_stem)",
      "    clean_stem = re.sub(r'[_.-]{2,}', '_', clean_stem)",
      "",
      "    # Try to extract patient ID",
      "    for pattern in PATIENT_PATTERNS:",
      "        match = re.match(pattern, clean_stem, re.IGNORECASE)",
      "        if match:",
      "            info['patient'] = match.group(1)",
      "            break",
      "",
      "    # Sample is the cleaned stem",
      "    info['sample'] = clean_stem if clean_stem else filename.split('.')[0]",
      "",
      "    # Default patient to sample if not extracted",
      "    if 'patient' not in info:",
      "        info['patient'] = info['sample']",
      "",
      "    return info",
      "",
      "",
      "def infer_tumor_normal_status(sample_name: str) -> Optional[int]:",
      '    """',
      "    Infer tumor (1) or normal (0) status from sample name.",
      "",
      "    Args:",
      "        sample_name: Sample identifier",
      "",
      "    Returns:",
      "        1 for tumor, 0 for normal, None if cannot determine",
      '    """',
      "    name_lower = sample_name.lower()",
      "",
      "    # Check tumor indicators",
      "    for pattern in TUMOR_KEYWORDS:",
      "        if re.search(pattern, name_lower, re.IGNORECASE):",
      "            return 1",
      "",
      "    # Check normal indicators",
      "    for pattern in NORMAL_KEYWORDS:",
      "        if re.search(pattern, name_lower, re.IGNORECASE):",
      "            return 0",
      "",
      "    return None",
      "",
      "",
      "def extract_replicate_number(sample_name: str) -> Optional[int]:",
      '    """',
      "    Extract replicate number from sample name.",
      "",
      "    Args:",
      "        sample_name: Sample identifier",
      "",
      "    Returns:",
      "        Replicate number if found, None otherwise",
      '    """',
      "    for pattern in REPLICATE_PATTERNS:",
      "        match = re.search(pattern, sample_name, re.IGNORECASE)",
      "        if match:",
      "            try:",
      "                return int(match.group(1))",
      "            except ValueError:",
      "                continue",
      "    return None",
      "",
      "",
      "def _get_pattern_score(filename: str, patterns: List[Tuple[str, int]]) -> int:",
      '    """Get highest matching pattern score."""',
      "    max_score = 0",
      "    for pattern, score in patterns:",
      "        if re.search(pattern, filename, re.IGNORECASE):",
      "            max_score = max(max_score, score)",
      "    return max_score",
      "",
      "",
      "def _get_sample_key(filepath: str) -> str:",
      '    """Generate a key for grouping related files."""',
      "    info = extract_sample_info(filepath)",
      "    sample = info['sample']",
      "    lane = info.get('lane', 'L001')",
      "",
      "    # Include lane in key for multi-lane samples",
      '    if lane != "L001":',
      '        return f"{sample}_{lane}"',
      "    return sample",
      "",
      "",
      "def match_read_pairs(files) -> Dict[str, Dict]:",
      '    """',
      "    Match R1/R2 read pairs using scored pattern matching.",
      "",
      "    Args:",
      "        files: List of FileInfo objects (from file_discovery)",
      "",
      "    Returns:",
      "        Dict mapping sample_key to {'r1': path, 'r2': path, 'info': dict}",
      '    """',
      "    # Classify files",
      "    r1_files = []",
      "    r2_files = []",
      "",
      "    for file in files:",
      "        filename = file.name if hasattr(file, 'name') else os.path.basename(str(file))",
      "        filepath = file.path if hasattr(file, 'path') else str(file)",
      "",
      "        r1_score = _get_pattern_score(filename, R1_PATTERNS)",
      "        r2_score = _get_pattern_score(filename, R2_PATTERNS)",
      "",
      "        if r2_score > r1_score and r2_score > 0:",
      "            r2_files.append((filepath, r2_score))",
      "        elif r1_score > 0:",
      "            r1_files.append((filepath, r1_score))",
      "        else:",
      "            # No clear indicator - assume R1 (single-end or non-standard naming)",
      "            r1_files.append((filepath, 0))",
      "",
      "    # Build pairs by matching sample keys",
      "    pairs = {}",
      "",
      "    # Process R1 files first",
      "    for r1_path, score in r1_files:",
      "        key = _get_sample_key(r1_path)",
      "        info = extract_sample_info(r1_path)",
      "",
      "        if key not in pairs:",
      "            pairs[key] = {",
      "                'r1': r1_path,",
      "                'r2': None,",
      "                'info': info,",
      "                'score': score",
      "            }",
      "        else:",
      "            # Multiple R1 files for same sample (should not happen)",
      "            pairs[key]['r1'] = r1_path",
      "",
      "    # Match R2 files",
      "    for r2_path, score in r2_files:",
      "        key = _get_sample_key(r2_path)",
      "        info = extract_sample_info(r2_path)",
      "",
      "        if key in pairs:",
      "            pairs[key]['r2'] = r2_path",
      "        else:",
      "            # R2 without matching R1",
      "            pairs[key] = {",
      "                'r1': None,",
      "                'r2': r2_path,",
      "                'info': info,",
      "                'score': score",
      "            }",
      "",
      "    return pairs",
      "",
      "",
      "def infer_patient_groupings(sample_names: List[str]) -> Dict[str, str]:",
      '    """',
      "    Infer patient groupings from sample names.",
      "",
      "    Groups samples that share a common prefix pattern.",
      "",
      "    Args:",
      "        sample_names: List of sample identifiers",
      "",
      "    Returns:",
      "        Dict mapping sample_name to patient_id",
      '    """',
      "    patient_map = {}",
      "",
      "    for sample in sample_names:",
      "        # Try to find a patient pattern",
      "        for pattern in PATIENT_PATTERNS:",
      "            match = re.match(pattern, sample, re.IGNORECASE)",
      "            if match:",
      "                patient_map[sample] = match.group(1)",
      "                break",
      "",
      "        if sample not in patient_map:",
      "            # Default: each sample is its own patient",
      "            patient_map[sample] = sample",
      "",
      "    return patient_map",
      "",
    ].join("\n"),
    "scripts/utils/validators.py": [
      '"""',
      "Samplesheet validation utilities.",
      "",
      "Validates samplesheet rows against pipeline configuration before writing,",
      "catching errors early with helpful messages.",
      '"""',
      "",
      "import os",
      "from dataclasses import dataclass, field",
      "from pathlib import Path",
      "from typing import Dict, List, Optional",
      "import yaml",
      "",
      "",
      "@dataclass",
      "class ValidationResult:",
      '    """Result of samplesheet validation."""',
      "    valid: bool",
      "    errors: List[str] = field(default_factory=list)",
      "    warnings: List[str] = field(default_factory=list)",
      "    suggestions: List[str] = field(default_factory=list)",
      "",
      "    def __bool__(self):",
      "        return self.valid",
      "",
      "    def summary(self) -> str:",
      '        """Generate human-readable summary."""',
      "        lines = []",
      "        if self.errors:",
      '            lines.append("Errors:")',
      "            for e in self.errors:",
      '                lines.append(f"  - {e}")',
      "        if self.warnings:",
      '            lines.append("Warnings:")',
      "            for w in self.warnings:",
      '                lines.append(f"  - {w}")',
      "        if self.suggestions:",
      '            lines.append("Suggestions:")',
      "            for s in self.suggestions:",
      '                lines.append(f"  - {s}")',
      '        return "\\n".join(lines)',
      "",
      "",
      "def load_pipeline_config(pipeline: str) -> Optional[Dict]:",
      '    """Load pipeline configuration from YAML file."""',
      "    # Find config directory relative to this file",
      "    script_dir = Path(__file__).parent.parent.parent",
      '    config_path = script_dir / "config" / "pipelines" / f"{pipeline}.yaml"',
      "",
      "    if not config_path.exists():",
      "        return None",
      "",
      "    with open(config_path) as f:",
      "        return yaml.safe_load(f)",
      "",
      "",
      "def validate_samplesheet(",
      "    rows: List[Dict],",
      "    pipeline: str,",
      "    config: Optional[Dict] = None",
      ") -> ValidationResult:",
      '    """',
      "    Validate samplesheet rows against pipeline requirements.",
      "",
      "    Args:",
      "        rows: List of row dictionaries",
      "        pipeline: Pipeline name (e.g., 'rnaseq', 'sarek')",
      "        config: Optional pre-loaded config dict",
      "",
      "    Returns:",
      "        ValidationResult with errors, warnings, and suggestions",
      '    """',
      "    errors = []",
      "    warnings = []",
      "    suggestions = []",
      "",
      "    # Load config if not provided",
      "    if config is None:",
      "        config = load_pipeline_config(pipeline)",
      "",
      "    if config is None:",
      '        errors.append(f"Unknown pipeline: {pipeline}")',
      "        return ValidationResult(valid=False, errors=errors)",
      "",
      '    columns = config.get("samplesheet", {}).get("columns", [])',
      '    required_cols = [c["name"] for c in columns if c.get("required", False)]',
      "",
      "    if not rows:",
      '        errors.append("Samplesheet is empty - no samples found")',
      "        return ValidationResult(valid=False, errors=errors)",
      "",
      "    # Validate each row",
      "    for i, row in enumerate(rows):",
      "        row_num = i + 2  # Account for header row",
      "",
      "        # Check required columns",
      "        for col_name in required_cols:",
      '            col_config = next((c for c in columns if c["name"] == col_name), None)',
      "",
      "            # Skip columns with conditions that don't apply",
      '            if col_config and "condition" in col_config:',
      "                # Simple condition check - skip for now",
      "                # Full implementation would evaluate conditions",
      "                pass",
      "",
      '            if col_name not in row or row[col_name] is None or row[col_name] == "":',
      "                # Check if there's a default",
      '                if col_config and "default" in col_config:',
      "                    continue",
      "                errors.append(f\"Row {row_num}: Missing required column '{col_name}'\")",
      "",
      "        # Validate path columns exist",
      '        for col_name in ["fastq_1", "fastq_2", "bam", "bai"]:',
      "            if col_name in row and row[col_name]:",
      "                path = row[col_name]",
      "                if not os.path.exists(path):",
      '                    errors.append(f"Row {row_num}: File not found: {path}")',
      "                elif not os.path.isfile(path):",
      '                    errors.append(f"Row {row_num}: Not a file: {path}")',
      "",
      "        # Validate enum values",
      "        for col_config in columns:",
      '            col_name = col_config["name"]',
      '            if col_name in row and row[col_name] and "allowed" in col_config:',
      "                value = row[col_name]",
      '                allowed = col_config["allowed"]',
      "                if value not in allowed:",
      "                    errors.append(",
      "                        f\"Row {row_num}: Invalid value '{value}' for '{col_name}'. \"",
      '                        f"Allowed: {allowed}"',
      "                    )",
      "",
      "        # Check R1/R2 pairing consistency",
      '        r1 = row.get("fastq_1", "")',
      '        r2 = row.get("fastq_2", "")',
      "        if r1 and not r2:",
      '            warnings.append(f"Row {row_num}: Single-end data (no R2 file)")',
      "        elif r2 and not r1:",
      '            errors.append(f"Row {row_num}: R2 present but R1 missing")',
      "",
      "    # Check for duplicate samples",
      '    sample_col = "sample" if "sample" in rows[0] else "patient"',
      "    if sample_col in rows[0]:",
      '        samples = [r.get(sample_col, "") for r in rows]',
      "        duplicates = [s for s in set(samples) if samples.count(s) > 1]",
      "        if duplicates:",
      '            warnings.append(f"Duplicate sample names: {duplicates}")',
      "            suggestions.append(",
      '                "Duplicates may be intentional (multi-lane sequencing). "',
      '                "Verify sample grouping is correct."',
      "            )",
      "",
      "    # Pipeline-specific validation",
      '    if pipeline == "sarek":',
      "        _validate_sarek_specific(rows, errors, warnings, suggestions)",
      '    elif pipeline == "atacseq":',
      "        _validate_atacseq_specific(rows, errors, warnings, suggestions)",
      "",
      "    return ValidationResult(",
      "        valid=len(errors) == 0,",
      "        errors=errors,",
      "        warnings=warnings,",
      "        suggestions=suggestions",
      "    )",
      "",
      "",
      "def _validate_sarek_specific(",
      "    rows: List[Dict],",
      "    errors: List[str],",
      "    warnings: List[str],",
      "    suggestions: List[str]",
      "):",
      '    """Sarek-specific validation for tumor/normal pairing."""',
      "    # Group by patient",
      "    patients = {}",
      "    for row in rows:",
      '        patient = row.get("patient", "")',
      '        status = row.get("status")',
      "",
      "        if patient not in patients:",
      '            patients[patient] = {"tumor": 0, "normal": 0, "unknown": 0}',
      "",
      "        if status == 1:",
      '            patients[patient]["tumor"] += 1',
      "        elif status == 0:",
      '            patients[patient]["normal"] += 1',
      "        else:",
      '            patients[patient]["unknown"] += 1',
      "",
      "    # Check pairing",
      "    for patient, counts in patients.items():",
      '        if counts["tumor"] > 0 and counts["normal"] == 0:',
      "            warnings.append(",
      "                f\"Patient '{patient}': Tumor sample(s) without matched normal. \"",
      '                "Somatic calling works best with paired tumor-normal."',
      "            )",
      "            suggestions.append(",
      "                f\"For patient '{patient}': Add a normal sample or use tumor-only mode.\"",
      "            )",
      "",
      '        if counts["unknown"] > 0:',
      "            warnings.append(",
      "                f\"Patient '{patient}': {counts['unknown']} sample(s) with unknown status. \"",
      '                "Set status column to 0 (normal) or 1 (tumor)."',
      "            )",
      "",
      "",
      "def _validate_atacseq_specific(",
      "    rows: List[Dict],",
      "    errors: List[str],",
      "    warnings: List[str],",
      "    suggestions: List[str]",
      "):",
      '    """ATAC-seq specific validation for replicates."""',
      "    # Group by sample (condition)",
      "    samples = {}",
      "    for row in rows:",
      '        sample = row.get("sample", "")',
      '        replicate = row.get("replicate", 1)',
      "",
      "        if sample not in samples:",
      "            samples[sample] = []",
      "",
      "        samples[sample].append(replicate)",
      "",
      "    # Check replicates",
      "    for sample, reps in samples.items():",
      "        if len(reps) < 2:",
      "            warnings.append(",
      "                f\"Sample '{sample}': Only {len(reps)} replicate(s). \"",
      '                "Consensus peaks require 2+ replicates."',
      "            )",
      "",
      "        # Check for duplicate replicate numbers",
      "        if len(reps) != len(set(reps)):",
      "            errors.append(",
      "                f\"Sample '{sample}': Duplicate replicate numbers detected. \"",
      '                "Each replicate must have a unique number."',
      "            )",
      "",
      "    # Check all samples have R2 (ATAC-seq requires paired-end)",
      "    for i, row in enumerate(rows):",
      '        if not row.get("fastq_2"):',
      "            errors.append(",
      '                f"Row {i+2}: ATAC-seq requires paired-end data. R2 file missing."',
      "            )",
      "",
      "",
      "def validate_file_exists(path: str) -> bool:",
      '    """Check if file exists and is accessible."""',
      "    return os.path.isfile(path) and os.access(path, os.R_OK)",
      "",
      "",
      "def validate_absolute_path(path: str) -> bool:",
      '    """Check if path is absolute."""',
      "    return os.path.isabs(path)",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: bio-research-nextflow-development",
      "description: Run nf-core bioinformatics pipelines (rnaseq, sarek, atacseq) on sequencing data. Use when analyzing RNA-seq, WGS/WES, or ATAC-seq data—either local FASTQs or public datasets from GEO/SRA. Triggers on nf-core, Nextflow, FASTQ analysis, variant calling, gene expression, differential expression, GEO reanalysis, GSE/GSM/SRR accessions, or samplesheet creation.",
      "metadata:",
      "  plugin: bio-research",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# nf-core Pipeline Deployment",
      "",
      "Run nf-core bioinformatics pipelines on local or public sequencing data.",
      "",
      "**Target users:** Bench scientists and researchers without specialized bioinformatics training who need to run large-scale omics analyses—differential expression, variant calling, or chromatin accessibility analysis.",
      "",
      "## Workflow Checklist",
      "",
      "```",
      "- [ ] Step 0: Acquire data (if from GEO/SRA)",
      "- [ ] Step 1: Environment check (MUST pass)",
      "- [ ] Step 2: Select pipeline (confirm with user)",
      "- [ ] Step 3: Run test profile (MUST pass)",
      "- [ ] Step 4: Create samplesheet",
      "- [ ] Step 5: Configure & run (confirm genome with user)",
      "- [ ] Step 6: Verify outputs",
      "```",
      "",
      "---",
      "",
      "## Step 0: Acquire Data (GEO/SRA Only)",
      "",
      "**Skip this step if user has local FASTQ files.**",
      "",
      "For public datasets, fetch from GEO/SRA first. See [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) for the full workflow.",
      "",
      "**Quick start:**",
      "",
      "```bash",
      "# 1. Get study info",
      "python scripts/sra_geo_fetch.py info GSE110004",
      "",
      "# 2. Download (interactive mode)",
      "python scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i",
      "",
      "# 3. Generate samplesheet",
      "python scripts/sra_geo_fetch.py samplesheet GSE110004 --fastq-dir ./fastq -o samplesheet.csv",
      "```",
      "",
      "**DECISION POINT:** After fetching study info, confirm with user:",
      "- Which sample subset to download (if multiple data types)",
      "- Suggested genome and pipeline",
      "",
      "Then continue to Step 1.",
      "",
      "---",
      "",
      "## Step 1: Environment Check",
      "",
      "**Run first. Pipeline will fail without passing environment.**",
      "",
      "```bash",
      "python scripts/check_environment.py",
      "```",
      "",
      "All critical checks must pass. If any fail, provide fix instructions:",
      "",
      "### Docker issues",
      "",
      "| Problem | Fix |",
      "|---------|-----|",
      "| Not installed | Install from https://docs.docker.com/get-docker/ |",
      "| Permission denied | `sudo usermod -aG docker $USER` then re-login |",
      "| Daemon not running | `sudo systemctl start docker` |",
      "",
      "### Nextflow issues",
      "",
      "| Problem | Fix |",
      "|---------|-----|",
      "| Not installed | `curl -s https://get.nextflow.io \\| bash && mv nextflow ~/bin/` |",
      "| Version < 23.04 | `nextflow self-update` |",
      "",
      "### Java issues",
      "",
      "| Problem | Fix |",
      "|---------|-----|",
      "| Not installed / < 11 | `sudo apt install openjdk-11-jdk` |",
      "",
      "**Do not proceed until all checks pass.** For HPC/Singularity, see [references/troubleshooting.md](references/troubleshooting.md).",
      "",
      "---",
      "",
      "## Step 2: Select Pipeline",
      "",
      "**DECISION POINT: Confirm with user before proceeding.**",
      "",
      "| Data Type | Pipeline | Version | Goal |",
      "|-----------|----------|---------|------|",
      "| RNA-seq | `rnaseq` | 3.22.2 | Gene expression |",
      "| WGS/WES | `sarek` | 3.7.1 | Variant calling |",
      "| ATAC-seq | `atacseq` | 2.1.2 | Chromatin accessibility |",
      "",
      "Auto-detect from data:",
      "```bash",
      "python scripts/detect_data_type.py /path/to/data",
      "```",
      "",
      "For pipeline-specific details:",
      "- [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md)",
      "- [references/pipelines/sarek.md](references/pipelines/sarek.md)",
      "- [references/pipelines/atacseq.md](references/pipelines/atacseq.md)",
      "",
      "---",
      "",
      "## Step 3: Run Test Profile",
      "",
      "**Validates environment with small data. MUST pass before real data.**",
      "",
      "```bash",
      "nextflow run nf-core/<pipeline> -r <version> -profile test,docker --outdir test_output",
      "```",
      "",
      "| Pipeline | Command |",
      "|----------|---------|",
      "| rnaseq | `nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq` |",
      "| sarek | `nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek` |",
      "| atacseq | `nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq` |",
      "",
      "Verify:",
      "```bash",
      "ls test_output/multiqc/multiqc_report.html",
      'grep "Pipeline completed successfully" .nextflow.log',
      "```",
      "",
      "If test fails, see [references/troubleshooting.md](references/troubleshooting.md).",
      "",
      "---",
      "",
      "## Step 4: Create Samplesheet",
      "",
      "### Generate automatically",
      "",
      "```bash",
      "python scripts/generate_samplesheet.py /path/to/data <pipeline> -o samplesheet.csv",
      "```",
      "",
      "The script:",
      "- Discovers FASTQ/BAM/CRAM files",
      "- Pairs R1/R2 reads",
      "- Infers sample metadata",
      "- Validates before writing",
      "",
      "**For sarek:** Script prompts for tumor/normal status if not auto-detected.",
      "",
      "### Validate existing samplesheet",
      "",
      "```bash",
      "python scripts/generate_samplesheet.py --validate samplesheet.csv <pipeline>",
      "```",
      "",
      "### Samplesheet formats",
      "",
      "**rnaseq:**",
      "```csv",
      "sample,fastq_1,fastq_2,strandedness",
      "SAMPLE1,/abs/path/R1.fq.gz,/abs/path/R2.fq.gz,auto",
      "```",
      "",
      "**sarek:**",
      "```csv",
      "patient,sample,lane,fastq_1,fastq_2,status",
      "patient1,tumor,L001,/abs/path/tumor_R1.fq.gz,/abs/path/tumor_R2.fq.gz,1",
      "patient1,normal,L001,/abs/path/normal_R1.fq.gz,/abs/path/normal_R2.fq.gz,0",
      "```",
      "",
      "**atacseq:**",
      "```csv",
      "sample,fastq_1,fastq_2,replicate",
      "CONTROL,/abs/path/ctrl_R1.fq.gz,/abs/path/ctrl_R2.fq.gz,1",
      "```",
      "",
      "---",
      "",
      "## Step 5: Configure & Run",
      "",
      "### 5a. Check genome availability",
      "",
      "```bash",
      "python scripts/manage_genomes.py check <genome>",
      "# If not installed:",
      "python scripts/manage_genomes.py download <genome>",
      "```",
      "",
      "Common genomes: GRCh38 (human), GRCh37 (legacy), GRCm39 (mouse), R64-1-1 (yeast), BDGP6 (fly)",
      "",
      "### 5b. Decision points",
      "",
      "**DECISION POINT: Confirm with user:**",
      "",
      "1. **Genome:** Which reference to use",
      "2. **Pipeline-specific options:**",
      "   - **rnaseq:** aligner (star_salmon recommended, hisat2 for low memory)",
      "   - **sarek:** tools (haplotypecaller for germline, mutect2 for somatic)",
      "   - **atacseq:** read_length (50, 75, 100, or 150)",
      "",
      "### 5c. Run pipeline",
      "",
      "```bash",
      "nextflow run nf-core/<pipeline> \\",
      "    -r <version> \\",
      "    -profile docker \\",
      "    --input samplesheet.csv \\",
      "    --outdir results \\",
      "    --genome <genome> \\",
      "    -resume",
      "```",
      "",
      "**Key flags:**",
      "- `-r`: Pin version",
      "- `-profile docker`: Use Docker (or `singularity` for HPC)",
      "- `--genome`: iGenomes key",
      "- `-resume`: Continue from checkpoint",
      "",
      "**Resource limits (if needed):**",
      "```bash",
      "--max_cpus 8 --max_memory '32.GB' --max_time '24.h'",
      "```",
      "",
      "---",
      "",
      "## Step 6: Verify Outputs",
      "",
      "### Check completion",
      "",
      "```bash",
      "ls results/multiqc/multiqc_report.html",
      'grep "Pipeline completed successfully" .nextflow.log',
      "```",
      "",
      "### Key outputs by pipeline",
      "",
      "**rnaseq:**",
      "- `results/star_salmon/salmon.merged.gene_counts.tsv` - Gene counts",
      "- `results/star_salmon/salmon.merged.gene_tpm.tsv` - TPM values",
      "",
      "**sarek:**",
      "- `results/variant_calling/*/` - VCF files",
      "- `results/preprocessing/recalibrated/` - BAM files",
      "",
      "**atacseq:**",
      "- `results/macs2/narrowPeak/` - Peak calls",
      "- `results/bwa/mergedLibrary/bigwig/` - Coverage tracks",
      "",
      "---",
      "",
      "## Quick Reference",
      "",
      "For common exit codes and fixes, see [references/troubleshooting.md](references/troubleshooting.md).",
      "",
      "### Resume failed run",
      "",
      "```bash",
      "nextflow run nf-core/<pipeline> -resume",
      "```",
      "",
      "---",
      "",
      "## References",
      "",
      "- [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) - Downloading public GEO/SRA data",
      "- [references/troubleshooting.md](references/troubleshooting.md) - Common issues and fixes",
      "- [references/installation.md](references/installation.md) - Environment setup",
      "- [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md) - RNA-seq pipeline details",
      "- [references/pipelines/sarek.md](references/pipelines/sarek.md) - Variant calling details",
      "- [references/pipelines/atacseq.md](references/pipelines/atacseq.md) - ATAC-seq details",
      "",
      "---",
      "",
      "## Disclaimer",
      "",
      "This skill is provided as a prototype example demonstrating how to integrate nf-core bioinformatics pipelines into Claude Code for automated analysis workflows. The current implementation supports three pipelines (rnaseq, sarek, and atacseq), serving as a foundation that enables the community to expand support to the full set of nf-core pipelines.",
      "",
      "It is intended for educational and research purposes and should not be considered production-ready without appropriate validation for your specific use case. Users are responsible for ensuring their computing environment meets pipeline requirements and for verifying analysis results.",
      "",
      "Anthropic does not guarantee the accuracy of bioinformatics outputs, and users should follow standard practices for validating computational analyses. This integration is not officially endorsed by or affiliated with the nf-core community.",
      "",
      "## Attribution",
      "",
      "When publishing results, cite the appropriate pipeline. Citations are available in each nf-core repository's CITATIONS.md file (e.g., https://github.com/nf-core/rnaseq/blob/3.22.2/CITATIONS.md).",
      "",
      "## Licenses",
      "",
      "- **nf-core pipelines:** MIT License (https://nf-co.re/about)",
      "- **Nextflow:** Apache License, Version 2.0 (https://www.nextflow.io/about-us.html)",
      "- **NCBI SRA Toolkit:** Public Domain (https://github.com/ncbi/sra-tools/blob/master/LICENSE)",
      "",
    ].join("\n"),
  },
  "bio-research-scientific-problem-selection": {
    "references/01-intuition-pumps.md": [
      "# SKILL: Intuition Pumps for Scientific Problem Ideation",
      "",
      "## Overview",
      'This skill helps scientists generate high-quality research ideas by providing systematic prompts ("intuition pumps") and identifying common ideation traps. Based on the framework that most biological and chemical science projects involve **perturbing a system, measuring it, and analyzing the data**, this skill guides users through structured ideation that can significantly impact how they spend years of their career.',
      "",
      "## Core Framework",
      "",
      "### The Three Pillars of Scientific Work",
      "Research advances generally fall into one of these categories, each with two dimensions:",
      "",
      "**PERTURBATION**",
      "- *Logic*: Novel ways to manipulate biological systems (e.g., using CRISPR for deep mutational scanning)",
      "- *Technology*: New tools for manipulation (e.g., developing base editors, creating whole-genome CRISPR libraries)",
      "",
      "**MEASUREMENT**  ",
      "- *Logic*: Novel applications of existing measurement tools (e.g., using tissue clearing to study liver fibrosis)",
      "- *Technology*: New measurement capabilities (e.g., developing tissue-clearing techniques, super-resolution microscopy)",
      "",
      "**THEORY/COMPUTATION**",
      "- *Logic*: Using computational tools to make discoveries (e.g., applying AlphaFold to identify protein functions)",
      "- *Technology*: Building new algorithms or models (e.g., developing machine learning architectures for biological data)",
      "",
      "Understanding which quadrant resonates with the user can help identify their niche and guide ideation.",
      "",
      "## The Skill Workflow",
      "",
      "### Phase 1: Initial Discovery Questions (5-10 minutes)",
      "",
      "Before diving into intuition pumps, Claude should gather context by asking the user:",
      "",
      "1. **What is the user's general research area or field?** (e.g., immunology, synthetic biology, neuroscience, protein engineering)",
      "",
      "2. **What excites the user most about science?**",
      "   - Building new tools/technologies?",
      "   - Discovering fundamental principles?",
      "   - Solving practical problems?",
      "   - Understanding dynamic processes?",
      "",
      "3. **What are the user's existing strengths?** (Select all that apply)",
      "   - Specific techniques (please list)",
      "   - Computational skills",
      "   - Access to unique systems/models",
      "   - Domain expertise in a particular area",
      "",
      "4. **Current constraints:**",
      "   - Time horizon for this project? (months/years)",
      "   - Resources available?",
      "   - Must it connect to existing work, or can the user start fresh?",
      "",
      "5. **On a scale of 1-5, how would the user rate their current idea?**",
      "   - Likelihood of success: 1 (very risky) to 5 (highly feasible)",
      "   - Potential impact: 1 (incremental) to 5 (transformative)",
      "",
      "### Phase 2: Applying Intuition Pumps",
      "",
      "Based on the user's responses, Claude should guide them through relevant intuition pumps from this list:",
      "",
      "#### Intuition Pump #1: Make It Systematic",
      "**Prompt:** Take any one-off perturbation or measurement and make it systematic.",
      "",
      "**Examples:**",
      "- Instead of mutating one enzyme, measure kinetic parameters across an entire enzyme family",
      "- Instead of one CRISPR mutant → genome-wide screen with transcriptomic readout",
      "- Instead of imaging one condition → high-throughput imaging across thousands of conditions",
      "",
      "**Prompt for User:** What one-off experiment in your field could become a systematic survey?",
      "",
      "#### Intuition Pump #2: Identify Technology Limitations",
      "**Prompt:** What are the fundamental limitations of technologies you use? These limitations are opportunities.",
      "",
      "**Examples:**",
      "- Microscopy can't resolve beyond diffraction limit → super-resolution microscopy",
      "- DNA synthesis can't make complete genomes → develop assembly methods",
      "- Genetic screens have precise input but imprecise output → develop high-dimensional readouts",
      "- We do single gene KOs but networks are complex → develop combinatorial perturbation methods",
      "",
      "**Prompt for User:** What technology limitation frustrates you most? How might you turn that limitation into an opportunity?",
      "",
      '#### Intuition Pump #3: The "I Can\'t Imagine" Test',
      "**Prompt:** I can't imagine a future in which we don't have ____, but it doesn't exist yet.",
      "",
      "**Examples:**",
      "- The ability to design highly efficient enzymes like we design other proteins",
      "- The ability to deliver genome editing payloads to any cell type in vivo",
      "- 3D tomographic imaging of live cells at molecular resolution",
      "- Proteome-scale sequencing with the throughput of RNA-seq",
      "",
      "**Prompt for User:** What capability seems inevitable but doesn't exist yet in your field?",
      "",
      "#### Intuition Pump #4: Static vs. Dynamic Understanding",
      '**Prompt:** We understand biological "parts lists" but rarely understand dynamic processes.',
      "",
      "**Key Insight:** Most observations are single-timepoint, single-perturbation format. But biological systems are dynamic—like humans flowing through Grand Central Station or money through financial systems.",
      "",
      "**Examples:**",
      "- Understanding growth factor signaling like we understand turning a key in a car engine",
      "- Time-resolved cell atlases with lineage tracing through entire development",
      "- Following metabolite flux through pathways in real-time",
      "",
      "**Prompt for User:** What dynamic process in your field do we observe as static snapshots? How might you capture the full temporal or spatial dynamics?",
      "",
      "#### Intuition Pump #5: Pick a New Axis",
      "**Prompt:** We almost always use time as the x-axis for dynamic processes. What other coordinate could you use?",
      "",
      '**Example:** Instead of time, use "infection progression" markers to enable monitoring asynchronous cells',
      "",
      "**Prompt for User:** What non-temporal coordinate could reveal new biology in your system?",
      "",
      "#### Intuition Pump #6: Create a Technology Platform",
      "**Prompt:** Instead of answering one question, could you build a platform that enables many questions?",
      "",
      "**Examples:**",
      "- Antibodies for intracellular targets (not just extracellular)",
      "- AI that predicts perturbations needed to reach desired cell states",
      "- Universal genome delivery vehicles",
      "",
      "**Prompt for User:** What platform would transform how your field asks questions?",
      "",
      "#### Intuition Pump #7: Dogs That Don't Bark",
      "**Prompt:** Why doesn't something exist or occur? Absence can be as informative as presence.",
      "",
      "**Examples:**",
      "- Why are there no Gram-negative bacteria on human skin?",
      "- Why do some catalytically inactive enzymes persist through evolution?",
      "- Why don't certain cell types exist in certain tissues?",
      "",
      "**Prompt for User:** What absence puzzles you in your field?",
      "",
      "### Phase 3: Avoiding Common Traps",
      "",
      "After generating ideas, we must evaluate them critically. Here are the most common traps:",
      "",
      "#### Trap #1: The Truffle Hound",
      "**Warning:** Don't become so good at one system or technique that you fail to ask questions of biological import.",
      "",
      '**Bad:** "What is the role of p190 RhoGAP in wing development?"  ',
      '**Better:** "How do signaling pathways and cytoskeleton coordinate to control wing development?"',
      "",
      "**Self-Check:** Is the question driven by biological curiosity or by what the user is technically capable of?",
      "",
      "#### Trap #2: Applying Existing Tool to New System",
      '**Warning:** "Let\'s use CRISPR in my organism" can be valuable but risks crowding and incrementalism.',
      "",
      "**When It Works:** The user is enabling a field that truly needs this capability",
      "**When It Fails:** The tool is already widely applied; the contribution will be incremental",
      "",
      "**Self-Check:** Will this tool application open new biological questions, or just extend existing observations? Claude should help the user evaluate this honestly.",
      "",
      "#### Trap #3: Jumping on the First Idea",
      "**Warning:** Treating ideas with reverence instead of skepticism. Confirmation bias sets in quickly.",
      "",
      "**Better Approach:** Users should treat new ideas like leeches trying to steal their time. Look for the warts. Develop several ideas in parallel and comparison shop.",
      "",
      "**Self-Check:** Has the user critically evaluated at least 3-5 alternative approaches?",
      "",
      "#### Trap #4: Too Many Fixed Parameters",
      "**Warning:** Fixing too many parameters at the outset creates a poor technique-application match.",
      "",
      '**Example of Over-Constraining:** "I will use spatial transcriptomics to study antigen-presenting cell and T cell interactions in the tumor microenvironment."',
      "- This fixes: technique (spatial transcriptomics), cell types, and context",
      "- If any assumption fails, the project fails",
      "",
      "**Self-Check:** Has the user fixed more than 2 parameters before starting?",
      "",
      "#### Trap #5: Too Few Fixed Parameters",
      '**Warning:** "I want to do impactful work in cell engineering" → paralysis',
      "",
      "**Resolution:** Constraints engender creativity. Fix ONE parameter at a time and let creativity flow.",
      "",
      "**Self-Check:** Does the user have at least one concrete constraint to work with?",
      "",
      "### Phase 4: Literature Integration",
      "",
      "To ensure the idea has appropriate scope and hasn't been thoroughly explored, Claude should ask:",
      "",
      "1. **What are 2-3 key questions or gaps the idea addresses?**",
      "",
      "2. **What should be searched in PubMed to:**",
      "   - Understand the current state of the field?",
      "   - Identify related approaches?",
      "   - Find empirical knowledge from adjacent domains that could inform the approach?",
      "",
      "Claude should use PubMed to:",
      "- Assess how general/specific the problem is",
      "- Identify relevant methodological advances",
      "- Find analogous systems or approaches in other fields",
      "- Determine the degree of competition",
      "",
      "### Phase 5: Idea Refinement and Output",
      "",
      "After working through intuition pumps, avoiding traps, and reviewing literature, Claude should help the user:",
      "",
      "1. **Crystallize the Idea:**",
      "   - Biological question",
      "   - Technical approach (perturbation/measurement/theory: logic vs. technology)",
      "   - What's novel about this angle?",
      "",
      "2. **Articulate Fixed vs. Floating Parameters:**",
      "   - What MUST remain constant in the approach?",
      "   - What can be flexible if obstacles arise?",
      "",
      "3. **Identify Key Assumptions:**",
      "   - What must be true for this to work?",
      "   - Which assumptions are about biology vs. technology capabilities?",
      "",
      "4. **Sketch Alternative Paths:**",
      "   - If the primary approach fails, what's Plan B?",
      "   - Can the project be designed to succeed regardless of outcome?",
      "",
      "## Output Deliverable",
      "",
      "At the end of this skill, Claude should produce a **2-page Problem Ideation Document** containing:",
      "",
      "### Page 1: Core Idea",
      "- **Title:** Concise project name",
      "- **The Question:** What biological question is being asked?",
      "- **The Approach:** How will it be answered? (Specify perturbation/measurement/computation: logic vs. technology)",
      "- **What's Novel:** The unique angle",
      "- **Why It Matters:** Potential impact (generality × learning, or technology development)",
      "- **Intuition Pump(s) Used:** Which prompted this idea",
      "",
      "### Page 2: Critical Analysis",
      "- **Fixed vs. Floating Parameters:**",
      "  - Fixed: What must stay constant",
      "  - Floating: What can adapt",
      "",
      "- **Key Assumptions & Risk Assessment:**",
      "  - Biological assumptions (risk level 1-5)",
      "  - Technical assumptions (risk level 1-5)",
      "",
      "- **Traps Avoided:** Which pitfalls were navigated around?",
      "",
      "- **Alternative Approaches:** Plan B and Plan C",
      "",
      "- **Literature Context:**",
      "  - 3-5 key papers that inform or relate to this work",
      "  - Degree of competition (low/medium/high)",
      "  - The user's edge/advantage",
      "",
      "- **Next Steps:** First 3 concrete experiments or analyses",
      "",
      "## Key Principles to Remember",
      "",
      "1. **Reversal of Polarity:** Treat ideas with skepticism, not reverence. Look for flaws before falling in love.",
      "",
      "2. **Comparison Shopping:** Develop multiple ideas in parallel. The act of comparison improves decision-making.",
      "",
      "3. **Fix One Parameter at a Time:** Constraints engender creativity, but too many constraints prevent it.",
      "",
      "4. **Think in Ensembles:** The user is picking a family of possible projects, not a singular path. Flexibility is essential.",
      "",
      "5. **Balance Logic and Technology:** Novel biology can come from new tools OR clever application of existing tools.",
      "",
      "6. **Systematic Over One-Off:** High-throughput and systematic approaches often reveal more than single observations.",
      "",
      "7. **Dynamic Over Static:** Biological systems are dynamic. How can process be captured rather than snapshot?",
      "",
      "## Getting Started",
      "",
      "When the user is ready, Claude should guide them through the Phase 1 questions to begin the systematic ideation process. The key message: spending extra time on problem choice is the highest-leverage activity in science. A well-chosen problem executed reasonably well will have more impact than a mediocre problem executed brilliantly.",
      "",
      "---",
      "",
      '*This skill is based on the problem choice framework developed by Michael A. Fischbach and Christopher T. Walsh, as described in "Problem choice and decision trees in science and engineering" (Cell, 2024).*',
      "",
    ].join("\n"),
    "references/02-risk-assessment.md": [
      "# SKILL 2: Risk Assessment and Assumption Analysis",
      "",
      "## Overview",
      "This skill helps scientists systematically identify, quantify, and manage project risk through rigorous assumption analysis. The goal is not to eliminate risk—risk-free projects tend to be incremental—but to name it, quantify it, and work steadily to chip away at it. This skill builds directly on the Problem Ideation Document from Skill 1.",
      "",
      "## Core Principle",
      "",
      '**"Don\'t avoid risk; befriend it."**',
      "",
      "The most important concept in problem choice is the two-axis evaluation:",
      "- **X-axis:** Likelihood of success",
      "- **Y-axis:** Impact if successful",
      "",
      "This skill focuses on the X-axis, helping users move their project rightward through systematic risk analysis.",
      "",
      "## Why This Matters",
      "",
      "A project with a high-risk assumption that won't read out for >2 years is problematic. One that requires multiple miracles to succeed should be avoided or refined. The human tendency is to stay in a safe local space, work laterally, and put off facing existential risks—like an ostrich burying its head in the sand. This skill helps users face risk head-on.",
      "",
      "## The Skill Workflow",
      "",
      "### Phase 1: Extract Headquarter Assumptions (10-15 minutes)",
      "",
      "First, Claude should gather information about the user's project from Skill 1:",
      "",
      "1. **Headquarter Summary** (from Skill 1):",
      "   - The biological question",
      "   - The technical approach",
      "   - What's novel about it",
      "",
      "2. **Headquarter Horizon:**",
      "   - How long is this project expected to take? (months/years)",
      "   - What is the user's role? (graduate student, postdoc, PI, startup founder)",
      "",
      "3. **Initial Risk Sense:**",
      "   - What keeps the user up at night about this project?",
      "   - What's the scariest assumption?",
      "",
      "### Phase 2: Comprehensive Assumption Listing",
      "",
      "Claude should work with the user to list EVERY assumption the project makes from inception through conclusion. Assumptions fall into two categories:",
      "",
      "#### Type A: Assumptions About Biological Reality",
      "These are facts about the world that either are or aren't true. They won't change during the project.",
      "",
      "**Examples:**",
      "- New cell types exist beyond what's currently known",
      "- A particular gene regulates the process being studied",
      "- Two proteins physically interact",
      "- A pathway functions in the organism of interest",
      "- The biological effect size is detectable",
      "",
      "#### Type B: Assumptions About Technical Capability",
      "These are about whether technology can do what's needed. These CAN change during the project as methods improve.",
      "",
      "**Examples:**",
      "- A specific cell type can be isolated",
      "- Sequencing will generate high-quality data",
      "- An assay has sufficient throughput",
      "- Computational analysis can distinguish signal from noise",
      "- Gene editing will work in the system",
      "",
      "**Claude should ask:**",
      "1. What must be true about the biology for this to work?",
      "2. What must the technology be able to do?",
      "3. What about the experimental design—what assumptions are built in?",
      "4. What about the analysis—can it deliver what's needed?",
      "5. If everything works, can the findings be validated?",
      "6. Will the findings be interpretable and meaningful?",
      "",
      "### Phase 3: Risk Scoring (The Assumption Analysis Table)",
      "",
      "For each assumption, Claude should help the user assign two scores:",
      "",
      "#### Risk Level (1-5 scale):",
      "- **1** = Very likely to be true/work (>90% confidence)",
      "- **2** = Likely (70-90% confidence)",
      "- **3** = Uncertain (40-70% confidence)",
      "- **4** = Unlikely (10-40% confidence)",
      "- **5** = Very unlikely (<10% confidence)",
      "",
      "#### Time to Test (months):",
      "How long before the user will know if this assumption is valid?",
      "",
      "**Critical Rules:**",
      "1. Be brutally honest—try to convince oneself of being WRONG, not right",
      "2. Distinguish between biological vs. technical assumptions",
      "3. Consider whether technical assumptions might improve over time",
      "4. Note which assumptions depend on earlier assumptions succeeding",
      "",
      "### Phase 4: Risk Profile Evaluation",
      "",
      "Once the complete table is ready, Claude should analyze the risk profile:",
      "",
      "#### Red Flags to Identify:",
      "1. **The Late High-Risk Problem:** Risk level 4-5 assumption that won't read out until >18 months",
      "2. **The Multiple Miracles:** More than 2-3 assumptions with risk level 4-5",
      "3. **The Dependency Chain:** High-risk assumptions stacked in sequence",
      "4. **The Ostrich Pattern:** Starting with low-risk work while avoiding the high-risk tests",
      "",
      "#### Green Lights:",
      "1. **Early Go/No-Go:** Highest-risk assumption testable in <6 months",
      "2. **Multiple Candidates:** Headquarter can succeed with several different outcomes",
      "3. **Graceful Degradation:** If assumption X fails, assumption Y provides alternative path",
      "4. **Risk Distribution:** High-risk assumptions balanced across timeline",
      "",
      "**Rule of Thumb:** If you have a risk level 5 assumption three years out, pick another project.",
      "",
      "### Phase 5: Risk Mitigation Strategies",
      "",
      "For each high-risk assumption (level 4-5), Claude should help develop mitigation strategies:",
      "",
      "#### Strategy 1: Move High-Risk Tests Earlier",
      "**Question:** Can a quicker, cruder test be designed that answers most of what's needed?",
      "",
      "**Example:** Instead of waiting 2 years to validate a new cell type exists, consider:",
      "- Using existing markers as a proxy",
      "- Testing in a simpler model system first",
      "- Using computational predictions to increase confidence",
      "",
      "#### Strategy 2: Multiple Candidates Approach",
      "**Question:** Can multiple candidates be tested in parallel to increase likelihood of success?",
      "",
      "**Example:** Instead of:",
      "- Testing one kinase → Test a panel of 10 kinases",
      "- Building one engineered organism → Build and test a library",
      "- Pursuing one therapeutic target → Pursue 3 related targets",
      "",
      "#### Strategy 3: Reframe the Question",
      "**Question:** Can the project scope be adjusted to reduce critical assumptions while maintaining impact?",
      "",
      "**Example from lecture:**",
      "- **Original:** Identify NEW enteroendocrine cell types (high risk: they may not exist)",
      "- **Reframed:** Better characterize KNOWN but incompletely understood cell types (lower risk)",
      "",
      "#### Strategy 4: Change the System",
      "**Question:** Is there a different biological system with similar scientific value but lower technical risk?",
      "",
      "**Example from lecture:**",
      "- **Original:** Intestinal epithelium (hard to manipulate genetically)",
      "- **Alternative:** Liver (easier genetic manipulation options exist)",
      "",
      "#### Strategy 5: Add Complementary Approaches",
      "**Question:** Can a parallel approach be added that de-risks the main assumption?",
      "",
      "**Example from lecture:**",
      "- Add spatial transcriptomics to scRNA-seq",
      "- This provides biogeographic context and validates cell type existence earlier",
      "",
      "### Phase 6: Go/No-Go Experiment Design",
      "",
      "For the top 3 highest-risk assumptions, Claude should help design the critical go/no-go experiments:",
      "",
      "**For each, specify:**",
      "1. **The Question:** Exactly what is being tested?",
      "2. **The Experiment:** Most direct test possible (even if crude)",
      '3. **Success Criteria:** What result means "go"?',
      '4. **Failure Response:** What result means "pivot" or "stop"?',
      "5. **Timeline:** How soon can this be run?",
      "6. **Resources:** What is needed?",
      "",
      "**Key Principle:** Cut right to the critical go/no-go experiment. Don't just start with easy stuff—the risk points aren't going away.",
      "",
      "### Phase 7: Literature Validation",
      "",
      "Claude should search PubMed to help calibrate risk assessments:",
      "",
      "**Search for:**",
      "1. **Precedents:** Has anyone done something similar? (Reduces technical risk)",
      "2. **Biological Evidence:** What's known about the system? (Informs biological risk)",
      "3. **Technical Benchmarks:** How well do the methods work in practice?",
      "4. **Adjacent Successes:** Has anyone solved related problems?",
      "",
      "**Questions to ask the user:**",
      "- What specific searches would help calibrate risk?",
      "- Are there particular papers that informed the assumptions?",
      "- Are there technical benchmarks to look up?",
      "",
      "### Phase 8: Revised Headquarter Plan",
      "",
      "Based on the risk analysis, Claude should help create a revised plan:",
      "",
      "#### Option A: De-Risk the Current Plan",
      "- Reorder experiments to test high-risk assumptions early",
      "- Add complementary approaches",
      "- Design multiple-candidate strategies",
      "",
      "#### Option B: Reframe the Headquarter",
      "- Adjust scope while maintaining impact",
      "- Change biological system",
      "- Modify technical approach",
      "",
      "#### Option C: Pick a Different Headquarter",
      'Sometimes the honest answer is: "This has too many miracles." That\'s valuable to know BEFORE investing years.',
      "",
      "## Output Deliverable",
      "",
      "Claude should produce a **2-page Risk Assessment Document**:",
      "",
      "### Page 1: Assumption Analysis Table",
      "",
      "| Assumption | Type* | Risk† | Time‡ | Notes |",
      "|------------|-------|-------|-------|-------|",
      "| [Assumption 1] | Bio/Tech | 1-5 | X mo | [Rationale for score] |",
      "| [Assumption 2] | Bio/Tech | 1-5 | X mo | [Rationale for score] |",
      "| ... | ... | ... | ... | ... |",
      "",
      "*Bio = Biological reality, Tech = Technical capability  ",
      "†Risk: 1=very likely to 5=very unlikely  ",
      "‡Time to test in months",
      "",
      "#### Risk Profile Summary:",
      "- **Total Assumptions:** X",
      "- **High Risk (4-5):** X assumptions",
      "- **Late High Risk (>18mo):** X assumptions",
      "- **Critical Path:** [Identify the chain of dependent assumptions]",
      "- **Overall Assessment:** [Green/Yellow/Red light with explanation]",
      "",
      "### Page 2: Risk Mitigation Plan",
      "",
      "#### Top 3 High-Risk Assumptions:",
      "For each:",
      "1. **The Assumption:** [Stated clearly]",
      "2. **Current Risk Level & Timeline:** X (risk) at Y months",
      "3. **Why This Risk Exists:** [Explanation]",
      "4. **Mitigation Strategy:** [From Strategies 1-5 above]",
      "5. **Go/No-Go Experiment:**",
      "   - Experiment design",
      "   - Success criteria",
      "   - Timeline",
      "   - What you'll do if it fails",
      "",
      "#### Revised Headquarter Timeline:",
      "```",
      "Month 0-6:   [Early go/no-go experiments]",
      "Month 6-12:  [Based on go/no-go results]",
      "Month 12-18: [...]",
      "Month 18+:   [...]",
      "```",
      "",
      "#### Contingency Plans:",
      "- **If assumption X fails:** [Plan B]",
      "- **If assumption Y fails:** [Plan C]",
      "- **Multiple success paths:** [How project can succeed different ways]",
      "",
      "#### Decision Points:",
      "- **Month X:** Evaluate [assumptions A, B] → Go/Pivot/Stop decision",
      "- **Month Y:** Evaluate [assumptions C, D] → Go/Pivot/Stop decision",
      "",
      "## Practical Examples",
      "",
      "### Example 1: ScRNA-Seq for Enteroendocrine Cells",
      "",
      "**High-Risk Assumptions Identified:**",
      '1. "New cell types can be validated experimentally" (Risk 5, 24 months)',
      '2. "Knockout will yield biologically relevant phenotype" (Risk 5, 30 months)',
      "",
      "**Problem:** Two risk-5 assumptions at 24+ months = RED FLAG",
      "",
      "**Mitigation Applied:**",
      "- Reframe to study known but poorly characterized cells (reduces Risk 5→3)",
      "- Switch to liver instead of intestine (improves validation timeline: 30→18 months)",
      "- Add spatial transcriptomics (provides earlier validation checkpoint at 16 months)",
      "",
      "### Example 2: Bacterial Therapy for Chronic Kidney Disease",
      "",
      "**High-Risk Assumption Identified:**",
      '"Key uremic toxins leading to effects can be determined" (Risk 4, unknown timeline)',
      "",
      "**Problem:** Critical assumption with unclear path to resolution",
      "",
      "**Mitigation Applied:**",
      "- Focus on known lead toxins (IS and PCS) rather than discovering new ones",
      "- Add parallel track: test multiple toxin candidates",
      "- Design study where learning toxin identity IS the outcome (multiple success paths)",
      "",
      "## Key Principles to Remember",
      "",
      "1. **Try to Convince Yourself You're Wrong:** The goal is critical evaluation, not confirmation bias.",
      "",
      "2. **Ignore Everything But Key Risk Points:** Don't get distracted by easy tasks. The high-risk assumptions aren't going away.",
      "",
      "3. **Early and Often:** Design go/no-go experiments at the earliest feasible moment.",
      "",
      "4. **Be Candid About Risk:** When presenting ideas, acknowledging risk makes your case MORE convincing, not less.",
      "",
      "5. **No Risk, No Interest:** The goal isn't zero risk—it's understood, quantified, manageable risk.",
      "",
      "6. **Risk Can Change:** Technical assumptions may improve as methods advance. Build this into your planning.",
      "",
      "7. **Compare Risk Profiles:** Evaluate multiple projects in parallel to compare risk profiles and make better choices.",
      "",
      "8. **Watch for the Ostrich Pattern:** Are you avoiding the scary experiment? That's human nature, but a critical failure mode.",
      "",
      "## Warning Signs",
      "",
      "**Warning signs include:**",
      "- Risk level 5 assumptions >2 years out",
      "- More than 3 assumptions at risk level 4-5",
      "- Highest-risk assumptions at the END of the timeline",
      '- Rationalizing why high-risk assumptions will "probably work out"',
      '- Planning to "start with the easy stuff" while avoiding risk tests',
      "- Inability to articulate clear go/no-go criteria",
      "",
      "**Good shape indicators:**",
      "- Highest-risk tests happen in first 6 months",
      "- Multiple paths to success exist",
      "- Clear plans for what to do if key assumptions fail",
      "- Risk is distributed across the timeline",
      "- Testing assumptions, not confirming hopes",
      "",
      "## Getting Started",
      "",
      "Claude should begin with Phase 1 by asking for:",
      "1. The project summary from Skill 1",
      "2. Headquarter timeline expectations",
      "3. What concerns the user most about this project",
      "",
      "Together, Claude and the user will build a rigorous risk assessment that dramatically improves the likelihood of success by helping avoid years of work on projects with insurmountable obstacles.",
      "",
      "---",
      "",
      "*Remember: Spending time on risk analysis is the most valuable investment a scientist can make. A well-understood risk profile enables moving forward with confidence or pivoting with clarity—both are valuable outcomes.*",
      "",
    ].join("\n"),
    "references/03-optimization-function.md": [
      "# SKILL 3: Optimization Function Selection",
      "",
      "## Overview",
      "This skill helps scientists articulate HOW their project should be evaluated and define what success means. While Skill 2 focused on likelihood of success (the X-axis), this skill focuses on impact if successful (the Y-axis). The key insight: **value is in the eye of a belief system**—the value creation framework must be explicitly stated and led with.",
      "",
      "## Core Principle",
      "",
      '**"Pick the right optimization function."**',
      "",
      "Different types of projects should be evaluated by different metrics. A common source of conflict between trainees and PIs, or authors and referees, is a misunderstanding about which category a project falls under. The root cause is often failure to articulate evaluation criteria clearly.",
      "",
      "## The Fundamental Truth",
      "",
      "The default state of:",
      "1. Every new discovery is **irrelevance**",
      "2. Every new technology is **non-use**",
      "3. Every company is **death**",
      "",
      "Scientists must actively work against these defaults by choosing the right metrics and scoring well on at least one axis.",
      "",
      "## The Skill Workflow",
      "",
      "### Phase 1: Headquarter Categorization (5 minutes)",
      "",
      "First, Claude should determine what type of project the user is pursuing:",
      "",
      "**Question 1: What is the primary goal?**",
      "A. Understand how biology works (fundamental knowledge)",
      "B. Enable new experiments or capabilities (tool/technology)",
      "C. Solve a practical problem (invention/application)",
      "D. Something else (please describe)",
      "",
      '**Question 2: What would "success" look like in 3-5 years?**',
      "- 1-2 sentences describing the ideal outcome",
      "",
      "**Question 3: Who cares if this succeeds?**",
      "- Academic researchers in the subfield?",
      "- Broader scientific community across fields?",
      "- Clinicians or practitioners?",
      "- Industry partners or companies?",
      "- General public or specific communities?",
      "- All of the above?",
      "",
      "Based on the answers, Claude should help identify the right optimization function.",
      "",
      "### Phase 2: Understanding the Three Main Frameworks",
      "",
      "#### Framework 1: Basic Science",
      "**Axes:** How much did we learn? × How general/fundamental is the object of study?",
      "",
      "**Philosophy:** A high score on EITHER axis yields substantial impact. You don't need both.",
      "",
      "**Examples:**",
      "- **High Generality, Medium Learning:** Ribosome stalling complex",
      "  - Updates understanding of translation (fundamental process)",
      "  - Scores well because translation is universal",
      "  ",
      "- **Medium Generality, High Learning:** Oxytricha germ-line nucleus",
      "  - Genomic acrobatics may not be common to other organisms",
      "  - BUT elegant mapping scores highly on how much we learned",
      "  - May yield tools for genome editing (bonus)",
      "  ",
      "- **High on Both Axes (Landmark):** RNA interference, biomolecular condensates",
      "  - These are rare—don't expect every project to be here",
      "  - But aim to score well on at least one axis",
      "",
      "**Key Questions:**",
      "- How many systems/organisms does this apply to?",
      "- Does it update understanding of a fundamental process?",
      "- Will textbooks need to be rewritten?",
      "- What new questions does this open?",
      "",
      "#### Framework 2: Technology Development",
      "**Axes:** How widely will it be used? × How critical is it for the application?",
      "",
      "**Philosophy:** Again, high score on EITHER axis is sufficient.",
      "",
      "**Examples:**",
      "- **Widely Used, Not Critical:** BLAST",
      "  - Used in countless projects",
      "  - Rarely THE critical tool, but enormous cumulative impact",
      "  ",
      "- **Not Widely Used, Highly Critical:** Cryo-electron tomography",
      "  - Too complicated for broad adoption",
      "  - But generates stunning data that's impossible to get otherwise",
      "  - When you need it, nothing else works",
      "  ",
      "- **High on Both Axes (Game-Changing):** ",
      "  - GFP, CRISPR, AlphaFold (the famous ones)",
      "  - But also: lentiviral delivery, cell sorting, massively parallel sequencing",
      "  - Technologies we cannot imagine living without",
      "",
      "**Key Questions:**",
      "- How many labs would adopt this?",
      "- For what fraction of experiments is this THE enabling technology?",
      "- What becomes possible that wasn't before?",
      "- How hard is it to implement?",
      "",
      "**Critical Rule:** A tool that won't be widely used AND isn't critical for an application probably isn't worth building.",
      "",
      "#### Framework 3: Typical Invention/Application",
      "**Axes:** How much good? × For how many people?",
      "",
      "**Philosophy:** Useful for translational work, frugal science, global health.",
      "",
      "**Examples:**",
      "- Foldscope: Paper microscope accessible to millions of students globally",
      "- Neglected tropical disease intervention: Quality-adjusted life years per $100",
      "- Medical device: Number of patients who can access treatment",
      "",
      "**Key Questions:**",
      "- What problem does this solve?",
      "- How many people have this problem?",
      "- How much better is their life if you solve it?",
      "- What's the cost per person helped?",
      "",
      "### Phase 3: Selecting and Articulating Your Framework",
      "",
      "Based on your Phase 1 responses, let me help you choose:",
      "",
      "**If you selected A (fundamental knowledge):** → Basic Science Framework",
      "**If you selected B (enable experiments):** → Technology Development Framework  ",
      "**If you selected C (solve practical problem):** → Invention Framework",
      "",
      "**Now, let's be explicit:**",
      "",
      '1. **State Your Framework:** "This project should be evaluated as [basic science/technology development/invention]."',
      "",
      "2. **Define Your Axes:** ",
      "   - X-axis measures: [specific metric]",
      "   - Y-axis measures: [specific metric]",
      "",
      "3. **Make Your Case:**",
      "   - X-axis score (Low/Medium/High): [Your assessment + reasoning]",
      "   - Y-axis score (Low/Medium/High): [Your assessment + reasoning]",
      "",
      "4. **Threshold Check:** ",
      "   - Do you score at least MEDIUM-HIGH on one axis?",
      "   - If both are LOW-MEDIUM, you have a problem",
      "",
      "### Phase 4: Alternative or Custom Metrics",
      "",
      "Sometimes standard frameworks don't fit. Examples where custom metrics work:",
      "",
      "**Alternative Metric Examples:**",
      "- **Frugal Science:** How many children in low/middle-income countries gain access to microscopy?",
      "- **Neglected Disease:** Quality-adjusted life years saved per $100 invested",
      "- **Sustainability:** Tons of CO₂ equivalent prevented × cost-effectiveness",
      "- **Equity:** Reduction in disparity metric × number of people affected",
      "",
      "**When to propose alternative metrics:**",
      "- Your work addresses a specific underserved need",
      "- Standard metrics miss your core value proposition",
      "- You're working in an emerging area without established norms",
      "- Your work crosses traditional boundaries",
      "",
      "**How to propose alternative metrics:**",
      "1. Explain why standard metrics are insufficient",
      "2. Define your proposed metric clearly",
      "3. Provide a value creation index (two axes)",
      "4. Show how your project scores on these axes",
      "",
      "### Phase 5: Comparative Assessment",
      "",
      "Even if absolute impact is hard to estimate, comparative assessment is valuable:",
      "",
      "**Exercise: Compare 3 Related Headquarters**",
      "",
      "For your project and two alternatives (either from literature or hypothetical):",
      "",
      "| Headquarter | Framework | X-Axis Score | Y-Axis Score | Overall |",
      "|---------|-----------|--------------|--------------|---------|",
      "| Yours | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |",
      "| Alt 1 | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |",
      "| Alt 2 | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |",
      "",
      "**Comparative Questions:**",
      "- Which would be most impactful if they all work?",
      "- Which has the best risk-adjusted impact?",
      "- Are you pursuing the best option?",
      "- If not, why? (Sometimes there are good reasons: resources, expertise, timing)",
      "",
      "### Phase 6: Avoiding Metric Mismatch",
      "",
      "**Common Mismatches:**",
      "",
      "#### Mismatch 1: Basic Science vs. Technology Evaluation",
      '**Scenario:** You\'re doing fundamental biology, but reviewers ask "How widely will this be used?"',
      "",
      "**Problem:** They're evaluating basic science with technology metrics",
      "",
      '**Solution:** Explicitly frame as basic science. Lead with: "This updates our understanding of [fundamental process], which is conserved across [many systems]."',
      "",
      "#### Mismatch 2: Technology vs. Basic Science Evaluation",
      '**Scenario:** You\'re building a tool, but reviewers ask "How much did we learn about biology?"',
      "",
      "**Problem:** They're evaluating technology with basic science metrics",
      "",
      '**Solution:** Explicitly frame as technology development. Lead with: "This enables experiments that are currently impossible, which [X] labs need for [Y] applications."',
      "",
      "#### Mismatch 3: Within-Category Confusion",
      "**Scenario:** Your basic science is specific but deep, but reviewers want broad generality",
      "",
      "**Problem:** They think both axes are required, rather than either/or",
      "",
      '**Solution:** Explicitly acknowledge: "While this may not be universal, the depth of mechanistic insight scores highly on the learning axis."',
      "",
      "#### Mismatch 4: Time Horizon Mismatch",
      "**Scenario:** You're working on long-term fundamental research, but reviewers want immediate impact",
      "",
      "**Problem:** Different value systems about when impact should materialize",
      "",
      "**Solution:** Articulate your time horizon explicitly and provide historical examples of similar timelines",
      "",
      "### Phase 7: Value System Discussion",
      "",
      "This is where Claude explicitly discusses the user's belief system about what matters:",
      "",
      "**Questions for Reflection:**",
      "",
      "1. **What drives the user?**",
      "   - Discovery and understanding?",
      "   - Enabling others?",
      "   - Solving problems?",
      "   - Building things?",
      "",
      "2. **What would make the user proud?**",
      "   - Paper in Cell/Nature/Science?",
      "   - Tool used by hundreds of labs?",
      "   - Treatment reaching patients?",
      "   - Opening a new field?",
      "",
      "3. **How does the user want to be remembered?**",
      '   - "Discovered X"',
      '   - "Built Y that enabled Z"',
      '   - "Solved problem W"',
      '   - "Trained students who went on to..."',
      "",
      "4. **Whose approval matters?**",
      "   - Specific senior scientists in the field?",
      "   - Broader community across fields?",
      "   - Practitioners who use tools?",
      "   - People whose lives are improved?",
      "",
      "**There are no wrong answers—but alignment matters:**",
      "- The project should match the user's value system",
      "- The evaluation framework should match the project type",
      "- Communication should lead with the framework",
      "",
      "### Phase 8: Literature Benchmarking",
      "",
      "Claude should use PubMed to benchmark impact in the user's area:",
      "",
      "**Searches should include:**",
      "",
      "1. **Impact Exemplars:** Papers the user considers high-impact in the field",
      "   - What framework did they use (implicitly or explicitly)?",
      "   - How did they score on the axes?",
      "   - What made them successful?",
      "",
      "2. **Analogous Headquarters:** Similar approaches or systems",
      "   - How were they evaluated?",
      "   - What impact did they achieve?",
      "   - What can be learned from their framing?",
      "",
      "3. **Field Expectations:** What's typical for the area?",
      "   - Are basic science papers common?",
      "   - Is technology development valued?",
      '   - What level of impact is "good enough"?',
      "",
      "**Questions to ask the user:**",
      "- What papers should be analyzed as benchmarks?",
      "- What search terms capture the field's impact exemplars?",
      "- Are there specific journals or authors whose framing to emulate?",
      "",
      "### Phase 9: Communication Strategy",
      "",
      "Once the framework is selected, here's how to lead with it:",
      "",
      "#### In Talks:",
      "**Opening Frame (within first 2 slides):**",
      '- "The goal of this work is to understand [fundamental process X] in [general system Y]" → Basic science',
      '- "We\'re developing a technology that will enable [critical experiment X] for [community Y]" → Technology',
      '- "This invention addresses [problem X] affecting [N] people" → Application',
      "",
      "#### In Papers:",
      "**Abstract Structure:**",
      "- State your framework implicitly through word choice",
      '- Basic science: "reveals," "demonstrates," "shows that"',
      '- Technology: "enables," "provides," "makes it possible to"',
      '- Application: "solves," "addresses," "improves"',
      "",
      "#### In Grants:",
      "**Broader Impact Section:**",
      "- Explicitly name your evaluation framework",
      "- Provide the two-axis assessment",
      "- Score yourself on each axis with evidence",
      "",
      "#### With Your PI/Committee:",
      "**Alignment Conversation:**",
      '- "I want to make sure we\'re aligned on how this should be evaluated"',
      '- "I see this as [framework], scoring [X] on [axis 1] and [Y] on [axis 2]"',
      '- "Do you agree, or do you see it differently?"',
      '- "This matters because..." [explain downstream implications]',
      "",
      "## Output Deliverable",
      "",
      "Claude should produce a **2-page Impact Assessment Document**:",
      "",
      "### Page 1: Framework and Scoring",
      "",
      "#### Headquarter Categorization:",
      "- **Type:** Basic Science / Technology Development / Invention / Custom",
      "- **Rationale:** [Why this categorization fits]",
      "",
      "#### Optimization Function:",
      "- **X-Axis:** [Metric name and definition]",
      "- **Y-Axis:** [Metric name and definition]",
      "- **Custom Rationale (if applicable):** [Why standard metrics don't fit]",
      "",
      "#### Self-Assessment:",
      "",
      "**X-Axis Score: [Low/Medium/High]**",
      "- Evidence: [Specific reasons for this score]",
      "- Examples: [Comparable projects or benchmarks]",
      "- PubMed Support: [Key papers that inform assessment]",
      "",
      "**Y-Axis Score: [Low/Medium/High]**",
      "- Evidence: [Specific reasons for this score]",
      "- Examples: [Comparable projects or benchmarks]",
      "- PubMed Support: [Key papers that inform assessment]",
      "",
      "**Overall Assessment:**",
      "- Score on at least one axis: ☑ Yes / ☐ No",
      "- Strong justification: ☑ Yes / ☐ No",
      "- Aligned with your values: ☑ Yes / ☐ No",
      "",
      "#### Visual Framework:",
      "```",
      "         [Your Headquarter Type]",
      "              ",
      "Y-Axis    |           ★ Your Headquarter",
      "[Metric]  |         /",
      "          |       /",
      "          |     /",
      "          |   /",
      "          |_________________",
      "              X-Axis [Metric]",
      "              ",
      "★ = Your project",
      "Reference projects plotted for context",
      "```",
      "",
      "### Page 2: Communication and Alignment",
      "",
      "#### Value System Alignment:",
      "- **What Drives You:** [Discovery/Enabling/Problem-solving/Building]",
      "- **Success Definition:** [What would make this worthwhile]",
      "- **Approval Sources:** [Whose opinion matters and why]",
      "- **Framework Fit:** [How project aligns with values]",
      "",
      "#### Potential Mismatches to Avoid:",
      "1. [Specific mismatch type]",
      "   - Scenario: [When this might happen]",
      "   - Prevention: [How to frame to avoid it]",
      "",
      "2. [Another mismatch]",
      "   - Scenario: [When this might happen]",
      "   - Prevention: [How to frame to avoid it]",
      "",
      "#### Communication Strategy:",
      "",
      "**For Talks:**",
      "- Opening frame: [Exact language to use in first 2 slides]",
      "- Key phrases: [Vocabulary that signals your framework]",
      "",
      "**For Papers:**",
      "- Abstract structure: [Framework-appropriate language]",
      "- Impact statement: [How to articulate in discussion]",
      "",
      "**For Grants:**",
      "- Broader impact: [How to score yourself explicitly]",
      "- Justification: [Evidence for scores]",
      "",
      "**For Mentors:**",
      "- Alignment question: [Exact question to ask]",
      "- Your perspective: [How you see it]",
      "- Discussion points: [What matters for alignment]",
      "",
      "#### Comparative Analysis:",
      "",
      "| Headquarter | Type | X-Score | Y-Score | Notes |",
      "|---------|------|---------|---------|-------|",
      "| Yours | [Type] | [L/M/H] | [L/M/H] | [Key strengths] |",
      "| Benchmark 1 | [Type] | [L/M/H] | [L/M/H] | [What you can learn] |",
      "| Benchmark 2 | [Type] | [L/M/H] | [L/M/H] | [What you can learn] |",
      "| Alternative | [Type] | [L/M/H] | [L/M/H] | [Why not pursuing] |",
      "",
      "#### Action Items:",
      "1. [Specific step to strengthen X-axis score or argument]",
      "2. [Specific step to strengthen Y-axis score or argument]",
      "3. [Communication alignment with key stakeholders]",
      "",
      "## Practical Examples",
      "",
      "### Example 1: Ribosome Stalling (Basic Science)",
      "- **Framework:** Basic science",
      "- **X-Axis (Generality):** HIGH—translation is universal",
      "- **Y-Axis (Learning):** MEDIUM—mechanism of one quality control system",
      "- **Assessment:** High on generality alone = substantial impact",
      '- **Communication:** "Updates our understanding of translation quality control"',
      "",
      "### Example 2: BLAST (Technology)",
      "- **Framework:** Technology development",
      "- **X-Axis (Widely Used):** VERY HIGH—used by virtually all molecular biologists",
      "- **Y-Axis (Critical):** LOW-MEDIUM—helpful but rarely essential",
      "- **Assessment:** Extreme breadth of use = enormous cumulative impact",
      '- **Communication:** "Enables rapid sequence comparison across all biological databases"',
      "",
      "### Example 3: Cryo-EM Tomography (Technology)",
      "- **Framework:** Technology development",
      "- **X-Axis (Widely Used):** LOW—complex, expensive, specialized",
      "- **Y-Axis (Critical):** VERY HIGH—generates impossible-to-get-otherwise data",
      "- **Assessment:** Extreme criticality for niche = high impact",
      '- **Communication:** "Enables 3D visualization of molecular machines in native cellular context"',
      "",
      "### Example 4: Foldscope (Invention)",
      "- **Framework:** Invention (custom metric)",
      "- **X-Axis (Good):** MEDIUM—functional microscopy",
      "- **Y-Axis (People):** VERY HIGH—millions of students globally",
      "- **Assessment:** Massive reach × modest utility = transformative for education",
      '- **Communication:** "Democratizes microscopy for global education"',
      "",
      "## Key Principles to Remember",
      "",
      "1. **Value Is in the Eye of a Belief System:** Make yours explicit.",
      "",
      "2. **Lead with Your Metric:** Don't assume others share your framework.",
      "",
      "3. **Either Axis Suffices:** You don't need both—just score well on one.",
      "",
      "4. **Articulate Early:** Discuss with mentors before you're 2 years in.",
      "",
      "5. **Avoid Default State:** Work actively against irrelevance/non-use.",
      "",
      "6. **Compare, Don't Absolute:** Even rough comparison beats ignoring impact.",
      "",
      "7. **Align Communication:** Your words should signal your framework.",
      "",
      "8. **Match Headquarter to Values:** Life is too short for misaligned work.",
      "",
      "## Warning Signs",
      "",
      "**Warning signs include:**",
      "- Inability to articulate which framework applies",
      "- Scoring LOW on both axes",
      "- Headquarter type and evaluation framework don't match",
      "- User and PI have different frameworks but haven't discussed it",
      "- Using basic science metrics for a tool or vice versa",
      "- Never explicitly discussing impact assessment",
      "",
      "**Good shape indicators:**",
      "- Clear statement of optimization function",
      "- MEDIUM-HIGH score on at least one axis",
      "- Framework matches project type",
      "- Alignment with key stakeholders",
      "- Communication signals framework clearly",
      "- Benchmarking against comparable work",
      "",
      "## Getting Started",
      "",
      "Claude should begin Phase 1 by asking:",
      "1. What is the primary goal? (A/B/C/D)",
      "2. What would success look like in 3-5 years?",
      "3. Who cares if this succeeds?",
      "",
      "Together, Claude and the user will select the right optimization function and position the work for maximum impact.",
      "",
      "---",
      "",
      "*Remember: Impact assessment isn't about ego—it's about ensuring work matters in the way the scientist wants it to matter. Explicit framing prevents years of misalignment.*",
      "",
    ].join("\n"),
    "references/04-parameter-strategy.md": [
      "# SKILL 4: Parameter Fixation Strategy",
      "",
      "## Overview",
      "This skill helps scientists strategically decide which parameters to fix and which to keep flexible in their project. The paradox: too many fixed parameters creates brittleness, but too few causes paralysis. The key is fixing ONE parameter thoughtfully and letting others float—constraints engender creativity.",
      "",
      "## Core Principle",
      "",
      '**"Fix one parameter; let the others float."**',
      "",
      'Most failure modes in ideation involve fixing too many parameters at the outset (system + method + application). Conversely, statements like "I want to do impactful work in cell engineering" are so broad they cause paralysis. The sweet spot: fix one meaningful constraint and let creativity flow within that boundary.',
      "",
      "## What Are Headquarter Parameters?",
      "",
      "Parameters are the choices that define your project:",
      "",
      "**Common Parameters:**",
      "- **System:** Which organism/cell type/tissue/molecule?",
      "- **Question:** What biological phenomenon to study?",
      "- **Tool/Method:** Which experimental approach?",
      "- **Application:** What practical use or goal?",
      "- **Output:** What form will results take?",
      "- **Collaborators:** Who will you work with?",
      "- **Timeline:** How fast must you move?",
      "- **Resources:** What's available/necessary?",
      "",
      "## The Skill Workflow",
      "",
      "### Phase 1: Parameter Inventory (10 minutes)",
      "",
      "First, let's identify what's already fixed in your current project idea:",
      "",
      "**Question 1: List your project parameters**",
      "",
      "For each category, indicate if it's **FIXED** (must stay) or **FLOATING** (could change):",
      "",
      "| Parameter Type | Your Choice | Status (F/FL) | Why Fixed? |",
      "|----------------|-------------|---------------|------------|",
      "| **System** | [organism/cell/tissue] | F / FL | [reason] |",
      "| **Question** | [biological phenomenon] | F / FL | [reason] |",
      "| **Tool/Method** | [techniques] | F / FL | [reason] |",
      "| **Application** | [use case/goal] | F / FL | [reason] |",
      "| **Timeline** | [duration] | F / FL | [reason] |",
      "| **Resources** | [equipment/funding] | F / FL | [reason] |",
      "",
      "**Question 2: Count your fixed parameters**",
      "- How many did you mark as FIXED? _____",
      "- If >2, you may have over-constrained the problem",
      "",
      "**Question 3: Why are they fixed?**",
      "For each fixed parameter, is it because:",
      "A. Your expertise/passion",
      "B. Lab resources/capabilities  ",
      "C. Advisor requirements",
      'D. You think it\'s the "best" solution',
      "E. Historical accident (you started this way)",
      "",
      "### Phase 2: The GLP-1 Example (Case Study)",
      "",
      "Let's learn from a concrete example:",
      "",
      "**Proposed Headquarter:** Engineer a T cell to produce GLP-1 (glucagon-like peptide-1) for continuous supply.",
      "",
      "**Analysis: What's Fixed?**",
      "1. Improving GLP-1 receptor agonist delivery characteristics (the problem)",
      "2. Using an engineered T cell (the solution)",
      "",
      "**Problem:** Two parameters fixed = poor technique-application match",
      "",
      "**Alternative Framings:**",
      "",
      "**If you fix Parameter 1 (GLP-1 delivery):**",
      "- Let the solution float",
      "- Better options: peptide engineering for extended half-life, oral peptides, small molecules, B cells (better protein secretion)",
      "- Why T cell is suboptimal: Not designed for protein secretion",
      "- **Best for:** Trainee in metabolism lab who cares about GLP-1",
      "",
      "**If you fix Parameter 2 (Engineered T cell):**",
      "- Let the application float",
      "- Better options: local-acting peptides (cytokines, chemokines, growth factors) for oncology/autoimmunity/regeneration",
      "- Why GLP-1 is suboptimal: Doesn't leverage T cell's natural capabilities",
      "- **Best for:** Trainee in immunology/cell engineering lab",
      "",
      "**Key Insight:** Which parameter you fix depends on YOUR interests and your lab's expertise. Both can lead to great projects, but they're DIFFERENT projects.",
      "",
      "### Phase 3: Diagnostic Questions",
      "",
      "**The Goldilocks Test:**",
      "",
      "**Too Many Fixed Parameters (>2):**",
      "- Are you forcing a technique-application match?",
      "- If one assumption fails, does everything fail?",
      "- Are you more attached to HOW than WHAT?",
      '- Does your project sound like: "Use X to do Y in Z"?',
      "",
      "**Too Few Fixed Parameters (0-1 very broad):**",
      "- Do you feel paralyzed where to start?",
      '- Is your statement super generic? ("Do impactful work in...")',
      "- Are you avoiding commitment?",
      "- Do you have decision fatigue?",
      "",
      "**Just Right (1-2 well-chosen):**",
      "- Do you have creative constraints?",
      "- Can you articulate why THIS constraint matters?",
      "- If one approach fails, do alternatives exist?",
      "- Does the constraint energize you?",
      "",
      "### Phase 4: The Illumina Example (Constraints Drive Innovation)",
      "",
      "**Historical Context:** Next-generation sequencing wasn't designed; we got Illumina's approach (many short reads).",
      "",
      "**Initial Constraint:** Short reads seemed like a limitation",
      '- Not what we would have "asked for"',
      "- Seemed inferior to long reads",
      "",
      "**Innovation Unleashed:**",
      "- Computational methods (assembly algorithms)",
      "- Novel applications (RNA-seq, ChIP-seq, ATAC-seq)",
      "- Unexpected uses (protein folding via sequencing)",
      "- Biochemical creativity to work within constraints",
      "",
      "**Lesson:** Constraints don't limit creativity—they focus it. If you feel stuck, fix ONE parameter and watch resourcefulness emerge.",
      "",
      "### Phase 5: Which Parameter Should You Fix?",
      "",
      "**Strategic Questions to Identify the Right Fixed Parameter:**",
      "",
      "1. **What can you prototype quickly?**",
      "   - What test article could you build rapidly?",
      "   - Which experimental conditions enable early go/no-go?",
      "   - What gives you fastest feedback?",
      "",
      "2. **What are people around you unusually good at?**",
      "   - Lab expertise?",
      "   - Core facility capabilities?",
      "   - Collaborator strengths?",
      "   - Your unique skill combination?",
      "",
      "3. **What do you enjoy so much you don't think of it as work?**",
      "   - System you're passionate about?",
      "   - Technique you love?",
      "   - Type of question that excites you?",
      "",
      "4. **What's your competitive advantage?**",
      "   - Unique resource access?",
      "   - Rare skill combination?",
      "   - Proprietary data/reagents?",
      "   - First-mover opportunity?",
      "",
      "**Common Strategic Choices:**",
      "",
      "**Fix the System (Let question & tool float):**",
      "- Good if: You're an expert in the organism/tissue/cell type",
      "- Enables: Asking multiple questions, trying various tools",
      '- Example: "I study *Drosophila* neural development; I\'ll let the specific questions and methods emerge"',
      "",
      "**Fix the Question (Let system & tool float):**",
      "- Good if: You care deeply about a biological phenomenon",
      "- Enables: Testing across systems, using best tool for each",
      "- Example: \"I want to understand phase separation; I'll study it wherever it's clearest\"",
      "",
      "**Fix the Tool (Let system & question float):**",
      "- Good if: You're developing or mastering a technology",
      "- Enables: Finding best applications, comparing across systems",
      "- Example: \"I'm building a new microscopy method; I'll find the most impactful uses\"",
      "",
      "**Fix the Application (Let system & tool float):**",
      "- Good if: You have a specific translational goal",
      "- Enables: Trying multiple approaches, testing in different models",
      '- Example: "I want to treat disease X; I\'m open to any validated approach"',
      "",
      "### Phase 6: Parameter Flexibility Matrix",
      "",
      "For your project, let's create a flexibility assessment:",
      "",
      "| Parameter | Currently | Should Be? | If Problem Arises, Could This Float? |",
      "|-----------|-----------|------------|--------------------------------------|",
      "| System | [F/FL] | [F/FL] | Yes / No / Maybe |",
      "| Question | [F/FL] | [F/FL] | Yes / No / Maybe |",
      "| Tool | [F/FL] | [F/FL] | Yes / No / Maybe |",
      "| Application | [F/FL] | [F/FL] | Yes / No / Maybe |",
      "| Timeline | [F/FL] | [F/FL] | Yes / No / Maybe |",
      "| Resources | [F/FL] | [F/FL] | Yes / No / Maybe |",
      "",
      "**Analysis:**",
      '- **Flexibility Score:** How many "Yes" or "Maybe"? _____',
      "- **Risk Assessment:** If <3 can float, you're brittle",
      "- **Pivot Potential:** Which parameters provide escape routes?",
      "",
      "### Phase 7: Scenario Planning",
      "",
      "For each fixed parameter, let's plan what happens if it becomes untenable:",
      "",
      "**Fixed Parameter 1: [Name it]**",
      "- **Why it's fixed:** [Your reason]",
      "- **Risk if this fails:** [What breaks]",
      "- **Contingency:** [What could you float instead]",
      "- **Alternative project:** [If you fixed something else]",
      "",
      "**Fixed Parameter 2: [Name it]**",
      "- **Why it's fixed:** [Your reason]  ",
      "- **Risk if this fails:** [What breaks]",
      "- **Contingency:** [What could you float instead]",
      "- **Alternative project:** [If you fixed something else]",
      "",
      "### Phase 8: The Unfixing Exercise",
      "",
      "Sometimes you need to unfix parameters to escape a rut:",
      "",
      "**Current State:** [Describe your over-constrained project]",
      "",
      "**Unfixing Experiment:**",
      "",
      "**Try 1: Unfix the System**",
      "- Keep question & tool",
      "- What other systems could you study?",
      "- Which would be easier/faster/more informative?",
      "",
      "**Try 2: Unfix the Tool**",
      "- Keep system & question",
      "- What other methods exist?",
      "- Which are more mature/accessible/powerful?",
      "",
      "**Try 3: Unfix the Question**",
      "- Keep system & tool",
      "- What other questions could you ask?",
      "- Which would be more impactful/feasible?",
      "",
      '**Evaluation:** Does any "unfixed" version seem better than your original? If yes, you over-constrained.',
      "",
      "### Phase 9: Literature Reality Check",
      "",
      "Let's use PubMed to see how others handled parameter fixation:",
      "",
      "**Search 1: Successful projects in your area**",
      "- What did they fix?",
      "- What did they let float?",
      "- Did they pivot from their initial parameter choices?",
      "",
      "**Search 2: Failed or stalled projects**",
      "- (Often in discussion sections or preprints)",
      "- Did they over-constrain?",
      "- What parameters trapped them?",
      "",
      "**Search 3: Method papers**",
      "- How did technology developers choose applications?",
      "- Did they fix the tool and let applications emerge?",
      "",
      "**Your Searches:**",
      "What specific papers should we analyze for parameter lessons?",
      "",
      "## Output Deliverable",
      "",
      "**2-Page Parameter Strategy Document**",
      "",
      "### Page 1: Current State and Analysis",
      "",
      "#### Parameter Inventory:",
      "| Parameter | Current Status | Strategic Rationale | Flexibility |",
      "|-----------|----------------|---------------------|-------------|",
      "| System | Fixed: [X] | [Why] | Can float if: [condition] |",
      "| Question | Floating: [Y,Z] | [Why] | Constrained by: [X] |",
      "| Tool | [Status] | [Why] | [Contingency] |",
      "| Application | [Status] | [Why] | [Contingency] |",
      "",
      "#### Diagnostic Summary:",
      "- **Fixed Parameters:** [Count and list]",
      "- **Assessment:** ☐ Too Many (>2) / ☐ Just Right (1-2) / ☐ Too Few (0, too broad)",
      "- **Primary Fixed Parameter:** [The one that matters most]",
      "- **Reason for Fixation:** [Expertise/Passion/Resources/Other]",
      "",
      "#### Goldilocks Test Results:",
      "- Over-constrained indicators: [Yes/No to each test]",
      "- Under-constrained indicators: [Yes/No to each test]",
      "- Verdict: [Analysis]",
      "",
      "### Page 2: Strategy and Contingencies",
      "",
      "#### Recommended Parameter Strategy:",
      "",
      "**Core Fixed Parameter:** [The one to keep]",
      "- **Rationale:** [Why this one]",
      "- **Your advantage:** [Expertise/access/passion]",
      "- **Enables:** [What becomes possible]",
      "",
      "**Parameters That Should Float:** [List]",
      "- [Parameter 1]: [How to explore alternatives]",
      "- [Parameter 2]: [How to explore alternatives]",
      "",
      "#### If Core Assumptions Fail:",
      "",
      "**Scenario 1: [Specific failure mode]**",
      "- **Unfix:** [Which parameter to let float]",
      "- **Alternative 1:** [New configuration]",
      "- **Alternative 2:** [Another option]",
      "",
      "**Scenario 2: [Another failure mode]**",
      "- **Unfix:** [Which parameter]",
      "- **Alternative 1:** [New configuration]",
      "- **Alternative 2:** [Another option]",
      "",
      "#### Headquarter Ensemble:",
      "```",
      "Core Fixed: [X]",
      "",
      "Possible Headquarters:",
      "1. [X] + [A] + [B1] → [Outcome]",
      "2. [X] + [A] + [B2] → [Outcome]",
      "3. [X] + [C] + [B1] → [Outcome]",
      "",
      "All share [X], but float other parameters",
      "```",
      "",
      "#### Strategic Questions Answered:",
      "1. **Quick prototype:** [How to test quickly]",
      "2. **Team strengths:** [Who's good at what]",
      "3. **Your passion:** [What energizes you]",
      "4. **Competitive advantage:** [Your edge]",
      "",
      "#### Historical Parallel:",
      "[Example like Illumina where constraints drove innovation in your field]",
      "- The constraint: [What seemed limiting]",
      "- The innovation: [How people worked within it]",
      "- Your application: [How this applies to your project]",
      "",
      "## Practical Examples",
      "",
      "### Example 1: GLP-1 T Cell Headquarter (Over-Constrained)",
      "- **Fixed:** GLP-1 delivery + T cell engineering",
      "- **Problem:** Poor technique-application match",
      "- **Solution:** Unfix one parameter",
      "  - Fix delivery, float cell type → Better options emerge",
      "  - Fix T cell, float payload → Better applications emerge",
      "",
      "### Example 2: Drosophila Neurobiologist (Well-Constrained)",
      "- **Fixed:** *Drosophila* nervous system",
      "- **Floating:** Specific questions, methods",
      "- **Works because:** Deep system expertise, many tools available",
      "- **Enables:** Pursuing most impactful questions as field evolves",
      "",
      '### Example 3: "Impactful Cell Engineering" (Under-Constrained)',
      "- **Fixed:** Nothing specific",
      "- **Problem:** Paralysis from too many options",
      "- **Solution:** Fix one meaningful constraint",
      "  - Option A: Fix CAR-T platform → Find best applications",
      "  - Option B: Fix autoimmune disease → Find best cell engineering approach",
      "  - Option C: Fix specific rare disease → Let methods emerge",
      "",
      "## Key Principles to Remember",
      "",
      "1. **Constraints Engender Creativity:** Limitations focus resourcefulness",
      "",
      "2. **One Parameter Rule:** Fix one meaningful constraint, let others float",
      "",
      "3. **Match to Your Strengths:** Fix the parameter you have advantage in",
      "",
      "4. **Technique-Application Match:** Don't force tools into wrong problems",
      "",
      "5. **Flexibility = Resilience:** Floating parameters provide pivot options",
      "",
      "6. **Historical Lesson:** Best technologies emerged from working within constraints (Illumina)",
      "",
      "7. **Not Forever:** Parameters can unfix mid-project when stuck",
      "",
      "## Warning Signs",
      "",
      "**Over-Constrained (Too Many Fixed):**",
      '- Headquarter sounds like: "Use X to study Y in Z"',
      "- When one assumption fails, everything fails",
      "- You're attached to HOW more than WHAT",
      "- Forcing a technique-application match",
      "",
      "**Under-Constrained (Too Few/Vague):**",
      '- Statement is incredibly broad ("impactful work in...")',
      "- Feeling paralyzed about where to start",
      "- Avoiding commitment due to infinite options",
      "- No clear next experimental step",
      "",
      "**Well-Constrained:**",
      "- One clear fixed parameter with good rationale",
      "- Multiple paths within that constraint",
      "- Energized by the focused challenge",
      "- If one approach fails, alternatives exist",
      "",
      "## Ready to Begin?",
      "",
      "Let's start with Phase 1. Please provide:",
      "1. Your current project description",
      "2. List of what you think is fixed vs. floating",
      "3. Your lab's core expertise",
      "4. What aspect excites you most",
      "",
      "Together we'll optimize your parameter strategy for maximum creativity and resilience.",
      "",
      "---",
      "",
      "*Remember: The right constraint is liberating, not limiting. It channels creativity into productive directions while maintaining flexibility for pivots.*",
      "",
    ].join("\n"),
    "references/05-decision-tree.md": [
      '# SKILL 5: Decision Tree Navigation ("The Altitude Dance")',
      "",
      "## Overview",
      "This skill teaches you to move fluidly between execution (Level 1: getting stuff done) and strategic evaluation (Level 2: critical thinking). Headquarters rarely unfold linearly—they require frequent course correction. Most trainees should spend MORE time on their project's decision tree.",
      "",
      "## Core Principle",
      '**"Learn the altitude dance"**',
      "",
      "Move back and forth frequently between:",
      "- **Level 1:** Full immersion in experimental details or coding",
      "- **Level 2:** Step back, clear your head, evaluate as if someone else did the work",
      "",
      "These cannot be done simultaneously. The key to navigating a project's decision tree is alternating between these levels deliberately.",
      "",
      "## Key Concepts",
      "",
      "**Why Decision Trees Matter:**",
      "Once you're in a project, the landscape changes:",
      "- You've learned from initial experiments",
      "- New papers have been published",
      "- Technology has advanced",
      "- Your assumptions have been tested",
      "",
      "At any decision point, you should rarely follow your plan from 2 years ago—there will be a better alternative.",
      "",
      "**The Altitude Levels:**",
      "- **Level 1 (Ground Level):** Doing the work, troubleshooting, optimizing",
      "- **Level 2 (Strategic Altitude):** What did we learn? What should we do next?",
      "- **Level 3 (Field Altitude):** How does this fit in the broader landscape?",
      "- **Level 4 (Career Altitude):** Is this the right use of my finite time?",
      "",
      "**Common Failure Modes:**",
      "1. **Stuck in Level 1:** Troubleshooting endlessly without reassessing the plan",
      "2. **Only Level 2:** Brilliant strategist but never rolls up sleeves",
      "3. **No rhythm:** Switching randomly instead of deliberately",
      "",
      "## Workflow",
      "",
      "### Phase 1: Map Your Decision Tree",
      "",
      "For your project, identify:",
      "1. **Initial plan:** What was the intended path?",
      "2. **Branch points:** Where might alternative paths emerge?",
      "3. **Decision criteria:** What determines which branch to take?",
      "4. **New information:** What could change the landscape?",
      "",
      "### Phase 2: Establish Your Rhythm",
      "",
      "**Recommended Schedule:**",
      "- **Daily:** Level 1 work (experiments, coding, analysis)",
      "- **Weekly:** Level 2 evaluation (1-2 hours, ideally Friday afternoon)",
      "- **Monthly:** Level 3 field review (read new papers, attend seminars)",
      "- **Quarterly:** Level 4 career check-in (with mentor)",
      "",
      "**Level 2 Weekly Protocol:**",
      "1. Clear your head (walk, coffee, change of scene)",
      "2. Review what happened this week",
      "3. Ask: What did we learn?",
      "4. Ask: What should happen next?",
      "5. Update decision tree",
      "6. Plan next week's Level 1 work",
      "",
      "### Phase 3: Decision Points",
      "",
      "At each major branch point:",
      "",
      "**Example: Genetic Screen Hits Wall**",
      "",
      "Instead of endless troubleshooting:",
      "- **Alternative 1:** Redo computational analysis with larger genome dataset",
      "- **Alternative 2:** Use AlphaFold models to search for similar folds",
      "- **Alternative 3:** Print and test larger candidate set (DNA synthesis cheaper now)",
      "",
      "**Framework:**",
      "1. **Acknowledge the stuck point**",
      "2. **Step to Level 2:** Evaluate with fresh eyes",
      "3. **Consider: What's newly possible?** (technology, knowledge)",
      "4. **Generate 3 alternatives**",
      "5. **Decide:** Troubleshoot more vs. pursue alternative",
      "",
      "## Output: Decision Tree Map",
      "- Visual map of your project's decision points",
      "- Update frequency schedule",
      "- Criteria for each branch point",
      "- Protocol for getting unstuck",
      "",
    ].join("\n"),
    "references/06-adversity-planning.md": [
      '# SKILL 6: Adversity Response Planning ("The Adversity Feature")',
      "",
      "## Overview",
      'This skill helps you prepare for inevitable crises and reframe them as opportunities. The term "adversity feature" (like a "rock garden" on a mountain bike trail) captures the mindset: adversity is not an obstacle—it\'s an opportunity to develop skill and improve your project.',
      "",
      "## Core Principle",
      "**\"Capitalize on the 'adversity feature'\"**",
      "",
      "Adversity in a project is inevitable AND opportune:",
      "- **Inevitable:** Almost every project suffers existential crisis or sharp turn",
      "- **Opportune:** Two valuable outcomes possible:",
      "  1. Fix the problem AND upgrade the project simultaneously",
      "  2. Develop reasoning-your-way-out skills (best growth opportunity)",
      "",
      "## Key Concepts",
      "",
      "**Why Adversity Is Inevitable:**",
      "- Technology doesn't work as advertised",
      "- Biological assumptions prove false",
      "- You get scooped",
      "- Key collaborator leaves",
      "- Funding runs out",
      "- Results don't support hypothesis",
      "",
      "**Why Adversity Is Opportune:**",
      "- Forces you to think deeply about alternatives",
      "- Removes sunk-cost bias (path is blocked anyway)",
      "- Often leads to better projects than original plan",
      "- Develops critical problem-solving skills",
      "- Makes you resourceful",
      "",
      "**The Crisis Mindset:**",
      '- **Wrong:** "This is a disaster that delays me"',
      "- **Right:** \"This is the crisis I've been waiting for—don't waste it\"",
      "",
      "## Workflow",
      "",
      "### Phase 1: Anticipate Failure Modes",
      "",
      "For your project, list likely adversity scenarios:",
      "1. **Technical failures:** Method doesn't work, signal too low, etc.",
      "2. **Biological surprises:** System behaves unexpectedly",
      "3. **Competition:** Someone scoops you",
      "4. **Resource issues:** Funding, equipment, access",
      "5. **Timeline pressures:** Takes longer than expected",
      "",
      "For each, rate:",
      "- Likelihood (Low/Medium/High)",
      "- Impact if it happens (Low/Medium/High)",
      "- When it might surface (early/mid/late)",
      "",
      "### Phase 2: Upgrade Opportunities",
      "",
      "For each high-likelihood or high-impact failure mode:",
      "",
      "**Question 1: How could you fix this AND make the project better?**",
      'Not just: "Get it working"',
      'Instead: "Use this as opportunity to improve the approach"',
      "",
      "**Example: Your Cell Type Can't Be Isolated**",
      "- Fix: Develop new isolation method",
      "- Upgrade: Make method work for whole class of cell types",
      "- Result: Better project (technology paper) + original biology",
      "",
      "**Question 2: What skill would you develop by solving this?**",
      "- Computational: Learn new analysis method",
      "- Technical: Master challenging technique",
      "- Conceptual: Reason through biological complexity",
      "",
      "### Phase 3: The Ensemble View",
      "",
      "**Critical Insight:** You're not picking ONE project path—you're picking an ENSEMBLE of possible projects that share core elements.",
      "",
      "**Your Headquarter Ensemble:**",
      "```",
      "Core Theme: [What stays constant]",
      "",
      "Path 1: [Original plan]",
      "Path 2: [If assumption A fails]",
      "Path 3: [If technical barrier B encountered]",
      "Path 4: [If scooped on C]",
      "",
      "All paths lead to impactful results, just different ones",
      "```",
      "",
      "This reframing is liberating: when adversity strikes, you're not failing—you're discovering which path in the ensemble you're actually on.",
      "",
      "### Phase 4: Historical Examples",
      "",
      "**Example 1: PROTAC Discovery**",
      "- **Original Plan:** Create molecules to degrade specific kinase",
      "- **Crisis:** Didn't work for intended target",
      "- **Upgrade:** Test across kinome systematically",
      "- **Result:** Better project (mapped degradable kinome, discovered that target engagement ≠ degradation)",
      "- **Impact:** More influential than if original plan succeeded",
      "",
      "**Example 2: Steroid Receptor Study**",
      "- **Original Plan:** Identify THE receptor for a steroid",
      "- **Crisis:** Binds multiple receptors at different affinities",
      "- **Upgrade:** Reframe question: How does finite receptor pool sense infinite lipids?",
      "- **Result:** Combinatorial sensing model (like piano chords)",
      '- **Impact:** More interesting than "receptor X binds steroid Y"',
      "",
      "## Output: Adversity Playbook",
      "",
      "**Page 1: Anticipated Crises**",
      "| Crisis | Likelihood | Impact | Timeline | Growth Opportunity |",
      "|--------|-----------|--------|----------|-------------------|",
      "| [Crisis 1] | H/M/L | H/M/L | Early/Mid/Late | [Skill developed] |",
      "",
      "**Page 2: Upgrade Strategies**",
      "For each high-priority crisis:",
      "- **The Crisis:** [Description]",
      "- **Fix Strategy:** [How to solve it]",
      "- **Upgrade Strategy:** [How to make project better while fixing]",
      "- **Alternative Path:** [New direction if fix doesn't work]",
      "- **Ensemble Position:** [How this fits in project family]",
      "",
      "**Page 3: Resilience Rituals**",
      "- **Weekly check-in:** Review what went wrong, what was learned",
      "- **Monthly ensemble review:** Update the family of possible projects",
      "- **Crisis protocol:** When major setback hits, take 2 days to think before acting",
      "- **Growth tracking:** Document skills developed through adversity",
      "",
    ].join("\n"),
    "references/07-problem-inversion.md": [
      '# SKILL 7: Problem Inversion Strategies ("Turn It On Its Head")',
      "",
      "## Overview",
      "This skill provides three concrete strategies for navigating around obstacles by reframing problems. When stuck, instead of pushing harder on the current approach, try inverting the problem.",
      "",
      "## Core Principle",
      '**"Turn a problem on its head"**',
      "",
      "Three powerful strategies:",
      "1. **Unfix parameters** (covered in Skill 4, applied here in crisis)",
      "2. **Don't achieve goal A? Achieve comparable goal B**",
      '3. **"I have the answer; what is the question?"**',
      "",
      "## Strategy 1: Unfix Parameters (In Crisis Mode)",
      "",
      "**When to Use:** Run-of-the-mill issues in project execution",
      "",
      '**Approach:** Let a "sacred" fixed parameter float',
      "",
      "**Example from Lecture:**",
      "- **Stuck:** Spatial transcriptomics of APC-T cell interactions in tumor microenvironment",
      "- **All fixed:** Technique, cell types, context",
      "- **Inversion:**",
      "  - Unfix technique → What else could measure these interactions?",
      "  - Unfix cell types → What other interactions matter in tumors?",
      "  - Unfix context → Where else do APC-T interactions matter?",
      "",
      "**Your Application:**",
      "For each fixed parameter in your project:",
      "- What if this floated?",
      "- What alternatives exist?",
      "- Which would be easier/faster/more informative?",
      "",
      "## Strategy 2: Comparable Goal Substitution",
      "",
      "**When to Use:** Existential threats to project (can't achieve original goal)",
      "",
      "**Approach:** Achieve a different but equally valuable goal",
      "",
      "**Mindset Shift:**",
      '- **Wrong:** "I failed to do X"',
      '- **Right:** "The world needs Y instead, which I CAN do"',
      "",
      "**Example from Lectures: PROTAC Story**",
      "- **Goal A (Failed):** Degrade specific therapeutic target",
      "- **Goal B (Achieved):** Map which kinases ARE degradable",
      "- **Value:** B is more impactful (general principle + method validation)",
      "- **Learning:** Target engagement ≠ degradation (important discovery)",
      "",
      "**Framework:**",
      "1. **Original goal:** [What you wanted]",
      "2. **Why it failed:** [Specific reason]",
      "3. **What CAN you do with current data/tools:** [Capabilities]",
      "4. **Comparable goals:**",
      "   - Option 1: [Different but related goal]",
      "   - Option 2: [Another alternative]",
      "   - Option 3: [Yet another]",
      "5. **Which is most valuable:** [Analysis]",
      "6. **How to frame it:** [Communication strategy]",
      "",
      "## Strategy 3: Answer Seeking Question",
      "",
      "**When to Use:** End-of-project challenges (interpretation, framing, application)",
      "",
      "**Approach:** You got an answer, but not to your original question. What question DOES your data answer?",
      "",
      "**Mindset Shift:**",
      '- **Wrong:** "This doesn\'t answer my question"',
      '- **Right:** "What interesting question does this answer?"',
      "",
      "**Example from Lectures: Steroid Receptor**",
      "- **Original Question:** What is THE receptor for this steroid?",
      "- **Answer Obtained:** Binds multiple receptors at different affinities",
      "- **Problem:** Can't answer original question (no single receptor)",
      '- **Inversion:** "What question does this answer?"',
      "- **New Question:** How does finite receptor pool sense infinite lipids?",
      '- **Answer:** Combinatorial sensing (pattern = unique "chord")',
      "- **Impact:** More interesting than intended finding",
      "",
      "**Framework:**",
      "1. **Original question:** [What you asked]",
      "2. **Data obtained:** [What you actually found]",
      "3. **Why it doesn't answer:** [The mismatch]",
      "4. **What DOES the data show clearly:** [Solid findings]",
      "5. **What questions could these answer:**",
      "   - Question 1: [Option]",
      "   - Question 2: [Option]",
      "   - Question 3: [Option]",
      "6. **Which is most interesting:** [Assessment]",
      "7. **How to reframe paper/project:** [New framing]",
      "",
      "## Workflow",
      "",
      "### Phase 1: Identify Your Obstacle",
      "- **Type:** Technical / Biological / Competitive / Interpretive",
      "- **Severity:** Run-of-mill / Existential / End-stage",
      "- **Description:** [What's blocking you]",
      "",
      "### Phase 2: Select Strategy",
      "",
      "| Obstacle Type | Recommended Strategy |",
      "|--------------|---------------------|",
      "| Technical barrier, mid-project | Strategy 1 (Unfix parameters) |",
      "| Can't achieve original goal | Strategy 2 (Comparable goal) |",
      "| Have data, unclear what it means | Strategy 3 (Answer seeking question) |",
      "",
      "### Phase 3: Apply Strategy",
      "",
      "Work through the relevant framework above with your specific situation.",
      "",
      "### Phase 4: Evaluate Alternatives",
      "",
      "For each alternative generated:",
      "- **Scientific value:** How interesting is this?",
      "- **Feasibility:** How hard to execute?",
      "- **Timeline:** How long will it take?",
      "- **Impact:** How does this compare to original plan?",
      "- **Your advantage:** Do you still have edge here?",
      "",
      "## Output: Problem Inversion Analysis",
      "",
      "**Page 1: Current Situation**",
      "- **Obstacle:** [Clear description]",
      "- **Why you're stuck:** [Root cause]",
      "- **Original plan:** [What you intended]",
      "- **Current capability:** [What you CAN do]",
      "",
      "**Page 2: Strategy Applications**",
      "",
      "**Strategy 1 (Unfix Parameters):**",
      "| Fixed Parameter | If This Floated | Alternative Approaches | Assessment |",
      "|----------------|-----------------|----------------------|------------|",
      "| [Param 1] | [Consequences] | [Options] | [Value] |",
      "",
      "**Strategy 2 (Comparable Goals):**",
      "| Original Goal | Why It Failed | Comparable Goal | Value Assessment |",
      "|--------------|---------------|----------------|------------------|",
      "| [Goal A] | [Reason] | [Goal B] | [Compare impact] |",
      "",
      "**Strategy 3 (Answer → Question):**",
      "- **Data obtained:** [What you have]",
      "- **Question 1 it could answer:** [Option 1]",
      "- **Question 2 it could answer:** [Option 2]",
      "- **Question 3 it could answer:** [Option 3]",
      "- **Most interesting:** [Selection + reasoning]",
      "",
      "**Page 3: Recommended Path**",
      "- **Selected strategy:** [1, 2, or 3]",
      "- **New direction:** [Specific plan]",
      '- **Why this is better:** [Not just "it works" but "it\'s more interesting"]',
      "- **Communication approach:** [How to frame this pivot]",
      "- **Timeline:** [New schedule]",
      "",
    ].join("\n"),
    "references/08-integration-synthesis.md": [
      "# SKILL 8: Integration and Synthesis",
      "",
      "## Overview",
      "This final individual skill synthesizes all previous skills into a coherent project plan and communication strategy. You'll create a complete package that demonstrates thoughtful problem selection and rigorous planning.",
      "",
      "## Core Principle",
      '**"Tell a compelling story with your choices"**',
      "",
      "Humans love stories. Your project should have:",
      "- **Setting:** Background and problem framing",
      "- **Problem statement:** Clear, general enough to be interesting, specific enough to be distinctive",
      "- **New idea/approach:** Your angle (perturbation/measurement/theory: logic vs. technology)",
      '- **Iteration:** Loop of "we wondered X → did Y → found Z → interpreted as W"',
      "- **Conclusion:** What we learned and/or what's now possible",
      "- **Passion:** Authentic enthusiasm",
      "",
      "## Workflow",
      "",
      "### Phase 1: Gather Your Skill Outputs",
      "",
      "Collect your completed documents:",
      "- ☐ Skill 1: Problem Ideation Document",
      "- ☐ Skill 2: Risk Assessment Matrix",
      "- ☐ Skill 3: Impact Assessment Document",
      "- ☐ Skill 4: Parameter Strategy Document",
      "- ☐ Skill 5: Decision Tree Map",
      "- ☐ Skill 6: Adversity Playbook",
      "- ☐ Skill 7: Problem Inversion Analysis (if applicable)",
      "",
      "### Phase 2: Create Narrative Arc",
      "",
      "**Story Structure for Your Headquarter:**",
      "",
      "**1. Setting (Background)**",
      "- What's known in the field?",
      "- What's the gap or opportunity?",
      "- Why does this matter?",
      "",
      "**2. Problem Statement**",
      "- General enough: connects to broad principle",
      "- Specific enough: distinctive and tractable",
      "- Your framing from Skill 1",
      "",
      "**3. Your Approach**",
      "- Perturbation/Measurement/Theory",
      "- Logic vs. Technology",
      "- What's novel about your angle (from Skill 1)",
      "- How your optimization function shapes approach (from Skill 3)",
      "",
      "**4. Strategy**",
      "- Fixed vs. floating parameters (from Skill 4)",
      "- Decision points mapped out (from Skill 5)",
      "- Risk mitigation built in (from Skill 2)",
      "- Adversity contingencies (from Skill 6)",
      "",
      "**5. Why You**",
      "- Your competitive advantage",
      "- Lab expertise",
      "- Your passion and alignment",
      "- Timeline and resources",
      "",
      "### Phase 3: Communication Formats",
      "",
      "**Format 1: 3-Slide, 5-Minute Presentation**",
      "",
      "**Slide 1: The Opportunity**",
      "- Setting + Problem statement",
      "- One key figure or schematic",
      "- Why this matters (optimization function)",
      "",
      "**Slide 2: Your Approach**",
      "- New idea/angle",
      "- Key experiments or analyses",
      "- What makes this feasible",
      "- Decision tree highlights",
      "",
      "**Slide 3: Impact and Timeline**",
      "- What you'll learn or enable",
      "- Success metrics",
      "- Timeline with milestones",
      "- Your advantage",
      "",
      "**Slide Design Tips:**",
      "- Minimal text (bullets are fine here)",
      "- Strong visuals",
      "- Tell story, don't catalog facts",
      "- Passion shows through",
      "",
      "**Format 2: 1-Page Written Summary**",
      "",
      "**Paragraph 1:** Setting and problem (2-3 sentences)",
      "**Paragraph 2:** Your approach and novelty (3-4 sentences)",
      "**Paragraph 3:** Why it will work (risk mitigation, your advantage) (2-3 sentences)",
      "**Paragraph 4:** Impact and timeline (2-3 sentences)",
      "",
      "**Total:** ~250-300 words that could be abstract or summary",
      "",
      "**Format 3: 1-Minute Elevator Pitch**",
      "",
      "**Structure:**",
      '- "I\'m working on [problem] because [why it matters]"',
      '- "Current approaches are limited by [gap]"',
      '- "My angle is [approach] which is novel because [what\'s new]"',
      '- "This will [impact] and I have [advantage]"',
      "",
      "**Practice until:** Natural, passionate, memorable",
      "",
      "### Phase 4: Integration Document",
      "",
      "**Complete Headquarter Plan Integrating All Skills:**",
      "",
      "**Section 1: Problem Selection Rationale**",
      "- How you generated this idea (Skill 1 intuition pumps)",
      "- Why this problem matters (Skill 3 optimization function)",
      "- Your competitive advantage",
      "",
      "**Section 2: Risk Management**",
      "- Assumption analysis table (Skill 2)",
      "- Go/no-go experiments",
      "- Timeline with checkpoints",
      "- Mitigation strategies",
      "",
      "**Section 3: Execution Strategy**",
      "- Fixed vs. floating parameters (Skill 4)",
      "- Decision tree navigation plan (Skill 5)",
      "- Adversity response protocols (Skill 6)",
      "- Headquarter ensemble (alternative paths)",
      "",
      "**Section 4: Communication Plan**",
      "- Presentations (3-slide deck)",
      "- Written summary (1-page)",
      "- Elevator pitch (1-minute)",
      "- Key messages for different audiences",
      "",
      "**Section 5: Career Alignment**",
      "- How this fits your trajectory",
      "- Skills you'll develop",
      "- Network you'll build",
      "- Next steps after this project",
      "",
      "## Output: Complete Headquarter Package",
      "",
      "**Document 1: Integrated Headquarter Plan (4-6 pages)**",
      "- All sections above",
      "- References to individual skill outputs",
      "- Timeline and milestones",
      "- Resource requirements",
      "",
      "**Document 2: Communication Materials**",
      "- 3-slide presentation",
      "- 1-page summary",
      "- Elevator pitch script",
      "- Talking points for different audiences",
      "",
      "**Document 3: Living Documents**",
      "- Decision tree (to update regularly)",
      "- Risk assessment (to review quarterly)",
      "- Adversity playbook (to consult in crisis)",
      "- Parameter strategy (to revisit if stuck)",
      "",
      "## Key Principles",
      "",
      "1. **Integration, Not Duplication:** Each skill output serves a purpose in the whole",
      "2. **Story Over Catalog:** Communicate choices, not just facts",
      "3. **Passion Matters:** Authentic enthusiasm is persuasive",
      "4. **Living Plan:** This evolves; revisit quarterly",
      "5. **Alignment:** Headquarter, values, and career fit together",
      "6. **Preparation:** You've thought through contingencies",
      "7. **Communication:** You can pitch this clearly to anyone",
      "",
      "## Ready to Synthesize",
      "",
      "With all skills complete, you now have a comprehensive, thoughtful, rigorous approach to problem selection and project planning. This is the highest-leverage work you can do in science.",
      "",
    ].join("\n"),
    "references/09-meta-framework.md": [
      "# SKILL 9: Meta-Framework - Complete Problem Selection Workflow",
      "",
      "## Overview",
      "This meta-skill orchestrates the complete problem selection process, guiding users through Skills 1-8 in a systematic, iterative way. This skill should be used when comprehensive support is needed from ideation through execution planning, with integrated literature searches and coherent documentation.",
      "",
      "## When to Use This Skill",
      "",
      "**Use Skill 9 (Complete Workflow) when:**",
      "- Starting a new project from scratch",
      "- Major project pivot or reframe needed",
      "- Grant/fellowship application requiring systematic planning",
      "- Thesis committee meeting preparation",
      "- Startup company planning",
      "- Want comprehensive, documented problem selection process",
      "",
      "**Use Individual Skills when:**",
      "- You're at a specific stage (e.g., just need risk assessment)",
      "- Quick consultation on one aspect",
      "- Updating one component of existing plan",
      "- Teaching/learning one concept",
      "",
      "## The Complete Workflow",
      "",
      "### Overview of the Journey",
      "",
      "```",
      "START: Vague idea or area of interest",
      "    ↓",
      "[SKILL 1] → Problem Ideation Document",
      "    ↓",
      "[SKILL 2] → Risk Assessment Matrix",
      "    ↓",
      "[SKILL 3] → Impact Assessment Document",
      "    ↓",
      "[SKILL 4] → Parameter Strategy Document",
      "    ↓",
      "[SKILL 5] → Decision Tree Map",
      "    ↓",
      "[SKILL 6] → Adversity Playbook",
      "    ↓",
      "[SKILL 7] → Problem Inversion Analysis (if needed)",
      "    ↓",
      "[SKILL 8] → Integrated Headquarter Plan + Communication Materials",
      "    ↓",
      "END: Comprehensive, rigorous project ready to execute",
      "```",
      "",
      "**Estimated Time:**",
      "- **Intensive:** 1 week of focused work (full-time)",
      "- **Distributed:** 4-6 weeks with other commitments",
      "- **With iterations:** Add 50% more time",
      "",
      "**You'll invest time once to save years of potential missteps.**",
      "",
      "## Phase-by-Phase Workflow",
      "",
      "### Phase 1: Preparation (Before Starting)",
      "",
      "**Gather Your Context:**",
      "1. **Your background:**",
      "   - Research area/field",
      "   - Current position (grad student, postdoc, PI, etc.)",
      "   - Lab expertise and resources",
      "   - Timeline constraints",
      "",
      "2. **Your starting point:**",
      "   - Vague area of interest?",
      "   - Specific problem in mind?",
      "   - Must build on existing work?",
      "   - Starting completely fresh?",
      "",
      "3. **Your goals:**",
      "   - Publication target (journal tier, timeline)?",
      "   - Degree requirement (thesis chapter)?",
      "   - Funding application?",
      "   - Startup foundation?",
      "   - Career development?",
      "",
      "**Set Expectations:**",
      "- This process will challenge your assumptions",
      "- You may discover your initial idea needs major revision",
      "- That's the point—better to know now than after 2 years",
      "- Intellectual honesty is required; this only works if you're rigorous",
      "",
      "### Phase 2: Ideation (Skill 1) - ~1 week",
      "",
      "**What We'll Do:**",
      "1. Understand your context and constraints",
      "2. Work through relevant intuition pumps",
      "3. Avoid common ideation traps",
      "4. Generate 2-3 project ideas",
      "5. Preliminary literature search to calibrate scope",
      "6. Select most promising idea",
      "7. Create Problem Ideation Document (2 pages)",
      "",
      "**Literature Integration Point 1:**",
      "- Search PubMed for precedents and adjacent work",
      "- Assess generality of problem",
      "- Identify methodological advances",
      "- Determine competition level",
      "",
      "**Deliverable:**",
      "- Problem Ideation Document with core idea and initial analysis",
      "- List of 10-15 key papers",
      "- Preliminary assessment of novelty and feasibility",
      "",
      "**Checkpoint:** Do you have a clear, specific idea that excites you? If not, iterate on intuition pumps.",
      "",
      "### Phase 3: Risk Analysis (Skill 2) - ~3-5 days",
      "",
      "**What We'll Do:**",
      "1. Extract ALL assumptions from your idea",
      "2. Categorize (biological vs. technical)",
      "3. Score each assumption (risk 1-5, time to test)",
      "4. Identify high-risk late-reading assumptions",
      "5. Design go/no-go experiments",
      "6. Develop mitigation strategies",
      "7. Create Risk Assessment Matrix (2 pages)",
      "",
      "**Literature Integration Point 2:**",
      "- Search for technical precedents (has method worked before?)",
      "- Find biological evidence (what's known about your system?)",
      "- Identify benchmarks (success rates, effect sizes)",
      "- Assess timeline realism",
      "",
      "**Deliverable:**",
      "- Complete assumption analysis table",
      "- Top 3 high-risk assumptions with mitigation plans",
      "- Go/no-go experiment designs",
      "- Revised timeline with decision points",
      "",
      "**Checkpoint:** Is your risk profile acceptable? If risk-5 assumptions are >2 years out, return to Skill 1 to reframe.",
      "",
      "### Phase 4: Impact Assessment (Skill 3) - ~2-3 days",
      "",
      "**What We'll Do:**",
      "1. Categorize your project type",
      "2. Select appropriate optimization function",
      "3. Score yourself on both axes",
      "4. Compare to benchmarks",
      "5. Articulate value system alignment",
      "6. Develop communication strategy",
      "7. Create Impact Assessment Document (2 pages)",
      "",
      "**Literature Integration Point 3:**",
      "- Identify high-impact exemplars in your field",
      "- Analyze their framing and evaluation",
      "- Benchmark your potential impact",
      "- Understand field expectations",
      "",
      "**Deliverable:**",
      "- Clear optimization function selection",
      "- Self-assessment on both axes with justification",
      "- Comparative analysis vs. alternatives",
      "- Communication strategy for different audiences",
      "",
      "**Checkpoint:** Do you score MEDIUM-HIGH on at least one axis? If not, return to Skill 1 to find higher-impact angle.",
      "",
      "### Phase 5: Parameter Strategy (Skill 4) - ~2-3 days",
      "",
      "**What We'll Do:**",
      "1. Inventory all project parameters",
      "2. Identify which are fixed vs. floating",
      "3. Assess if you're over/under-constrained",
      "4. Select strategic fixed parameter",
      "5. Plan flexibility for contingencies",
      "6. Create Parameter Strategy Document (2 pages)",
      "",
      "**Literature Integration Point 4:**",
      "- How did successful projects handle parameters?",
      "- What parameter choices led to breakthroughs?",
      "- What over-constraints caused failures?",
      "",
      "**Deliverable:**",
      "- Complete parameter inventory",
      "- Strategic rationale for fixed/floating decisions",
      "- Flexibility matrix for contingencies",
      "- Headquarter ensemble (family of related projects)",
      "",
      "**Checkpoint:** Have you fixed 1-2 meaningful parameters while maintaining flexibility? If too rigid, adjust.",
      "",
      "### Phase 6: Decision Tree Planning (Skill 5) - ~2 days",
      "",
      "**What We'll Do:**",
      "1. Map your project's decision tree",
      "2. Identify major branch points",
      "3. Set criteria for each decision",
      "4. Establish Level 1 / Level 2 rhythm",
      "5. Create protocols for getting unstuck",
      "6. Create Decision Tree Map (1-2 pages)",
      "",
      "**No major literature search here** (unless you identify specific decision points needing technical information)",
      "",
      "**Deliverable:**",
      "- Visual decision tree",
      "- Decision criteria at each branch",
      "- Schedule for Level 2 evaluations",
      "- Protocol for course correction",
      "",
      "**Checkpoint:** Have you planned for regular strategic evaluation, not just execution?",
      "",
      "### Phase 7: Adversity Preparation (Skill 6) - ~2 days",
      "",
      "**What We'll Do:**",
      "1. Anticipate likely failure modes",
      "2. For each, identify upgrade opportunity",
      "3. Map your project ensemble",
      "4. Create crisis response protocols",
      "5. Create Adversity Playbook (2-3 pages)",
      "",
      "**Literature Integration Point 5:**",
      "- Historical examples of productive pivots",
      "- How did others capitalize on adversity?",
      "- What second-generation projects emerged from failures?",
      "",
      "**Deliverable:**",
      "- Anticipated crisis catalog",
      "- Upgrade strategies for each",
      "- Headquarter ensemble map",
      "- Resilience rituals and protocols",
      "",
      "**Checkpoint:** Are you prepared to see adversity as opportunity? Have you planned how to upgrade, not just fix?",
      "",
      "### Phase 8: Problem Inversion Toolkit (Skill 7) - ~1 day",
      "",
      "**What We'll Do:**",
      "1. Review three inversion strategies",
      "2. Pre-plan applications for your likely obstacles",
      "3. Create Problem Inversion Analysis (1-2 pages)",
      "",
      "**This is preparatory** - you may not need it now, but when crisis hits, you'll have framework ready.",
      "",
      "**Deliverable:**",
      "- Strategy 1 application planned",
      "- Strategy 2 options identified",
      "- Strategy 3 alternative questions brainstormed",
      "- Quick-reference guide for crisis",
      "",
      "**Checkpoint:** Do you have concrete strategies for inverting problems when stuck?",
      "",
      "### Phase 9: Integration and Synthesis (Skill 8) - ~3-5 days",
      "",
      "**What We'll Do:**",
      "1. Review all outputs from Skills 1-7",
      "2. Create cohesive narrative",
      "3. Develop communication materials:",
      "   - 3-slide presentation",
      "   - 1-page summary",
      "   - 1-minute elevator pitch",
      "4. Write integrated project plan (4-6 pages)",
      "5. Create living documents for ongoing use",
      "",
      "**Literature Integration Point 6:**",
      "- Final references for integrated plan",
      "- Key papers for each section",
      "- Communication examples from field leaders",
      "",
      "**Deliverable:**",
      "- Complete Integrated Headquarter Plan (4-6 pages)",
      "- 3-slide presentation deck",
      "- 1-page written summary",
      "- Elevator pitch script",
      "- Living documents (decision tree, risk matrix, etc.)",
      "",
      "**Checkpoint:** Can you communicate your project compellingly in 1 minute, 5 minutes, and 1 page? Do all pieces fit together coherently?",
      "",
      "## Iteration and Refinement",
      "",
      "### When to Iterate",
      "",
      "**Red Flags That Require Going Back:**",
      "",
      "**From Skill 2 (Risk):**",
      "- Risk-5 assumptions >2 years out → Return to Skill 1 (reframe problem)",
      "- >3 risk-4-5 assumptions → Return to Skill 1 (simplify or change approach)",
      "",
      "**From Skill 3 (Impact):**",
      "- Score LOW on both axes → Return to Skill 1 (find higher-impact angle)",
      "- Optimization function mismatch → Return to Skill 1 (reframe problem)",
      "",
      "**From Skill 4 (Parameters):**",
      "- >2 fixed parameters → Return to Skill 1 (over-constrained)",
      "- Zero fixed parameters → Return to Skill 1 (under-constrained)",
      "",
      "**From Skills 5-6:**",
      "- No clear decision points → Return to Skill 4 (need more flexibility)",
      "- Every failure mode is existential → Return to Skill 2 (too risky)",
      "",
      "### Iteration Protocol",
      "",
      "**Major Revision Needed:**",
      "1. **Pause and acknowledge:** The process is working—it caught a problem",
      "2. **Return to indicated skill:** Usually Skill 1 or 2",
      "3. **Bring forward what you learned:** Don't start from scratch",
      "4. **Revised idea → Run through workflow again:** Faster the second time",
      "5. **Multiple iterations OK:** Better than years on wrong project",
      "",
      "**Minor Refinement:**",
      "1. **Update specific document:** E.g., adjust parameter strategy",
      "2. **Check downstream effects:** Does this change anything else?",
      "3. **Update integration document:** Keep everything coherent",
      "",
      "## Literature Integration Strategy",
      "",
      "### Overall PubMed Approach",
      "",
      "**Throughout the workflow, use PubMed strategically:**",
      "",
      "1. **Skill 1 (Ideation):** Assess generality, find precedents, gauge competition",
      "2. **Skill 2 (Risk):** Technical feasibility, biological evidence, benchmarks",
      "3. **Skill 3 (Impact):** Field exemplars, evaluation frameworks, benchmarks",
      "4. **Skill 4 (Parameters):** Successful parameter choices, cautionary tales",
      "5. **Skill 6 (Adversity):** Productive pivots, upgrade examples",
      "6. **Skill 8 (Integration):** Communication models, comprehensive references",
      "",
      "**Search Strategy:**",
      "- Start broad (field overview)",
      "- Get specific (your exact approach)",
      "- Look adjacent (related systems/methods)",
      "- Find benchmarks (what's state-of-art?)",
      "- Identify competition (who else is doing this?)",
      "",
      "**Papers to Track:**",
      "- ~10-15 key papers from Skill 1",
      "- ~5-10 technical papers from Skill 2",
      "- ~5-10 impact exemplars from Skill 3",
      "- ~5 parameter lessons from Skill 4",
      "- ~3-5 pivot examples from Skill 6",
      "- **Total: ~30-50 papers** (your foundation)",
      "",
      "## Final Deliverable Package",
      "",
      "### What You'll Have at the End",
      "",
      "**Core Documents (Organized Folder):**",
      "1. `01_Problem_Ideation.pdf` (2 pages, Skill 1)",
      "2. `02_Risk_Assessment.pdf` (2 pages, Skill 2)",
      "3. `03_Impact_Assessment.pdf` (2 pages, Skill 3)",
      "4. `04_Parameter_Strategy.pdf` (2 pages, Skill 4)",
      "5. `05_Decision_Tree.pdf` (1-2 pages, Skill 5)",
      "6. `06_Adversity_Playbook.pdf` (2-3 pages, Skill 6)",
      "7. `07_Problem_Inversion.pdf` (1-2 pages, Skill 7)",
      "8. `08_Integrated_Plan.pdf` (4-6 pages, Skill 8)",
      "",
      "**Communication Materials:**",
      "- `Presentation_3slides.pptx`",
      "- `Summary_1page.pdf`",
      "- `Elevator_Pitch.txt`",
      "",
      "**Living Documents (for ongoing use):**",
      "- `Decision_Tree.pdf` (update monthly)",
      "- `Risk_Matrix.xlsx` (update quarterly)",
      "- `Adversity_Playbook.pdf` (consult in crisis)",
      "- `Parameter_Strategy.pdf` (revisit if stuck)",
      "",
      "**Reference Library:**",
      "- `Key_Papers.pdf` (annotated bibliography, 30-50 papers)",
      "- Organized by: Ideation / Technical / Impact / Pivots",
      "",
      "**Total: ~20-25 pages of documentation + supporting materials**",
      "",
      "## Using Your Outputs",
      "",
      "### For Different Purposes",
      "",
      "**Grant/Fellowship Applications:**",
      "- Start with Integrated Plan (Skill 8)",
      "- Include specific aims from Ideation (Skill 1)",
      "- Show risk mitigation from Risk Assessment (Skill 2)",
      "- Demonstrate impact from Impact Assessment (Skill 3)",
      "- Timeline from Decision Tree (Skill 5)",
      "",
      "**Thesis Committee Meetings:**",
      "- Present 3-slide deck (Skill 8)",
      "- Walk through decision tree (Skill 5)",
      "- Discuss risk mitigation (Skill 2)",
      "- Show parameter flexibility (Skill 4)",
      "- Demonstrate thoughtful planning",
      "",
      "**Lab Meetings:**",
      "- Use elevator pitch (Skill 8)",
      "- Show decision tree updates (Skill 5)",
      "- Discuss latest adversity and response (Skill 6)",
      "- Get input on parameter strategy (Skill 4)",
      "",
      "**Collaborator Conversations:**",
      "- Share 1-page summary (Skill 8)",
      "- Highlight where their expertise fits (Skill 4)",
      "- Show risk mitigation plan (Skill 2)",
      "- Discuss impact potential (Skill 3)",
      "",
      "**Personal Reflection:**",
      "- Quarterly: Review Decision Tree (Skill 5), update milestones",
      "- After setbacks: Consult Adversity Playbook (Skill 6)",
      "- When stuck: Use Problem Inversion (Skill 7)",
      "- Annual: Full workflow review, consider new projects",
      "",
      "## Maintenance and Updates",
      "",
      "### Living Documents Protocol",
      "",
      "**Monthly:**",
      "- Update Decision Tree (Skill 5)",
      "- Log adversities and responses (Skill 6)",
      "- Note new papers or competition",
      "- Adjust timeline if needed",
      "",
      "**Quarterly:**",
      "- Review Risk Matrix (Skill 2) - mark assumptions tested",
      "- Reassess Impact (Skill 3) - has evaluation changed?",
      "- Check Parameter Strategy (Skill 4) - still optimal?",
      "- Update Integrated Plan (Skill 8) - keep current",
      "",
      "**Annually:**",
      "- Complete workflow review",
      "- Consider new projects with fresh Skill 1 ideation",
      "- Archive old project docs",
      "- Extract lessons learned",
      "",
      "## Success Metrics",
      "",
      "### How Do You Know This Worked?",
      "",
      "**Immediate Indicators:**",
      "- Clearer project vision than before",
      "- Honest assessment of risks",
      "- Contingency plans for failures",
      "- Compelling communication materials",
      "- Alignment between project and values",
      "- Confidence in problem choice",
      "",
      "**6-Month Indicators:**",
      "- Major decisions made faster (have framework)",
      "- Adversity handled productively (used playbook)",
      "- No existential crises (risks were mitigated)",
      "- Regular Level 2 evaluation happening",
      "- Headquarter staying on-track or pivoting smartly",
      "",
      "**2-Year Indicators:**",
      "- Published results or strong progress",
      "- Avoided dead-end projects",
      "- Multiple high-quality options at decision points",
      "- Skills developed as planned",
      "- Career trajectory aligned with goals",
      "- Time well-spent (the ultimate measure)",
      "",
      "## Key Principles of the Meta-Framework",
      "",
      "1. **Systematic > Ad Hoc:** Process ensures nothing forgotten",
      "2. **Iterative > Linear:** Expect to loop back, that's good",
      "3. **Documented > Mental:** Writing forces clarity",
      "4. **Integrated > Fragmented:** All skills connect",
      "5. **Living > Static:** Update as you learn",
      "6. **Thoughtful > Fast:** Time invested now saves years later",
      "7. **Honest > Optimistic:** Rigor protects against wishful thinking",
      "8. **Prepared > Surprised:** Anticipate adversity",
      "9. **Flexible > Rigid:** Parameters float when needed",
      "10. **Passionate > Obligatory:** Alignment matters",
      "",
      "## Getting Started",
      "",
      "### First Steps",
      "",
      "**This Week:**",
      "1. Block time in calendar (1-2 hours to start)",
      "2. Gather your context (background, goals, constraints)",
      "3. Begin Skill 1 (Intuition Pumps)",
      "4. Let me know your starting point",
      "",
      "**This Month:**",
      "1. Work through Skills 1-4 (foundation)",
      "2. Share with mentor for alignment check",
      "3. Iterate if major changes needed",
      "4. Complete Skills 5-8 (execution planning)",
      "",
      "**This Quarter:**",
      "1. Begin project execution with living documents",
      "2. Monthly decision tree updates",
      "3. Quarterly risk assessment reviews",
      "4. Log adversities and responses",
      "",
      "**This Year:**",
      "1. Execute planned project",
      "2. Use frameworks when stuck",
      "3. Update living documents",
      "4. Evaluate process and refine",
      "",
      "## Ready to Begin?",
      "",
      "The complete meta-framework is substantial, but each step builds on the last. You'll move through:",
      "- ~2 weeks of intensive planning",
      "- Comprehensive documentation",
      "- Clear decision criteria",
      "- Communication materials",
      "- Living documents for ongoing guidance",
      "",
      "**Most importantly:** You'll KNOW you're working on a well-chosen problem with rigorous planning. That confidence is priceless.",
      "",
      "Let's start with Skill 1. Are you ready to begin?",
      "",
      "---",
      "",
      "*Remember: The highest-leverage work in science is choosing the right problem. This meta-framework ensures you spend your finite time wisely. The investment in systematic planning pays dividends for years.*",
      "",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: bio-research-scientific-problem-selection",
      'description: "This skill should be used when scientists need help with research problem selection, project ideation, troubleshooting stuck projects, or strategic scientific decisions. Use this skill when users ask to pitch a new research idea, work through a project problem, evaluate project risks, plan research strategy, navigate decision trees, or get help choosing what scientific problem to work on. Typical requests include \\"I have an idea for a project\\", \\"I\'m stuck on my research\\", \\"help me evaluate this project\\", \\"what should I work on\\", or \\"I need strategic advice about my research\\"."',
      "metadata:",
      "  plugin: bio-research",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Scientific Problem Selection Skills",
      "",
      'A conversational framework for systematic scientific problem selection based on Fischbach & Walsh\'s "Problem choice and decision trees in science and engineering" (Cell, 2024).',
      "",
      "## Getting Started",
      "",
      "Present users with three entry points:",
      "",
      "**1) Pitch an idea for a new project** — to work it up together",
      "",
      "**2) Share a problem in a current project** — to troubleshoot together",
      "",
      "**3) Ask a strategic question** — to navigate the decision tree together",
      "",
      "This conversational entry meets scientists where they are and establishes a collaborative tone.",
      "",
      "---",
      "",
      "## Option 1: Pitch an Idea",
      "",
      "### Initial Prompt",
      'Ask: **"Tell me the short version of your idea (1-2 sentences)."**',
      "",
      "### Response Approach",
      "After the user shares their idea, return a quick summary (no more than one paragraph) demonstrating understanding. Note the general area of research and rephrase the idea in a way that highlights its kernel—showing alignment and readiness to dive into details.",
      "",
      "### Follow-up Prompt",
      'Then ask for more detail: "Now give me a bit more detail. You might include, however briefly or even say where you are unsure:',
      "1. What exactly you want to do",
      "2. How you currently plan to do it",
      "3. If it works, why will it be a big deal",
      '4. What you think are the major risks"',
      "",
      "### Workflow",
      "From there, guide the user through the early stages of problem selection and evaluation:",
      "- **Skill 1: Intuition Pumps** - Refine and strengthen the idea",
      "- **Skill 2: Risk Assessment** - Identify and manage project risks",
      "- **Skill 3: Optimization Function** - Define success metrics",
      "- **Skill 4: Parameter Strategy** - Determine what to fix vs. keep flexible",
      "",
      "See `references/01-intuition-pumps.md`, `references/02-risk-assessment.md`, `references/03-optimization-function.md`, and `references/04-parameter-strategy.md` for detailed guidance.",
      "",
      "---",
      "",
      "## Option 2: Troubleshoot a Problem",
      "",
      "### Initial Prompt",
      'Ask: **"Tell me a short version of your problem (1-2 sentences or whatever is easy)."**',
      "",
      "### Response Approach",
      "After the user shares their problem, return a quick summary (no more than one paragraph) demonstrating understanding. Note the context of the project where the problem occurred and rephrase the problem—highlighting its core essence—so the user knows the situation is understood. Also raise additional questions that seem important to discuss.",
      "",
      "### Follow-up Prompt",
      'Then ask: "Now give me a bit more detail. You might include, however briefly:',
      "1. The overall goal of your project (if we have not talked about it before)",
      "2. What exactly went wrong",
      '3. Your current ideas for fixing it"',
      "",
      "### Workflow",
      "From there, guide the user through troubleshooting and decision tree navigation:",
      "- **Skill 5: Decision Tree Navigation** - Plan decision points and navigate between execution and strategic thinking",
      "- **Skill 4: Parameter Strategy** - Fix one parameter at a time, let others float",
      "- **Skill 6: Adversity Response** - Frame problems as opportunities for growth",
      "- **Skill 7: Problem Inversion** - Strategies for navigating around obstacles",
      "",
      "Always include workarounds that might be useful whether or not the problem can be fixed easily.",
      "",
      "See `references/05-decision-tree.md`, `references/06-adversity-planning.md`, `references/07-problem-inversion.md`, and `references/04-parameter-strategy.md` for detailed guidance.",
      "",
      "---",
      "",
      "## Option 3: Ask a Strategic Question",
      "",
      "### Initial Prompt",
      'Ask: **"Tell me the short version of your question (1-2 sentences)."**',
      "",
      "### Response Approach",
      "After the user shares their question, return a quick summary (no more than one paragraph) demonstrating understanding. Note the broader context and rephrase the question—highlighting its crux—to confirm alignment with their thinking.",
      "",
      "### Follow-up Prompt",
      'Then ask: "Now give me a bit more detail. You might include, however briefly:',
      "1. The setting (i.e., is this about a current or future project)",
      "2. A bit more detail about what you're thinking\"",
      "",
      "### Workflow",
      "From there, draw on the specific modules from the problem choice framework most appropriate to the question:",
      "- **Skills 1-4** for future project planning (ideation, risk, optimization, parameters)",
      "- **Skills 5-7** for current project navigation (decision trees, adversity, inversion)",
      "- **Skill 8** for communication and synthesis",
      "- **Skill 9** for comprehensive workflow orchestration",
      "",
      "See the complete reference materials in the `references/` folder.",
      "",
      "---",
      "",
      "## Core Framework Concepts",
      "",
      "### The Central Insight",
      "**Problem Choice >> Execution Quality**",
      "",
      "Even brilliant execution of a mediocre problem yields incremental impact. Good execution of an important problem yields substantial impact.",
      "",
      "### The Time Paradox",
      "Scientists typically spend:",
      "- **Days** choosing a problem",
      "- **Years** solving it",
      "",
      "This imbalance limits impact. These skills help invest more time choosing wisely.",
      "",
      "### Evaluation Axes",
      "**For Evaluating Ideas:**",
      "- **X-axis:** Likelihood of success",
      "- **Y-axis:** Impact if successful",
      "",
      "Skills help move ideas rightward (more feasible) and upward (more impactful).",
      "",
      "### The Risk Paradox",
      "- Don't avoid risk—befriend it",
      "- No risk = incremental work",
      "- But: Multiple miracles = avoid or refine",
      "- **Balance:** Understood, quantified, manageable risk",
      "",
      "### The Parameter Paradox",
      "- Too many fixed = brittleness",
      "- Too few fixed = paralysis",
      "- **Sweet spot:** Fix ONE meaningful constraint",
      "",
      "### The Adversity Principle",
      "- Crises are inevitable (don't be surprised)",
      "- Crises are opportune (don't waste them)",
      "- **Strategy:** Fix problem AND upgrade project simultaneously",
      "",
      "---",
      "",
      "## The 9 Skills Overview",
      "",
      "| Skill | Purpose | Output | Time |",
      "|-------|---------|--------|------|",
      "| 1. Intuition Pumps | Generate high-quality research ideas | Problem Ideation Document | ~1 week |",
      "| 2. Risk Assessment | Identify and manage project risks | Risk Assessment Matrix | 3-5 days |",
      "| 3. Optimization Function | Define success metrics | Impact Assessment Document | 2-3 days |",
      "| 4. Parameter Strategy | Decide what to fix vs. keep flexible | Parameter Strategy Document | 2-3 days |",
      "| 5. Decision Tree Navigation | Plan decision points and altitude dance | Decision Tree Map | 2 days |",
      "| 6. Adversity Response | Prepare for crises as opportunities | Adversity Playbook | 2 days |",
      "| 7. Problem Inversion | Navigate around obstacles | Problem Inversion Analysis | 1 day |",
      "| 8. Integration & Synthesis | Synthesize into coherent plan | Headquarter Communication Package | 3-5 days |",
      "| 9. Meta-Framework | Orchestrate complete workflow | Complete Headquarter Package | 1-6 weeks |",
      "",
      "---",
      "",
      "## Skill Workflow",
      "",
      "```",
      "SKILL 1: Intuition Pumps",
      "         | (generates idea)",
      "         v",
      "SKILL 2: Risk Assessment",
      "         | (evaluates feasibility)",
      "         v",
      "SKILL 3: Optimization Function",
      "         | (defines success metrics)",
      "         v",
      "SKILL 4: Parameter Strategy",
      "         | (determines flexibility)",
      "         v",
      "SKILL 5: Decision Tree",
      "         | (plans execution and evaluation)",
      "         v",
      "SKILL 6: Adversity Planning",
      "         | (prepares for failure modes)",
      "         v",
      "SKILL 7: Problem Inversion",
      "         | (provides pivot strategies)",
      "         v",
      "SKILL 8: Integration & Communication",
      "         | (synthesizes into coherent plan)",
      "         v",
      "SKILL 9: Meta-Skill",
      "         (orchestrates complete workflow)",
      "```",
      "",
      "---",
      "",
      "## Key Design Principles",
      "",
      "1. **Conversational Entry** - Meet users where they are with three clear starting points",
      "2. **Thoughtful Interaction** - Ask clarifying questions; low confidence prompts additional input",
      "3. **Literature Integration** - Use PubMed searches at strategic points for validation",
      "4. **Concrete Outputs** - Every skill produces tangible 1-2 page documents",
      "5. **Building Specificity** - Progressive detail emerges through targeted questions",
      "6. **Flexibility** - Skills work independently, sequentially, or iteratively",
      "7. **Scientific Rigor** - Claims about generality and feasibility should be evidence-based",
      "",
      "---",
      "",
      "## Who Should Use These Skills",
      "",
      "### Graduate Students (Primary Audience)",
      "- **When:** Choosing thesis projects, qualifying exams, committee meetings",
      "- **Focus:** Skills 1-3 (ideation, risk, impact) + Skill 9 (complete workflow)",
      "- **Timeline:** 2-4 weeks for comprehensive planning",
      "",
      "### Postdocs",
      "- **When:** Starting new position, planning independent projects, fellowship applications",
      "- **Focus:** All skills, emphasizing independence and risk management",
      "- **Timeline:** 1-2 weeks intensive planning",
      "",
      "### Principal Investigators",
      "- **When:** New lab, new direction, mentoring trainees, grant cycles",
      "- **Focus:** Skills 1, 3, 4, 6 (ideation, impact, parameters, adversity)",
      "- **Timeline:** Ongoing, integrate into lab culture",
      "",
      "### Startup Founders",
      "- **When:** Company inception, pivot decisions, investor pitches",
      "- **Focus:** Skills 1-4 (ideation through parameters) + Skill 8 (communication)",
      "- **Timeline:** 1-2 weeks for initial planning, revisit quarterly",
      "",
      "---",
      "",
      "## Reference Materials",
      "",
      "Detailed skill documentation is available in the `references/` folder:",
      "",
      "| File | Content | Search Patterns |",
      "|------|---------|-----------------|",
      "| `01-intuition-pumps.md` | Generate research ideas | `Intuition Pump #`, `Trap #`, `Phase [0-9]` |",
      "| `02-risk-assessment.md` | Risk identification | `Risk.*1-5`, `go/no-go`, `assumption` |",
      "| `03-optimization-function.md` | Success metrics | `Generality.*Learning`, `optimization`, `impact` |",
      "| `04-parameter-strategy.md` | Parameter fixation | `fixed.*float`, `constraint`, `parameter` |",
      "| `05-decision-tree.md` | Decision tree navigation | `altitude`, `Level [0-9]`, `decision` |",
      "| `06-adversity-planning.md` | Adversity response | `adversity`, `crisis`, `ensemble` |",
      "| `07-problem-inversion.md` | Problem inversion strategies | `Strategy [0-9]`, `inversion`, `goal` |",
      "| `08-integration-synthesis.md` | Integration and synthesis | `narrative`, `communication`, `story` |",
      "| `09-meta-framework.md` | Complete workflow | `Phase`, `workflow`, `orchestrat` |",
      "",
      "---",
      "",
      "## Expected Outcomes",
      "",
      "### Immediate (After Completing Workflow)",
      "- Clear project vision",
      "- Honest risk assessment",
      "- Contingency plans",
      "- Communication materials ready",
      "- Confidence in problem choice",
      "",
      "### 6-Month",
      "- Faster decisions (have framework)",
      "- Productive adversity handling",
      "- No existential crises (risks mitigated)",
      "",
      "### 2-Year",
      "- Published results or strong progress",
      "- Avoided dead-end projects",
      "- Career aligned with goals",
      "- **Time well-spent** (ultimate measure)",
      "",
      "---",
      "",
      "## Foundational Reference",
      "",
      '**Fischbach, M.A., & Walsh, C.T. (2024).** "Problem choice and decision trees in science and engineering." *Cell*, 187, 1828-1833.',
      "",
      "Based on course BIOE 395 taught at Stanford University.",
      "",
    ].join("\n"),
  },
  "bio-research-scvi-tools": {
    "references/atac_peakvi.md": [
      "# scATAC-seq Analysis with PeakVI",
      "",
      "This reference covers single-cell ATAC-seq analysis using PeakVI for dimensionality reduction, batch correction, and differential accessibility.",
      "",
      "## Overview",
      "",
      "PeakVI is a deep generative model for scATAC-seq data that:",
      "- Models binary accessibility (peak open/closed)",
      "- Handles batch effects",
      "- Provides latent representation for clustering",
      "- Enables differential accessibility analysis",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import numpy as np",
      "import anndata as ad",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      "```",
      "",
      "## Step 1: Load and Prepare ATAC Data",
      "",
      "### From 10x Genomics (Cell Ranger ATAC)",
      "",
      "```python",
      "# Peak-cell matrix from fragments",
      "# Usually in filtered_peak_bc_matrix format",
      "",
      'adata = sc.read_10x_h5("filtered_peak_bc_matrix.h5")',
      "",
      "# Or from mtx format",
      'adata = sc.read_10x_mtx("filtered_peak_bc_matrix/")',
      "",
      "# Check structure",
      'print(f"Cells: {adata.n_obs}, Peaks: {adata.n_vars}")',
      'print(f"Sparsity: {1 - adata.X.nnz / (adata.n_obs * adata.n_vars):.2%}")',
      "```",
      "",
      "### From ArchR/Signac",
      "",
      "```python",
      "# Export from ArchR (in R)",
      '# saveArchRProject(proj, outputDirectory="atac_export", load=FALSE)',
      "# Then read the exported files in Python",
      "",
      "# From Signac:",
      "# Export peak matrix and metadata",
      "```",
      "",
      "## Step 2: Quality Control",
      "",
      "```python",
      "# Calculate QC metrics",
      "sc.pp.calculate_qc_metrics(adata, inplace=True)",
      "",
      "# Key metrics for ATAC:",
      "# - n_genes_by_counts: peaks per cell (should rename)",
      "# - total_counts: fragments per cell",
      "adata.obs['n_peaks'] = adata.obs['n_genes_by_counts']",
      "adata.obs['total_fragments'] = adata.obs['total_counts']",
      "",
      "# Filter cells",
      "adata = adata[adata.obs['n_peaks'] > 500].copy()",
      "adata = adata[adata.obs['n_peaks'] < 50000].copy()  # Remove potential doublets",
      "",
      "# Filter peaks (accessible in at least n cells)",
      "sc.pp.filter_genes(adata, min_cells=10)",
      "",
      'print(f"After QC: {adata.shape}")',
      "```",
      "",
      "### Binarize Data",
      "",
      "```python",
      "# PeakVI works with binary accessibility",
      "# Binarize if not already binary",
      "adata.X = (adata.X > 0).astype(np.float32)",
      "",
      "# Verify",
      'print(f"Unique values: {np.unique(adata.X.data)}")',
      "```",
      "",
      "## Step 3: Feature Selection",
      "",
      "Unlike RNA-seq, peak selection for ATAC is less established. Options:",
      "",
      "### Option A: Most Accessible Peaks",
      "",
      "```python",
      "# Select top peaks by accessibility frequency",
      "peak_accessibility = np.array(adata.X.sum(axis=0)).flatten()",
      "top_peaks = np.argsort(peak_accessibility)[-50000:]  # Top 50k peaks",
      "",
      "adata = adata[:, top_peaks].copy()",
      "```",
      "",
      "### Option B: Variable Peaks",
      "",
      "```python",
      "# Select peaks with high variance",
      "# (Most informative for clustering)",
      "from sklearn.feature_selection import VarianceThreshold",
      "",
      "selector = VarianceThreshold(threshold=0.05)",
      "selector.fit(adata.X)",
      "adata = adata[:, selector.get_support()].copy()",
      "```",
      "",
      "### Option C: Peaks Near Genes",
      "",
      "```python",
      "# Keep peaks within promoter regions or gene bodies",
      "# Requires peak annotation",
      "# gene_peaks = peaks with gene annotation",
      "# adata = adata[:, adata.var['near_gene']].copy()",
      "```",
      "",
      "## Step 4: Add Batch Information",
      "",
      "```python",
      "# Add batch annotation if multiple samples",
      "adata.obs['batch'] = adata.obs['sample_id']  # Or appropriate column",
      "",
      "print(adata.obs['batch'].value_counts())",
      "```",
      "",
      "## Step 5: Setup and Train PeakVI",
      "",
      "```python",
      "# Setup AnnData",
      "scvi.model.PEAKVI.setup_anndata(",
      "    adata,",
      '    batch_key="batch"  # Optional, omit for single batch',
      ")",
      "",
      "# Create model",
      "model = scvi.model.PEAKVI(",
      "    adata,",
      "    n_latent=20,      # Latent dimensions",
      "    n_layers_encoder=2,",
      "    n_layers_decoder=2",
      ")",
      "",
      "# Train",
      "model.train(",
      "    max_epochs=200,",
      "    early_stopping=True,",
      "    batch_size=128",
      ")",
      "",
      "# Check training",
      "model.history['elbo_train'].plot()",
      "```",
      "",
      "## Step 6: Get Latent Representation",
      "",
      "```python",
      "# Latent space for downstream analysis",
      'adata.obsm["X_PeakVI"] = model.get_latent_representation()',
      "",
      "# Clustering and visualization",
      'sc.pp.neighbors(adata, use_rep="X_PeakVI", n_neighbors=15)',
      "sc.tl.umap(adata)",
      "sc.tl.leiden(adata, resolution=0.5)",
      "",
      "# Visualize",
      "sc.pl.umap(adata, color=['leiden', 'batch'], ncols=2)",
      "```",
      "",
      "## Step 7: Differential Accessibility",
      "",
      "```python",
      "# Differential accessibility between clusters",
      "da_results = model.differential_accessibility(",
      "    groupby='leiden',",
      "    group1='0',",
      "    group2='1'",
      ")",
      "",
      "# Filter significant peaks",
      "da_sig = da_results[",
      "    (da_results['is_da_fdr_0.05']) &",
      "    (abs(da_results['lfc_mean']) > 1)",
      "]",
      "",
      'print(f"Significant DA peaks: {len(da_sig)}")',
      "print(da_sig.head())",
      "```",
      "",
      "### DA Between Conditions",
      "",
      "```python",
      "# Compare conditions within cell type",
      "adata_subset = adata[adata.obs['cell_type'] == 'CD4 T cells'].copy()",
      "",
      "da_condition = model.differential_accessibility(",
      "    groupby='condition',",
      "    group1='treated',",
      "    group2='control'",
      ")",
      "```",
      "",
      "## Step 8: Peak Annotation",
      "",
      "```python",
      "# Annotate peaks with nearest genes",
      "# Using pybedtools or similar",
      "",
      "# Example peak name format: chr1:1000-2000",
      "# Parse into bed format for annotation",
      "",
      "import pandas as pd",
      "",
      "def parse_peak_names(peak_names):",
      '    """Parse peak names into bed format."""',
      "    records = []",
      "    for peak in peak_names:",
      "        chrom, coords = peak.split(':')",
      "        start, end = coords.split('-')",
      "        records.append({",
      "            'chrom': chrom,",
      "            'start': int(start),",
      "            'end': int(end),",
      "            'peak': peak",
      "        })",
      "    return pd.DataFrame(records)",
      "",
      "peak_bed = parse_peak_names(adata.var_names)",
      "```",
      "",
      "## Step 9: Motif Analysis",
      "",
      "```python",
      "# Export significant peaks for motif analysis",
      "# Use HOMER, MEME, or chromVAR",
      "",
      "# Export peak sequences",
      "sig_peaks = da_sig.index.tolist()",
      "peak_bed_sig = peak_bed[peak_bed['peak'].isin(sig_peaks)]",
      "peak_bed_sig.to_csv(\"significant_peaks.bed\", sep='\\t', index=False, header=False)",
      "",
      "# Then run HOMER:",
      "# findMotifsGenome.pl significant_peaks.bed hg38 motif_output/ -size 200",
      "```",
      "",
      "## Step 10: Gene Activity Scores",
      "",
      "```python",
      "# Compute gene activity from peak accessibility",
      "# (Requires peak-gene annotations)",
      "",
      "def compute_gene_activity(adata, peak_gene_map):",
      '    """',
      "    Compute gene activity scores from peak accessibility.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        ATAC data with peaks",
      "    peak_gene_map : dict",
      "        Mapping of peaks to genes",
      "        ",
      "    Returns",
      "    -------",
      "    AnnData with gene activity scores",
      '    """',
      "    from scipy.sparse import csr_matrix",
      "    ",
      "    genes = list(set(peak_gene_map.values()))",
      "    gene_matrix = np.zeros((adata.n_obs, len(genes)))",
      "    ",
      "    for i, gene in enumerate(genes):",
      "        gene_peaks = [p for p, g in peak_gene_map.items() if g == gene]",
      "        if gene_peaks:",
      "            peak_idx = [list(adata.var_names).index(p) for p in gene_peaks if p in adata.var_names]",
      "            if peak_idx:",
      "                gene_matrix[:, i] = np.array(adata.X[:, peak_idx].sum(axis=1)).flatten()",
      "    ",
      "    adata_gene = ad.AnnData(",
      "        X=csr_matrix(gene_matrix),",
      "        obs=adata.obs.copy(),",
      "        var=pd.DataFrame(index=genes)",
      "    )",
      "    ",
      "    return adata_gene",
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def analyze_scatac(",
      "    adata,",
      "    batch_key=None,",
      "    n_top_peaks=50000,",
      "    n_latent=20,",
      "    resolution=0.5",
      "):",
      '    """',
      "    Complete scATAC-seq analysis with PeakVI.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Raw peak-cell matrix",
      "    batch_key : str, optional",
      "        Batch annotation column",
      "    n_top_peaks : int",
      "        Number of top peaks to use",
      "    n_latent : int",
      "        Latent dimensions",
      "    resolution : float",
      "        Leiden clustering resolution",
      "        ",
      "    Returns",
      "    -------",
      "    Tuple of (processed AnnData, trained model)",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    import numpy as np",
      "    ",
      "    adata = adata.copy()",
      "    ",
      "    # QC",
      "    sc.pp.calculate_qc_metrics(adata, inplace=True)",
      "    adata = adata[adata.obs['n_genes_by_counts'] > 500].copy()",
      "    sc.pp.filter_genes(adata, min_cells=10)",
      "    ",
      "    # Binarize",
      "    adata.X = (adata.X > 0).astype(np.float32)",
      "    ",
      "    # Select top peaks",
      "    if adata.n_vars > n_top_peaks:",
      "        peak_accessibility = np.array(adata.X.sum(axis=0)).flatten()",
      "        top_peaks = np.argsort(peak_accessibility)[-n_top_peaks:]",
      "        adata = adata[:, top_peaks].copy()",
      "    ",
      "    # Setup PeakVI",
      "    scvi.model.PEAKVI.setup_anndata(adata, batch_key=batch_key)",
      "    ",
      "    # Train",
      "    model = scvi.model.PEAKVI(adata, n_latent=n_latent)",
      "    model.train(max_epochs=200, early_stopping=True)",
      "    ",
      "    # Latent representation",
      '    adata.obsm["X_PeakVI"] = model.get_latent_representation()',
      "    ",
      "    # Clustering",
      '    sc.pp.neighbors(adata, use_rep="X_PeakVI")',
      "    sc.tl.umap(adata)",
      "    sc.tl.leiden(adata, resolution=resolution)",
      "    ",
      "    return adata, model",
      "",
      "# Usage",
      "adata, model = analyze_scatac(",
      "    adata,",
      '    batch_key="sample",',
      "    n_top_peaks=50000",
      ")",
      "",
      "# Visualize",
      "sc.pl.umap(adata, color=['leiden', 'sample'])",
      "",
      "# Differential accessibility",
      "da_results = model.differential_accessibility(",
      "    groupby='leiden',",
      "    group1='0',",
      "    group2='1'",
      ")",
      "```",
      "",
      "## Integration with scRNA-seq",
      "",
      "For multiome data or separate RNA/ATAC from same cells:",
      "",
      "```python",
      "# See MultiVI for joint RNA+ATAC analysis",
      "# Or use WNN (weighted nearest neighbors) approach",
      "",
      "# Transfer labels from RNA to ATAC using shared latent space",
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Training slow | Too many peaks | Subset to top 50k peaks |",
      "| Poor clustering | Too few informative peaks | Use variable peaks |",
      "| Batch dominates | Strong technical effects | Ensure batch_key is set |",
      "| Memory error | Large peak matrix | Use sparse format, reduce peaks |",
      "",
      "## Key References",
      "",
      '- Ashuach et al. (2022) "PeakVI: A deep generative model for single-cell chromatin accessibility analysis"',
      "",
    ].join("\n"),
    "references/batch_correction_sysvi.md": [
      "# Advanced Batch Correction with sysVI",
      "",
      "This reference covers system-level batch correction using sysVI, designed for integrating data across major technological or study differences.",
      "",
      "## Overview",
      "",
      "sysVI (System Variational Inference) extends scVI for scenarios where:",
      "- Batch effects are very strong (different technologies)",
      "- Standard scVI over-corrects biological signal",
      '- You need to separate "system" effects from biological variation',
      "",
      "## When to Use sysVI vs scVI",
      "",
      "| Scenario | Recommended Model |",
      "|----------|-------------------|",
      "| Same technology, different samples | scVI |",
      "| 10x v2 vs 10x v3 | scVI (usually) |",
      "| 10x vs Smart-seq2 | sysVI |",
      "| Different sequencing depths | scVI with covariates |",
      "| Cross-study integration | sysVI |",
      "| Atlas-scale integration | sysVI |",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import numpy as np",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      "```",
      "",
      "## Understanding sysVI Architecture",
      "",
      "sysVI separates variation into:",
      "1. **Biological variation**: Cell type, state, trajectory",
      "2. **System variation**: Technology, study, lab effects",
      "",
      "```",
      "                    ┌─────────────────┐",
      "Input counts ──────►│    Encoder      │",
      "                    │                 │",
      "System info ───────►│  (conditioned)  │",
      "                    └────────┬────────┘",
      "                             │",
      "                    ┌────────▼────────┐",
      "                    │   Latent z      │",
      "                    │  (biological)   │",
      "                    └────────┬────────┘",
      "                             │",
      "                    ┌────────▼────────┐",
      "System info ───────►│    Decoder      │",
      "                    │  (conditioned)  │",
      "                    └────────┬────────┘",
      "                             │",
      "                    Reconstructed counts",
      "```",
      "",
      "## Basic sysVI Workflow",
      "",
      "### Step 1: Prepare Data",
      "",
      "```python",
      "# Load datasets from different systems",
      'adata1 = sc.read_h5ad("10x_data.h5ad")',
      'adata2 = sc.read_h5ad("smartseq_data.h5ad")',
      "",
      "# Add system labels",
      'adata1.obs["system"] = "10x"',
      'adata2.obs["system"] = "Smart-seq2"',
      "",
      "# Add batch labels (within system)",
      "# e.g., different samples within each technology",
      "",
      "# Concatenate",
      "adata = sc.concat([adata1, adata2])",
      "",
      "# Store raw counts",
      'adata.layers["counts"] = adata.X.copy()',
      "```",
      "",
      "### Step 2: HVG Selection",
      "",
      "```python",
      "# Select HVGs considering both batch and system",
      "sc.pp.highly_variable_genes(",
      "    adata,",
      "    n_top_genes=4000,  # More genes for cross-system",
      '    flavor="seurat_v3",',
      '    batch_key="system",  # Consider system for HVG',
      '    layer="counts"',
      ")",
      "",
      "# Optionally: ensure overlap between systems",
      "# Check HVGs are expressed in both systems",
      'adata = adata[:, adata.var["highly_variable"]].copy()',
      "```",
      "",
      "### Step 3: Setup and Train sysVI",
      "",
      "```python",
      "# Setup AnnData",
      "# Note: sysVI may be accessed differently depending on version",
      "# Check scvi-tools documentation for current API",
      "",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="sample",           # Within-system batches',
      '    categorical_covariate_keys=["system"]  # System-level covariate',
      ")",
      "",
      "# For true sysVI (if available in your version)",
      "# scvi.model.SysVI.setup_anndata(...)",
      "",
      "# Create model with system awareness",
      "model = scvi.model.SCVI(",
      "    adata,",
      "    n_latent=30,",
      "    n_layers=2,",
      '    gene_likelihood="nb"',
      ")",
      "",
      "# Train",
      "model.train(max_epochs=300)",
      "```",
      "",
      "### Step 4: Extract Representations",
      "",
      "```python",
      "# Get latent representation",
      'adata.obsm["X_integrated"] = model.get_latent_representation()',
      "",
      "# Clustering and visualization",
      'sc.pp.neighbors(adata, use_rep="X_integrated")',
      "sc.tl.umap(adata)",
      "sc.tl.leiden(adata)",
      "",
      "# Check integration",
      'sc.pl.umap(adata, color=["system", "leiden", "cell_type"])',
      "```",
      "",
      "## Alternative: Harmony + scVI",
      "",
      "For cross-system integration, combining methods can work well:",
      "",
      "```python",
      "import scanpy.external as sce",
      "",
      "# First run PCA",
      "sc.pp.pca(adata)",
      "",
      "# Apply Harmony for initial alignment",
      'sce.pp.harmony_integrate(adata, key="system")',
      "",
      "# Then train scVI on Harmony-corrected embedding",
      "# Or use Harmony representation directly",
      "```",
      "",
      "## Alternative: Using Covariates in scVI",
      "",
      "For moderate system effects:",
      "",
      "```python",
      "# Include system as categorical covariate",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="sample",',
      '    categorical_covariate_keys=["system", "technology_version"]',
      ")",
      "",
      "model = scvi.model.SCVI(adata, n_latent=30)",
      "model.train()",
      "```",
      "",
      "## Alternative: Separate Models + Integration",
      "",
      "For very different systems:",
      "",
      "```python",
      "# Train separate models",
      'scvi.model.SCVI.setup_anndata(adata1, layer="counts", batch_key="sample")',
      "model1 = scvi.model.SCVI(adata1)",
      "model1.train()",
      "",
      'scvi.model.SCVI.setup_anndata(adata2, layer="counts", batch_key="sample")',
      "model2 = scvi.model.SCVI(adata2)",
      "model2.train()",
      "",
      "# Get latent spaces",
      'adata1.obsm["X_scVI"] = model1.get_latent_representation()',
      'adata2.obsm["X_scVI"] = model2.get_latent_representation()',
      "",
      "# Align with CCA or Harmony",
      "# ... additional alignment step",
      "```",
      "",
      "## Evaluating Cross-System Integration",
      "",
      "### Visual Assessment",
      "",
      "```python",
      "import matplotlib.pyplot as plt",
      "",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))",
      "",
      "# Color by system",
      'sc.pl.umap(adata, color="system", ax=axes[0], show=False, title="By System")',
      "",
      "# Color by cell type",
      'sc.pl.umap(adata, color="cell_type", ax=axes[1], show=False, title="By Cell Type")',
      "",
      "# Color by expression of marker",
      'sc.pl.umap(adata, color="CD3D", ax=axes[2], show=False, title="CD3D Expression")',
      "",
      "plt.tight_layout()",
      "```",
      "",
      "### Quantitative Metrics",
      "",
      "```python",
      "# Using scib-metrics",
      "from scib_metrics.benchmark import Benchmarker",
      "",
      "bm = Benchmarker(",
      "    adata,",
      '    batch_key="system",',
      '    label_key="cell_type",',
      '    embedding_obsm_keys=["X_integrated"]',
      ")",
      "",
      "bm.benchmark()",
      "",
      "# Key metrics:",
      "# - Batch mixing (ASW_batch, Graph connectivity)",
      "# - Bio conservation (NMI, ARI, ASW_label)",
      "```",
      "",
      "### LISI Scores",
      "",
      "```python",
      "# Local Inverse Simpson's Index",
      "from scib_metrics import lisi",
      "",
      "# Batch LISI (higher = better mixing)",
      "batch_lisi = lisi.ilisi_graph(",
      "    adata,",
      '    batch_key="system",',
      '    use_rep="X_integrated"',
      ")",
      "",
      "# Cell type LISI (lower = better preservation)",
      "ct_lisi = lisi.clisi_graph(",
      "    adata,",
      '    label_key="cell_type", ',
      '    use_rep="X_integrated"',
      ")",
      "",
      'print(f"Batch LISI: {batch_lisi.mean():.3f}")',
      'print(f"Cell type LISI: {ct_lisi.mean():.3f}")',
      "```",
      "",
      "## Handling Specific Challenges",
      "",
      "### Different Gene Sets",
      "",
      "```python",
      "# Find common genes",
      "common_genes = adata1.var_names.intersection(adata2.var_names)",
      'print(f"Common genes: {len(common_genes)}")',
      "",
      "# If too few, use gene mapping",
      "# Or impute missing genes",
      "```",
      "",
      "### Different Sequencing Depths",
      "",
      "```python",
      "# Add depth as continuous covariate",
      'adata.obs["log_counts"] = np.log1p(adata.obs["total_counts"])',
      "",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="sample",',
      '    continuous_covariate_keys=["log_counts"]',
      ")",
      "```",
      "",
      "### Unbalanced Cell Types",
      "",
      "```python",
      "# Check cell type distribution per system",
      "import pandas as pd",
      "",
      'ct_dist = pd.crosstab(adata.obs["system"], adata.obs["cell_type"], normalize="index")',
      "print(ct_dist)",
      "",
      "# If very unbalanced, consider:",
      "# 1. Subsample to balance",
      "# 2. Use scANVI with labels to preserve rare types",
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def integrate_cross_system(",
      "    adatas: dict,",
      '    system_key: str = "system",',
      '    batch_key: str = "batch",',
      '    cell_type_key: str = "cell_type",',
      "    n_top_genes: int = 4000,",
      "    n_latent: int = 30",
      "):",
      '    """',
      "    Integrate datasets from different technological systems.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adatas : dict",
      "        Dictionary of {system_name: AnnData}",
      "    system_key : str",
      "        Key for system annotation",
      "    batch_key : str",
      "        Key for within-system batch",
      "    cell_type_key : str",
      "        Key for cell type labels (optional)",
      "    n_top_genes : int",
      "        Number of HVGs",
      "    n_latent : int",
      "        Latent dimensions",
      "        ",
      "    Returns",
      "    -------",
      "    Integrated AnnData with model",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    ",
      "    # Add system labels and concatenate",
      "    for system_name, adata in adatas.items():",
      "        adata.obs[system_key] = system_name",
      "    ",
      "    adata = sc.concat(list(adatas.values()))",
      "    ",
      "    # Find common genes",
      "    for name, ad in adatas.items():",
      "        if name == list(adatas.keys())[0]:",
      "            common_genes = set(ad.var_names)",
      "        else:",
      "            common_genes = common_genes.intersection(ad.var_names)",
      "    ",
      "    adata = adata[:, list(common_genes)].copy()",
      '    print(f"Common genes: {len(common_genes)}")',
      "    ",
      "    # Store counts",
      '    adata.layers["counts"] = adata.X.copy()',
      "    ",
      "    # HVG selection",
      "    sc.pp.highly_variable_genes(",
      "        adata,",
      "        n_top_genes=n_top_genes,",
      '        flavor="seurat_v3",',
      "        batch_key=system_key,",
      '        layer="counts"',
      "    )",
      '    adata = adata[:, adata.var["highly_variable"]].copy()',
      "    ",
      "    # Setup with system as covariate",
      "    scvi.model.SCVI.setup_anndata(",
      "        adata,",
      '        layer="counts",',
      "        batch_key=batch_key if batch_key in adata.obs else None,",
      "        categorical_covariate_keys=[system_key]",
      "    )",
      "    ",
      "    # Train",
      "    model = scvi.model.SCVI(adata, n_latent=n_latent, n_layers=2)",
      "    model.train(max_epochs=300, early_stopping=True)",
      "    ",
      "    # Get representation",
      '    adata.obsm["X_integrated"] = model.get_latent_representation()',
      "    ",
      "    # Clustering",
      '    sc.pp.neighbors(adata, use_rep="X_integrated")',
      "    sc.tl.umap(adata)",
      "    sc.tl.leiden(adata)",
      "    ",
      "    return adata, model",
      "",
      "# Usage",
      "adatas = {",
      '    "10x_v3": sc.read_h5ad("10x_v3_data.h5ad"),',
      '    "Smart-seq2": sc.read_h5ad("smartseq_data.h5ad"),',
      '    "Drop-seq": sc.read_h5ad("dropseq_data.h5ad")',
      "}",
      "",
      "adata_integrated, model = integrate_cross_system(adatas)",
      "",
      "# Visualize",
      'sc.pl.umap(adata_integrated, color=["system", "leiden"])',
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Systems don't mix | Effects too strong | Use more genes, increase n_latent |",
      "| Over-correction | Model too aggressive | Reduce n_layers, use scANVI |",
      "| Few common genes | Different platforms | Use gene name mapping |",
      "| One system dominates | Unbalanced sizes | Subsample larger dataset |",
      "",
      "## Key References",
      "",
      '- Lopez et al. (2018) "Deep generative modeling for single-cell transcriptomics"',
      '- Luecken et al. (2022) "Benchmarking atlas-level data integration in single-cell genomics"',
      "",
    ].join("\n"),
    "references/citeseq_totalvi.md": [
      "# CITE-seq Analysis with totalVI",
      "",
      "This reference covers multi-modal analysis of CITE-seq data (RNA + surface proteins) using totalVI.",
      "",
      "## Overview",
      "",
      "CITE-seq combines:",
      "- scRNA-seq (transcriptome)",
      "- Protein surface markers (antibody-derived tags, ADT)",
      "",
      "totalVI jointly models both modalities to:",
      "- Integrate across batches",
      "- Denoise protein signal",
      "- Learn joint latent representation",
      "- Enable cross-modal imputation",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import mudata as md",
      "import numpy as np",
      "import pandas as pd",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      "```",
      "",
      "## Step 1: Load CITE-seq Data",
      "",
      "### From 10x Genomics (Cell Ranger)",
      "",
      "```python",
      "# 10x outputs separate gene expression and feature barcoding",
      'adata_rna = sc.read_10x_h5("filtered_feature_bc_matrix.h5", gex_only=False)',
      "",
      "# Separate RNA and protein",
      "adata_protein = adata_rna[:, adata_rna.var['feature_types'] == 'Antibody Capture'].copy()",
      "adata_rna = adata_rna[:, adata_rna.var['feature_types'] == 'Gene Expression'].copy()",
      "",
      'print(f"RNA: {adata_rna.shape}")',
      'print(f"Protein: {adata_protein.shape}")',
      "```",
      "",
      "### From MuData",
      "",
      "```python",
      "# If data is in MuData format",
      'mdata = md.read_h5mu("cite_seq.h5mu")',
      "",
      "adata_rna = mdata['rna'].copy()",
      "adata_protein = mdata['protein'].copy()",
      "```",
      "",
      "### Combine into Single AnnData",
      "",
      "```python",
      "# totalVI expects protein data in obsm",
      "adata = adata_rna.copy()",
      "",
      "# Add protein expression to obsm",
      "adata.obsm[\"protein_expression\"] = adata_protein.X.toarray() if hasattr(adata_protein.X, 'toarray') else adata_protein.X",
      "",
      "# Store protein names",
      'adata.uns["protein_names"] = list(adata_protein.var_names)',
      "```",
      "",
      "## Step 2: Quality Control",
      "",
      "### RNA QC",
      "",
      "```python",
      "# Standard RNA QC",
      "# Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes",
      "    adata.var['mt'] = (",
      "        adata.var_names.str.startswith('MT-') |",
      "        adata.var_names.str.startswith('mt-') |",
      "        adata.var_names.str.startswith('Mt-')",
      "    )",
      "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)",
      "",
      "# Filter cells",
      "adata = adata[adata.obs['n_genes_by_counts'] > 200].copy()",
      "adata = adata[adata.obs['pct_counts_mt'] < 20].copy()",
      "",
      "# Filter genes",
      "sc.pp.filter_genes(adata, min_cells=3)",
      "```",
      "",
      "### Protein QC",
      "",
      "```python",
      "# Protein QC",
      'protein_counts = adata.obsm["protein_expression"]',
      'print(f"Protein counts per cell: min={protein_counts.sum(1).min():.0f}, max={protein_counts.sum(1).max():.0f}")',
      "",
      "# Check for isotype controls",
      "# Isotype controls should have low counts",
      'protein_names = adata.uns["protein_names"]',
      "for i, name in enumerate(protein_names):",
      "    if 'isotype' in name.lower() or 'control' in name.lower():",
      '        print(f"{name}: mean={protein_counts[:, i].mean():.1f}")',
      "```",
      "",
      "## Step 3: Data Preparation",
      "",
      "### Store Raw Counts",
      "",
      "```python",
      "# Store RNA counts",
      'adata.layers["counts"] = adata.X.copy()',
      "",
      "# Protein must be raw ADT counts (NOT CLR-normalized)",
      "# WARNING: If importing from Seurat, ensure you use raw counts, not CLR-normalized data",
      '# Seurat\'s NormalizeData(normalization.method = "CLR") transforms counts - use the original assay',
      "```",
      "",
      "### HVG Selection for RNA",
      "",
      "```python",
      "# Select HVGs for RNA",
      "# Note: totalVI uses all proteins regardless of HVG",
      "",
      "sc.pp.highly_variable_genes(",
      "    adata,",
      "    n_top_genes=4000,  # Use more for CITE-seq",
      '    flavor="seurat_v3",',
      '    batch_key="batch" if "batch" in adata.obs else None,',
      '    layer="counts"',
      ")",
      "",
      "# Subset to HVGs",
      'adata = adata[:, adata.var["highly_variable"]].copy()',
      "```",
      "",
      "## Step 4: Setup and Train totalVI",
      "",
      "```python",
      "# Setup AnnData for totalVI",
      "scvi.model.TOTALVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    protein_expression_obsm_key="protein_expression",',
      '    batch_key="batch"  # Optional',
      ")",
      "",
      "# Create model",
      "model = scvi.model.TOTALVI(",
      "    adata,",
      "    n_latent=20,",
      '    latent_distribution="normal"  # or "ln" for log-normal',
      ")",
      "",
      "# Train",
      "model.train(",
      "    max_epochs=200,",
      "    early_stopping=True,",
      "    batch_size=128",
      ")",
      "",
      "# Check training",
      "model.history['elbo_train'].plot()",
      "```",
      "",
      "## Step 5: Get Latent Representation",
      "",
      "```python",
      "# Joint latent space",
      'adata.obsm["X_totalVI"] = model.get_latent_representation()',
      "",
      "# Clustering and visualization",
      'sc.pp.neighbors(adata, use_rep="X_totalVI")',
      "sc.tl.umap(adata)",
      "sc.tl.leiden(adata, resolution=1.0)",
      "",
      "sc.pl.umap(adata, color=['leiden', 'batch'])",
      "```",
      "",
      "## Step 6: Denoised Protein Expression",
      "",
      "```python",
      "# Get denoised protein values",
      "# This removes background noise from protein measurements",
      "",
      "_, protein_denoised = model.get_normalized_expression(",
      "    return_mean=True,",
      '    transform_batch="batch1"  # Optional: normalize to specific batch',
      ")",
      "",
      "# Add to adata",
      'adata.obsm["protein_denoised"] = protein_denoised',
      "",
      "# Visualize denoised proteins",
      'protein_names = adata.uns["protein_names"]',
      "for i, protein in enumerate(protein_names[:5]):",
      '    adata.obs[f"denoised_{protein}"] = protein_denoised[:, i]',
      "",
      'sc.pl.umap(adata, color=[f"denoised_{p}" for p in protein_names[:5]])',
      "```",
      "",
      "## Step 7: Normalized RNA Expression",
      "",
      "```python",
      "# Get normalized RNA expression",
      "rna_normalized, _ = model.get_normalized_expression(",
      "    return_mean=True",
      ")",
      "",
      "# Store",
      'adata.layers["totalVI_normalized"] = rna_normalized',
      "```",
      "",
      "## Step 8: Differential Expression",
      "",
      "### RNA Differential Expression",
      "",
      "```python",
      "# DE between clusters",
      "de_rna = model.differential_expression(",
      '    groupby="leiden",',
      '    group1="0",',
      '    group2="1"',
      ")",
      "",
      "# Filter significant genes",
      "de_sig = de_rna[",
      "    (de_rna['is_de_fdr_0.05']) &",
      "    (abs(de_rna['lfc_mean']) > 1)",
      "]",
      "",
      'print(f"Significant DE genes: {len(de_sig)}")',
      "```",
      "",
      "### Protein Differential Expression",
      "",
      "```python",
      "# Protein DE",
      "de_protein = model.differential_expression(",
      '    groupby="leiden",',
      '    group1="0",',
      '    group2="1",',
      '    mode="protein"',
      ")",
      "",
      "print(de_protein.head(20))",
      "```",
      "",
      "## Step 9: Visualization",
      "",
      "### Protein Expression on UMAP",
      "",
      "```python",
      "# Denoised protein on UMAP",
      "import matplotlib.pyplot as plt",
      "",
      'proteins_to_plot = ["CD3", "CD4", "CD8", "CD19", "CD14"]',
      "",
      "fig, axes = plt.subplots(1, len(proteins_to_plot), figsize=(4*len(proteins_to_plot), 4))",
      "for ax, protein in zip(axes, proteins_to_plot):",
      '    idx = adata.uns["protein_names"].index(protein)',
      "    sc.pl.umap(",
      "        adata,",
      '        color=adata.obsm["protein_denoised"][:, idx],',
      "        ax=ax,",
      "        title=protein,",
      "        show=False",
      "    )",
      "plt.tight_layout()",
      "```",
      "",
      "### Joint Heatmap",
      "",
      "```python",
      "# Heatmap of top genes and proteins per cluster",
      "sc.pl.dotplot(",
      "    adata,",
      "    var_names=de_sig.index[:20].tolist(),",
      '    groupby="leiden",',
      '    layer="totalVI_normalized"',
      ")",
      "```",
      "",
      "## Step 10: Cell Type Annotation",
      "",
      "```python",
      "# Use both RNA and protein markers for annotation",
      "",
      "# RNA markers",
      "rna_markers = {",
      "    'T cells': ['CD3D', 'CD3E'],",
      "    'CD4 T': ['CD4'],",
      "    'CD8 T': ['CD8A', 'CD8B'],",
      "    'B cells': ['CD19', 'MS4A1'],",
      "    'Monocytes': ['CD14', 'LYZ']",
      "}",
      "",
      "# Check denoised protein expression",
      'for i, protein in enumerate(adata.uns["protein_names"]):',
      "    if any(m in protein for m in ['CD3', 'CD4', 'CD8', 'CD19', 'CD14']):",
      '        print(f"{protein}: cluster means")',
      "        for cluster in adata.obs['leiden'].unique():",
      "            mask = adata.obs['leiden'] == cluster",
      '            mean_expr = adata.obsm["protein_denoised"][mask, i].mean()',
      '            print(f"  Cluster {cluster}: {mean_expr:.2f}")',
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def analyze_citeseq(",
      "    adata_rna,",
      "    adata_protein,",
      "    batch_key=None,",
      "    n_top_genes=4000,",
      "    n_latent=20",
      "):",
      '    """',
      "    Complete CITE-seq analysis with totalVI.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata_rna : AnnData",
      "        RNA expression (raw counts)",
      "    adata_protein : AnnData",
      "        Protein expression (raw counts)",
      "    batch_key : str, optional",
      "        Batch column in obs",
      "    n_top_genes : int",
      "        Number of HVGs",
      "    n_latent : int",
      "        Latent dimensions",
      "        ",
      "    Returns",
      "    -------",
      "    Tuple of (processed AnnData, trained model)",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    ",
      "    # Ensure same cells",
      "    common_cells = adata_rna.obs_names.intersection(adata_protein.obs_names)",
      "    adata = adata_rna[common_cells].copy()",
      "    adata_protein = adata_protein[common_cells].copy()",
      "    ",
      "    # Add protein to obsm",
      "    adata.obsm[\"protein_expression\"] = adata_protein.X.toarray() if hasattr(adata_protein.X, 'toarray') else adata_protein.X",
      '    adata.uns["protein_names"] = list(adata_protein.var_names)',
      "    ",
      "    # RNA QC",
      "    # Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes",
      "    adata.var['mt'] = (",
      "        adata.var_names.str.startswith('MT-') |",
      "        adata.var_names.str.startswith('mt-') |",
      "        adata.var_names.str.startswith('Mt-')",
      "    )",
      "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)",
      "    adata = adata[adata.obs['pct_counts_mt'] < 20].copy()",
      "    sc.pp.filter_genes(adata, min_cells=3)",
      "    ",
      "    # Store counts",
      '    adata.layers["counts"] = adata.X.copy()',
      "    ",
      "    # HVG selection",
      "    sc.pp.highly_variable_genes(",
      "        adata,",
      "        n_top_genes=n_top_genes,",
      '        flavor="seurat_v3",',
      "        batch_key=batch_key,",
      '        layer="counts"',
      "    )",
      '    adata = adata[:, adata.var["highly_variable"]].copy()',
      "    ",
      "    # Setup totalVI",
      "    scvi.model.TOTALVI.setup_anndata(",
      "        adata,",
      '        layer="counts",',
      '        protein_expression_obsm_key="protein_expression",',
      "        batch_key=batch_key",
      "    )",
      "    ",
      "    # Train",
      "    model = scvi.model.TOTALVI(adata, n_latent=n_latent)",
      "    model.train(max_epochs=200, early_stopping=True)",
      "    ",
      "    # Get representations",
      '    adata.obsm["X_totalVI"] = model.get_latent_representation()',
      "    rna_norm, protein_denoised = model.get_normalized_expression(return_mean=True)",
      '    adata.layers["totalVI_normalized"] = rna_norm',
      '    adata.obsm["protein_denoised"] = protein_denoised',
      "    ",
      "    # Clustering",
      '    sc.pp.neighbors(adata, use_rep="X_totalVI")',
      "    sc.tl.umap(adata)",
      "    sc.tl.leiden(adata)",
      "    ",
      "    return adata, model",
      "",
      "# Usage",
      "adata, model = analyze_citeseq(",
      "    adata_rna,",
      "    adata_protein,",
      '    batch_key="batch"',
      ")",
      "",
      "# Visualize",
      "sc.pl.umap(adata, color=['leiden', 'batch'])",
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Protein signal noisy | Background not removed | Use get_normalized_expression with denoising |",
      "| Batch effects persist | Need batch_key | Ensure batch_key is specified |",
      "| Memory error | Too many genes | Reduce n_top_genes |",
      "| Poor protein clustering | Few proteins | Normal - totalVI uses RNA for structure |",
      "",
      "## Key References",
      "",
      '- Gayoso et al. (2021) "Joint probabilistic modeling of single-cell multi-omic data with totalVI"',
      "",
    ].join("\n"),
    "references/data_preparation.md": [
      "# Data Preparation for scvi-tools",
      "",
      "This reference covers how to properly prepare AnnData objects for use with scvi-tools models.",
      "",
      "## Overview",
      "",
      "Proper data preparation is critical for scvi-tools. Key requirements:",
      "1. **Raw counts** (not normalized)",
      "2. **Highly variable gene selection**",
      "3. **Proper setup_anndata() call**",
      "",
      "## Step 1: Load and Inspect Data",
      "",
      "```python",
      "import scanpy as sc",
      "import scvi",
      "import numpy as np",
      "",
      "# Load data",
      'adata = sc.read_h5ad("data.h5ad")',
      "",
      "# Check what's in adata.X",
      'print(f"Shape: {adata.shape}")',
      'print(f"X dtype: {adata.X.dtype}")',
      'print(f"X contains integers: {np.allclose(adata.X.data, adata.X.data.astype(int))}")',
      'print(f"X min: {adata.X.min()}, max: {adata.X.max()}")',
      "```",
      "",
      "### Verify Raw Counts",
      "",
      "```python",
      "# scvi-tools needs INTEGER counts",
      "# If X appears normalized, check for raw counts",
      "",
      "if hasattr(adata, 'raw') and adata.raw is not None:",
      '    print("Found adata.raw")',
      "    # Use raw counts",
      "    adata = adata.raw.to_adata()",
      "    ",
      "# Or check layers",
      "if 'counts' in adata.layers:",
      '    print("Found counts layer")',
      "    # Will specify layer in setup_anndata",
      "```",
      "",
      "## Step 2: Basic Filtering",
      "",
      "```python",
      "# Filter cells (standard QC)",
      "sc.pp.filter_cells(adata, min_genes=200)",
      "sc.pp.filter_cells(adata, max_genes=5000)",
      "",
      "# Calculate mito percent if not present",
      "# Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes",
      "adata.var['mt'] = (",
      "    adata.var_names.str.startswith('MT-') |",
      "    adata.var_names.str.startswith('mt-') |",
      "    adata.var_names.str.startswith('Mt-')",
      ")",
      "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)",
      "adata = adata[adata.obs['pct_counts_mt'] < 20].copy()",
      "",
      "# Filter genes",
      "sc.pp.filter_genes(adata, min_cells=3)",
      "",
      'print(f"After filtering: {adata.shape}")',
      "```",
      "",
      "## Step 3: Store Raw Counts",
      "",
      "**Critical**: Always preserve raw counts before any normalization.",
      "",
      "```python",
      "# Store raw counts in a layer",
      'adata.layers["counts"] = adata.X.copy()',
      "",
      "# Now you can normalize for other purposes (HVG selection)",
      "# But scvi will use the counts layer",
      "```",
      "",
      "## Step 4: Highly Variable Gene Selection",
      "",
      "scvi-tools works best with 1,500-5,000 HVGs.",
      "",
      "### For Single-Batch Data",
      "",
      "```python",
      "# Normalize for HVG selection only",
      "adata_hvg = adata.copy()",
      "sc.pp.normalize_total(adata_hvg, target_sum=1e4)",
      "sc.pp.log1p(adata_hvg)",
      "",
      "# Select HVGs",
      "sc.pp.highly_variable_genes(",
      "    adata_hvg,",
      "    n_top_genes=2000,",
      '    flavor="seurat"  # or "cell_ranger"',
      ")",
      "",
      "# Transfer HVG annotation",
      "adata.var['highly_variable'] = adata_hvg.var['highly_variable']",
      "```",
      "",
      "### For Multi-Batch Data (Recommended)",
      "",
      "```python",
      "# Use seurat_v3 flavor with batch_key",
      "# This selects genes variable across batches",
      "sc.pp.highly_variable_genes(",
      "    adata,",
      "    n_top_genes=2000,",
      '    flavor="seurat_v3",',
      '    batch_key="batch",  # Your batch column',
      '    layer="counts"      # Use raw counts',
      ")",
      "```",
      "",
      "### Subset to HVGs",
      "",
      "```python",
      "# Subset to highly variable genes",
      "adata = adata[:, adata.var['highly_variable']].copy()",
      'print(f"After HVG selection: {adata.shape}")',
      "```",
      "",
      "## Step 5: Setup AnnData",
      "",
      "The `setup_anndata()` function registers data for the model.",
      "",
      "### Basic Setup",
      "",
      "```python",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts"  # Specify layer with raw counts',
      ")",
      "```",
      "",
      "### With Batch Information",
      "",
      "```python",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch"  # Column in adata.obs',
      ")",
      "```",
      "",
      "### With Cell Type Labels (for scANVI)",
      "",
      "```python",
      "scvi.model.SCANVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch",',
      '    labels_key="cell_type"  # Column with cell type labels',
      ")",
      "```",
      "",
      "### With Continuous Covariates",
      "",
      "```python",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch",',
      '    continuous_covariate_keys=["percent_mito", "n_genes"]',
      ")",
      "```",
      "",
      "### With Categorical Covariates",
      "",
      "```python",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch",',
      '    categorical_covariate_keys=["donor", "technology"]',
      ")",
      "```",
      "",
      "## Multi-Modal Data Setup",
      "",
      "### CITE-seq (for totalVI)",
      "",
      "```python",
      "# Protein data in adata.obsm",
      "# RNA in adata.X, protein in separate matrix",
      "",
      "# Add protein data",
      'adata.obsm["protein_expression"] = protein_counts  # numpy array',
      "",
      "# Setup for totalVI",
      "scvi.model.TOTALVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch",',
      '    protein_expression_obsm_key="protein_expression"',
      ")",
      "```",
      "",
      "### Multiome RNA+ATAC (for MultiVI)",
      "",
      "```python",
      "# RNA and ATAC in separate AnnData objects or MuData",
      "",
      "import mudata as md",
      "",
      "# If using MuData",
      'mdata = md.read("multiome.h5mu")',
      "",
      "scvi.model.MULTIVI.setup_mudata(",
      "    mdata,",
      '    rna_layer="counts",',
      "    protein_layer=None,",
      '    batch_key="batch",',
      '    modalities={"rna": "rna", "accessibility": "atac"}',
      ")",
      "```",
      "",
      "## Complete Preparation Pipeline",
      "",
      "For a complete preparation function, use `prepare_adata()` from `scripts/model_utils.py`:",
      "",
      "```python",
      "from model_utils import prepare_adata",
      "",
      "# Prepare data with QC, HVG selection, and layer setup",
      "adata = prepare_adata(",
      "    adata,",
      '    batch_key="batch",',
      "    n_top_genes=2000,",
      "    min_genes=200,",
      "    max_mito_pct=20",
      ")",
      "",
      "# Then setup for your model",
      "import scvi",
      'scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "```",
      "",
      "This function handles:",
      "- Mitochondrial QC filtering",
      "- Cell and gene filtering",
      "- Storing counts in layer",
      "- HVG selection (batch-aware if batch_key provided)",
      "- Subsetting to HVGs",
      "",
      "## Checking Setup",
      "",
      "```python",
      "# View registered data",
      "print(adata.uns['_scvi_manager_uuid'])",
      "print(adata.uns['_scvi_adata_minify_type'])",
      "",
      "# For scVI",
      "scvi.model.SCVI.view_anndata_setup(adata)",
      "```",
      "",
      "## Common Issues and Solutions",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      '| "X should contain integers" | Normalized data in X | Use layer="counts" |',
      '| "batch_key not found" | Wrong column name | Check adata.obs.columns |',
      "| Sparse matrix errors | Incompatible format | Convert: adata.X = adata.X.toarray() |",
      "| Memory error | Too many genes | Subset to HVGs first |",
      "| NaN in data | Missing values | Filter or impute |",
      "",
      "## Data Format Reference",
      "",
      "### Required",
      "",
      '- `adata.X` or `adata.layers["counts"]`: Raw integer counts (sparse OK)',
      "- `adata.obs`: Cell metadata DataFrame",
      "- `adata.var`: Gene metadata DataFrame",
      "",
      "### Recommended",
      "",
      '- `adata.obs["batch"]`: Batch/sample identifiers',
      '- `adata.var["highly_variable"]`: HVG boolean mask',
      "",
      "### For scANVI",
      "",
      '- `adata.obs["labels"]`: Cell type annotations',
      '- Can include "Unknown" for unlabeled cells',
      "",
    ].join("\n"),
    "references/environment_setup.md": [
      "# Environment Setup for scvi-tools",
      "",
      "This reference covers installation and environment configuration for scvi-tools.",
      "",
      "## Installation Options",
      "",
      "### Option 1: Conda Environment (Recommended)",
      "",
      "```bash",
      "# Create environment with GPU support",
      "conda create -n scvi-env python=3.10",
      "conda activate scvi-env",
      "",
      "# Install scvi-tools",
      "pip install scvi-tools",
      "",
      "# For GPU acceleration (recommended for large datasets)",
      "pip install torch --index-url https://download.pytorch.org/whl/cu118",
      "",
      "# Common dependencies",
      "pip install scanpy leidenalg",
      "```",
      "",
      "### Option 2: Pip Only",
      "",
      "```bash",
      "# Create virtual environment",
      "python -m venv scvi-env",
      "source scvi-env/bin/activate  # Linux/Mac",
      "# scvi-env\\Scripts\\activate   # Windows",
      "",
      "# Install",
      "pip install scvi-tools scanpy",
      "```",
      "",
      "### Option 3: With Spatial Analysis Support",
      "",
      "```bash",
      "conda create -n scvi-spatial python=3.10",
      "conda activate scvi-spatial",
      "",
      "pip install scvi-tools scanpy squidpy",
      "```",
      "",
      "### Option 4: With MuData Support (Multiome)",
      "",
      "```bash",
      "pip install scvi-tools mudata muon",
      "```",
      "",
      "## Verify Installation",
      "",
      "```python",
      "import scvi",
      "import torch",
      "import scanpy as sc",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      'print(f"scanpy version: {sc.__version__}")',
      'print(f"PyTorch version: {torch.__version__}")',
      'print(f"GPU available: {torch.cuda.is_available()}")',
      "",
      "if torch.cuda.is_available():",
      '    print(f"GPU device: {torch.cuda.get_device_name(0)}")',
      '    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")',
      "```",
      "",
      "## GPU Configuration",
      "",
      "### Check CUDA Version",
      "",
      "```bash",
      "nvidia-smi",
      "nvcc --version",
      "```",
      "",
      "### PyTorch CUDA Versions",
      "",
      "| CUDA Version | PyTorch Install Command |",
      "|--------------|------------------------|",
      "| CUDA 11.8 | `pip install torch --index-url https://download.pytorch.org/whl/cu118` |",
      "| CUDA 12.1 | `pip install torch --index-url https://download.pytorch.org/whl/cu121` |",
      "| CPU only | `pip install torch --index-url https://download.pytorch.org/whl/cpu` |",
      "",
      "### Memory Management",
      "",
      "```python",
      "import torch",
      "",
      "# Clear GPU cache between models",
      "torch.cuda.empty_cache()",
      "",
      "# Monitor memory usage",
      'print(f"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")',
      'print(f"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")',
      "```",
      "",
      "## Common Issues",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| `CUDA out of memory` | GPU memory exhausted | Reduce batch_size, use smaller model |",
      "| `No GPU detected` | CUDA not installed | Install CUDA toolkit matching PyTorch |",
      "| `Version mismatch` | PyTorch/CUDA incompatibility | Reinstall PyTorch with correct CUDA version |",
      "| `Import error scvi` | Missing dependencies | `pip install scvi-tools[all]` |",
      "",
      "## Jupyter Setup",
      "",
      "```bash",
      "# Install Jupyter kernel",
      "pip install ipykernel",
      'python -m ipykernel install --user --name scvi-env --display-name "scvi-tools"',
      "",
      "# For interactive plots",
      "pip install matplotlib seaborn",
      "```",
      "",
      "## Recommended Package Versions",
      "",
      "For reproducibility, pin versions:",
      "",
      "```bash",
      "pip install \\",
      "    scvi-tools>=1.0.0 \\",
      "    scanpy>=1.9.0 \\",
      "    anndata>=0.9.0 \\",
      "    torch>=2.0.0",
      "```",
      "",
      "## Version Compatibility Guide",
      "",
      "### scvi-tools 1.x vs 0.x API Changes",
      "",
      "The 1.x release introduced breaking changes. Key differences:",
      "",
      "| Operation | 0.x API (deprecated) | 1.x API (current) |",
      "|-----------|---------------------|-------------------|",
      "| Setup data | `scvi.data.setup_anndata(adata, ...)` | `scvi.model.SCVI.setup_anndata(adata, ...)` |",
      "| Register data | `scvi.data.register_tensor_from_anndata(...)` | Built into `setup_anndata` |",
      "| View setup | `scvi.data.view_anndata_setup(adata)` | `scvi.model.SCVI.view_anndata_setup(adata)` |",
      "",
      "### Migration from 0.x to 1.x",
      "",
      "```python",
      "# OLD (0.x) - DEPRECATED",
      "import scvi",
      'scvi.data.setup_anndata(adata, layer="counts", batch_key="batch")',
      "model = scvi.model.SCVI(adata)",
      "",
      "# NEW (1.x) - CURRENT",
      "import scvi",
      'scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "model = scvi.model.SCVI(adata)",
      "```",
      "",
      "### Model-Specific Setup (1.x)",
      "",
      "Each model has its own setup method:",
      "",
      "```python",
      "# scVI",
      'scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "",
      "# scANVI",
      'scvi.model.SCANVI.setup_anndata(adata, layer="counts", batch_key="batch", labels_key="cell_type")',
      "",
      "# totalVI",
      'scvi.model.TOTALVI.setup_anndata(adata, layer="counts", protein_expression_obsm_key="protein")',
      "",
      "# MultiVI (uses MuData)",
      'scvi.model.MULTIVI.setup_mudata(mdata, rna_layer="counts", atac_layer="counts")',
      "",
      "# PeakVI",
      'scvi.model.PEAKVI.setup_anndata(adata, batch_key="batch")',
      "",
      "# veloVI",
      'scvi.external.VELOVI.setup_anndata(adata, spliced_layer="spliced", unspliced_layer="unspliced")',
      "```",
      "",
      "### Minimum Version Requirements",
      "",
      "| Package | Minimum Version | Notes |",
      "|---------|-----------------|-------|",
      "| scvi-tools | 1.0.0 | Required for current API |",
      "| scanpy | 1.9.0 | HVG selection improvements |",
      "| anndata | 0.9.0 | Improved MuData support |",
      "| torch | 2.0.0 | Performance improvements |",
      "| mudata | 0.2.0 | Required for MultiVI |",
      "| scvelo | 0.2.5 | Required for veloVI |",
      "",
      "### Check Your Versions",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import anndata",
      "import torch",
      "",
      'print(f"scvi-tools: {scvi.__version__}")',
      'print(f"scanpy: {sc.__version__}")',
      'print(f"anndata: {anndata.__version__}")',
      'print(f"torch: {torch.__version__}")',
      "",
      "# Check if using 1.x API",
      "if hasattr(scvi.model.SCVI, 'setup_anndata'):",
      '    print("Using scvi-tools 1.x API")',
      "else:",
      '    print("WARNING: Using deprecated 0.x API - please upgrade")',
      "```",
      "",
      "### Known Compatibility Issues",
      "",
      "| Issue | Affected Versions | Solution |",
      "|-------|-------------------|----------|",
      "| `setup_anndata` not found | scvi-tools < 1.0 | Upgrade to 1.0+ |",
      "| MuData errors | mudata < 0.2 | `pip install mudata>=0.2.0` |",
      "| CUDA version mismatch | Any | Reinstall PyTorch for your CUDA |",
      "| numpy 2.0 issues | Early 2024 builds | `pip install numpy<2.0` |",
      "",
      "### Upgrading scvi-tools",
      "",
      "```bash",
      "# Upgrade to latest",
      "pip install --upgrade scvi-tools",
      "",
      "# Upgrade all dependencies",
      "pip install --upgrade scvi-tools scanpy anndata torch",
      "",
      "# If you have issues, clean install",
      "pip uninstall scvi-tools",
      "pip cache purge",
      "pip install scvi-tools",
      "```",
      "",
      "## Testing Installation",
      "",
      "```python",
      "# Quick test with sample data",
      "import scvi",
      "import scanpy as sc",
      "",
      "# Load test dataset",
      "adata = scvi.data.heart_cell_atlas_subsampled()",
      'print(f"Loaded test data: {adata.shape}")',
      "",
      "# Setup and create model (quick test)",
      'scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="cell_source")',
      "model = scvi.model.SCVI(adata, n_latent=10)",
      'print("Model created successfully")',
      "",
      "# Quick training test (1 epoch)",
      "model.train(max_epochs=1)",
      'print("Training works!")',
      "```",
      "",
    ].join("\n"),
    "references/label_transfer.md": [
      "# Label Transfer and Reference Mapping with scANVI",
      "",
      "This reference covers using scANVI for transferring cell type annotations from a reference atlas to query data.",
      "",
      "## Overview",
      "",
      'Reference mapping (also called "label transfer") uses a pre-trained model on annotated reference data to predict cell types in new, unannotated query data. This is faster than re-clustering and more consistent across studies.',
      "",
      "scANVI excels at this because it:",
      "- Jointly embeds reference and query in shared space",
      "- Transfers labels probabilistically",
      "- Handles batch effects between reference and query",
      "",
      "## When to Use Reference Mapping",
      "",
      "- Annotating new dataset using existing atlas",
      "- Consistent annotation across multiple studies",
      "- Speed: no need to re-cluster and manually annotate",
      "- Quality: leverage expert-curated reference annotations",
      "",
      "## Workflow Options",
      "",
      "1. **Train new model**: Train scANVI on reference, then map query",
      "2. **Use pre-trained model**: Load existing model (e.g., from Model Hub)",
      "3. **scArches**: Extend existing model with query data (preserves reference)",
      "",
      "## Option 1: Train scANVI on Reference",
      "",
      "### Step 1: Prepare Reference Data",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "",
      "# Load reference atlas",
      'adata_ref = sc.read_h5ad("reference_atlas.h5ad")',
      "",
      "# Check annotations",
      'print(f"Reference cells: {adata_ref.n_obs}")',
      "print(f\"Cell types: {adata_ref.obs['cell_type'].nunique()}\")",
      "print(adata_ref.obs['cell_type'].value_counts())",
      "",
      "# Ensure raw counts",
      'adata_ref.layers["counts"] = adata_ref.raw.X.copy() if adata_ref.raw else adata_ref.X.copy()',
      "",
      "# HVG selection",
      "sc.pp.highly_variable_genes(",
      "    adata_ref,",
      "    n_top_genes=3000,",
      '    flavor="seurat_v3",',
      '    batch_key="batch" if "batch" in adata_ref.obs else None,',
      '    layer="counts"',
      ")",
      'adata_ref = adata_ref[:, adata_ref.var["highly_variable"]].copy()',
      "```",
      "",
      "### Step 2: Train scANVI on Reference",
      "",
      "```python",
      "# First train scVI (unlabeled)",
      "scvi.model.SCVI.setup_anndata(",
      "    adata_ref,",
      '    layer="counts",',
      '    batch_key="batch"',
      ")",
      "",
      "scvi_ref = scvi.model.SCVI(adata_ref, n_latent=30)",
      "scvi_ref.train(max_epochs=200)",
      "",
      "# Initialize scANVI from scVI",
      "scanvi_ref = scvi.model.SCANVI.from_scvi_model(",
      "    scvi_ref,",
      '    labels_key="cell_type",',
      '    unlabeled_category="Unknown"',
      ")",
      "",
      "# Train scANVI",
      "scanvi_ref.train(max_epochs=50)",
      "",
      "# Save for later use",
      'scanvi_ref.save("scanvi_reference_model/")',
      "```",
      "",
      "### Step 3: Prepare Query Data",
      "",
      "```python",
      "# Load query data",
      'adata_query = sc.read_h5ad("query_data.h5ad")',
      "",
      "# CRITICAL: Use same genes as reference",
      "common_genes = adata_ref.var_names.intersection(adata_query.var_names)",
      'print(f"Common genes: {len(common_genes)}")',
      "",
      "# Subset query to reference genes",
      "adata_query = adata_query[:, adata_ref.var_names].copy()",
      "",
      "# Handle missing genes (set to 0)",
      "missing_genes = set(adata_ref.var_names) - set(adata_query.var_names)",
      "if missing_genes:",
      "    # Add missing genes with zero expression",
      "    import numpy as np",
      "    from scipy.sparse import csr_matrix",
      "    ",
      "    zero_matrix = csr_matrix((adata_query.n_obs, len(missing_genes)))",
      "    # ... concat and reorder to match reference",
      "    ",
      "# Store counts",
      'adata_query.layers["counts"] = adata_query.X.copy()',
      "```",
      "",
      "### Step 4: Map Query to Reference",
      "",
      "```python",
      "# Prepare query data for mapping",
      "scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)",
      "",
      "# Create query model from reference",
      "scanvi_query = scvi.model.SCANVI.load_query_data(",
      "    adata_query,",
      "    scanvi_ref",
      ")",
      "",
      "# Fine-tune on query (optional but recommended)",
      "scanvi_query.train(",
      "    max_epochs=100,",
      '    plan_kwargs={"weight_decay": 0.0}',
      ")",
      "",
      "# Get predictions",
      'adata_query.obs["predicted_cell_type"] = scanvi_query.predict()',
      "",
      "# Get prediction probabilities",
      "soft_predictions = scanvi_query.predict(soft=True)",
      'adata_query.obs["prediction_score"] = soft_predictions.max(axis=1)',
      "```",
      "",
      "### Step 5: Evaluate Predictions",
      "",
      "```python",
      "# Confidence scores",
      "print(f\"Mean prediction confidence: {adata_query.obs['prediction_score'].mean():.3f}\")",
      "",
      "# Low confidence predictions",
      "low_conf = adata_query.obs['prediction_score'] < 0.5",
      'print(f"Low confidence cells: {low_conf.sum()} ({low_conf.mean()*100:.1f}%)")',
      "",
      "# Visualize",
      'sc.pp.neighbors(adata_query, use_rep="X_scANVI")',
      "sc.tl.umap(adata_query)",
      "sc.pl.umap(adata_query, color=['predicted_cell_type', 'prediction_score'])",
      "```",
      "",
      "## Option 2: Use Pre-Trained Models",
      "",
      "### From Model Hub",
      "",
      "```python",
      "# scvi-tools maintains models on HuggingFace",
      "# Check: https://huggingface.co/scvi-tools",
      "",
      "# Example: Load pre-trained model",
      "from huggingface_hub import hf_hub_download",
      "",
      "model_path = hf_hub_download(",
      '    repo_id="scvi-tools/example-model",',
      '    filename="model.pt"',
      ")",
      "",
      "# Load model",
      "model = scvi.model.SCANVI.load(model_path, adata=adata_query)",
      "```",
      "",
      "### From Published Atlas",
      "",
      "```python",
      "# Many atlases provide pre-trained models",
      "# Example workflow with CellTypist-style model",
      "",
      "# Download reference model",
      '# model = scvi.model.SCANVI.load("atlas_model/", adata=adata_query)',
      "```",
      "",
      "## Option 3: scArches for Incremental Updates",
      "",
      "scArches extends a reference model without retraining from scratch:",
      "",
      "```python",
      "# Load existing reference model",
      'scanvi_ref = scvi.model.SCANVI.load("reference_model/")',
      "",
      "# Surgery: prepare for query integration",
      "scanvi_ref.freeze_layers()",
      "",
      "# Map query data",
      "scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)",
      "scanvi_query = scvi.model.SCANVI.load_query_data(adata_query, scanvi_ref)",
      "",
      "# Train only query-specific parameters",
      "scanvi_query.train(",
      "    max_epochs=200,",
      '    plan_kwargs={"weight_decay": 0.0}',
      ")",
      "```",
      "",
      "## Visualize Reference and Query Together",
      "",
      "```python",
      "# Concatenate for joint visualization",
      'adata_ref.obs["dataset"] = "reference"',
      'adata_query.obs["dataset"] = "query"',
      "",
      "# Get latent representations",
      'adata_ref.obsm["X_scANVI"] = scanvi_ref.get_latent_representation()',
      'adata_query.obsm["X_scANVI"] = scanvi_query.get_latent_representation()',
      "",
      "# Combine",
      "adata_combined = sc.concat([adata_ref, adata_query])",
      "",
      "# Compute combined UMAP",
      'sc.pp.neighbors(adata_combined, use_rep="X_scANVI")',
      "sc.tl.umap(adata_combined)",
      "",
      "# Plot",
      "sc.pl.umap(",
      "    adata_combined,",
      '    color=["dataset", "cell_type", "predicted_cell_type"],',
      "    ncols=2",
      ")",
      "```",
      "",
      "## Quality Control for Predictions",
      "",
      "### Confidence Filtering",
      "",
      "```python",
      "# Filter predictions by confidence",
      "confidence_threshold = 0.7",
      "",
      "high_conf = adata_query[adata_query.obs['prediction_score'] >= confidence_threshold].copy()",
      "low_conf = adata_query[adata_query.obs['prediction_score'] < confidence_threshold].copy()",
      "",
      'print(f"High confidence: {len(high_conf)} ({len(high_conf)/len(adata_query)*100:.1f}%)")',
      'print(f"Low confidence: {len(low_conf)} ({len(low_conf)/len(adata_query)*100:.1f}%)")',
      "```",
      "",
      "### Marker Validation",
      "",
      "```python",
      "# Validate predictions with known markers",
      "markers = {",
      "    'T cells': ['CD3D', 'CD3E'],",
      "    'B cells': ['CD19', 'MS4A1'],",
      "    'Monocytes': ['CD14', 'LYZ']",
      "}",
      "",
      "for ct, genes in markers.items():",
      "    ct_cells = adata_query[adata_query.obs['predicted_cell_type'] == ct]",
      "    if len(ct_cells) > 0:",
      "        for gene in genes:",
      "            if gene in adata_query.var_names:",
      "                expr = ct_cells[:, gene].X.mean()",
      '                print(f"{ct} - {gene}: {expr:.3f}")',
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def transfer_labels(",
      "    adata_ref,",
      "    adata_query,",
      '    cell_type_key="cell_type",',
      "    batch_key=None,",
      "    n_top_genes=3000,",
      "    confidence_threshold=0.5",
      "):",
      '    """',
      "    Transfer cell type labels from reference to query.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata_ref : AnnData",
      "        Annotated reference data",
      "    adata_query : AnnData",
      "        Unannotated query data",
      "    cell_type_key : str",
      "        Column with cell type annotations in reference",
      "    batch_key : str, optional",
      "        Batch column",
      "    n_top_genes : int",
      "        Number of HVGs",
      "    confidence_threshold : float",
      "        Minimum confidence for predictions",
      "        ",
      "    Returns",
      "    -------",
      "    AnnData with predictions",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    ",
      "    # Prepare reference",
      "    adata_ref = adata_ref.copy()",
      '    adata_ref.layers["counts"] = adata_ref.X.copy()',
      "    ",
      "    sc.pp.highly_variable_genes(",
      "        adata_ref,",
      "        n_top_genes=n_top_genes,",
      '        flavor="seurat_v3",',
      "        batch_key=batch_key,",
      '        layer="counts"',
      "    )",
      '    adata_ref = adata_ref[:, adata_ref.var["highly_variable"]].copy()',
      "    ",
      "    # Train reference model",
      '    scvi.model.SCVI.setup_anndata(adata_ref, layer="counts", batch_key=batch_key)',
      "    scvi_ref = scvi.model.SCVI(adata_ref, n_latent=30)",
      "    scvi_ref.train(max_epochs=200)",
      "    ",
      "    scanvi_ref = scvi.model.SCANVI.from_scvi_model(",
      "        scvi_ref,",
      "        labels_key=cell_type_key,",
      '        unlabeled_category="Unknown"',
      "    )",
      "    scanvi_ref.train(max_epochs=50)",
      "    ",
      "    # Prepare query",
      "    adata_query = adata_query[:, adata_ref.var_names].copy()",
      '    adata_query.layers["counts"] = adata_query.X.copy()',
      "    ",
      "    # Map query",
      "    scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)",
      "    scanvi_query = scvi.model.SCANVI.load_query_data(adata_query, scanvi_ref)",
      '    scanvi_query.train(max_epochs=100, plan_kwargs={"weight_decay": 0.0})',
      "    ",
      "    # Get predictions",
      '    adata_query.obs["predicted_cell_type"] = scanvi_query.predict()',
      "    soft = scanvi_query.predict(soft=True)",
      '    adata_query.obs["prediction_score"] = soft.max(axis=1)',
      "    ",
      "    # Mark low confidence",
      '    adata_query.obs["confident_prediction"] = adata_query.obs["prediction_score"] >= confidence_threshold',
      "    ",
      "    # Add latent representation",
      '    adata_query.obsm["X_scANVI"] = scanvi_query.get_latent_representation()',
      "    ",
      "    return adata_query, scanvi_ref, scanvi_query",
      "",
      "# Usage",
      "adata_annotated, ref_model, query_model = transfer_labels(",
      "    adata_ref,",
      "    adata_query,",
      '    cell_type_key="cell_type"',
      ")",
      "",
      "# Visualize",
      'sc.pp.neighbors(adata_annotated, use_rep="X_scANVI")',
      "sc.tl.umap(adata_annotated)",
      "sc.pl.umap(adata_annotated, color=['predicted_cell_type', 'prediction_score'])",
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Many low-confidence predictions | Query has novel cell types | Manually annotate low-confidence cells |",
      "| Wrong predictions | Reference doesn't match tissue | Use tissue-appropriate reference |",
      "| Gene mismatch | Different gene naming | Convert gene IDs |",
      "| All same prediction | Query too different | Check data quality, try different reference |",
      "",
      "## Key References",
      "",
      '- Xu et al. (2021) "Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models"',
      '- Lotfollahi et al. (2022) "Mapping single-cell data to reference atlases by transfer learning"',
      "",
    ].join("\n"),
    "references/multiome_multivi.md": [
      "# Multiome Analysis with MultiVI",
      "",
      "This reference covers joint RNA and ATAC-seq analysis from multiome experiments using MultiVI.",
      "",
      "## Overview",
      "",
      "MultiVI is a deep generative model for analyzing multiome data (simultaneous RNA-seq and ATAC-seq from the same cells). It:",
      "- Learns a joint latent representation across modalities",
      "- Handles missing modalities (RNA-only or ATAC-only cells)",
      "- Enables batch correction across experiments",
      "- Supports imputation of missing modalities",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import mudata as md",
      "import numpy as np",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      "```",
      "",
      "## Data Formats",
      "",
      "### Option 1: MuData (Recommended)",
      "",
      "```python",
      "# Load multiome data as MuData",
      'mdata = md.read("multiome.h5mu")',
      "",
      "# Structure:",
      "# mdata.mod['rna']  - AnnData with RNA counts",
      "# mdata.mod['atac'] - AnnData with ATAC counts",
      "",
      "print(f\"RNA: {mdata.mod['rna'].shape}\")",
      "print(f\"ATAC: {mdata.mod['atac'].shape}\")",
      "```",
      "",
      "### Option 2: Separate AnnData Objects",
      "",
      "```python",
      "# Load separately",
      'adata_rna = sc.read_h5ad("rna.h5ad")',
      'adata_atac = sc.read_h5ad("atac.h5ad")',
      "",
      "# Ensure same cells in same order",
      "common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)",
      "adata_rna = adata_rna[common_cells].copy()",
      "adata_atac = adata_atac[common_cells].copy()",
      "```",
      "",
      "## Step 1: Prepare RNA Data",
      "",
      "```python",
      "# RNA preprocessing (standard scvi-tools pipeline)",
      "adata_rna = mdata.mod['rna'].copy()",
      "",
      "# Filter",
      "sc.pp.filter_cells(adata_rna, min_genes=200)",
      "sc.pp.filter_genes(adata_rna, min_cells=3)",
      "",
      "# Store counts",
      'adata_rna.layers["counts"] = adata_rna.X.copy()',
      "",
      "# HVG selection",
      "sc.pp.highly_variable_genes(",
      "    adata_rna,",
      "    n_top_genes=4000,",
      '    flavor="seurat_v3",',
      '    layer="counts",',
      '    batch_key="batch"  # If multiple batches',
      ")",
      "",
      "# Subset to HVGs",
      "adata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()",
      "```",
      "",
      "## Step 2: Prepare ATAC Data",
      "",
      "```python",
      "# ATAC preprocessing",
      "adata_atac = mdata.mod['atac'].copy()",
      "",
      "# Filter peaks",
      "sc.pp.filter_genes(adata_atac, min_cells=10)",
      "",
      "# Binarize accessibility",
      "adata_atac.X = (adata_atac.X > 0).astype(np.float32)",
      "",
      "# Select top accessible peaks (if too many)",
      "if adata_atac.n_vars > 50000:",
      "    peak_accessibility = np.array(adata_atac.X.sum(axis=0)).flatten()",
      "    top_peaks = np.argsort(peak_accessibility)[-50000:]",
      "    adata_atac = adata_atac[:, top_peaks].copy()",
      "",
      "# Store in layer",
      'adata_atac.layers["counts"] = adata_atac.X.copy()',
      "```",
      "",
      "## Step 3: Create Combined MuData",
      "",
      "```python",
      "# Ensure matching cells",
      "common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)",
      "adata_rna = adata_rna[common_cells].copy()",
      "adata_atac = adata_atac[common_cells].copy()",
      "",
      "# Create MuData",
      "mdata = md.MuData({",
      '    "rna": adata_rna,',
      '    "atac": adata_atac',
      "})",
      "",
      'print(f"Combined multiome: {mdata.n_obs} cells")',
      "print(f\"RNA features: {mdata.mod['rna'].n_vars}\")",
      "print(f\"ATAC features: {mdata.mod['atac'].n_vars}\")",
      "```",
      "",
      "## Step 4: Setup MultiVI",
      "",
      "```python",
      "# Setup MuData for MultiVI",
      "scvi.model.MULTIVI.setup_mudata(",
      "    mdata,",
      '    rna_layer="counts",',
      '    atac_layer="counts",',
      '    batch_key="batch",  # Optional',
      "    modalities={",
      '        "rna_layer": "rna",',
      '        "batch_key": "rna",',
      '        "atac_layer": "atac"',
      "    }",
      ")",
      "```",
      "",
      "## Step 5: Train MultiVI",
      "",
      "```python",
      "# Create model",
      "model = scvi.model.MULTIVI(",
      "    mdata,",
      "    n_latent=20,",
      "    n_layers_encoder=2,",
      "    n_layers_decoder=2",
      ")",
      "",
      "# Train",
      "model.train(",
      "    max_epochs=300,",
      "    early_stopping=True,",
      "    early_stopping_patience=10,",
      "    batch_size=128",
      ")",
      "",
      "# Check training",
      "model.history['elbo_train'].plot()",
      "```",
      "",
      "## Step 6: Get Joint Representation",
      "",
      "```python",
      "# Latent representation",
      "latent = model.get_latent_representation()",
      "",
      "# Add to MuData",
      'mdata.obsm["X_MultiVI"] = latent',
      "",
      "# Clustering on joint space",
      'sc.pp.neighbors(mdata, use_rep="X_MultiVI")',
      "sc.tl.umap(mdata)",
      "sc.tl.leiden(mdata, resolution=1.0)",
      "",
      "# Visualize",
      "sc.pl.umap(mdata, color=['leiden', 'batch'], ncols=2)",
      "```",
      "",
      "## Step 7: Modality-Specific Analysis",
      "",
      "### Impute Missing Modality",
      "",
      "```python",
      "# Impute RNA expression for ATAC-only cells",
      "# (Useful when integrating with ATAC-only datasets)",
      "imputed_rna = model.get_normalized_expression(",
      '    modality="rna"',
      ")",
      "",
      "# Impute accessibility for RNA-only cells",
      "imputed_atac = model.get_accessibility_estimates()",
      "```",
      "",
      "### Differential Analysis",
      "",
      "```python",
      "# Differential expression (RNA)",
      "de_results = model.differential_expression(",
      '    groupby="leiden",',
      '    group1="0",',
      '    group2="1"',
      ")",
      "",
      "# Differential accessibility (ATAC)",
      "da_results = model.differential_accessibility(",
      '    groupby="leiden",',
      '    group1="0",',
      '    group2="1"',
      ")",
      "```",
      "",
      "## Handling Partial Data",
      "",
      "MultiVI can integrate datasets with only one modality:",
      "",
      "```python",
      "# Dataset 1: Full multiome",
      "# Dataset 2: RNA only",
      "# Dataset 3: ATAC only",
      "",
      "# Mark missing modalities",
      "mdata.obs['modality'] = 'paired'  # For cells with both",
      "# For RNA-only cells, ATAC data should be missing/NaN",
      "# For ATAC-only cells, RNA data should be missing/NaN",
      "",
      "# MultiVI handles this automatically during training",
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def analyze_multiome(",
      "    adata_rna,",
      "    adata_atac,",
      "    batch_key=None,",
      "    n_top_genes=4000,",
      "    n_top_peaks=50000,",
      "    n_latent=20,",
      "    max_epochs=300",
      "):",
      '    """',
      "    Complete multiome analysis with MultiVI.",
      "",
      "    Parameters",
      "    ----------",
      "    adata_rna : AnnData",
      "        RNA count data",
      "    adata_atac : AnnData",
      "        ATAC peak data",
      "    batch_key : str, optional",
      "        Batch column name",
      "    n_top_genes : int",
      "        Number of HVGs for RNA",
      "    n_top_peaks : int",
      "        Number of top peaks for ATAC",
      "    n_latent : int",
      "        Latent dimensions",
      "    max_epochs : int",
      "        Maximum training epochs",
      "",
      "    Returns",
      "    -------",
      "    MuData with joint representation",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    import mudata as md",
      "    import numpy as np",
      "",
      "    # Get common cells",
      "    common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)",
      "    adata_rna = adata_rna[common_cells].copy()",
      "    adata_atac = adata_atac[common_cells].copy()",
      "",
      "    # RNA preprocessing",
      "    sc.pp.filter_genes(adata_rna, min_cells=3)",
      '    adata_rna.layers["counts"] = adata_rna.X.copy()',
      "",
      "    if batch_key:",
      "        sc.pp.highly_variable_genes(",
      "            adata_rna, n_top_genes=n_top_genes,",
      '            flavor="seurat_v3", layer="counts", batch_key=batch_key',
      "        )",
      "    else:",
      "        sc.pp.normalize_total(adata_rna, target_sum=1e4)",
      "        sc.pp.log1p(adata_rna)",
      "        sc.pp.highly_variable_genes(adata_rna, n_top_genes=n_top_genes)",
      '        adata_rna.X = adata_rna.layers["counts"].copy()',
      "",
      "    adata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()",
      "",
      "    # ATAC preprocessing",
      "    sc.pp.filter_genes(adata_atac, min_cells=10)",
      "    adata_atac.X = (adata_atac.X > 0).astype(np.float32)",
      "",
      "    if adata_atac.n_vars > n_top_peaks:",
      "        peak_acc = np.array(adata_atac.X.sum(axis=0)).flatten()",
      "        top_idx = np.argsort(peak_acc)[-n_top_peaks:]",
      "        adata_atac = adata_atac[:, top_idx].copy()",
      "",
      '    adata_atac.layers["counts"] = adata_atac.X.copy()',
      "",
      "    # Create MuData",
      '    mdata = md.MuData({"rna": adata_rna, "atac": adata_atac})',
      "",
      "    # Setup and train",
      "    scvi.model.MULTIVI.setup_mudata(",
      "        mdata,",
      '        rna_layer="counts",',
      '        atac_layer="counts",',
      "        batch_key=batch_key,",
      '        modalities={"rna_layer": "rna", "batch_key": "rna", "atac_layer": "atac"}',
      "    )",
      "",
      "    model = scvi.model.MULTIVI(mdata, n_latent=n_latent)",
      "    model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      "    # Add representation",
      '    mdata.obsm["X_MultiVI"] = model.get_latent_representation()',
      "",
      "    # Cluster",
      '    sc.pp.neighbors(mdata, use_rep="X_MultiVI")',
      "    sc.tl.umap(mdata)",
      "    sc.tl.leiden(mdata)",
      "",
      "    return mdata, model",
      "",
      "",
      "# Usage",
      "mdata, model = analyze_multiome(",
      "    adata_rna,",
      "    adata_atac,",
      '    batch_key="sample"',
      ")",
      "",
      "sc.pl.umap(mdata, color=['leiden', 'sample'])",
      "```",
      "",
      "## Peak-to-Gene Linking",
      "",
      "```python",
      "# Link ATAC peaks to genes based on correlation in latent space",
      "# This identifies regulatory relationships",
      "",
      "def link_peaks_to_genes(model, mdata, distance_threshold=100000):",
      '    """',
      "    Link peaks to nearby genes based on correlation.",
      "",
      "    Parameters",
      "    ----------",
      "    model : MULTIVI",
      "        Trained model",
      "    mdata : MuData",
      "        Multiome data",
      "    distance_threshold : int",
      "        Maximum distance (bp) to link peak to gene",
      "",
      "    Returns",
      "    -------",
      "    DataFrame of peak-gene links",
      '    """',
      "    # Get imputed values",
      "    rna_imputed = model.get_normalized_expression()",
      "    atac_imputed = model.get_accessibility_estimates()",
      "",
      "    # Correlate peak accessibility with gene expression",
      "    # for peaks near gene promoters",
      "    # ... (requires genomic coordinates)",
      "",
      "    return peak_gene_links",
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Different cell counts | Cells missing in one modality | Use common cells only |",
      "| Training instability | Imbalanced modalities | Normalize feature counts |",
      "| Poor clustering | Too few features | Increase n_top_genes/peaks |",
      "| Memory error | Large ATAC matrix | Reduce peak count, use sparse |",
      "| Batch dominates | Strong technical effects | Ensure batch_key is set |",
      "",
      "## Key References",
      "",
      '- Ashuach et al. (2023) "MultiVI: deep generative model for the integration of multimodal data"',
      "",
    ].join("\n"),
    "references/rna_velocity_velovi.md": [
      "# RNA Velocity with veloVI",
      "",
      "This reference covers RNA velocity analysis using veloVI, a deep learning approach that improves upon traditional velocity methods.",
      "",
      "## Overview",
      "",
      "RNA velocity estimates the future state of cells by modeling:",
      "- **Unspliced RNA**: Newly transcribed, contains introns",
      "- **Spliced RNA**: Mature mRNA, introns removed",
      "",
      "The ratio of unspliced to spliced indicates whether a gene is being upregulated or downregulated.",
      "",
      "## Why veloVI?",
      "",
      "Traditional methods (velocyto, scVelo) have limitations:",
      "- Assume steady-state or dynamical model",
      "- Sensitive to noise",
      "- Don't handle batch effects",
      "",
      "veloVI addresses these with:",
      "- Probabilistic modeling",
      "- Better uncertainty quantification",
      "- Integration with scVI framework",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scvelo as scv",
      "import scanpy as sc",
      "import numpy as np",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      'print(f"scvelo version: {scv.__version__}")',
      "```",
      "",
      "## Step 1: Generate Spliced/Unspliced Counts",
      "",
      "### From BAM Files (velocyto)",
      "",
      "```bash",
      "# Run velocyto on Cell Ranger output",
      "velocyto run10x /path/to/cellranger_output /path/to/genes.gtf",
      "",
      "# Output: velocyto.loom file with spliced/unspliced layers",
      "```",
      "",
      "### From kb-python (kallisto|bustools)",
      "",
      "```bash",
      "# Faster alternative using kallisto",
      "kb count \\",
      "    --workflow lamanno \\",
      "    -i index.idx \\",
      "    -g t2g.txt \\",
      "    -c1 spliced_t2c.txt \\",
      "    -c2 unspliced_t2c.txt \\",
      "    -x 10xv3 \\",
      "    -o output \\",
      "    R1.fastq.gz R2.fastq.gz",
      "```",
      "",
      "## Step 2: Load Velocity Data",
      "",
      "```python",
      "# Load loom file from velocyto",
      'adata = scv.read("velocyto_output.loom")',
      "",
      "# Or load from kb-python",
      'adata = sc.read_h5ad("adata.h5ad")',
      '# Spliced in adata.layers["spliced"]',
      '# Unspliced in adata.layers["unspliced"]',
      "",
      "# Check layers",
      'print("Available layers:", list(adata.layers.keys()))',
      "print(f\"Spliced shape: {adata.layers['spliced'].shape}\")",
      "print(f\"Unspliced shape: {adata.layers['unspliced'].shape}\")",
      "```",
      "",
      "### Merge with Existing AnnData",
      "",
      "```python",
      "# If you have separate loom and h5ad",
      'ldata = scv.read("velocyto.loom")',
      'adata = sc.read_h5ad("processed.h5ad")',
      "",
      "# Merge velocity data into processed adata",
      "adata = scv.utils.merge(adata, ldata)",
      "```",
      "",
      "## Step 3: Preprocessing for Velocity",
      "",
      "```python",
      "# Filter and normalize",
      "scv.pp.filter_and_normalize(",
      "    adata,",
      "    min_shared_counts=20,",
      "    n_top_genes=2000",
      ")",
      "",
      "# Compute moments (for scVelo comparison)",
      "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)",
      "```",
      "",
      "## Step 4: Run veloVI",
      "",
      "### Setup AnnData",
      "",
      "```python",
      "# Setup for veloVI",
      "scvi.model.VELOVI.setup_anndata(",
      "    adata,",
      '    spliced_layer="spliced",',
      '    unspliced_layer="unspliced"',
      ")",
      "```",
      "",
      "### Train Model",
      "",
      "```python",
      "# Create and train veloVI model",
      "vae = scvi.model.VELOVI(adata)",
      "",
      "vae.train(",
      "    max_epochs=500,",
      "    early_stopping=True,",
      "    batch_size=256",
      ")",
      "",
      "# Check training",
      'vae.history["elbo_train"].plot()',
      "```",
      "",
      "### Get Velocity Estimates",
      "",
      "```python",
      "# Get latent time",
      "latent_time = vae.get_latent_time(n_samples=25)",
      'adata.obs["veloVI_latent_time"] = latent_time',
      "",
      "# Get velocity",
      "velocities = vae.get_velocity(n_samples=25)",
      'adata.layers["veloVI_velocity"] = velocities',
      "",
      "# Get expression states",
      'adata.layers["veloVI_expression"] = vae.get_expression_fit(n_samples=25)',
      "```",
      "",
      "## Step 5: Visualize Velocity",
      "",
      "### Velocity Streamlines",
      "",
      "```python",
      "# Compute velocity graph",
      'scv.tl.velocity_graph(adata, vkey="veloVI_velocity")',
      "",
      "# Plot streamlines on UMAP",
      "scv.pl.velocity_embedding_stream(",
      "    adata,",
      '    basis="umap",',
      '    vkey="veloVI_velocity",',
      '    color="cell_type"',
      ")",
      "```",
      "",
      "### Velocity Arrows",
      "",
      "```python",
      "# Individual cell arrows",
      "scv.pl.velocity_embedding(",
      "    adata,",
      '    basis="umap",',
      '    vkey="veloVI_velocity",',
      "    arrow_length=3,",
      "    arrow_size=2,",
      '    color="cell_type"',
      ")",
      "```",
      "",
      "### Latent Time",
      "",
      "```python",
      "# Plot latent time (pseudotime from velocity)",
      'sc.pl.umap(adata, color="veloVI_latent_time", cmap="viridis")',
      "```",
      "",
      "## Step 6: Compare with scVelo",
      "",
      "```python",
      "# Run standard scVelo for comparison",
      'scv.tl.velocity(adata, mode="dynamical")',
      "scv.tl.velocity_graph(adata)",
      "",
      "# Compare velocity fields",
      "fig, axes = plt.subplots(1, 2, figsize=(12, 5))",
      "",
      "scv.pl.velocity_embedding_stream(",
      '    adata, basis="umap", ax=axes[0], ',
      '    title="scVelo", show=False',
      ")",
      "",
      "scv.pl.velocity_embedding_stream(",
      '    adata, basis="umap", vkey="veloVI_velocity",',
      '    ax=axes[1], title="veloVI", show=False',
      ")",
      "",
      "plt.tight_layout()",
      "```",
      "",
      "## Step 7: Gene-Level Analysis",
      "",
      "### Velocity Phase Portraits",
      "",
      "```python",
      "# Plot phase portrait for specific genes",
      'genes = ["SOX2", "PAX6", "DCX", "NEUROD1"]',
      "",
      "scv.pl.velocity(",
      "    adata,",
      "    var_names=genes,",
      '    vkey="veloVI_velocity",',
      "    colorbar=True",
      ")",
      "```",
      "",
      "### Gene Dynamics",
      "",
      "```python",
      "# Plot expression over latent time",
      "for gene in genes:",
      "    fig, ax = plt.subplots(figsize=(6, 4))",
      "    ",
      "    sc.pl.scatter(",
      "        adata,",
      '        x="veloVI_latent_time",',
      "        y=gene,",
      '        color="cell_type",',
      "        ax=ax,",
      "        show=False",
      "    )",
      '    ax.set_xlabel("Latent Time")',
      '    ax.set_ylabel(f"{gene} Expression")',
      "```",
      "",
      "### Driver Genes",
      "",
      "```python",
      "# Find genes driving velocity",
      "scv.tl.rank_velocity_genes(",
      "    adata,",
      '    vkey="veloVI_velocity",',
      '    groupby="cell_type"',
      ")",
      "",
      "# Get top genes per cluster",
      'df = scv.get_df(adata, "rank_velocity_genes/names")',
      "print(df.head(10))",
      "```",
      "",
      "## Step 8: Uncertainty Quantification",
      "",
      "veloVI provides uncertainty estimates:",
      "",
      "```python",
      "# Get velocity with uncertainty",
      "velocity_mean, velocity_std = vae.get_velocity(",
      "    n_samples=100,",
      "    return_mean=True,",
      "    return_numpy=True",
      ")",
      "",
      "# Store uncertainty",
      'adata.layers["velocity_uncertainty"] = velocity_std',
      "",
      "# Visualize uncertainty",
      'adata.obs["mean_velocity_uncertainty"] = velocity_std.mean(axis=1)',
      'sc.pl.umap(adata, color="mean_velocity_uncertainty")',
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def run_velocity_analysis(",
      "    adata,",
      '    spliced_layer="spliced",',
      '    unspliced_layer="unspliced",',
      "    n_top_genes=2000,",
      "    max_epochs=500",
      "):",
      '    """',
      "    Complete RNA velocity analysis with veloVI.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data with spliced/unspliced layers",
      "    spliced_layer : str",
      "        Layer name for spliced counts",
      "    unspliced_layer : str",
      "        Layer name for unspliced counts",
      "    n_top_genes : int",
      "        Number of velocity genes",
      "    max_epochs : int",
      "        Training epochs",
      "        ",
      "    Returns",
      "    -------",
      "    AnnData with velocity and model",
      '    """',
      "    import scvi",
      "    import scvelo as scv",
      "    import scanpy as sc",
      "    ",
      "    adata = adata.copy()",
      "    ",
      "    # Preprocessing",
      "    scv.pp.filter_and_normalize(",
      "        adata,",
      "        min_shared_counts=20,",
      "        n_top_genes=n_top_genes",
      "    )",
      "    ",
      "    # Compute moments (needed for some visualizations)",
      "    scv.pp.moments(adata, n_pcs=30, n_neighbors=30)",
      "    ",
      "    # Setup veloVI",
      "    scvi.model.VELOVI.setup_anndata(",
      "        adata,",
      "        spliced_layer=spliced_layer,",
      "        unspliced_layer=unspliced_layer",
      "    )",
      "    ",
      "    # Train",
      "    model = scvi.model.VELOVI(adata)",
      "    model.train(max_epochs=max_epochs, early_stopping=True)",
      "    ",
      "    # Get results",
      '    adata.obs["latent_time"] = model.get_latent_time(n_samples=25)',
      '    adata.layers["velocity"] = model.get_velocity(n_samples=25)',
      "    ",
      "    # Compute velocity graph for visualization",
      '    scv.tl.velocity_graph(adata, vkey="velocity")',
      "    ",
      "    # Compute UMAP if not present",
      '    if "X_umap" not in adata.obsm:',
      "        sc.pp.neighbors(adata)",
      "        sc.tl.umap(adata)",
      "    ",
      "    return adata, model",
      "",
      "# Usage",
      "adata_velocity, model = run_velocity_analysis(adata)",
      "",
      "# Visualize",
      "scv.pl.velocity_embedding_stream(",
      "    adata_velocity,",
      '    basis="umap",',
      '    vkey="velocity",',
      '    color="cell_type"',
      ")",
      "",
      'sc.pl.umap(adata_velocity, color="latent_time")',
      "```",
      "",
      "## Advanced: Batch-Aware Velocity",
      "",
      "```python",
      "# For multi-batch data, include batch in model",
      "scvi.model.VELOVI.setup_anndata(",
      "    adata,",
      '    spliced_layer="spliced",',
      '    unspliced_layer="unspliced",',
      '    batch_key="batch"',
      ")",
      "",
      "model = scvi.model.VELOVI(adata)",
      "model.train()",
      "```",
      "",
      "## Interpreting Results",
      "",
      "### Good Velocity Signal",
      "",
      "- Streamlines follow expected differentiation",
      "- Latent time correlates with known biology",
      "- Phase portraits show clear dynamics",
      "",
      "### Poor Velocity Signal",
      "",
      "- Random/chaotic streamlines",
      "- No correlation with known markers",
      "- May indicate:",
      "  - Insufficient unspliced reads",
      "  - Cells at steady state",
      "  - Technical issues",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| No velocity signal | Low unspliced counts | Check sequencing depth, use kb-python |",
      "| Reversed direction | Wrong root assignment | Manually set root cells |",
      "| Noisy streamlines | Too many genes | Reduce n_top_genes |",
      "| Memory error | Large dataset | Reduce batch_size |",
      "",
      "## Key References",
      "",
      '- Gayoso et al. (2023) "Deep generative modeling of transcriptional dynamics for RNA velocity analysis in single cells"',
      '- La Manno et al. (2018) "RNA velocity of single cells"',
      '- Bergen et al. (2020) "Generalizing RNA velocity to transient cell states through dynamical modeling"',
      "",
    ].join("\n"),
    "references/scarches_mapping.md": [
      "# Reference Mapping with scArches",
      "",
      "This reference covers using scArches for mapping query data to pre-trained reference models without retraining from scratch.",
      "",
      "## Overview",
      "",
      "scArches (single-cell architecture surgery) enables:",
      "- Mapping new data to existing reference atlases",
      "- Extending models with new batches/studies",
      "- Transfer learning without full retraining",
      "- Preserving reference structure while integrating query",
      "",
      "## When to Use scArches",
      "",
      "| Scenario | Approach |",
      "|----------|----------|",
      "| Map query to existing atlas | scArches query mapping |",
      "| Extend atlas with new data | scArches model surgery |",
      "| No pre-trained model available | Train scANVI from scratch |",
      "| Query very different from reference | Consider retraining |",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import numpy as np",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      "```",
      "",
      "## Workflow 1: Map Query to Pre-Trained Reference",
      "",
      "### Step 1: Load Pre-Trained Reference Model",
      "",
      "```python",
      "# Load saved reference model",
      "# The model must have been trained with scvi-tools",
      'reference_model = scvi.model.SCVI.load("reference_model/")',
      "",
      "# Or load scANVI for label transfer",
      'reference_model = scvi.model.SCANVI.load("reference_scanvi_model/")',
      "",
      "# Check model info",
      'print(f"Model type: {type(reference_model)}")',
      'print(f"Training data shape: {reference_model.adata.shape}")',
      "```",
      "",
      "### Step 2: Prepare Query Data",
      "",
      "```python",
      "# Load query data",
      'adata_query = sc.read_h5ad("query_data.h5ad")',
      "",
      "# CRITICAL: Match genes to reference",
      "reference_genes = reference_model.adata.var_names",
      "query_genes = adata_query.var_names",
      "",
      "# Check overlap",
      "common_genes = reference_genes.intersection(query_genes)",
      'print(f"Reference genes: {len(reference_genes)}")',
      'print(f"Query genes: {len(query_genes)}")',
      'print(f"Overlap: {len(common_genes)}")',
      "",
      "# Subset query to reference genes",
      "adata_query = adata_query[:, reference_genes].copy()",
      "",
      "# Handle missing genes (filled with zeros automatically by prepare_query_anndata)",
      "```",
      "",
      "### Step 3: Prepare Query AnnData",
      "",
      "```python",
      "# Store raw counts",
      'adata_query.layers["counts"] = adata_query.X.copy()',
      "",
      "# Prepare query for mapping",
      "# This aligns the query data structure to match the reference",
      "scvi.model.SCVI.prepare_query_anndata(adata_query, reference_model)",
      "```",
      "",
      "### Step 4: Create Query Model",
      "",
      "```python",
      "# Create query model from reference",
      "# This initializes with reference weights",
      "query_model = scvi.model.SCVI.load_query_data(",
      "    adata_query,",
      "    reference_model",
      ")",
      "",
      "# The query model inherits:",
      "# - Reference architecture",
      "# - Reference encoder weights (frozen by default)",
      "# - Decoder is fine-tuned for query",
      "```",
      "",
      "### Step 5: Fine-Tune on Query",
      "",
      "```python",
      "# Fine-tune the query model",
      "# This adjusts decoder weights for query-specific effects",
      "query_model.train(",
      "    max_epochs=200,",
      "    plan_kwargs={",
      '        "weight_decay": 0.0  # Less regularization for fine-tuning',
      "    }",
      ")",
      "",
      "# Check training",
      "query_model.history['elbo_train'].plot()",
      "```",
      "",
      "### Step 6: Get Query Representation",
      "",
      "```python",
      "# Get latent representation",
      "# Query cells are embedded in same space as reference",
      'adata_query.obsm["X_scVI"] = query_model.get_latent_representation()',
      "",
      "# Visualize",
      'sc.pp.neighbors(adata_query, use_rep="X_scVI")',
      "sc.tl.umap(adata_query)",
      "sc.pl.umap(adata_query, color=['cell_type', 'batch'])",
      "```",
      "",
      "## Workflow 2: scANVI Query Mapping with Label Transfer",
      "",
      "For transferring cell type labels from reference to query:",
      "",
      "### Step 1: Load scANVI Reference",
      "",
      "```python",
      "# Reference must be scANVI model (trained with labels)",
      'reference_scanvi = scvi.model.SCANVI.load("scanvi_reference/")',
      "",
      "# Check available labels",
      'print("Reference cell types:")',
      "print(reference_scanvi.adata.obs['cell_type'].value_counts())",
      "```",
      "",
      "### Step 2: Prepare and Map Query",
      "",
      "```python",
      "# Prepare query",
      'adata_query.layers["counts"] = adata_query.X.copy()',
      "adata_query = adata_query[:, reference_scanvi.adata.var_names].copy()",
      "",
      "scvi.model.SCANVI.prepare_query_anndata(adata_query, reference_scanvi)",
      "",
      "# Create query model",
      "query_scanvi = scvi.model.SCANVI.load_query_data(",
      "    adata_query,",
      "    reference_scanvi",
      ")",
      "",
      "# Fine-tune",
      "query_scanvi.train(",
      "    max_epochs=100,",
      '    plan_kwargs={"weight_decay": 0.0}',
      ")",
      "```",
      "",
      "### Step 3: Get Predictions",
      "",
      "```python",
      "# Predict cell types",
      "predictions = query_scanvi.predict()",
      'adata_query.obs["predicted_cell_type"] = predictions',
      "",
      "# Get prediction probabilities",
      "soft_predictions = query_scanvi.predict(soft=True)",
      'adata_query.obs["prediction_confidence"] = soft_predictions.max(axis=1)',
      "",
      "# Latent representation",
      'adata_query.obsm["X_scANVI"] = query_scanvi.get_latent_representation()',
      "",
      "# Visualize predictions",
      'sc.pp.neighbors(adata_query, use_rep="X_scANVI")',
      "sc.tl.umap(adata_query)",
      "sc.pl.umap(adata_query, color=['predicted_cell_type', 'prediction_confidence'])",
      "```",
      "",
      "### Step 4: Evaluate Predictions",
      "",
      "```python",
      "# Distribution of predictions",
      "print(adata_query.obs['predicted_cell_type'].value_counts())",
      "",
      "# Confidence statistics",
      "print(f\"Mean confidence: {adata_query.obs['prediction_confidence'].mean():.3f}\")",
      "print(f\"Low confidence (<0.5): {(adata_query.obs['prediction_confidence'] < 0.5).sum()}\")",
      "",
      "# Filter low-confidence predictions",
      "high_conf = adata_query[adata_query.obs['prediction_confidence'] >= 0.7].copy()",
      'print(f"High confidence cells: {len(high_conf)} ({len(high_conf)/len(adata_query)*100:.1f}%)")',
      "```",
      "",
      "## Workflow 3: Model Surgery (Extending Reference)",
      "",
      "Extend an existing reference model with new data:",
      "",
      "### Step 1: Freeze Reference Layers",
      "",
      "```python",
      "# Load reference model",
      'reference_model = scvi.model.SCVI.load("reference_model/")',
      "",
      "# Get reference representation (before surgery)",
      "adata_ref = reference_model.adata",
      'adata_ref.obsm["X_scVI_before"] = reference_model.get_latent_representation()',
      "```",
      "",
      "### Step 2: Prepare Combined Data",
      "",
      "```python",
      "# Add batch information",
      'adata_ref.obs["dataset"] = "reference"',
      'adata_query.obs["dataset"] = "query"',
      "",
      "# Combine",
      "adata_combined = sc.concat([adata_ref, adata_query])",
      'adata_combined.layers["counts"] = adata_combined.X.copy()',
      "```",
      "",
      "### Step 3: Surgery Approach",
      "",
      "```python",
      "# Option A: Use load_query_data (recommended)",
      "scvi.model.SCVI.prepare_query_anndata(adata_query, reference_model)",
      "extended_model = scvi.model.SCVI.load_query_data(adata_query, reference_model)",
      "extended_model.train(max_epochs=200)",
      "",
      "# Option B: Retrain with combined data (if query is large)",
      "# This doesn't preserve reference exactly but may give better results",
      "scvi.model.SCVI.setup_anndata(",
      "    adata_combined,",
      '    layer="counts",',
      '    batch_key="dataset"',
      ")",
      "new_model = scvi.model.SCVI(adata_combined, n_latent=30)",
      "new_model.train(max_epochs=200)",
      "```",
      "",
      "## Joint Visualization",
      "",
      "Visualize reference and query together:",
      "",
      "```python",
      "# Get latent representations",
      'adata_ref.obsm["X_scVI"] = reference_model.get_latent_representation()',
      'adata_query.obsm["X_scVI"] = query_model.get_latent_representation()',
      "",
      "# Combine for visualization",
      'adata_ref.obs["source"] = "reference"',
      'adata_query.obs["source"] = "query"',
      "adata_combined = sc.concat([adata_ref, adata_query])",
      "",
      "# Compute joint UMAP",
      'sc.pp.neighbors(adata_combined, use_rep="X_scVI")',
      "sc.tl.umap(adata_combined)",
      "",
      "# Visualize",
      "import matplotlib.pyplot as plt",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))",
      "",
      'sc.pl.umap(adata_combined, color="source", ax=axes[0], show=False, title="Source")',
      'sc.pl.umap(adata_combined, color="cell_type", ax=axes[1], show=False, title="Cell Type")',
      'sc.pl.umap(adata_combined, color="batch", ax=axes[2], show=False, title="Batch")',
      "",
      "plt.tight_layout()",
      "```",
      "",
      "## Using Public Atlas Models",
      "",
      "### From HuggingFace Model Hub",
      "",
      "```python",
      "from huggingface_hub import hf_hub_download",
      "",
      "# Download model files",
      "model_dir = hf_hub_download(",
      '    repo_id="scvi-tools/model-name",  # Replace with actual repo',
      '    filename="model.pt",',
      '    local_dir="./downloaded_model/"',
      ")",
      "",
      "# Load model",
      "atlas_model = scvi.model.SCANVI.load(model_dir)",
      "```",
      "",
      "### From CellxGene",
      "",
      "```python",
      "# Many CellxGene datasets provide pre-trained models",
      "# Check dataset documentation for model availability",
      "# https://cellxgene.cziscience.com/",
      "",
      "# Example workflow:",
      "# 1. Download reference dataset and model",
      '# 2. Load model: model = scvi.model.SCANVI.load("cellxgene_model/")',
      "# 3. Map your query data using steps above",
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def map_query_to_reference(",
      "    adata_query,",
      "    reference_model_path,",
      '    model_type="scanvi",',
      "    max_epochs=100,",
      "    confidence_threshold=0.5",
      "):",
      '    """',
      "    Map query data to pre-trained reference model.",
      "",
      "    Parameters",
      "    ----------",
      "    adata_query : AnnData",
      "        Query data with raw counts",
      "    reference_model_path : str",
      "        Path to saved reference model",
      "    model_type : str",
      '        "scvi" or "scanvi"',
      "    max_epochs : int",
      "        Fine-tuning epochs",
      "    confidence_threshold : float",
      "        Minimum prediction confidence (for scANVI)",
      "",
      "    Returns",
      "    -------",
      "    Mapped AnnData with predictions (if scANVI)",
      '    """',
      "    import scvi",
      "",
      "    # Load reference",
      '    if model_type == "scanvi":',
      "        reference_model = scvi.model.SCANVI.load(reference_model_path)",
      "        ModelClass = scvi.model.SCANVI",
      "    else:",
      "        reference_model = scvi.model.SCVI.load(reference_model_path)",
      "        ModelClass = scvi.model.SCVI",
      "",
      "    # Prepare query",
      "    adata_query = adata_query.copy()",
      "    adata_query = adata_query[:, reference_model.adata.var_names].copy()",
      '    adata_query.layers["counts"] = adata_query.X.copy()',
      "",
      "    # Map query",
      "    ModelClass.prepare_query_anndata(adata_query, reference_model)",
      "    query_model = ModelClass.load_query_data(adata_query, reference_model)",
      "",
      "    # Fine-tune",
      "    query_model.train(",
      "        max_epochs=max_epochs,",
      '        plan_kwargs={"weight_decay": 0.0}',
      "    )",
      "",
      "    # Get results",
      '    rep_key = "X_scANVI" if model_type == "scanvi" else "X_scVI"',
      "    adata_query.obsm[rep_key] = query_model.get_latent_representation()",
      "",
      '    if model_type == "scanvi":',
      '        adata_query.obs["predicted_cell_type"] = query_model.predict()',
      "        soft = query_model.predict(soft=True)",
      '        adata_query.obs["prediction_confidence"] = soft.max(axis=1)',
      '        adata_query.obs["confident"] = adata_query.obs["prediction_confidence"] >= confidence_threshold',
      "",
      "    # Compute UMAP",
      "    sc.pp.neighbors(adata_query, use_rep=rep_key)",
      "    sc.tl.umap(adata_query)",
      "",
      "    return adata_query, query_model",
      "",
      "",
      "# Usage",
      "adata_mapped, model = map_query_to_reference(",
      "    adata_query,",
      '    "reference_scanvi_model/",',
      '    model_type="scanvi"',
      ")",
      "",
      "# Visualize",
      "sc.pl.umap(adata_mapped, color=['predicted_cell_type', 'prediction_confidence'])",
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Gene mismatch | Different gene naming | Convert gene IDs (Ensembl ↔ Symbol) |",
      "| Many low-confidence | Query has novel types | Manually annotate low-confidence cells |",
      "| Poor mapping | Query too different | Consider retraining with combined data |",
      "| Memory error | Large query | Process in batches |",
      "| Version mismatch | Different scvi-tools version | Use same version as reference training |",
      "",
      "## Key References",
      "",
      '- Lotfollahi et al. (2022) "Mapping single-cell data to reference atlases by transfer learning"',
      '- Xu et al. (2021) "Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models"',
      "",
    ].join("\n"),
    "references/scrna_integration.md": [
      "# scRNA-seq Integration with scVI and scANVI",
      "",
      "This reference covers batch correction and dataset integration using scVI (unsupervised) and scANVI (semi-supervised with cell type labels).",
      "",
      "## Overview",
      "",
      "Single-cell datasets often have batch effects from:",
      "- Different donors/patients",
      "- Different experimental batches",
      "- Different technologies (10x v2 vs v3)",
      "- Different studies",
      "",
      "scVI and scANVI learn a shared latent space where batch effects are removed while biological variation is preserved.",
      "",
      "## When to Use Which Model",
      "",
      "| Model | Use When | Labels Needed |",
      "|-------|----------|---------------|",
      "| **scVI** | No labels available, exploratory analysis | No |",
      "| **scANVI** | Have partial/full labels, want better preservation | Yes (partial OK) |",
      "",
      "## scVI Integration Workflow",
      "",
      "### Step 1: Prepare Data",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "",
      "# Load datasets",
      'adata1 = sc.read_h5ad("dataset1.h5ad")',
      'adata2 = sc.read_h5ad("dataset2.h5ad")',
      "",
      "# Add batch annotation",
      'adata1.obs["batch"] = "batch1"',
      'adata2.obs["batch"] = "batch2"',
      "",
      "# Concatenate",
      'adata = sc.concat([adata1, adata2], label="batch")',
      "",
      "# Ensure we have raw counts",
      "# If data is normalized, recover from .raw",
      "if hasattr(adata, 'raw') and adata.raw is not None:",
      "    adata = adata.raw.to_adata()",
      "",
      "# Store counts",
      'adata.layers["counts"] = adata.X.copy()',
      "```",
      "",
      "### Step 2: HVG Selection Across Batches",
      "",
      "```python",
      "# Select HVGs considering batch",
      "sc.pp.highly_variable_genes(",
      "    adata,",
      "    n_top_genes=2000,",
      '    flavor="seurat_v3",',
      '    batch_key="batch",',
      '    layer="counts"',
      ")",
      "",
      "# Subset to HVGs",
      'adata = adata[:, adata.var["highly_variable"]].copy()',
      "```",
      "",
      "### Step 3: Setup and Train scVI",
      "",
      "```python",
      "# Register data with scVI",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch"',
      ")",
      "",
      "# Create model",
      "model = scvi.model.SCVI(",
      "    adata,",
      "    n_latent=30,          # Latent dimensions",
      "    n_layers=2,           # Encoder/decoder depth",
      '    gene_likelihood="nb"  # negative binomial (or "zinb")',
      ")",
      "",
      "# Train",
      "model.train(",
      "    max_epochs=200,",
      "    early_stopping=True,",
      "    early_stopping_patience=10,",
      "    batch_size=128",
      ")",
      "",
      "# Plot training history",
      'model.history["elbo_train"].plot()',
      "```",
      "",
      "### Step 4: Get Integrated Representation",
      "",
      "```python",
      "# Get latent representation",
      'adata.obsm["X_scVI"] = model.get_latent_representation()',
      "",
      "# Use for clustering and visualization",
      'sc.pp.neighbors(adata, use_rep="X_scVI", n_neighbors=15)',
      "sc.tl.umap(adata)",
      "sc.tl.leiden(adata, resolution=1.0)",
      "",
      "# Visualize integration",
      'sc.pl.umap(adata, color=["batch", "leiden"], ncols=2)',
      "```",
      "",
      "### Step 5: Save Model",
      "",
      "```python",
      "# Save model for later use",
      'model.save("scvi_model/")',
      "",
      "# Load model",
      'model = scvi.model.SCVI.load("scvi_model/", adata=adata)',
      "```",
      "",
      "## scANVI Integration Workflow",
      "",
      "scANVI extends scVI with cell type labels for better biological preservation.",
      "",
      "### Step 1: Prepare Data with Labels",
      "",
      "```python",
      "# Labels should be in adata.obs",
      '# Use "Unknown" for unlabeled cells',
      'print(adata.obs["cell_type"].value_counts())',
      "",
      "# For partially labeled data",
      "# Mark unlabeled cells",
      'adata.obs["cell_type_scanvi"] = adata.obs["cell_type"].copy()',
      '# adata.obs.loc[unlabeled_mask, "cell_type_scanvi"] = "Unknown"',
      "```",
      "",
      "### Step 2: Option A - Train scANVI from Scratch",
      "",
      "```python",
      "# Setup for scANVI",
      "scvi.model.SCANVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch",',
      '    labels_key="cell_type"',
      ")",
      "",
      "# Create model",
      "scanvi_model = scvi.model.SCANVI(",
      "    adata,",
      "    n_latent=30,",
      "    n_layers=2",
      ")",
      "",
      "# Train",
      "scanvi_model.train(max_epochs=200)",
      "```",
      "",
      "### Step 2: Option B - Initialize scANVI from scVI (Recommended)",
      "",
      "```python",
      "# First train scVI",
      'scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "scvi_model = scvi.model.SCVI(adata, n_latent=30)",
      "scvi_model.train(max_epochs=200)",
      "",
      "# Initialize scANVI from scVI",
      "scanvi_model = scvi.model.SCANVI.from_scvi_model(",
      "    scvi_model,",
      '    labels_key="cell_type",',
      '    unlabeled_category="Unknown"  # For partially labeled data',
      ")",
      "",
      "# Fine-tune scANVI (fewer epochs needed)",
      "scanvi_model.train(max_epochs=50)",
      "```",
      "",
      "### Step 3: Get Results",
      "",
      "```python",
      "# Latent representation",
      'adata.obsm["X_scANVI"] = scanvi_model.get_latent_representation()',
      "",
      "# Predicted labels for unlabeled cells",
      "predictions = scanvi_model.predict()",
      'adata.obs["predicted_cell_type"] = predictions',
      "",
      "# Prediction probabilities",
      "soft_predictions = scanvi_model.predict(soft=True)",
      "",
      "# Visualization",
      'sc.pp.neighbors(adata, use_rep="X_scANVI")',
      "sc.tl.umap(adata)",
      'sc.pl.umap(adata, color=["batch", "cell_type", "predicted_cell_type"])',
      "```",
      "",
      "## Comparing Integration Quality",
      "",
      "### Visual Assessment",
      "",
      "```python",
      "import matplotlib.pyplot as plt",
      "",
      "fig, axes = plt.subplots(1, 3, figsize=(15, 4))",
      "",
      "# Before integration (on PCA)",
      "sc.pp.pca(adata)",
      'sc.pl.pca(adata, color="batch", ax=axes[0], title="Before (PCA)", show=False)',
      "",
      "# After scVI",
      'sc.pp.neighbors(adata, use_rep="X_scVI")',
      "sc.tl.umap(adata)",
      'sc.pl.umap(adata, color="batch", ax=axes[1], title="After scVI", show=False)',
      "",
      "# After scANVI",
      'sc.pp.neighbors(adata, use_rep="X_scANVI")',
      "sc.tl.umap(adata)",
      'sc.pl.umap(adata, color="batch", ax=axes[2], title="After scANVI", show=False)',
      "",
      "plt.tight_layout()",
      "```",
      "",
      "### Quantitative Metrics (scib)",
      "",
      "```python",
      "# pip install scib-metrics",
      "",
      "from scib_metrics.benchmark import Benchmarker",
      "",
      "bm = Benchmarker(",
      "    adata,",
      '    batch_key="batch",',
      '    label_key="cell_type",',
      '    embedding_obsm_keys=["X_pca", "X_scVI", "X_scANVI"]',
      ")",
      "",
      "bm.benchmark()",
      "bm.plot_results_table()",
      "```",
      "",
      "## Differential Expression",
      "",
      "scVI provides differential expression that accounts for batch effects:",
      "",
      "```python",
      "# DE between groups",
      "de_results = model.differential_expression(",
      '    groupby="cell_type",',
      '    group1="T cells",',
      '    group2="B cells"',
      ")",
      "",
      "# Filter significant",
      "de_sig = de_results[",
      '    (de_results["is_de_fdr_0.05"] == True) &',
      '    (abs(de_results["lfc_mean"]) > 1)',
      "]",
      "",
      "print(de_sig.head(20))",
      "```",
      "",
      "## Advanced: Multiple Categorical Covariates",
      "",
      "```python",
      "# Include additional covariates beyond batch",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    batch_key="batch",',
      '    categorical_covariate_keys=["donor", "technology"]',
      ")",
      "",
      "model = scvi.model.SCVI(adata, n_latent=30)",
      "model.train()",
      "```",
      "",
      "## Training Tips",
      "",
      "### For Large Datasets (>100k cells)",
      "",
      "```python",
      "model.train(",
      "    max_epochs=100,      # Fewer epochs needed",
      "    batch_size=256,      # Larger batches",
      "    train_size=0.9,      # Less validation",
      "    early_stopping=True",
      ")",
      "```",
      "",
      "### For Small Datasets (<10k cells)",
      "",
      "```python",
      "model = scvi.model.SCVI(",
      "    adata,",
      "    n_latent=10,         # Smaller latent space",
      "    n_layers=1,          # Simpler model",
      "    dropout_rate=0.2     # More regularization",
      ")",
      "",
      "model.train(",
      "    max_epochs=400,",
      "    batch_size=64",
      ")",
      "```",
      "",
      "### Monitoring Training",
      "",
      "```python",
      "# Check training curves",
      "import matplotlib.pyplot as plt",
      "",
      "fig, ax = plt.subplots()",
      'ax.plot(model.history["elbo_train"], label="Train")',
      'ax.plot(model.history["elbo_validation"], label="Validation")',
      'ax.set_xlabel("Epoch")',
      'ax.set_ylabel("ELBO")',
      "ax.legend()",
      "",
      "# Should see convergence without overfitting",
      "```",
      "",
      "## Complete Pipeline",
      "",
      "```python",
      "def integrate_datasets(",
      "    adatas,",
      '    batch_key="batch",',
      "    labels_key=None,",
      "    n_top_genes=2000,",
      "    n_latent=30",
      "):",
      '    """',
      "    Integrate multiple scRNA-seq datasets.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adatas : dict",
      "        Dictionary of {batch_name: AnnData}",
      "    batch_key : str",
      "        Key for batch annotation",
      "    labels_key : str, optional",
      "        Key for cell type labels (uses scANVI if provided)",
      "    n_top_genes : int",
      "        Number of HVGs",
      "    n_latent : int",
      "        Latent dimensions",
      "        ",
      "    Returns",
      "    -------",
      "    AnnData with integrated representation",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    ",
      "    # Add batch labels and concatenate",
      "    for batch_name, adata in adatas.items():",
      "        adata.obs[batch_key] = batch_name",
      "    ",
      "    adata = sc.concat(list(adatas.values()), label=batch_key)",
      "    ",
      "    # Store counts",
      '    adata.layers["counts"] = adata.X.copy()',
      "    ",
      "    # HVG selection",
      "    sc.pp.highly_variable_genes(",
      "        adata,",
      "        n_top_genes=n_top_genes,",
      '        flavor="seurat_v3",',
      "        batch_key=batch_key,",
      '        layer="counts"',
      "    )",
      '    adata = adata[:, adata.var["highly_variable"]].copy()',
      "    ",
      "    # Train model",
      "    if labels_key and labels_key in adata.obs.columns:",
      "        # Use scANVI",
      '        scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key=batch_key)',
      "        scvi_model = scvi.model.SCVI(adata, n_latent=n_latent)",
      "        scvi_model.train(max_epochs=200)",
      "        ",
      "        model = scvi.model.SCANVI.from_scvi_model(",
      "            scvi_model,",
      "            labels_key=labels_key,",
      '            unlabeled_category="Unknown"',
      "        )",
      "        model.train(max_epochs=50)",
      '        rep_key = "X_scANVI"',
      "    else:",
      "        # Use scVI",
      '        scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key=batch_key)',
      "        model = scvi.model.SCVI(adata, n_latent=n_latent)",
      "        model.train(max_epochs=200)",
      '        rep_key = "X_scVI"',
      "    ",
      "    # Add representation",
      "    adata.obsm[rep_key] = model.get_latent_representation()",
      "    ",
      "    # Compute neighbors and UMAP",
      "    sc.pp.neighbors(adata, use_rep=rep_key)",
      "    sc.tl.umap(adata)",
      "    sc.tl.leiden(adata)",
      "    ",
      "    return adata, model",
      "",
      "# Usage",
      "adatas = {",
      '    "study1": sc.read_h5ad("study1.h5ad"),',
      '    "study2": sc.read_h5ad("study2.h5ad"),',
      '    "study3": sc.read_h5ad("study3.h5ad")',
      "}",
      "",
      "adata_integrated, model = integrate_datasets(",
      "    adatas,",
      '    labels_key="cell_type"',
      ")",
      "",
      'sc.pl.umap(adata_integrated, color=["batch", "leiden", "cell_type"])',
      "```",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Batches not mixing | Too few shared genes | Use more HVGs, check gene overlap |",
      "| Over-correction | Biological variation removed | Use scANVI with labels |",
      "| Training diverges | Learning rate too high | Reduce lr, increase batch_size |",
      "| NaN loss | Bad data | Check for all-zero cells/genes |",
      "| Memory error | Too many cells | Reduce batch_size, use GPU |",
      "",
    ].join("\n"),
    "references/spatial_deconvolution.md": [
      "# Spatial Transcriptomics Analysis",
      "",
      "This reference covers spatial transcriptomics analysis using scvi-tools methods: DestVI for deconvolution and resolVI for building spatial models.",
      "",
      "## Overview",
      "",
      "Spatial transcriptomics technologies like Visium capture gene expression at defined spatial locations, but many platforms have multi-cellular resolution. scvi-tools provides two main approaches:",
      "",
      "- **DestVI**: Deconvolution - estimates cell type proportions at each spot using a single-cell reference",
      "- **resolVI**: Builds a spatial model that learns gene expression patterns accounting for spatial context",
      "",
      "## Available Methods in scvi-tools",
      "",
      "| Method | Description | Use Case |",
      "|--------|-------------|----------|",
      "| **DestVI** | Variational inference for deconvolution | Estimate cell type proportions per spot |",
      "| **resolVI** | Spatial gene expression model | Learn spatially-aware representations |",
      "| **CondSCVI** | Reference model for DestVI | Required for DestVI workflow |",
      "",
      "## Prerequisites",
      "",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import squidpy as sq",
      "import numpy as np",
      "",
      'print(f"scvi-tools version: {scvi.__version__}")',
      "```",
      "",
      "---",
      "",
      "## Part 1: DestVI Deconvolution",
      "",
      "### Step 1: Load Spatial Data",
      "",
      "```python",
      "# Load Visium data",
      'adata_spatial = sc.read_visium("spaceranger_output/")',
      "",
      "# Check structure",
      'print(f"Spots: {adata_spatial.n_obs}")',
      'print(f"Genes: {adata_spatial.n_vars}")',
      "print(f\"Spatial coordinates: {adata_spatial.obsm['spatial'].shape}\")",
      "",
      "# Basic QC",
      "sc.pp.calculate_qc_metrics(adata_spatial, inplace=True)",
      "adata_spatial = adata_spatial[adata_spatial.obs['n_genes_by_counts'] > 200].copy()",
      "",
      "# Store counts",
      'adata_spatial.layers["counts"] = adata_spatial.X.copy()',
      "```",
      "",
      "### Step 2: Load Single-Cell Reference",
      "",
      "```python",
      "# Load reference single-cell data",
      'adata_sc = sc.read_h5ad("reference_scrna.h5ad")',
      "",
      "# Requirements:",
      "# - Raw counts",
      "# - Cell type annotations",
      'print(f"Reference cells: {adata_sc.n_obs}")',
      "print(f\"Cell types: {adata_sc.obs['cell_type'].nunique()}\")",
      "print(adata_sc.obs['cell_type'].value_counts())",
      "",
      "# Store counts",
      'adata_sc.layers["counts"] = adata_sc.X.copy()',
      "```",
      "",
      "### Step 3: Prepare Data",
      "",
      "```python",
      "# DestVI requires gene overlap between reference and spatial",
      "common_genes = adata_sc.var_names.intersection(adata_spatial.var_names)",
      'print(f"Common genes: {len(common_genes)}")',
      "",
      "adata_sc = adata_sc[:, common_genes].copy()",
      "adata_spatial = adata_spatial[:, common_genes].copy()",
      "```",
      "",
      "### Step 4: Train Reference Model (CondSCVI)",
      "",
      "```python",
      "# Train conditional scVI on reference data",
      "scvi.model.CondSCVI.setup_anndata(",
      "    adata_sc,",
      '    layer="counts",',
      '    labels_key="cell_type"',
      ")",
      "",
      "sc_model = scvi.model.CondSCVI(",
      "    adata_sc,",
      "    n_latent=20",
      ")",
      "",
      "sc_model.train(max_epochs=200)",
      "sc_model.history['elbo_train'].plot()",
      "```",
      "",
      "### Step 5: Train DestVI",
      "",
      "```python",
      "# Setup spatial data",
      "scvi.model.DestVI.setup_anndata(",
      "    adata_spatial,",
      '    layer="counts"',
      ")",
      "",
      "# Train DestVI using reference model",
      "spatial_model = scvi.model.DestVI.from_rna_model(",
      "    adata_spatial,",
      "    sc_model",
      ")",
      "",
      "spatial_model.train(max_epochs=500)",
      "```",
      "",
      "### Step 6: Get Cell Type Proportions",
      "",
      "```python",
      "# Infer cell type proportions at each spot",
      "proportions = spatial_model.get_proportions()",
      "",
      "# Add to adata",
      "for ct in adata_sc.obs['cell_type'].unique():",
      "    adata_spatial.obs[f'prop_{ct}'] = proportions[ct]",
      "",
      "# Visualize",
      "sq.pl.spatial_scatter(",
      "    adata_spatial,",
      "    color=[f'prop_{ct}' for ct in adata_sc.obs['cell_type'].unique()[:6]],",
      "    ncols=3",
      ")",
      "```",
      "",
      "---",
      "",
      "## Part 2: resolVI Spatial Model",
      "",
      "resolVI is a semi-supervised method that learns cell type assignments and spatially-aware representations directly from spatial data, optionally using initial cell type predictions.",
      "",
      "**Note**: resolVI is in `scvi.external` (not `scvi.model`).",
      "",
      "### Step 1: Prepare Spatial Data",
      "",
      "```python",
      "# Load and preprocess",
      'adata = sc.read_visium("spaceranger_output/")',
      "",
      "# QC",
      "sc.pp.calculate_qc_metrics(adata, inplace=True)",
      "adata = adata[adata.obs['n_genes_by_counts'] > 200].copy()",
      "",
      "# Store counts",
      'adata.layers["counts"] = adata.X.copy()',
      "",
      "# HVG selection",
      "sc.pp.highly_variable_genes(",
      "    adata,",
      "    n_top_genes=4000,",
      '    flavor="seurat_v3",',
      '    layer="counts"',
      ")",
      "adata = adata[:, adata.var['highly_variable']].copy()",
      "",
      "# Optional: Get initial cell type predictions (e.g., from a reference)",
      '# adata.obs["predicted_celltype"] = ...',
      "```",
      "",
      "### Step 2: Setup and Train resolVI",
      "",
      "```python",
      "# Setup for resolVI (note: scvi.external, not scvi.model)",
      "scvi.external.RESOLVI.setup_anndata(",
      "    adata,",
      '    labels_key="predicted_celltype",  # Initial cell type predictions',
      '    layer="counts"',
      ")",
      "",
      "# Create model (semisupervised=True uses the labels)",
      "model = scvi.external.RESOLVI(adata, semisupervised=True)",
      "",
      "# Train",
      "model.train(max_epochs=50)",
      "```",
      "",
      "### Step 3: Get Cell Type Predictions",
      "",
      "```python",
      "# Get refined cell type predictions",
      "# soft=True returns probabilities, soft=False returns labels",
      "cell_type_probs = model.predict(adata, num_samples=3, soft=True)",
      "cell_type_labels = model.predict(adata, num_samples=3, soft=False)",
      "",
      'adata.obs["resolvi_celltype"] = cell_type_labels',
      "",
      "# Visualize",
      'sq.pl.spatial_scatter(adata, color="resolvi_celltype")',
      "```",
      "",
      "### Step 4: Get Latent Representation",
      "",
      "```python",
      "# Get latent representation",
      'adata.obsm["X_resolVI"] = model.get_latent_representation(adata)',
      "",
      "# Cluster based on spatial representation",
      'sc.pp.neighbors(adata, use_rep="X_resolVI")',
      "sc.tl.umap(adata)",
      "sc.tl.leiden(adata, resolution=0.5)",
      "",
      "# Visualize clusters spatially",
      'sq.pl.spatial_scatter(adata, color="leiden")',
      "```",
      "",
      "### Step 5: Differential Expression",
      "",
      "```python",
      "# DE between cell types using resolVI",
      "de_results = model.differential_expression(",
      "    adata,",
      '    groupby="resolvi_celltype",',
      '    group1="T_cell",',
      '    group2="Tumor"',
      ")",
      "",
      "print(de_results.head(20))",
      "```",
      "",
      "### Step 6: Niche Abundance Analysis",
      "",
      "```python",
      "# Analyze how cell type neighborhoods differ between conditions",
      "# Requires spatial neighbor graph",
      'sq.gr.spatial_neighbors(adata, coord_type="generic")',
      "",
      "niche_results = model.differential_niche_abundance(",
      '    groupby="resolvi_celltype",',
      '    group1="T_cell",',
      '    group2="Tumor",',
      '    neighbor_key="spatial_neighbors"',
      ")",
      "```",
      "",
      "### Step 7: Query Mapping (Transfer to New Data)",
      "",
      "```python",
      "# Map new spatial data to trained model",
      'query_adata = sc.read_visium("new_sample/")',
      'query_adata.layers["counts"] = query_adata.X.copy()',
      "",
      "# Prepare and load query",
      "model.prepare_query_anndata(query_adata, reference_model=model)",
      "query_model = model.load_query_data(query_adata, reference_model=model)",
      "",
      "# Fine-tune on query",
      "query_model.train(max_epochs=20)",
      "",
      "# Get predictions for query",
      "query_labels = query_model.predict(query_adata, num_samples=3, soft=False)",
      "```",
      "",
      "---",
      "",
      "## Visualization",
      "",
      "### Spatial Proportions",
      "",
      "```python",
      "import matplotlib.pyplot as plt",
      "",
      "# Plot multiple cell type proportions",
      "cell_types = ['T_cell', 'Tumor', 'Fibroblast', 'Macrophage']",
      "fig, axes = plt.subplots(2, 2, figsize=(12, 12))",
      "",
      "for ax, ct in zip(axes.flat, cell_types):",
      "    sq.pl.spatial_scatter(",
      "        adata_spatial,",
      "        color=f'prop_{ct}',",
      "        ax=ax,",
      "        title=ct,",
      "        show=False",
      "    )",
      "",
      "plt.tight_layout()",
      "```",
      "",
      "### Enrichment by Region",
      "",
      "```python",
      "# Cluster spatial data",
      "sc.pp.neighbors(adata_spatial)",
      "sc.tl.leiden(adata_spatial, resolution=0.5)",
      "",
      "# Compare proportions across regions",
      "import pandas as pd",
      "",
      "cell_types = adata_sc.obs['cell_type'].unique()",
      "prop_cols = [f'prop_{ct}' for ct in cell_types]",
      "region_props = adata_spatial.obs.groupby('leiden')[prop_cols].mean()",
      "print(region_props)",
      "",
      "# Heatmap",
      "import seaborn as sns",
      "plt.figure(figsize=(10, 6))",
      "sns.heatmap(region_props.T, annot=True, cmap='viridis')",
      "plt.title('Cell Type Proportions by Region')",
      "```",
      "",
      "### Spatial Cell Type Interactions",
      "",
      "```python",
      "# Neighborhood enrichment using cell type assignments",
      "sq.gr.spatial_neighbors(adata_spatial)",
      "",
      '# Create "dominant cell type" annotation',
      "prop_cols = [f'prop_{ct}' for ct in cell_types]",
      "adata_spatial.obs['dominant_type'] = adata_spatial.obs[prop_cols].idxmax(axis=1)",
      "adata_spatial.obs['dominant_type'] = adata_spatial.obs['dominant_type'].str.replace('prop_', '')",
      "",
      "# Co-occurrence analysis",
      "sq.gr.co_occurrence(adata_spatial, cluster_key='dominant_type')",
      "sq.pl.co_occurrence(adata_spatial, cluster_key='dominant_type')",
      "```",
      "",
      "---",
      "",
      "## Complete DestVI Pipeline",
      "",
      "```python",
      "def deconvolve_spatial(",
      "    adata_spatial,",
      "    adata_ref,",
      '    cell_type_key="cell_type",',
      "    n_latent=20,",
      "    max_epochs_ref=200,",
      "    max_epochs_spatial=500",
      "):",
      '    """',
      "    Perform spatial deconvolution using DestVI.",
      "",
      "    Parameters",
      "    ----------",
      "    adata_spatial : AnnData",
      "        Spatial transcriptomics data",
      "    adata_ref : AnnData",
      "        Single-cell reference with cell type annotations",
      "    cell_type_key : str",
      "        Column in adata_ref.obs with cell type labels",
      "    n_latent : int",
      "        Latent dimensions",
      "    max_epochs_ref : int",
      "        Training epochs for reference model",
      "    max_epochs_spatial : int",
      "        Training epochs for spatial model",
      "",
      "    Returns",
      "    -------",
      "    AnnData with cell type proportions in obs",
      '    """',
      "    import scvi",
      "",
      "    # Get common genes",
      "    common_genes = adata_ref.var_names.intersection(adata_spatial.var_names)",
      "    adata_ref = adata_ref[:, common_genes].copy()",
      "    adata_spatial = adata_spatial[:, common_genes].copy()",
      "",
      "    # Ensure counts are stored",
      '    if "counts" not in adata_ref.layers:',
      '        adata_ref.layers["counts"] = adata_ref.X.copy()',
      '    if "counts" not in adata_spatial.layers:',
      '        adata_spatial.layers["counts"] = adata_spatial.X.copy()',
      "",
      "    # Train reference model",
      "    scvi.model.CondSCVI.setup_anndata(",
      "        adata_ref,",
      '        layer="counts",',
      "        labels_key=cell_type_key",
      "    )",
      "",
      "    ref_model = scvi.model.CondSCVI(adata_ref, n_latent=n_latent)",
      "    ref_model.train(max_epochs=max_epochs_ref)",
      "",
      "    # Train spatial model",
      '    scvi.model.DestVI.setup_anndata(adata_spatial, layer="counts")',
      "",
      "    spatial_model = scvi.model.DestVI.from_rna_model(",
      "        adata_spatial,",
      "        ref_model",
      "    )",
      "    spatial_model.train(max_epochs=max_epochs_spatial)",
      "",
      "    # Get proportions",
      "    proportions = spatial_model.get_proportions()",
      "",
      "    cell_types = adata_ref.obs[cell_type_key].unique()",
      "    for ct in cell_types:",
      "        adata_spatial.obs[f'prop_{ct}'] = proportions[ct]",
      "",
      "    # Add dominant type",
      "    prop_cols = [f'prop_{ct}' for ct in cell_types]",
      "    adata_spatial.obs['dominant_type'] = adata_spatial.obs[prop_cols].idxmax(axis=1)",
      "    adata_spatial.obs['dominant_type'] = adata_spatial.obs['dominant_type'].str.replace('prop_', '')",
      "",
      "    return adata_spatial, ref_model, spatial_model",
      "",
      "# Usage",
      "adata_spatial, ref_model, spatial_model = deconvolve_spatial(",
      "    adata_spatial,",
      "    adata_sc,",
      '    cell_type_key="cell_type"',
      ")",
      "",
      "# Visualize",
      "sq.pl.spatial_scatter(",
      "    adata_spatial,",
      "    color=['dominant_type', 'prop_T_cell', 'prop_Tumor'],",
      "    ncols=3",
      ")",
      "```",
      "",
      "---",
      "",
      "## Troubleshooting",
      "",
      "| Issue | Cause | Solution |",
      "|-------|-------|----------|",
      "| Few common genes | Different gene naming | Convert gene names (Ensembl ↔ Symbol) |",
      "| Poor deconvolution | Reference doesn't match | Use tissue-matched reference |",
      "| All spots same type | Over-smoothing | Adjust model parameters, check reference diversity |",
      "| NaN proportions | Missing cell types | Ensure all expected types in reference |",
      "| Training slow | Large spatial dataset | Reduce max_epochs, increase batch_size |",
      "",
      "## Key References",
      "",
      '- Lopez et al. (2022) "DestVI identifies continuums of cell types in spatial transcriptomics data"',
      "- [scvi-tools spatial tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)",
      "",
    ].join("\n"),
    "references/troubleshooting.md": [
      "# Troubleshooting Guide for scvi-tools",
      "",
      "This reference provides a consolidated guide for diagnosing and resolving common issues across all scvi-tools models.",
      "",
      "## Quick Diagnosis",
      "",
      "| Symptom | Likely Cause | Quick Fix |",
      "|---------|--------------|-----------|",
      '| "X should contain integers" | Normalized data in X | Use `layer="counts"` in setup |',
      "| CUDA out of memory | GPU memory exhausted | Reduce `batch_size`, use smaller model |",
      "| Training loss is NaN | Bad data or learning rate | Check for all-zero cells/genes |",
      "| Batches not mixing | Too few shared features | Increase HVGs, check gene overlap |",
      "| Over-correction | Too aggressive integration | Use scANVI with labels |",
      "| Import error | Missing dependencies | `pip install scvi-tools[all]` |",
      "",
      "## Data Format Issues",
      "",
      "### Issue: CITE-seq protein data from Seurat is CLR-normalized",
      "",
      '**Cause**: Seurat\'s `NormalizeData(normalization.method = "CLR")` transforms raw ADT counts. totalVI requires raw integer counts for protein data.',
      "",
      "**Symptoms**:",
      "- Protein values are not integers",
      "- Protein values contain negative numbers",
      "- Model training produces poor results",
      "",
      "**Solution**:",
      "```python",
      "# Check if protein data is normalized",
      'protein = adata.obsm["protein_expression"]',
      'print(f"Min value: {protein.min()}")  # Should be 0 if raw counts',
      'print(f"Contains integers: {np.allclose(protein, protein.astype(int))}")',
      "",
      "# If importing from Seurat, use the raw counts assay, not the normalized one",
      "# In R/Seurat, export the RNA assay's counts slot, not the data slot",
      '# GetAssayData(seurat_obj, assay = "ADT", slot = "counts")',
      "```",
      "",
      '### Issue: "layer not found" or "X should contain integers"',
      "",
      "**Cause**: scvi-tools requires raw integer counts, not normalized data.",
      "",
      "**Solution**:",
      "```python",
      "# Check if X contains integers",
      "import numpy as np",
      'print(f"X max: {adata.X.max()}")',
      'print(f"Contains integers: {np.allclose(adata.X.data, adata.X.data.astype(int))}")',
      "",
      "# If normalized, recover from raw",
      "if hasattr(adata, 'raw') and adata.raw is not None:",
      "    adata = adata.raw.to_adata()",
      "",
      "# Or use existing counts layer",
      'adata.layers["counts"] = adata.X.copy()',
      'scvi.model.SCVI.setup_anndata(adata, layer="counts")',
      "```",
      "",
      "### Issue: Sparse matrix errors",
      "",
      "**Cause**: Incompatible sparse format or dense array expected.",
      "",
      "**Solution**:",
      "```python",
      "from scipy.sparse import csr_matrix",
      "",
      "# Convert to CSR format (most compatible)",
      "if hasattr(adata.X, 'toarray'):",
      "    adata.X = csr_matrix(adata.X)",
      "",
      "# Or convert to dense if small enough",
      "if adata.n_obs * adata.n_vars < 1e8:",
      "    adata.X = adata.X.toarray()",
      "```",
      "",
      "### Issue: NaN or Inf values in data",
      "",
      "**Cause**: Missing values or corrupted data.",
      "",
      "**Solution**:",
      "```python",
      "import numpy as np",
      "",
      "# Check for issues",
      "X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X",
      'print(f"NaN count: {np.isnan(X).sum()}")',
      'print(f"Inf count: {np.isinf(X).sum()}")',
      'print(f"Negative count: {(X < 0).sum()}")',
      "",
      "# Replace NaN/Inf with 0",
      "X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)",
      "X = np.clip(X, 0, None)  # Ensure non-negative",
      "adata.X = csr_matrix(X)",
      "```",
      "",
      "### Issue: batch_key or labels_key not found",
      "",
      "**Cause**: Column name mismatch in adata.obs.",
      "",
      "**Solution**:",
      "```python",
      "# List available columns",
      "print(adata.obs.columns.tolist())",
      "",
      "# Check for similar names",
      "for col in adata.obs.columns:",
      "    if 'batch' in col.lower() or 'sample' in col.lower():",
      '        print(f"Potential batch column: {col}")',
      "```",
      "",
      "## GPU and Memory Issues",
      "",
      "### Issue: CUDA out of memory",
      "",
      "**Cause**: Model or batch doesn't fit in GPU memory.",
      "",
      "**Solutions** (try in order):",
      "",
      "```python",
      "# 1. Reduce batch size",
      "model.train(batch_size=64)  # Default is 128",
      "",
      "# 2. Use smaller model architecture",
      "model = scvi.model.SCVI(",
      "    adata,",
      "    n_latent=10,   # Default is 10-30",
      "    n_layers=1     # Default is 1-2",
      ")",
      "",
      "# 3. Subset to fewer genes",
      "sc.pp.highly_variable_genes(adata, n_top_genes=1500)",
      "adata = adata[:, adata.var['highly_variable']].copy()",
      "",
      "# 4. Clear GPU cache between models",
      "import torch",
      "torch.cuda.empty_cache()",
      "",
      "# 5. Use CPU if GPU is too small",
      'model.train(accelerator="cpu")',
      "```",
      "",
      "### Issue: No GPU detected",
      "",
      "**Cause**: CUDA not installed or version mismatch.",
      "",
      "**Diagnosis**:",
      "```python",
      "import torch",
      'print(f"CUDA available: {torch.cuda.is_available()}")',
      'print(f"PyTorch version: {torch.__version__}")',
      'print(f"CUDA version: {torch.version.cuda}")',
      "```",
      "",
      "**Solution**:",
      "```bash",
      "# Check system CUDA",
      "nvidia-smi",
      "nvcc --version",
      "",
      "# Reinstall PyTorch with matching CUDA",
      "pip install torch --index-url https://download.pytorch.org/whl/cu118  # For CUDA 11.8",
      "# Or",
      "pip install torch --index-url https://download.pytorch.org/whl/cu121  # For CUDA 12.1",
      "```",
      "",
      "### Issue: Memory error with large datasets",
      "",
      "**Cause**: Dataset too large for system RAM.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Process in chunks (for very large data)",
      "# Subsample for initial exploration",
      "adata_sample = adata[np.random.choice(adata.n_obs, 50000, replace=False)].copy()",
      "",
      "# 2. Use backed mode for AnnData",
      "adata = sc.read_h5ad(\"large_data.h5ad\", backed='r')",
      "",
      "# 3. Reduce gene count aggressively",
      "adata = adata[:, adata.var['highly_variable']].copy()",
      "```",
      "",
      "## Training Issues",
      "",
      "### Issue: Training loss is NaN",
      "",
      "**Cause**: Numerical instability, bad data, or learning rate issues.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Check for problematic cells/genes",
      "sc.pp.filter_cells(adata, min_genes=200)",
      "sc.pp.filter_genes(adata, min_cells=3)",
      "",
      "# 2. Remove cells with zero counts",
      "adata = adata[adata.X.sum(axis=1) > 0].copy()",
      "",
      "# 3. Use gradient clipping (built into scvi-tools)",
      "model.train(max_epochs=200, early_stopping=True)",
      "```",
      "",
      "### Issue: Training doesn't converge",
      "",
      "**Cause**: Insufficient epochs, poor hyperparameters, or data issues.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Train longer",
      "model.train(max_epochs=400)",
      "",
      "# 2. Check training curves",
      "import matplotlib.pyplot as plt",
      "plt.plot(model.history['elbo_train'])",
      "plt.plot(model.history['elbo_validation'])",
      "plt.xlabel('Epoch')",
      "plt.ylabel('ELBO')",
      "plt.legend(['Train', 'Validation'])",
      "",
      "# 3. Adjust model size for data size",
      "# Small data (<10k cells): smaller model",
      "model = scvi.model.SCVI(adata, n_latent=10, n_layers=1, dropout_rate=0.2)",
      "",
      "# Large data (>100k cells): can use larger model",
      "model = scvi.model.SCVI(adata, n_latent=30, n_layers=2)",
      "```",
      "",
      "### Issue: Overfitting (validation loss increases)",
      "",
      "**Cause**: Model too complex or trained too long.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Enable early stopping",
      "model.train(early_stopping=True, early_stopping_patience=10)",
      "",
      "# 2. Add regularization",
      "model = scvi.model.SCVI(adata, dropout_rate=0.2)",
      "",
      "# 3. Reduce model complexity",
      "model = scvi.model.SCVI(adata, n_layers=1)",
      "```",
      "",
      "## Integration Issues",
      "",
      "### Issue: Batches don't mix",
      "",
      "**Cause**: Too few shared features, strong biological differences, or technical issues.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Check gene overlap between batches",
      "for batch in adata.obs['batch'].unique():",
      "    batch_genes = adata[adata.obs['batch'] == batch].var_names",
      '    print(f"{batch}: {len(batch_genes)} genes")',
      "",
      "# 2. Use more HVGs",
      'sc.pp.highly_variable_genes(adata, n_top_genes=4000, batch_key="batch")',
      "",
      "# 3. Train longer",
      "model.train(max_epochs=400)",
      "",
      "# 4. Increase latent dimensions",
      "model = scvi.model.SCVI(adata, n_latent=50)",
      "```",
      "",
      "### Issue: Over-correction (biological signal lost)",
      "",
      "**Cause**: Model removes too much variation.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Use scANVI with cell type labels",
      'scvi.model.SCANVI.from_scvi_model(scvi_model, labels_key="cell_type")',
      "",
      "# 2. Reduce model capacity",
      "model = scvi.model.SCVI(adata, n_latent=10)",
      "",
      "# 3. Use categorical covariates instead of batch_key",
      "scvi.model.SCVI.setup_anndata(",
      "    adata,",
      '    layer="counts",',
      '    categorical_covariate_keys=["batch"]  # Less aggressive than batch_key',
      ")",
      "```",
      "",
      "### Issue: One batch dominates clusters",
      "",
      "**Cause**: Unbalanced batch sizes or incomplete integration.",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Check batch distribution",
      "print(adata.obs['batch'].value_counts())",
      "",
      "# 2. Subsample to balance",
      "from sklearn.utils import resample",
      "balanced = []",
      "min_size = adata.obs['batch'].value_counts().min()",
      "for batch in adata.obs['batch'].unique():",
      "    batch_data = adata[adata.obs['batch'] == batch]",
      "    balanced.append(batch_data[np.random.choice(len(batch_data), min_size, replace=False)])",
      "adata_balanced = sc.concat(balanced)",
      "```",
      "",
      "## Model-Specific Issues",
      "",
      "### scANVI: Poor label transfer",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Check label distribution",
      "print(adata.obs['cell_type'].value_counts())",
      "",
      "# 2. Use Unknown for low-confidence cells",
      "adata.obs.loc[adata.obs['prediction_score'] < 0.5, 'cell_type'] = 'Unknown'",
      "",
      "# 3. Train scVI longer before scANVI",
      "scvi_model.train(max_epochs=300)",
      'scanvi_model = scvi.model.SCANVI.from_scvi_model(scvi_model, labels_key="cell_type")',
      "scanvi_model.train(max_epochs=100)",
      "```",
      "",
      "### totalVI: Noisy protein signal",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Use denoised protein values",
      "_, protein_denoised = model.get_normalized_expression(return_mean=True)",
      "",
      "# 2. Check isotype controls",
      "# Isotype controls should have low expression",
      'for i, name in enumerate(adata.uns["protein_names"]):',
      "    if 'isotype' in name.lower():",
      "        print(f\"{name}: mean={adata.obsm['protein_expression'][:, i].mean():.1f}\")",
      "```",
      "",
      "### PeakVI: Poor clustering",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Use more variable peaks",
      "from sklearn.feature_selection import VarianceThreshold",
      "selector = VarianceThreshold(threshold=0.05)",
      "adata = adata[:, selector.fit(adata.X).get_support()].copy()",
      "",
      "# 2. Binarize data",
      "adata.X = (adata.X > 0).astype(np.float32)",
      "```",
      "",
      "### MultiVI: Different cell counts between modalities",
      "",
      "**Solutions**:",
      "```python",
      "# Ensure same cells in same order",
      "common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)",
      "adata_rna = adata_rna[common_cells].copy()",
      "adata_atac = adata_atac[common_cells].copy()",
      "```",
      "",
      "### DestVI: Poor deconvolution",
      "",
      "**Solutions**:",
      "```python",
      "# 1. Check gene overlap",
      "common_genes = adata_ref.var_names.intersection(adata_spatial.var_names)",
      'print(f"Common genes: {len(common_genes)}")  # Should be >1000',
      "",
      "# 2. Use tissue-matched reference",
      "# Reference should contain all cell types expected in spatial data",
      "",
      "# 3. Check reference quality",
      "print(adata_ref.obs['cell_type'].value_counts())",
      "```",
      "",
      "## Version Compatibility",
      "",
      "### scvi-tools 1.x vs 0.x API changes",
      "",
      "Key differences:",
      "```python",
      "# 0.x API",
      "scvi.data.setup_anndata(adata, ...)",
      "",
      "# 1.x API (current)",
      "scvi.model.SCVI.setup_anndata(adata, ...)",
      "```",
      "",
      "### Check versions",
      "```python",
      "import scvi",
      "import scanpy as sc",
      "import anndata",
      "import torch",
      "",
      'print(f"scvi-tools: {scvi.__version__}")',
      'print(f"scanpy: {sc.__version__}")',
      'print(f"anndata: {anndata.__version__}")',
      'print(f"torch: {torch.__version__}")',
      "```",
      "",
      "### Recommended versions (as of late 2024)",
      "```",
      "scvi-tools>=1.0.0",
      "scanpy>=1.9.0",
      "anndata>=0.9.0",
      "torch>=2.0.0",
      "```",
      "",
      "## Getting Help",
      "",
      "1. **Check documentation**: https://docs.scvi-tools.org/",
      "2. **GitHub issues**: https://github.com/scverse/scvi-tools/issues",
      "3. **Discourse forum**: https://discourse.scverse.org/",
      "4. **Tutorials**: https://docs.scvi-tools.org/en/stable/tutorials/index.html",
      "",
      "When reporting issues, include:",
      "- scvi-tools version (`scvi.__version__`)",
      "- Python version",
      "- Full error traceback",
      "- Minimal reproducible example",
      "",
    ].join("\n"),
    "scripts/cluster_embed.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Cluster and embed data using scvi-tools latent representation.",
      "",
      "Computes neighbors, UMAP, and Leiden clustering on the latent space.",
      "Input should have latent representation from train_model.py.",
      "",
      "Usage:",
      "    python cluster_embed.py input.h5ad output_dir/",
      "    python cluster_embed.py input.h5ad output_dir/ --resolution 0.5 --use-rep X_scVI",
      '"""',
      "",
      "import argparse",
      "import os",
      "import sys",
      "",
      "",
      "def cluster_and_embed(",
      "    adata,",
      "    use_rep=None,",
      "    n_neighbors=15,",
      "    resolution=1.0,",
      "    min_dist=0.3",
      "):",
      '    """',
      "    Cluster and compute UMAP embedding.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data with latent representation in obsm",
      "    use_rep : str, optional",
      "        Key in obsm to use (auto-detects if None)",
      "    n_neighbors : int",
      "        Number of neighbors for graph",
      "    resolution : float",
      "        Leiden clustering resolution",
      "    min_dist : float",
      "        UMAP min_dist parameter",
      "",
      "    Returns",
      "    -------",
      "    AnnData with neighbors, UMAP, and leiden clustering",
      '    """',
      "    import scanpy as sc",
      "",
      "    # Auto-detect representation",
      "    if use_rep is None:",
      '        candidates = ["X_scANVI", "X_scVI", "X_totalVI", "X_PeakVI", "X_MultiVI"]',
      "        for key in candidates:",
      "            if key in adata.obsm:",
      "                use_rep = key",
      "                break",
      "",
      "        if use_rep is None:",
      "            # Fall back to PCA",
      '            if "X_pca" not in adata.obsm:',
      '                print("No scvi-tools embedding found, computing PCA...")',
      "                sc.pp.pca(adata)",
      '            use_rep = "X_pca"',
      "",
      '    print(f"Using representation: {use_rep}")',
      '    print(f"Embedding shape: {adata.obsm[use_rep].shape}")',
      "",
      "    # Compute neighbors",
      '    print(f"Computing neighbors (n={n_neighbors})...")',
      "    sc.pp.neighbors(adata, use_rep=use_rep, n_neighbors=n_neighbors)",
      "",
      "    # UMAP",
      '    print(f"Computing UMAP (min_dist={min_dist})...")',
      "    sc.tl.umap(adata, min_dist=min_dist)",
      "",
      "    # Leiden clustering",
      '    print(f"Computing Leiden clustering (resolution={resolution})...")',
      "    sc.tl.leiden(adata, resolution=resolution)",
      "",
      "    n_clusters = adata.obs['leiden'].nunique()",
      '    print(f"Found {n_clusters} clusters")',
      "",
      "    return adata",
      "",
      "",
      "def plot_results(adata, output_dir, batch_key=None, labels_key=None):",
      '    """Generate and save visualization plots."""',
      "    import scanpy as sc",
      "    import matplotlib.pyplot as plt",
      "",
      "    plots = []",
      "",
      "    # Always plot clusters",
      '    plots.append(("leiden", "Clusters"))',
      "",
      "    # Plot batch if available",
      "    if batch_key is not None and batch_key in adata.obs.columns:",
      '        plots.append((batch_key, f"Batch ({batch_key})"))',
      "",
      "    # Plot labels if available",
      "    if labels_key is not None and labels_key in adata.obs.columns:",
      '        plots.append((labels_key, f"Labels ({labels_key})"))',
      "",
      "    # Check for common columns",
      "    for col in adata.obs.columns:",
      "        if col not in [p[0] for p in plots]:",
      "            if 'cell' in col.lower() and 'type' in col.lower():",
      "                plots.append((col, col))",
      "            elif 'predict' in col.lower():",
      "                plots.append((col, col))",
      "",
      "    # Limit to 6 plots",
      "    plots = plots[:6]",
      "",
      "    # Create figure",
      "    n_plots = len(plots)",
      "    n_cols = min(3, n_plots)",
      "    n_rows = (n_plots + n_cols - 1) // n_cols",
      "",
      "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))",
      "    if n_plots == 1:",
      "        axes = [axes]",
      "    else:",
      "        axes = axes.flatten()",
      "",
      "    for i, (color, title) in enumerate(plots):",
      "        try:",
      "            sc.pl.umap(adata, color=color, ax=axes[i], show=False, title=title)",
      "        except Exception as e:",
      '            axes[i].set_title(f"Could not plot {color}: {e}")',
      "",
      "    # Hide unused axes",
      "    for i in range(len(plots), len(axes)):",
      "        axes[i].set_visible(False)",
      "",
      "    plt.tight_layout()",
      "",
      '    plot_path = os.path.join(output_dir, "umap_clusters.png")',
      '    plt.savefig(plot_path, dpi=150, bbox_inches="tight")',
      "    plt.close()",
      '    print(f"UMAP plot saved to {plot_path}")',
      "",
      "    # Save cluster counts",
      "    cluster_counts = adata.obs['leiden'].value_counts().sort_index()",
      '    counts_path = os.path.join(output_dir, "cluster_counts.csv")',
      "    cluster_counts.to_csv(counts_path)",
      '    print(f"Cluster counts saved to {counts_path}")',
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Cluster and embed using scvi-tools latent space",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # Basic clustering",
      "    python cluster_embed.py adata_trained.h5ad results/",
      "",
      "    # Custom resolution",
      "    python cluster_embed.py adata_trained.h5ad results/ --resolution 0.5",
      "",
      "    # Specify representation",
      "    python cluster_embed.py adata_trained.h5ad results/ --use-rep X_scANVI",
      "",
      "    # Include batch and label columns in plots",
      "    python cluster_embed.py adata_trained.h5ad results/ --batch-key batch --labels-key cell_type",
      '        """',
      "    )",
      '    parser.add_argument("input", help="Input h5ad file with latent representation")',
      '    parser.add_argument("output_dir", help="Output directory")',
      '    parser.add_argument("--use-rep", help="Representation key in obsm (auto-detects)")',
      '    parser.add_argument("--n-neighbors", type=int, default=15, help="Neighbors for graph (default: 15)")',
      '    parser.add_argument("--resolution", type=float, default=1.0, help="Leiden resolution (default: 1.0)")',
      '    parser.add_argument("--min-dist", type=float, default=0.3, help="UMAP min_dist (default: 0.3)")',
      '    parser.add_argument("--batch-key", help="Batch column for plotting")',
      '    parser.add_argument("--labels-key", help="Labels column for plotting")',
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scanpy required. Install with: pip install scanpy")',
      "        sys.exit(1)",
      "",
      "    # Create output directory",
      "    os.makedirs(args.output_dir, exist_ok=True)",
      "",
      "    # Load data",
      '    print(f"Loading {args.input}...")',
      "    adata = sc.read_h5ad(args.input)",
      '    print(f"Data: {adata.shape}")',
      "",
      "    # Cluster and embed",
      "    adata = cluster_and_embed(",
      "        adata,",
      "        use_rep=args.use_rep,",
      "        n_neighbors=args.n_neighbors,",
      "        resolution=args.resolution,",
      "        min_dist=args.min_dist",
      "    )",
      "",
      "    # Save results",
      '    adata_path = os.path.join(args.output_dir, "adata_clustered.h5ad")',
      "    adata.write_h5ad(adata_path)",
      '    print(f"AnnData saved to {adata_path}")',
      "",
      "    # Plot",
      "    plot_results(adata, args.output_dir, args.batch_key, args.labels_key)",
      "",
      '    print("\\nDone!")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/differential_expression.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Differential expression analysis using scvi-tools models.",
      "",
      "Uses the trained model's differential_expression method which accounts",
      "for batch effects and uses the generative model for inference.",
      "",
      "Usage:",
      "    python differential_expression.py model_dir/ adata.h5ad output.csv --groupby leiden",
      '    python differential_expression.py model_dir/ adata.h5ad output.csv --groupby cell_type --group1 "T cells" --group2 "B cells"',
      '"""',
      "",
      "import argparse",
      "import os",
      "import sys",
      "",
      "",
      "def run_de_analysis(",
      "    model,",
      "    adata,",
      "    groupby,",
      "    group1=None,",
      "    group2=None,",
      "    n_genes=None",
      "):",
      '    """',
      "    Run differential expression analysis.",
      "",
      "    Parameters",
      "    ----------",
      "    model : scvi model",
      "        Trained model with differential_expression method",
      "    adata : AnnData",
      "        Data used for training",
      "    groupby : str",
      "        Column in obs to group by",
      "    group1 : str, optional",
      "        First group (if None, computes for all groups)",
      "    group2 : str, optional",
      "        Second group (rest if None)",
      "    n_genes : int, optional",
      "        Limit to top N genes per group",
      "",
      "    Returns",
      "    -------",
      "    DataFrame with DE results",
      '    """',
      "    import pandas as pd",
      "",
      "    if group1 is not None:",
      "        # Specific comparison",
      "        print(f\"Comparing {group1} vs {group2 or 'rest'}...\")",
      "        de_results = model.differential_expression(",
      "            groupby=groupby,",
      "            group1=group1,",
      "            group2=group2",
      "        )",
      "",
      "        # Add comparison info",
      '        de_results["comparison"] = f"{group1}_vs_{group2 or \'rest\'}"',
      "",
      "    else:",
      "        # All pairwise or one-vs-rest",
      "        groups = adata.obs[groupby].unique()",
      '        print(f"Computing DE for {len(groups)} groups...")',
      "",
      "        all_results = []",
      "        for group in groups:",
      '            print(f"  Processing {group}...")',
      "            try:",
      "                de = model.differential_expression(",
      "                    groupby=groupby,",
      "                    group1=group",
      "                )",
      '                de["group"] = group',
      "                all_results.append(de)",
      "            except Exception as e:",
      '                print(f"  Warning: Failed for {group}: {e}")',
      "",
      "        de_results = pd.concat(all_results, ignore_index=False)",
      "",
      "    # Filter to significant",
      '    if "is_de_fdr_0.05" in de_results.columns:',
      '        n_sig = de_results["is_de_fdr_0.05"].sum()',
      '        print(f"Found {n_sig} significant DE genes (FDR < 0.05)")',
      "",
      "    # Limit to top genes if requested",
      '    if n_genes is not None and "lfc_mean" in de_results.columns:',
      '        if "group" in de_results.columns:',
      "            # Top N per group",
      '            de_results = de_results.groupby("group").apply(',
      '                lambda x: x.nlargest(n_genes, "lfc_mean")',
      "            ).reset_index(drop=True)",
      "        else:",
      '            de_results = de_results.nlargest(n_genes, "lfc_mean")',
      "",
      "    return de_results",
      "",
      "",
      "def plot_volcano(de_results, output_path, group_name=None):",
      '    """Create volcano plot of DE results."""',
      "    import matplotlib.pyplot as plt",
      "    import numpy as np",
      "",
      '    if "lfc_mean" not in de_results.columns:',
      '        print("Cannot create volcano plot: missing lfc_mean column")',
      "        return",
      "",
      "    fig, ax = plt.subplots(figsize=(8, 6))",
      "",
      "    # Get values",
      '    lfc = de_results["lfc_mean"].values',
      '    if "bayes_factor" in de_results.columns:',
      '        y_val = de_results["bayes_factor"].values',
      '        y_label = "Bayes Factor"',
      '    elif "proba_de" in de_results.columns:',
      '        y_val = -np.log10(1 - de_results["proba_de"].values + 1e-10)',
      '        y_label = "-log10(1 - P(DE))"',
      "    else:",
      "        y_val = np.ones(len(lfc))",
      '        y_label = ""',
      "",
      "    # Color by significance",
      '    if "is_de_fdr_0.05" in de_results.columns:',
      '        sig = de_results["is_de_fdr_0.05"].values',
      '        colors = ["red" if s else "gray" for s in sig]',
      "    else:",
      '        colors = "gray"',
      "",
      "    ax.scatter(lfc, y_val, c=colors, alpha=0.5, s=10)",
      '    ax.axvline(0, color="black", linestyle="--", alpha=0.5)',
      '    ax.set_xlabel("Log Fold Change")',
      "    ax.set_ylabel(y_label)",
      "",
      '    title = "Differential Expression"',
      "    if group_name:",
      '        title += f": {group_name}"',
      "    ax.set_title(title)",
      "",
      "    plt.tight_layout()",
      '    plt.savefig(output_path, dpi=150, bbox_inches="tight")',
      "    plt.close()",
      '    print(f"Volcano plot saved to {output_path}")',
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Differential expression with scvi-tools",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # DE for all clusters (one-vs-rest)",
      "    python differential_expression.py model/ adata.h5ad de_results.csv --groupby leiden",
      "",
      "    # Specific comparison",
      "    python differential_expression.py model/ adata.h5ad de_results.csv \\\\",
      '        --groupby cell_type --group1 "T cells" --group2 "B cells"',
      "",
      "    # Top 50 genes per cluster",
      "    python differential_expression.py model/ adata.h5ad de_results.csv \\\\",
      "        --groupby leiden --n-genes 50",
      '        """',
      "    )",
      '    parser.add_argument("model_dir", help="Directory containing saved model")',
      '    parser.add_argument("input", help="Input h5ad file (same as training)")',
      '    parser.add_argument("output", help="Output CSV file for DE results")',
      '    parser.add_argument("--groupby", required=True, help="Column to group by")',
      '    parser.add_argument("--group1", help="First group for comparison")',
      '    parser.add_argument("--group2", help="Second group (default: rest)")',
      '    parser.add_argument("--n-genes", type=int, help="Limit to top N genes per group")',
      '    parser.add_argument("--model-type", choices=["scvi", "scanvi", "totalvi"],',
      '                        default="scvi", help="Model type (default: scvi)")',
      '    parser.add_argument("--plot", action="store_true", help="Generate volcano plot")',
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        import scvi",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scvi-tools and scanpy required")',
      "        sys.exit(1)",
      "",
      "    # Load data",
      '    print(f"Loading {args.input}...")',
      "    adata = sc.read_h5ad(args.input)",
      "",
      "    # Load model",
      '    print(f"Loading model from {args.model_dir}...")',
      '    if args.model_type == "scvi":',
      "        model = scvi.model.SCVI.load(args.model_dir, adata=adata)",
      '    elif args.model_type == "scanvi":',
      "        model = scvi.model.SCANVI.load(args.model_dir, adata=adata)",
      '    elif args.model_type == "totalvi":',
      "        model = scvi.model.TOTALVI.load(args.model_dir, adata=adata)",
      "",
      "    # Run DE",
      "    de_results = run_de_analysis(",
      "        model,",
      "        adata,",
      "        groupby=args.groupby,",
      "        group1=args.group1,",
      "        group2=args.group2,",
      "        n_genes=args.n_genes",
      "    )",
      "",
      "    # Save results",
      "    de_results.to_csv(args.output)",
      '    print(f"DE results saved to {args.output}")',
      "",
      "    # Plot",
      "    if args.plot:",
      '        plot_path = args.output.replace(".csv", "_volcano.png")',
      "        plot_volcano(de_results, plot_path, args.group1)",
      "",
      '    print("\\nDone!")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/integrate_datasets.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Integrate multiple datasets using scvi-tools.",
      "",
      "Concatenates multiple h5ad files and runs batch correction with scVI or scANVI.",
      "",
      "Usage:",
      "    python integrate_datasets.py output_dir/ dataset1.h5ad dataset2.h5ad dataset3.h5ad",
      "    python integrate_datasets.py output_dir/ *.h5ad --batch-names study1,study2,study3",
      '"""',
      "",
      "import argparse",
      "import os",
      "import sys",
      "",
      "",
      "def integrate_datasets(",
      "    adatas,",
      "    batch_names=None,",
      "    labels_key=None,",
      "    n_top_genes=2000,",
      "    n_latent=30,",
      "    max_epochs=200",
      "):",
      '    """',
      "    Integrate multiple datasets.",
      "",
      "    Parameters",
      "    ----------",
      "    adatas : list of AnnData",
      "        Datasets to integrate",
      "    batch_names : list of str, optional",
      "        Names for each dataset (default: dataset_0, dataset_1, ...)",
      "    labels_key : str, optional",
      "        Cell type column (uses scANVI if provided)",
      "    n_top_genes : int",
      "        Number of HVGs",
      "    n_latent : int",
      "        Latent dimensions",
      "    max_epochs : int",
      "        Training epochs",
      "",
      "    Returns",
      "    -------",
      "    Integrated AnnData and trained model",
      '    """',
      "    import scvi",
      "    import scanpy as sc",
      "    import numpy as np",
      "",
      "    # Assign batch names",
      "    if batch_names is None:",
      '        batch_names = [f"dataset_{i}" for i in range(len(adatas))]',
      "",
      "    if len(batch_names) != len(adatas):",
      '        raise ValueError(f"Number of batch names ({len(batch_names)}) must match datasets ({len(adatas)})")',
      "",
      "    # Add batch labels",
      "    for adata, name in zip(adatas, batch_names):",
      '        adata.obs["batch"] = name',
      '        print(f"{name}: {adata.shape}")',
      "",
      "    # Find common genes",
      "    common_genes = set(adatas[0].var_names)",
      "    for adata in adatas[1:]:",
      "        common_genes = common_genes.intersection(adata.var_names)",
      "    common_genes = list(common_genes)",
      '    print(f"\\nCommon genes: {len(common_genes)}")',
      "",
      "    # Subset to common genes",
      "    adatas = [adata[:, common_genes].copy() for adata in adatas]",
      "",
      "    # Concatenate",
      '    print("Concatenating datasets...")',
      '    adata = sc.concat(adatas, label="batch", keys=batch_names)',
      '    print(f"Combined: {adata.shape}")',
      "",
      "    # Store counts",
      '    adata.layers["counts"] = adata.X.copy()',
      "",
      "    # HVG selection",
      '    print(f"Selecting {n_top_genes} HVGs...")',
      "    sc.pp.highly_variable_genes(",
      "        adata,",
      "        n_top_genes=n_top_genes,",
      '        flavor="seurat_v3",',
      '        batch_key="batch",',
      '        layer="counts"',
      "    )",
      '    adata = adata[:, adata.var["highly_variable"]].copy()',
      "",
      "    # Train model",
      "    if labels_key is not None and labels_key in adata.obs.columns:",
      '        print(f"\\nTraining scANVI with labels ({labels_key})...")',
      "",
      "        # First train scVI",
      '        scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "        scvi_model = scvi.model.SCVI(adata, n_latent=n_latent)",
      "        scvi_model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      "        # Then scANVI",
      "        model = scvi.model.SCANVI.from_scvi_model(",
      "            scvi_model,",
      "            labels_key=labels_key,",
      '            unlabeled_category="Unknown"',
      "        )",
      "        model.train(max_epochs=max_epochs // 4)",
      "",
      '        adata.obsm["X_scANVI"] = model.get_latent_representation()',
      '        rep_key = "X_scANVI"',
      "",
      "    else:",
      '        print("\\nTraining scVI...")',
      '        scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "        model = scvi.model.SCVI(adata, n_latent=n_latent)",
      "        model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      '        adata.obsm["X_scVI"] = model.get_latent_representation()',
      '        rep_key = "X_scVI"',
      "",
      "    # Cluster",
      '    print("\\nClustering...")',
      "    sc.pp.neighbors(adata, use_rep=rep_key)",
      "    sc.tl.umap(adata)",
      "    sc.tl.leiden(adata)",
      "",
      "    print(f\"Found {adata.obs['leiden'].nunique()} clusters\")",
      "",
      "    return adata, model",
      "",
      "",
      "def plot_integration(adata, output_dir, labels_key=None):",
      '    """Plot integration results."""',
      "    import scanpy as sc",
      "    import matplotlib.pyplot as plt",
      "",
      "    plots = [",
      '        ("batch", "By Batch"),',
      '        ("leiden", "Clusters")',
      "    ]",
      "",
      "    if labels_key is not None and labels_key in adata.obs.columns:",
      '        plots.append((labels_key, f"Cell Types ({labels_key})"))',
      "",
      '    if "predicted_cell_type" in adata.obs.columns:',
      '        plots.append(("predicted_cell_type", "Predicted Types"))',
      "",
      "    n_plots = len(plots)",
      "    fig, axes = plt.subplots(1, n_plots, figsize=(5 * n_plots, 4))",
      "    if n_plots == 1:",
      "        axes = [axes]",
      "",
      "    for ax, (color, title) in zip(axes, plots):",
      "        sc.pl.umap(adata, color=color, ax=ax, show=False, title=title)",
      "",
      "    plt.tight_layout()",
      '    plot_path = os.path.join(output_dir, "integration.png")',
      '    plt.savefig(plot_path, dpi=150, bbox_inches="tight")',
      "    plt.close()",
      '    print(f"Integration plot saved to {plot_path}")',
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Integrate multiple datasets with scvi-tools",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # Integrate multiple files",
      "    python integrate_datasets.py results/ data1.h5ad data2.h5ad data3.h5ad",
      "",
      "    # With custom batch names",
      "    python integrate_datasets.py results/ *.h5ad --batch-names ctrl,treat1,treat2",
      "",
      "    # With cell type labels (uses scANVI)",
      "    python integrate_datasets.py results/ *.h5ad --labels-key cell_type",
      '        """',
      "    )",
      '    parser.add_argument("output_dir", help="Output directory")',
      '    parser.add_argument("inputs", nargs="+", help="Input h5ad files")',
      '    parser.add_argument("--batch-names", help="Comma-separated batch names")',
      '    parser.add_argument("--labels-key", help="Cell type column (uses scANVI)")',
      '    parser.add_argument("--n-hvgs", type=int, default=2000, help="Number of HVGs (default: 2000)")',
      '    parser.add_argument("--n-latent", type=int, default=30, help="Latent dimensions (default: 30)")',
      '    parser.add_argument("--max-epochs", type=int, default=200, help="Max epochs (default: 200)")',
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        import scvi",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scvi-tools and scanpy required")',
      "        sys.exit(1)",
      "",
      "    # Create output directory",
      "    os.makedirs(args.output_dir, exist_ok=True)",
      "",
      "    # Parse batch names",
      "    batch_names = None",
      "    if args.batch_names:",
      '        batch_names = args.batch_names.split(",")',
      "",
      "    # Load datasets",
      '    print("Loading datasets...")',
      "    adatas = []",
      "    for path in args.inputs:",
      '        print(f"  Loading {path}...")',
      "        adatas.append(sc.read_h5ad(path))",
      "",
      "    # Integrate",
      "    adata, model = integrate_datasets(",
      "        adatas,",
      "        batch_names=batch_names,",
      "        labels_key=args.labels_key,",
      "        n_top_genes=args.n_hvgs,",
      "        n_latent=args.n_latent,",
      "        max_epochs=args.max_epochs",
      "    )",
      "",
      "    # Save results",
      '    adata_path = os.path.join(args.output_dir, "integrated.h5ad")',
      "    adata.write_h5ad(adata_path)",
      '    print(f"\\nIntegrated data saved to {adata_path}")',
      "",
      '    model_path = os.path.join(args.output_dir, "model")',
      "    model.save(model_path)",
      '    print(f"Model saved to {model_path}")',
      "",
      "    # Plot",
      "    plot_integration(adata, args.output_dir, args.labels_key)",
      "",
      '    print("\\nDone!")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/model_utils.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Utility functions for scvi-tools model training and evaluation.",
      "",
      "Usage:",
      "    from model_utils import prepare_adata, train_scvi, evaluate_integration",
      '"""',
      "",
      "import numpy as np",
      "import scanpy as sc",
      "from typing import Optional, List, Dict, Tuple",
      "import warnings",
      "",
      "",
      "def get_mito_genes(adata) -> np.ndarray:",
      '    """',
      "    Identify mitochondrial genes for both human and mouse data.",
      "",
      "    Handles common prefixes:",
      "    - Human: MT- (e.g., MT-CO1, MT-ND1)",
      "    - Mouse: mt- or Mt- (e.g., mt-Co1, Mt-Nd1)",
      "",
      "    Returns",
      "    -------",
      "    Boolean array indicating mitochondrial genes",
      '    """',
      "    return (",
      "        adata.var_names.str.startswith('MT-') |",
      "        adata.var_names.str.startswith('mt-') |",
      "        adata.var_names.str.startswith('Mt-')",
      "    )",
      "",
      "",
      "def prepare_adata(",
      "    adata,",
      "    batch_key: Optional[str] = None,",
      "    n_top_genes: int = 2000,",
      "    min_genes: int = 200,",
      "    max_genes: int = 5000,",
      "    max_mito_pct: float = 20.0,",
      "    min_cells: int = 3,",
      "    copy: bool = True",
      "):",
      '    """',
      "    Prepare AnnData for scvi-tools models.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Raw count data",
      "    batch_key : str, optional",
      "        Column for batch information",
      "    n_top_genes : int",
      "        Number of highly variable genes",
      "    min_genes : int",
      "        Minimum genes per cell",
      "    max_genes : int",
      "        Maximum genes per cell",
      "    max_mito_pct : float",
      "        Maximum mitochondrial percentage",
      "    min_cells : int",
      "        Minimum cells per gene",
      "    copy : bool",
      "        Return copy of data",
      "        ",
      "    Returns",
      "    -------",
      "    AnnData prepared for scvi-tools",
      '    """',
      "    if copy:",
      "        adata = adata.copy()",
      "    ",
      "    # Calculate QC metrics",
      "    adata.var['mt'] = get_mito_genes(adata)",
      "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)",
      "    ",
      "    # Filter cells",
      "    adata = adata[adata.obs['n_genes_by_counts'] >= min_genes].copy()",
      "    adata = adata[adata.obs['n_genes_by_counts'] <= max_genes].copy()",
      "    adata = adata[adata.obs['pct_counts_mt'] < max_mito_pct].copy()",
      "    ",
      "    # Filter genes",
      "    sc.pp.filter_genes(adata, min_cells=min_cells)",
      "    ",
      "    # Store raw counts",
      '    adata.layers["counts"] = adata.X.copy()',
      "    ",
      "    # HVG selection",
      "    if batch_key and batch_key in adata.obs.columns:",
      "        sc.pp.highly_variable_genes(",
      "            adata,",
      "            n_top_genes=n_top_genes,",
      '            flavor="seurat_v3",',
      "            batch_key=batch_key,",
      '            layer="counts"',
      "        )",
      "    else:",
      "        # Need to normalize for non-seurat_v3 flavor",
      "        sc.pp.normalize_total(adata, target_sum=1e4)",
      "        sc.pp.log1p(adata)",
      "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes)",
      "        # Restore counts to X",
      '        adata.X = adata.layers["counts"].copy()',
      "    ",
      "    # Subset to HVGs",
      "    adata = adata[:, adata.var['highly_variable']].copy()",
      "    ",
      '    print(f"Prepared AnnData: {adata.shape}")',
      "    if batch_key:",
      '        print(f"Batches: {adata.obs[batch_key].nunique()}")',
      "    ",
      "    return adata",
      "",
      "",
      "def train_scvi(",
      "    adata,",
      "    batch_key: Optional[str] = None,",
      "    labels_key: Optional[str] = None,",
      "    n_latent: int = 30,",
      "    n_layers: int = 2,",
      "    max_epochs: int = 200,",
      "    early_stopping: bool = True,",
      "    use_gpu: bool = True",
      "):",
      '    """',
      "    Train scVI or scANVI model.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Prepared data with counts layer",
      "    batch_key : str, optional",
      "        Batch column",
      "    labels_key : str, optional",
      "        Cell type labels (uses scANVI if provided)",
      "    n_latent : int",
      "        Latent dimensions",
      "    n_layers : int",
      "        Encoder/decoder layers",
      "    max_epochs : int",
      "        Maximum training epochs",
      "    early_stopping : bool",
      "        Use early stopping",
      "    use_gpu : bool",
      "        Use GPU if available",
      "        ",
      "    Returns",
      "    -------",
      "    Trained model",
      '    """',
      "    import scvi",
      "    ",
      "    # Setup AnnData",
      "    scvi.model.SCVI.setup_anndata(",
      "        adata,",
      '        layer="counts",',
      "        batch_key=batch_key",
      "    )",
      "    ",
      "    if labels_key and labels_key in adata.obs.columns:",
      "        # Train scVI first",
      "        scvi_model = scvi.model.SCVI(",
      "            adata,",
      "            n_latent=n_latent,",
      "            n_layers=n_layers",
      "        )",
      "        scvi_model.train(",
      "            max_epochs=max_epochs,",
      "            early_stopping=early_stopping",
      "        )",
      "        ",
      "        # Initialize scANVI",
      "        model = scvi.model.SCANVI.from_scvi_model(",
      "            scvi_model,",
      "            labels_key=labels_key,",
      '            unlabeled_category="Unknown"',
      "        )",
      "        model.train(max_epochs=max_epochs // 4)",
      "        ",
      "        # Store representation",
      '        adata.obsm["X_scANVI"] = model.get_latent_representation()',
      "    else:",
      "        # Train scVI only",
      "        model = scvi.model.SCVI(",
      "            adata,",
      "            n_latent=n_latent,",
      "            n_layers=n_layers",
      "        )",
      "        model.train(",
      "            max_epochs=max_epochs,",
      "            early_stopping=early_stopping",
      "        )",
      "        ",
      "        # Store representation",
      '        adata.obsm["X_scVI"] = model.get_latent_representation()',
      "    ",
      "    return model",
      "",
      "",
      "def evaluate_integration(",
      "    adata,",
      "    batch_key: str,",
      "    label_key: str,",
      '    embedding_key: str = "X_scVI"',
      ") -> Dict[str, float]:",
      '    """',
      "    Evaluate integration quality using basic metrics.",
      "    ",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Integrated data",
      "    batch_key : str",
      "        Batch column",
      "    label_key : str",
      "        Cell type column",
      "    embedding_key : str",
      "        Key in obsm for embedding",
      "        ",
      "    Returns",
      "    -------",
      "    Dictionary of metrics",
      '    """',
      "    from sklearn.metrics import silhouette_score",
      "    from sklearn.neighbors import NearestNeighbors",
      "    ",
      "    X = adata.obsm[embedding_key]",
      "    batch = adata.obs[batch_key].values",
      "    labels = adata.obs[label_key].values",
      "    ",
      "    metrics = {}",
      "    ",
      "    # Silhouette scores",
      "    try:",
      "        # Cell type silhouette (higher = better separation)",
      '        metrics["silhouette_label"] = silhouette_score(X, labels)',
      "        ",
      "        # Batch silhouette (lower = better mixing)",
      '        metrics["silhouette_batch"] = silhouette_score(X, batch)',
      "    except Exception as e:",
      '        warnings.warn(f"Silhouette calculation failed: {e}")',
      "    ",
      "    # Batch mixing in neighbors",
      "    try:",
      "        nn = NearestNeighbors(n_neighbors=50)",
      "        nn.fit(X)",
      "        distances, indices = nn.kneighbors(X)",
      "        ",
      "        batch_mixing = []",
      "        for i in range(len(X)):",
      "            neighbor_batches = batch[indices[i]]",
      "            unique_batches = len(np.unique(neighbor_batches))",
      "            batch_mixing.append(unique_batches / len(np.unique(batch)))",
      "        ",
      '        metrics["batch_mixing"] = np.mean(batch_mixing)',
      "    except Exception as e:",
      '        warnings.warn(f"Batch mixing calculation failed: {e}")',
      "    ",
      "    return metrics",
      "",
      "",
      "def get_marker_genes(",
      "    model,",
      "    adata,",
      "    groupby: str,",
      "    n_genes: int = 10",
      ") -> Dict[str, List[str]]:",
      '    """',
      "    Get marker genes using scVI differential expression.",
      "    ",
      "    Parameters",
      "    ----------",
      "    model : scvi model",
      "        Trained scVI/scANVI model",
      "    adata : AnnData",
      "        Data used for training",
      "    groupby : str",
      "        Column to group cells by",
      "    n_genes : int",
      "        Number of top markers per group",
      "        ",
      "    Returns",
      "    -------",
      "    Dictionary of {group: [marker_genes]}",
      '    """',
      "    markers = {}",
      "    groups = adata.obs[groupby].unique()",
      "    ",
      "    for group in groups:",
      "        # Get DE results for this group vs rest",
      "        de_results = model.differential_expression(",
      "            groupby=groupby,",
      "            group1=group",
      "        )",
      "        ",
      "        # Filter and sort",
      "        de_sig = de_results[",
      '            (de_results["is_de_fdr_0.05"] == True) &',
      '            (de_results["lfc_mean"] > 0.5)',
      '        ].sort_values("lfc_mean", ascending=False)',
      "        ",
      "        markers[group] = de_sig.index[:n_genes].tolist()",
      "    ",
      "    return markers",
      "",
      "",
      "def plot_training_history(model, save_path: Optional[str] = None):",
      '    """',
      "    Plot model training history.",
      "    ",
      "    Parameters",
      "    ----------",
      "    model : scvi model",
      "        Trained model",
      "    save_path : str, optional",
      "        Path to save figure",
      '    """',
      "    import matplotlib.pyplot as plt",
      "    ",
      "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))",
      "    ",
      "    # ELBO",
      '    if "elbo_train" in model.history:',
      '        axes[0].plot(model.history["elbo_train"], label="Train")',
      '    if "elbo_validation" in model.history:',
      '        axes[0].plot(model.history["elbo_validation"], label="Validation")',
      '    axes[0].set_xlabel("Epoch")',
      '    axes[0].set_ylabel("ELBO")',
      "    axes[0].legend()",
      '    axes[0].set_title("Training Loss")',
      "    ",
      "    # Reconstruction",
      '    if "reconstruction_loss_train" in model.history:',
      '        axes[1].plot(model.history["reconstruction_loss_train"], label="Train")',
      '    if "reconstruction_loss_validation" in model.history:',
      '        axes[1].plot(model.history["reconstruction_loss_validation"], label="Validation")',
      '    axes[1].set_xlabel("Epoch")',
      '    axes[1].set_ylabel("Reconstruction Loss")',
      "    axes[1].legend()",
      '    axes[1].set_title("Reconstruction Loss")',
      "    ",
      "    plt.tight_layout()",
      "    ",
      "    if save_path:",
      '        plt.savefig(save_path, dpi=150, bbox_inches="tight")',
      "    ",
      "    return fig",
      "",
      "",
      "def save_results(",
      "    model,",
      "    adata,",
      "    output_dir: str,",
      "    save_model: bool = True,",
      "    save_adata: bool = True,",
      "    plot_umap: bool = True",
      "):",
      '    """',
      "    Save model, processed data, and visualization.",
      "",
      "    Parameters",
      "    ----------",
      "    model : scvi model",
      "        Trained model",
      "    adata : AnnData",
      "        Processed data with latent representation",
      "    output_dir : str",
      "        Output directory path",
      "    save_model : bool",
      "        Save the trained model",
      "    save_adata : bool",
      "        Save the processed AnnData",
      "    plot_umap : bool",
      "        Generate and save UMAP plot",
      '    """',
      "    import os",
      "    import scanpy as sc",
      "    import matplotlib.pyplot as plt",
      "",
      "    os.makedirs(output_dir, exist_ok=True)",
      "",
      "    # Save model",
      "    if save_model:",
      '        model_path = os.path.join(output_dir, "model")',
      "        model.save(model_path)",
      '        print(f"Model saved to {model_path}")',
      "",
      "    # Save AnnData",
      "    if save_adata:",
      '        adata_path = os.path.join(output_dir, "adata_processed.h5ad")',
      "        adata.write(adata_path)",
      '        print(f"AnnData saved to {adata_path}")',
      "",
      "    # Generate UMAP if needed",
      "    if plot_umap:",
      "        # Determine which embedding to use",
      '        if "X_scANVI" in adata.obsm:',
      '            rep_key = "X_scANVI"',
      '        elif "X_scVI" in adata.obsm:',
      '            rep_key = "X_scVI"',
      "        else:",
      "            rep_key = None",
      "",
      "        if rep_key is not None:",
      "            # Compute neighbors and UMAP if not present",
      '            if "X_umap" not in adata.obsm:',
      "                sc.pp.neighbors(adata, use_rep=rep_key)",
      "                sc.tl.umap(adata)",
      "",
      "            # Plot",
      "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))",
      "",
      "            # Plot by batch if available",
      "            batch_cols = [c for c in adata.obs.columns if 'batch' in c.lower()]",
      "            if batch_cols:",
      '                sc.pl.umap(adata, color=batch_cols[0], ax=axes[0], show=False, title="By Batch")',
      "",
      "            # Plot by cluster",
      '            if "leiden" not in adata.obs:',
      "                sc.tl.leiden(adata)",
      '            sc.pl.umap(adata, color="leiden", ax=axes[1], show=False, title="Clusters")',
      "",
      "            plt.tight_layout()",
      '            plot_path = os.path.join(output_dir, "umap.png")',
      '            plt.savefig(plot_path, dpi=150, bbox_inches="tight")',
      "            plt.close()",
      '            print(f"UMAP plot saved to {plot_path}")',
      "",
      "",
      "def auto_select_model(adata) -> str:",
      '    """',
      "    Suggest the best scvi-tools model based on available data.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data to analyze",
      "",
      "    Returns",
      "    -------",
      "    String with model recommendation and reasoning",
      '    """',
      "    suggestions = []",
      "",
      "    # Check for multi-modal data",
      "    if 'protein_expression' in adata.obsm:",
      "        suggestions.append({",
      "            'model': 'totalVI',",
      "            'reason': 'CITE-seq data detected (protein + RNA)',",
      "            'priority': 1",
      "        })",
      "",
      "    if 'spliced' in adata.layers and 'unspliced' in adata.layers:",
      "        suggestions.append({",
      "            'model': 'veloVI',",
      "            'reason': 'RNA velocity data detected (spliced + unspliced)',",
      "            'priority': 1",
      "        })",
      "",
      "    # Check for ATAC data indicators",
      "    if adata.n_vars > 100000:  # Many peaks suggest ATAC",
      "        suggestions.append({",
      "            'model': 'PeakVI',",
      "            'reason': f'Large number of features ({adata.n_vars}) suggests ATAC-seq peaks',",
      "            'priority': 2",
      "        })",
      "",
      "    # Check for labels",
      "    label_cols = [c for c in adata.obs.columns if 'cell' in c.lower() or 'type' in c.lower() or 'label' in c.lower()]",
      "    has_labels = len(label_cols) > 0",
      "",
      "    # Check for batch info",
      "    batch_cols = [c for c in adata.obs.columns if 'batch' in c.lower() or 'sample' in c.lower()]",
      "    has_batch = len(batch_cols) > 0",
      "",
      "    if has_batch:",
      "        if has_labels:",
      "            suggestions.append({",
      "                'model': 'scANVI',",
      "                'reason': f'Batch info ({batch_cols[0]}) + labels ({label_cols[0]}) available',",
      "                'priority': 1",
      "            })",
      "        else:",
      "            suggestions.append({",
      "                'model': 'scVI',",
      "                'reason': f'Batch info ({batch_cols[0]}) available, no labels',",
      "                'priority': 1",
      "            })",
      "    else:",
      "        suggestions.append({",
      "            'model': 'scVI',",
      "            'reason': 'Standard scRNA-seq analysis',",
      "            'priority': 2",
      "        })",
      "",
      "    # Sort by priority",
      "    suggestions.sort(key=lambda x: x['priority'])",
      "",
      "    # Format output",
      '    lines = ["Recommended models (in order of priority):"]',
      "    for i, s in enumerate(suggestions, 1):",
      "        lines.append(f\"  {i}. {s['model']}: {s['reason']}\")",
      "",
      '    return "\\n".join(lines)',
      "",
      "",
      "def compare_integrations(",
      "    adata,",
      "    batch_key: str,",
      "    label_key: str,",
      "    embedding_keys: List[str] = None",
      ") -> Dict[str, Dict[str, float]]:",
      '    """',
      "    Compare multiple integration methods using standard metrics.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data with integration embeddings in obsm",
      "    batch_key : str",
      "        Batch column in obs",
      "    label_key : str",
      "        Cell type column in obs",
      "    embedding_keys : list, optional",
      "        Keys in obsm to compare (default: auto-detect)",
      "",
      "    Returns",
      "    -------",
      "    Dictionary of {embedding: {metric: value}}",
      '    """',
      "    from sklearn.metrics import silhouette_score",
      "",
      "    # Auto-detect embeddings",
      "    if embedding_keys is None:",
      "        embedding_keys = [k for k in adata.obsm.keys()",
      "                         if k.startswith('X_') and 'umap' not in k.lower()]",
      "",
      "    results = {}",
      "",
      "    for key in embedding_keys:",
      "        if key not in adata.obsm:",
      "            continue",
      "",
      "        X = adata.obsm[key]",
      "        batch = adata.obs[batch_key].values",
      "        labels = adata.obs[label_key].values",
      "",
      "        metrics = {}",
      "",
      "        try:",
      "            # Silhouette scores",
      '            metrics["silhouette_label"] = silhouette_score(X, labels)',
      '            metrics["silhouette_batch"] = silhouette_score(X, batch)',
      "",
      "            # Combined score (higher label preservation, lower batch separation = better)",
      '            metrics["integration_score"] = metrics["silhouette_label"] - metrics["silhouette_batch"]',
      "",
      "        except Exception as e:",
      '            metrics["error"] = str(e)',
      "",
      "        results[key] = metrics",
      "",
      "    return results",
      "",
      "",
      "def quick_clustering(",
      "    adata,",
      "    use_rep: str = None,",
      "    resolution: float = 1.0,",
      "    n_neighbors: int = 15",
      "):",
      '    """',
      "    Quick clustering pipeline on latent representation.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data with latent representation",
      "    use_rep : str, optional",
      "        Key in obsm (auto-detects scVI/scANVI if not specified)",
      "    resolution : float",
      "        Leiden clustering resolution",
      "    n_neighbors : int",
      "        Number of neighbors for graph",
      "",
      "    Returns",
      "    -------",
      "    AnnData with neighbors, UMAP, and leiden clustering",
      '    """',
      "    import scanpy as sc",
      "",
      "    # Auto-detect representation",
      "    if use_rep is None:",
      '        if "X_scANVI" in adata.obsm:',
      '            use_rep = "X_scANVI"',
      '        elif "X_scVI" in adata.obsm:',
      '            use_rep = "X_scVI"',
      '        elif "X_totalVI" in adata.obsm:',
      '            use_rep = "X_totalVI"',
      '        elif "X_PeakVI" in adata.obsm:',
      '            use_rep = "X_PeakVI"',
      '        elif "X_MultiVI" in adata.obsm:',
      '            use_rep = "X_MultiVI"',
      "        else:",
      '            raise ValueError("No scvi-tools embedding found in obsm")',
      "",
      '    print(f"Using representation: {use_rep}")',
      "",
      "    # Compute neighbors",
      "    sc.pp.neighbors(adata, use_rep=use_rep, n_neighbors=n_neighbors)",
      "",
      "    # UMAP",
      "    sc.tl.umap(adata)",
      "",
      "    # Leiden clustering",
      "    sc.tl.leiden(adata, resolution=resolution)",
      "",
      "    print(f\"Found {adata.obs['leiden'].nunique()} clusters\")",
      "",
      "    return adata",
      "",
      "",
      'if __name__ == "__main__":',
      '    print("scvi-tools model utilities")',
      '    print("\\nAvailable functions:")',
      '    print("  - prepare_adata: Standard data preparation (QC, HVG, layer setup)")',
      '    print("  - train_scvi: Train scVI or scANVI with sensible defaults")',
      '    print("  - evaluate_integration: Compute batch mixing and silhouette metrics")',
      '    print("  - get_marker_genes: Extract markers using scVI differential expression")',
      '    print("  - plot_training_history: Visualize training convergence")',
      '    print("  - save_results: Save model, data, and visualizations")',
      '    print("  - auto_select_model: Suggest best model for your data")',
      '    print("  - compare_integrations: Compare multiple integration embeddings")',
      '    print("  - quick_clustering: Quick clustering on latent representation")',
      "",
    ].join("\n"),
    "scripts/prepare_data.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Prepare AnnData for scvi-tools models.",
      "",
      "This script handles QC filtering, HVG selection, and layer setup.",
      "Output is ready for any scvi-tools model.",
      "",
      "Usage:",
      "    python prepare_data.py input.h5ad output.h5ad --batch-key batch --n-hvgs 2000",
      "    python prepare_data.py input.h5ad output.h5ad --no-filter  # Skip QC filtering",
      '"""',
      "",
      "import argparse",
      "import sys",
      "",
      "",
      "def prepare_data(",
      "    adata,",
      "    batch_key=None,",
      "    n_top_genes=2000,",
      "    min_genes=200,",
      "    max_genes=5000,",
      "    max_mito_pct=20.0,",
      "    min_cells=3,",
      "    skip_filter=False",
      "):",
      '    """',
      "    Prepare AnnData for scvi-tools.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Raw count data",
      "    batch_key : str, optional",
      "        Batch column for batch-aware HVG selection",
      "    n_top_genes : int",
      "        Number of highly variable genes",
      "    min_genes : int",
      "        Minimum genes per cell",
      "    max_genes : int",
      "        Maximum genes per cell",
      "    max_mito_pct : float",
      "        Maximum mitochondrial percentage",
      "    min_cells : int",
      "        Minimum cells per gene",
      "    skip_filter : bool",
      "        Skip QC filtering (use if already filtered)",
      "",
      "    Returns",
      "    -------",
      "    AnnData prepared for scvi-tools",
      '    """',
      "    import scanpy as sc",
      "    import numpy as np",
      "    from model_utils import get_mito_genes",
      "",
      "    adata = adata.copy()",
      '    print(f"Input: {adata.shape[0]} cells, {adata.shape[1]} genes")',
      "",
      "    if not skip_filter:",
      "        # Calculate QC metrics",
      "        adata.var['mt'] = get_mito_genes(adata)",
      "        sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)",
      "",
      "        # Filter cells",
      "        n_before = adata.n_obs",
      "        adata = adata[adata.obs['n_genes_by_counts'] >= min_genes].copy()",
      "        adata = adata[adata.obs['n_genes_by_counts'] <= max_genes].copy()",
      "        adata = adata[adata.obs['pct_counts_mt'] < max_mito_pct].copy()",
      '        print(f"Filtered cells: {n_before} → {adata.n_obs}")',
      "",
      "        # Filter genes",
      "        n_genes_before = adata.n_vars",
      "        sc.pp.filter_genes(adata, min_cells=min_cells)",
      '        print(f"Filtered genes: {n_genes_before} → {adata.n_vars}")',
      "",
      "    # Store raw counts in layer",
      '    adata.layers["counts"] = adata.X.copy()',
      "",
      "    # HVG selection",
      "    if batch_key is not None and batch_key in adata.obs.columns:",
      '        print(f"Selecting {n_top_genes} HVGs (batch-aware: {batch_key})")',
      "        sc.pp.highly_variable_genes(",
      "            adata,",
      "            n_top_genes=n_top_genes,",
      '            flavor="seurat_v3",',
      "            batch_key=batch_key,",
      '            layer="counts"',
      "        )",
      "    else:",
      '        print(f"Selecting {n_top_genes} HVGs")',
      "        # Need to normalize for non-seurat_v3",
      "        sc.pp.normalize_total(adata, target_sum=1e4)",
      "        sc.pp.log1p(adata)",
      "        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes)",
      "        # Restore counts to X",
      '        adata.X = adata.layers["counts"].copy()',
      "",
      "    # Subset to HVGs",
      "    n_hvg = adata.var['highly_variable'].sum()",
      "    adata = adata[:, adata.var['highly_variable']].copy()",
      '    print(f"Selected {n_hvg} highly variable genes")',
      "",
      '    print(f"Output: {adata.shape[0]} cells, {adata.shape[1]} genes")',
      "",
      "    return adata",
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Prepare AnnData for scvi-tools",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # Basic preparation",
      "    python prepare_data.py raw.h5ad prepared.h5ad",
      "",
      "    # With batch-aware HVG selection",
      "    python prepare_data.py raw.h5ad prepared.h5ad --batch-key sample",
      "",
      "    # Custom parameters",
      "    python prepare_data.py raw.h5ad prepared.h5ad --n-hvgs 3000 --max-mito 15",
      "",
      "    # Skip filtering (data already QC'd)",
      "    python prepare_data.py filtered.h5ad prepared.h5ad --no-filter",
      '        """',
      "    )",
      '    parser.add_argument("input", help="Input h5ad file")',
      '    parser.add_argument("output", help="Output h5ad file")',
      '    parser.add_argument("--batch-key", help="Batch column for HVG selection")',
      '    parser.add_argument("--n-hvgs", type=int, default=2000, help="Number of HVGs (default: 2000)")',
      '    parser.add_argument("--min-genes", type=int, default=200, help="Min genes per cell (default: 200)")',
      '    parser.add_argument("--max-genes", type=int, default=5000, help="Max genes per cell (default: 5000)")',
      '    parser.add_argument("--max-mito", type=float, default=20.0, help="Max mito %% (default: 20)")',
      '    parser.add_argument("--min-cells", type=int, default=3, help="Min cells per gene (default: 3)")',
      '    parser.add_argument("--no-filter", action="store_true", help="Skip QC filtering")',
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scanpy required. Install with: pip install scanpy")',
      "        sys.exit(1)",
      "",
      "    # Load data",
      '    print(f"Loading {args.input}...")',
      "    adata = sc.read_h5ad(args.input)",
      "",
      "    # Prepare",
      "    adata = prepare_data(",
      "        adata,",
      "        batch_key=args.batch_key,",
      "        n_top_genes=args.n_hvgs,",
      "        min_genes=args.min_genes,",
      "        max_genes=args.max_genes,",
      "        max_mito_pct=args.max_mito,",
      "        min_cells=args.min_cells,",
      "        skip_filter=args.no_filter",
      "    )",
      "",
      "    # Save",
      '    print(f"Saving to {args.output}...")',
      "    adata.write_h5ad(args.output)",
      '    print("Done!")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/train_model.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Train scvi-tools models.",
      "",
      "Supports scVI, scANVI, totalVI, PeakVI, and other models.",
      "Input should be prepared with prepare_data.py or equivalent.",
      "",
      "Usage:",
      "    python train_model.py input.h5ad output_dir/ --model scvi --batch-key batch",
      "    python train_model.py input.h5ad output_dir/ --model scanvi --batch-key batch --labels-key cell_type",
      '"""',
      "",
      "import argparse",
      "import os",
      "import sys",
      "",
      "",
      "def train_scvi(adata, batch_key=None, n_latent=30, n_layers=2, max_epochs=200):",
      '    """Train scVI model."""',
      "    import scvi",
      "",
      "    scvi.model.SCVI.setup_anndata(",
      "        adata,",
      '        layer="counts",',
      "        batch_key=batch_key",
      "    )",
      "",
      "    model = scvi.model.SCVI(",
      "        adata,",
      "        n_latent=n_latent,",
      "        n_layers=n_layers",
      "    )",
      "",
      "    model.train(",
      "        max_epochs=max_epochs,",
      "        early_stopping=True,",
      "        early_stopping_patience=10",
      "    )",
      "",
      '    adata.obsm["X_scVI"] = model.get_latent_representation()',
      '    return model, "X_scVI"',
      "",
      "",
      "def train_scanvi(adata, batch_key=None, labels_key=None, n_latent=30, n_layers=2, max_epochs=200):",
      '    """Train scANVI model (scVI + labels)."""',
      "    import scvi",
      "",
      "    # First train scVI",
      "    scvi.model.SCVI.setup_anndata(",
      "        adata,",
      '        layer="counts",',
      "        batch_key=batch_key",
      "    )",
      "",
      "    scvi_model = scvi.model.SCVI(",
      "        adata,",
      "        n_latent=n_latent,",
      "        n_layers=n_layers",
      "    )",
      "    scvi_model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      "    # Initialize scANVI from scVI",
      "    model = scvi.model.SCANVI.from_scvi_model(",
      "        scvi_model,",
      "        labels_key=labels_key,",
      '        unlabeled_category="Unknown"',
      "    )",
      "",
      "    # Fine-tune scANVI",
      "    model.train(max_epochs=max_epochs // 4)",
      "",
      '    adata.obsm["X_scANVI"] = model.get_latent_representation()',
      '    return model, "X_scANVI"',
      "",
      "",
      'def train_totalvi(adata, batch_key=None, protein_key="protein_expression", n_latent=20, max_epochs=200):',
      '    """Train totalVI model for CITE-seq."""',
      "    import scvi",
      "    import numpy as np",
      "",
      "    scvi.model.TOTALVI.setup_anndata(",
      "        adata,",
      '        layer="counts",',
      "        batch_key=batch_key,",
      "        protein_expression_obsm_key=protein_key",
      "    )",
      "",
      "    model = scvi.model.TOTALVI(",
      "        adata,",
      "        n_latent=n_latent",
      "    )",
      "",
      "    model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      '    adata.obsm["X_totalVI"] = model.get_latent_representation()',
      "",
      "    # Also get denoised protein - convert to numpy array for h5ad compatibility",
      "    _, protein_denoised = model.get_normalized_expression(return_mean=True)",
      "    if hasattr(protein_denoised, 'values'):",
      '        adata.obsm["protein_denoised"] = protein_denoised.values',
      "    else:",
      '        adata.obsm["protein_denoised"] = np.array(protein_denoised)',
      "",
      '    return model, "X_totalVI"',
      "",
      "",
      "def train_peakvi(adata, batch_key=None, n_latent=20, max_epochs=200):",
      '    """Train PeakVI model for scATAC-seq."""',
      "    import scvi",
      "    import numpy as np",
      "",
      "    # Binarize if not already",
      "    if adata.X.max() > 1:",
      '        print("Binarizing ATAC data...")',
      "        adata.X = (adata.X > 0).astype(np.float32)",
      "",
      "    scvi.model.PEAKVI.setup_anndata(",
      "        adata,",
      "        batch_key=batch_key",
      "    )",
      "",
      "    model = scvi.model.PEAKVI(",
      "        adata,",
      "        n_latent=n_latent",
      "    )",
      "",
      "    model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      '    adata.obsm["X_PeakVI"] = model.get_latent_representation()',
      '    return model, "X_PeakVI"',
      "",
      "",
      "def train_velovi(adata, max_epochs=500):",
      '    """Train veloVI model for RNA velocity.',
      "",
      "    Note: Requires scvelo preprocessing. If Ms/Mu layers don't exist,",
      "    will run preprocessing automatically.",
      '    """',
      "    import scvi",
      "    import scvelo as scv",
      "",
      "    # Check if data needs preprocessing",
      '    if "Ms" not in adata.layers or "Mu" not in adata.layers:',
      '        print("Preprocessing data for veloVI (scvelo moments)...")',
      "",
      "        # Filter and normalize",
      "        scv.pp.filter_and_normalize(adata, min_shared_counts=30, n_top_genes=2000)",
      "",
      "        # Calculate moments (creates Ms, Mu layers)",
      "        scv.pp.moments(adata, n_pcs=30, n_neighbors=30)",
      "",
      '        print(f"After preprocessing: {adata.shape}")',
      "",
      "    # VELOVI is in scvi.external, not scvi.model",
      "    scvi.external.VELOVI.setup_anndata(",
      "        adata,",
      '        spliced_layer="Ms",',
      '        unspliced_layer="Mu"',
      "    )",
      "",
      "    model = scvi.external.VELOVI(adata)",
      "    model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      "    # Get latent representation (cells x latent_dim)",
      '    adata.obsm["X_veloVI"] = model.get_latent_representation()',
      "",
      "    # Get velocity (cells x genes)",
      '    adata.layers["velocity"] = model.get_velocity()',
      "",
      "    # Get latent time per gene (cells x genes) - store mean across genes as summary",
      "    latent_time_df = model.get_latent_time()",
      '    adata.obs["latent_time_mean"] = latent_time_df.mean(axis=1).values',
      "",
      '    return model, "X_veloVI"',
      "",
      "",
      "def train_multivi(adata, batch_key=None, n_latent=20, max_epochs=300):",
      '    """Train MultiVI model for multiome (RNA + ATAC).',
      "",
      "    Note: Expects MuData or AnnData with both RNA and ATAC data.",
      "    For AnnData, ATAC peaks should be concatenated with genes,",
      "    or use MuData format.",
      '    """',
      "    import scvi",
      "    import numpy as np",
      "",
      "    # Check if this is MuData",
      "    try:",
      "        import mudata as md",
      "        if isinstance(adata, md.MuData):",
      "            # Setup for MuData",
      "            scvi.model.MULTIVI.setup_mudata(",
      "                adata,",
      '                rna_layer="counts",',
      '                atac_layer="counts",',
      "                batch_key=batch_key,",
      "                modalities={",
      '                    "rna_layer": "rna",',
      '                    "batch_key": "rna",',
      '                    "atac_layer": "atac"',
      "                }",
      "            )",
      "        else:",
      "            raise ValueError(\"MultiVI requires MuData format with 'rna' and 'atac' modalities\")",
      "    except ImportError:",
      '        raise ImportError("MultiVI requires mudata. Install with: pip install mudata")',
      "",
      "    model = scvi.model.MULTIVI(",
      "        adata,",
      "        n_latent=n_latent",
      "    )",
      "",
      "    model.train(max_epochs=max_epochs, early_stopping=True)",
      "",
      "    # Get latent representation",
      "    latent = model.get_latent_representation()",
      '    adata.obsm["X_MultiVI"] = latent',
      "",
      '    return model, "X_MultiVI"',
      "",
      "",
      "MODELS = {",
      '    "scvi": train_scvi,',
      '    "scanvi": train_scanvi,',
      '    "totalvi": train_totalvi,',
      '    "peakvi": train_peakvi,',
      '    "velovi": train_velovi,',
      '    "multivi": train_multivi,',
      "}",
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Train scvi-tools models",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # Train scVI for batch correction",
      "    python train_model.py prepared.h5ad results/ --model scvi --batch-key batch",
      "",
      "    # Train scANVI with cell type labels",
      "    python train_model.py prepared.h5ad results/ --model scanvi --batch-key batch --labels-key cell_type",
      "",
      "    # Train totalVI for CITE-seq",
      "    python train_model.py citeseq.h5ad results/ --model totalvi --batch-key batch",
      "",
      "    # Train PeakVI for ATAC-seq",
      "    python train_model.py atac.h5ad results/ --model peakvi",
      "",
      "    # Train veloVI for RNA velocity",
      "    python train_model.py velocity.h5ad results/ --model velovi",
      "",
      "    # Train MultiVI for multiome (RNA + ATAC) - requires MuData format",
      "    python train_model.py multiome.h5mu results/ --model multivi --batch-key batch",
      '        """',
      "    )",
      '    parser.add_argument("input", help="Input h5ad file (prepared)")',
      '    parser.add_argument("output_dir", help="Output directory for model and results")',
      '    parser.add_argument("--model", choices=list(MODELS.keys()), default="scvi",',
      '                        help="Model type (default: scvi)")',
      '    parser.add_argument("--batch-key", help="Batch column in obs")',
      '    parser.add_argument("--labels-key", help="Labels column (required for scanvi)")',
      '    parser.add_argument("--protein-key", default="protein_expression",',
      '                        help="Protein obsm key for totalvi")',
      '    parser.add_argument("--n-latent", type=int, default=30, help="Latent dimensions (default: 30)")',
      '    parser.add_argument("--n-layers", type=int, default=2, help="Encoder/decoder layers (default: 2)")',
      '    parser.add_argument("--max-epochs", type=int, default=200, help="Max training epochs (default: 200)")',
      "",
      "    args = parser.parse_args()",
      "",
      "    # Validate",
      '    if args.model == "scanvi" and args.labels_key is None:',
      '        print("Error: --labels-key required for scanvi model")',
      "        sys.exit(1)",
      "",
      "    try:",
      "        import scvi",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scvi-tools and scanpy required")',
      "        sys.exit(1)",
      "",
      "    # Create output directory",
      "    os.makedirs(args.output_dir, exist_ok=True)",
      "",
      "    # Load data",
      '    print(f"Loading {args.input}...")',
      "    if args.input.endswith('.h5mu') or args.model == \"multivi\":",
      "        try:",
      "            import mudata as md",
      "            adata = md.read(args.input)",
      '            print(f"MuData: {adata.n_obs} cells")',
      "            for mod_name, mod in adata.mod.items():",
      '                print(f"  {mod_name}: {mod.shape}")',
      "        except ImportError:",
      '            print("Error: mudata required for .h5mu files. Install with: pip install mudata")',
      "            sys.exit(1)",
      "    else:",
      "        adata = sc.read_h5ad(args.input)",
      '        print(f"Data: {adata.shape}")',
      "",
      "    # Check for counts layer",
      '    if "counts" not in adata.layers:',
      "        print(\"Warning: 'counts' layer not found, using X\")",
      '        adata.layers["counts"] = adata.X.copy()',
      "",
      "    # Train model",
      '    print(f"\\nTraining {args.model.upper()}...")',
      "",
      '    if args.model == "scvi":',
      "        model, rep_key = train_scvi(",
      "            adata, args.batch_key, args.n_latent, args.n_layers, args.max_epochs",
      "        )",
      '    elif args.model == "scanvi":',
      "        model, rep_key = train_scanvi(",
      "            adata, args.batch_key, args.labels_key, args.n_latent, args.n_layers, args.max_epochs",
      "        )",
      '    elif args.model == "totalvi":',
      "        model, rep_key = train_totalvi(",
      "            adata, args.batch_key, args.protein_key, args.n_latent, args.max_epochs",
      "        )",
      '    elif args.model == "peakvi":',
      "        model, rep_key = train_peakvi(",
      "            adata, args.batch_key, args.n_latent, args.max_epochs",
      "        )",
      '    elif args.model == "velovi":',
      "        model, rep_key = train_velovi(adata, args.max_epochs)",
      '    elif args.model == "multivi":',
      "        model, rep_key = train_multivi(adata, args.batch_key, args.n_latent, args.max_epochs)",
      "",
      '    print("Training complete!")',
      "",
      "    # Save model",
      '    model_path = os.path.join(args.output_dir, "model")',
      "    model.save(model_path)",
      '    print(f"Model saved to {model_path}")',
      "",
      "    # Save adata with latent representation",
      '    adata_path = os.path.join(args.output_dir, "adata_trained.h5ad")',
      "    adata.write_h5ad(adata_path)",
      '    print(f"AnnData saved to {adata_path}")',
      "",
      "    # Save training history plot",
      "    try:",
      "        import matplotlib.pyplot as plt",
      "",
      "        fig, ax = plt.subplots(figsize=(8, 4))",
      '        if "elbo_train" in model.history:',
      '            ax.plot(model.history["elbo_train"], label="Train")',
      '        if "elbo_validation" in model.history:',
      '            ax.plot(model.history["elbo_validation"], label="Validation")',
      '        ax.set_xlabel("Epoch")',
      '        ax.set_ylabel("ELBO")',
      "        ax.legend()",
      '        ax.set_title(f"{args.model.upper()} Training History")',
      "",
      '        plot_path = os.path.join(args.output_dir, "training_history.png")',
      '        plt.savefig(plot_path, dpi=150, bbox_inches="tight")',
      "        plt.close()",
      '        print(f"Training plot saved to {plot_path}")',
      "    except Exception as e:",
      '        print(f"Could not save training plot: {e}")',
      "",
      '    print("\\nDone! Next steps:")',
      '    print(f"  - Run clustering: python cluster_embed.py {adata_path} {args.output_dir}")',
      "    print(f\"  - Load model: scvi.model.{args.model.upper()}.load('{model_path}')\")",
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/transfer_labels.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Transfer cell type labels from reference to query using scANVI.",
      "",
      "Maps query cells to a pre-trained reference model and predicts cell types.",
      "",
      "Usage:",
      "    python transfer_labels.py reference_model/ query.h5ad output_dir/",
      "    python transfer_labels.py reference_model/ query.h5ad output_dir/ --confidence 0.7",
      '"""',
      "",
      "import argparse",
      "import os",
      "import sys",
      "",
      "",
      "def transfer_labels(",
      "    reference_model,",
      "    adata_query,",
      "    max_epochs=100,",
      "    confidence_threshold=0.5",
      "):",
      '    """',
      "    Transfer labels from reference to query.",
      "",
      "    Parameters",
      "    ----------",
      "    reference_model : SCANVI model",
      "        Pre-trained scANVI model",
      "    adata_query : AnnData",
      "        Query data to annotate",
      "    max_epochs : int",
      "        Fine-tuning epochs",
      "    confidence_threshold : float",
      "        Minimum confidence for predictions",
      "",
      "    Returns",
      "    -------",
      "    AnnData with predictions",
      '    """',
      "    import scvi",
      "    import numpy as np",
      "",
      "    # Get reference genes",
      "    ref_genes = reference_model.adata.var_names",
      '    print(f"Reference genes: {len(ref_genes)}")',
      "",
      "    # Check gene overlap",
      "    query_genes = adata_query.var_names",
      "    common = ref_genes.intersection(query_genes)",
      '    print(f"Query genes: {len(query_genes)}")',
      '    print(f"Common genes: {len(common)} ({len(common)/len(ref_genes)*100:.1f}%)")',
      "",
      "    if len(common) < len(ref_genes) * 0.5:",
      '        print("Warning: Less than 50% gene overlap. Results may be unreliable.")',
      "",
      "    # Subset query to reference genes",
      "    # Missing genes will be filled with zeros",
      "    adata_query = adata_query[:, adata_query.var_names.isin(ref_genes)].copy()",
      "",
      "    # Ensure counts layer",
      '    if "counts" not in adata_query.layers:',
      '        adata_query.layers["counts"] = adata_query.X.copy()',
      "",
      "    # Prepare query for mapping",
      '    print("Preparing query data...")',
      "    scvi.model.SCANVI.prepare_query_anndata(adata_query, reference_model)",
      "",
      "    # Create query model",
      '    print("Creating query model...")',
      "    query_model = scvi.model.SCANVI.load_query_data(",
      "        adata_query,",
      "        reference_model",
      "    )",
      "",
      "    # Fine-tune",
      '    print(f"Fine-tuning ({max_epochs} epochs)...")',
      "    query_model.train(",
      "        max_epochs=max_epochs,",
      '        plan_kwargs={"weight_decay": 0.0}',
      "    )",
      "",
      "    # Get predictions",
      '    print("Getting predictions...")',
      "    predictions = query_model.predict()",
      "    soft_predictions = query_model.predict(soft=True)",
      "",
      '    adata_query.obs["predicted_cell_type"] = predictions',
      '    adata_query.obs["prediction_confidence"] = soft_predictions.max(axis=1)',
      '    adata_query.obs["confident_prediction"] = adata_query.obs["prediction_confidence"] >= confidence_threshold',
      "",
      "    # Get latent representation",
      '    adata_query.obsm["X_scANVI"] = query_model.get_latent_representation()',
      "",
      "    # Stats",
      '    n_confident = adata_query.obs["confident_prediction"].sum()',
      '    print(f"\\nPrediction summary:")',
      '    print(f"  Total cells: {adata_query.n_obs}")',
      '    print(f"  Confident (>= {confidence_threshold}): {n_confident} ({n_confident/adata_query.n_obs*100:.1f}%)")',
      "    print(f\"  Mean confidence: {adata_query.obs['prediction_confidence'].mean():.3f}\")",
      "",
      '    print("\\nPredicted cell types:")',
      '    print(adata_query.obs["predicted_cell_type"].value_counts())',
      "",
      "    return adata_query, query_model",
      "",
      "",
      "def plot_predictions(adata, output_dir):",
      '    """Plot prediction results."""',
      "    import scanpy as sc",
      "    import matplotlib.pyplot as plt",
      "",
      "    # Compute UMAP if needed",
      '    if "X_umap" not in adata.obsm:',
      '        sc.pp.neighbors(adata, use_rep="X_scANVI")',
      "        sc.tl.umap(adata)",
      "",
      "    # Plot",
      "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))",
      "",
      '    sc.pl.umap(adata, color="predicted_cell_type", ax=axes[0], show=False,',
      '               title="Predicted Cell Type")',
      '    sc.pl.umap(adata, color="prediction_confidence", ax=axes[1], show=False,',
      '               title="Prediction Confidence", cmap="viridis")',
      '    sc.pl.umap(adata, color="confident_prediction", ax=axes[2], show=False,',
      '               title="Confident Predictions")',
      "",
      "    plt.tight_layout()",
      '    plot_path = os.path.join(output_dir, "predictions.png")',
      '    plt.savefig(plot_path, dpi=150, bbox_inches="tight")',
      "    plt.close()",
      '    print(f"Prediction plot saved to {plot_path}")',
      "",
      "",
      "def main():",
      "    parser = argparse.ArgumentParser(",
      '        description="Transfer cell type labels using scANVI",',
      "        formatter_class=argparse.RawDescriptionHelpFormatter,",
      '        epilog="""',
      "Examples:",
      "    # Basic label transfer",
      "    python transfer_labels.py reference_model/ query.h5ad results/",
      "",
      "    # With confidence threshold",
      "    python transfer_labels.py reference_model/ query.h5ad results/ --confidence 0.7",
      "",
      "    # More fine-tuning",
      "    python transfer_labels.py reference_model/ query.h5ad results/ --max-epochs 200",
      '        """',
      "    )",
      '    parser.add_argument("model_dir", help="Directory containing reference scANVI model")',
      '    parser.add_argument("query", help="Query h5ad file to annotate")',
      '    parser.add_argument("output_dir", help="Output directory")',
      '    parser.add_argument("--reference-adata", help="Reference adata used for training (if not saved with model)")',
      '    parser.add_argument("--max-epochs", type=int, default=100,',
      '                        help="Fine-tuning epochs (default: 100)")',
      '    parser.add_argument("--confidence", type=float, default=0.5,',
      '                        help="Confidence threshold (default: 0.5)")',
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        import scvi",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scvi-tools and scanpy required")',
      "        sys.exit(1)",
      "",
      "    # Create output directory",
      "    os.makedirs(args.output_dir, exist_ok=True)",
      "",
      "    # Load query data",
      '    print(f"Loading query data: {args.query}")',
      "    adata_query = sc.read_h5ad(args.query)",
      '    print(f"Query: {adata_query.shape}")',
      "",
      "    # Load reference model",
      '    print(f"Loading reference model: {args.model_dir}")',
      "    if args.reference_adata:",
      "        ref_adata = sc.read_h5ad(args.reference_adata)",
      "        reference_model = scvi.model.SCANVI.load(args.model_dir, adata=ref_adata)",
      "    else:",
      "        # Try loading without adata (works if model was saved with adata)",
      "        try:",
      "            reference_model = scvi.model.SCANVI.load(args.model_dir)",
      "        except ValueError as e:",
      '            if "no saved anndata" in str(e).lower():',
      '                print("Error: Model was saved without adata. Please provide --reference-adata")',
      "                sys.exit(1)",
      "            raise",
      '    print(f"Reference: {reference_model.adata.shape}")',
      "",
      "    # Transfer labels",
      "    adata_annotated, query_model = transfer_labels(",
      "        reference_model,",
      "        adata_query,",
      "        max_epochs=args.max_epochs,",
      "        confidence_threshold=args.confidence",
      "    )",
      "",
      "    # Save results",
      '    adata_path = os.path.join(args.output_dir, "query_annotated.h5ad")',
      "    adata_annotated.write_h5ad(adata_path)",
      '    print(f"\\nAnnotated data saved to {adata_path}")',
      "",
      "    # Save query model",
      '    model_path = os.path.join(args.output_dir, "query_model")',
      "    query_model.save(model_path)",
      '    print(f"Query model saved to {model_path}")',
      "",
      "    # Save predictions CSV",
      '    pred_df = adata_annotated.obs[["predicted_cell_type", "prediction_confidence", "confident_prediction"]]',
      '    pred_path = os.path.join(args.output_dir, "predictions.csv")',
      "    pred_df.to_csv(pred_path)",
      '    print(f"Predictions saved to {pred_path}")',
      "",
      "    # Plot",
      "    plot_predictions(adata_annotated, args.output_dir)",
      "",
      '    print("\\nDone!")',
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "scripts/validate_adata.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Validation utilities for checking AnnData compatibility with scvi-tools.",
      "",
      "Usage:",
      "    python validate_adata.py data.h5ad",
      "",
      "    # Or import as module",
      "    from validate_adata import validate_for_scvi, ValidationResult",
      "    result = validate_for_scvi(adata)",
      "    print(result.summary())",
      '"""',
      "",
      "import argparse",
      "import sys",
      "from dataclasses import dataclass, field",
      "from typing import Optional, List, Dict, Any",
      "import warnings",
      "",
      "",
      "@dataclass",
      "class ValidationResult:",
      '    """Results from AnnData validation."""',
      "",
      "    is_valid: bool = True",
      "    errors: List[str] = field(default_factory=list)",
      "    warnings: List[str] = field(default_factory=list)",
      "    info: Dict[str, Any] = field(default_factory=dict)",
      "    recommendations: List[str] = field(default_factory=list)",
      "",
      "    def add_error(self, msg: str):",
      '        """Add an error (makes validation fail)."""',
      "        self.errors.append(msg)",
      "        self.is_valid = False",
      "",
      "    def add_warning(self, msg: str):",
      '        """Add a warning (doesn\'t fail validation)."""',
      "        self.warnings.append(msg)",
      "",
      "    def add_recommendation(self, msg: str):",
      '        """Add a recommendation for improvement."""',
      "        self.recommendations.append(msg)",
      "",
      "    def summary(self) -> str:",
      '        """Generate summary report."""',
      "        lines = []",
      '        lines.append("=" * 60)',
      '        lines.append("scvi-tools AnnData Validation Report")',
      '        lines.append("=" * 60)',
      "",
      "        # Status",
      '        status = "PASSED" if self.is_valid else "FAILED"',
      '        lines.append(f"\\nStatus: {status}")',
      "",
      "        # Info",
      "        if self.info:",
      '            lines.append("\\n--- Data Summary ---")',
      "            for key, value in self.info.items():",
      '                lines.append(f"  {key}: {value}")',
      "",
      "        # Errors",
      "        if self.errors:",
      '            lines.append(f"\\n--- Errors ({len(self.errors)}) ---")',
      "            for i, err in enumerate(self.errors, 1):",
      '                lines.append(f"  {i}. {err}")',
      "",
      "        # Warnings",
      "        if self.warnings:",
      '            lines.append(f"\\n--- Warnings ({len(self.warnings)}) ---")',
      "            for i, warn in enumerate(self.warnings, 1):",
      '                lines.append(f"  {i}. {warn}")',
      "",
      "        # Recommendations",
      "        if self.recommendations:",
      '            lines.append(f"\\n--- Recommendations ({len(self.recommendations)}) ---")',
      "            for i, rec in enumerate(self.recommendations, 1):",
      '                lines.append(f"  {i}. {rec}")',
      "",
      '        lines.append("\\n" + "=" * 60)',
      '        return "\\n".join(lines)',
      "",
      "",
      "def validate_for_scvi(",
      "    adata,",
      "    layer: Optional[str] = None,",
      "    batch_key: Optional[str] = None,",
      "    labels_key: Optional[str] = None,",
      "    check_hvg: bool = True",
      ") -> ValidationResult:",
      '    """',
      "    Validate AnnData for scvi-tools compatibility.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data to validate",
      "    layer : str, optional",
      "        Layer containing counts (if None, checks X)",
      "    batch_key : str, optional",
      "        Expected batch column in obs",
      "    labels_key : str, optional",
      "        Expected labels column in obs",
      "    check_hvg : bool",
      "        Check for highly variable genes",
      "",
      "    Returns",
      "    -------",
      "    ValidationResult with errors, warnings, and recommendations",
      '    """',
      "    import numpy as np",
      "    from scipy.sparse import issparse",
      "",
      "    result = ValidationResult()",
      "",
      "    # Basic info",
      '    result.info["shape"] = f"{adata.n_obs} cells x {adata.n_vars} genes"',
      '    result.info["layers"] = list(adata.layers.keys()) if adata.layers else "None"',
      "",
      "    # Get data matrix to check",
      "    if layer is not None:",
      "        if layer not in adata.layers:",
      "            result.add_error(f\"Layer '{layer}' not found. Available: {list(adata.layers.keys())}\")",
      "            return result",
      "        X = adata.layers[layer]",
      '        result.info["checking"] = f"layer \'{layer}\'"',
      "    else:",
      "        X = adata.X",
      '        result.info["checking"] = "adata.X"',
      "",
      "    # Check for None or empty",
      "    if X is None:",
      '        result.add_error("Data matrix is None")',
      "        return result",
      "",
      "    if X.shape[0] == 0 or X.shape[1] == 0:",
      '        result.add_error(f"Data matrix is empty: shape {X.shape}")',
      "        return result",
      "",
      "    # Convert to array for checking",
      "    if issparse(X):",
      '        result.info["sparse"] = True',
      "        X_check = X.data  # Just check non-zero values",
      "    else:",
      '        result.info["sparse"] = False',
      "        X_check = X.flatten()",
      "",
      "    # Check for raw counts (integers)",
      "    if len(X_check) > 0:",
      "        is_integer = np.allclose(X_check, X_check.astype(int))",
      '        result.info["contains_integers"] = is_integer',
      "",
      "        if not is_integer:",
      "            result.add_error(",
      '                "Data does not contain integers (raw counts required). "',
      '                "Found float values - data may be normalized."',
      "            )",
      "            result.add_recommendation(",
      '                "Use adata.raw.to_adata() to recover raw counts, "',
      '                "or specify a layer with raw counts"',
      "            )",
      "",
      "    # Check for negative values",
      "    min_val = X.min()",
      "    if min_val < 0:",
      '        result.add_error(f"Data contains negative values (min={min_val})")',
      "",
      "    # Check for NaN/Inf",
      "    if issparse(X):",
      "        has_nan = np.isnan(X.data).any()",
      "        has_inf = np.isinf(X.data).any()",
      "    else:",
      "        has_nan = np.isnan(X).any()",
      "        has_inf = np.isinf(X).any()",
      "",
      "    if has_nan:",
      '        result.add_error("Data contains NaN values")',
      "    if has_inf:",
      '        result.add_error("Data contains Inf values")',
      "",
      "    # Check data range",
      "    max_val = X.max()",
      '    result.info["value_range"] = f"[{min_val}, {max_val}]"',
      "",
      "    if max_val < 10:",
      "        result.add_warning(",
      '            f"Maximum value is {max_val}, which is very low. "',
      '            "Data may be log-transformed or normalized."',
      "        )",
      "",
      "    # Check sparsity",
      "    if issparse(X):",
      "        sparsity = 1 - (X.nnz / (X.shape[0] * X.shape[1]))",
      '        result.info["sparsity"] = f"{sparsity:.1%}"',
      "",
      "        if sparsity < 0.5:",
      "            result.add_warning(",
      '                f"Data is only {sparsity:.1%} sparse. "',
      '                "Consider if this is expected for your data type."',
      "            )",
      "",
      "    # Check batch key",
      "    if batch_key is not None:",
      "        if batch_key not in adata.obs.columns:",
      "            result.add_error(",
      "                f\"batch_key '{batch_key}' not found in obs. \"",
      '                f"Available columns: {list(adata.obs.columns)}"',
      "            )",
      "        else:",
      "            n_batches = adata.obs[batch_key].nunique()",
      '            result.info["n_batches"] = n_batches',
      "",
      "            if n_batches == 1:",
      "                result.add_warning(",
      '                    "Only 1 batch found. Batch correction may not be needed."',
      "                )",
      "",
      "            # Check for small batches",
      "            batch_counts = adata.obs[batch_key].value_counts()",
      "            small_batches = batch_counts[batch_counts < 50]",
      "            if len(small_batches) > 0:",
      "                result.add_warning(",
      '                    f"{len(small_batches)} batches have fewer than 50 cells. "',
      '                    "Consider merging small batches."',
      "                )",
      "",
      "    # Check labels key",
      "    if labels_key is not None:",
      "        if labels_key not in adata.obs.columns:",
      "            result.add_error(",
      "                f\"labels_key '{labels_key}' not found in obs. \"",
      '                f"Available columns: {list(adata.obs.columns)}"',
      "            )",
      "        else:",
      "            n_labels = adata.obs[labels_key].nunique()",
      '            result.info["n_labels"] = n_labels',
      "",
      "            # Check for rare labels",
      "            label_counts = adata.obs[labels_key].value_counts()",
      "            rare_labels = label_counts[label_counts < 30]",
      "            if len(rare_labels) > 0:",
      "                result.add_warning(",
      '                    f"{len(rare_labels)} cell types have fewer than 30 cells. "',
      '                    "Rare types may not be well learned."',
      "                )",
      "",
      "    # Check HVG",
      "    if check_hvg:",
      "        if 'highly_variable' not in adata.var.columns:",
      "            result.add_recommendation(",
      '                "No highly variable genes found. Run sc.pp.highly_variable_genes() "',
      '                "and subset to HVGs for better performance."',
      "            )",
      "        else:",
      "            n_hvg = adata.var['highly_variable'].sum()",
      '            result.info["n_hvg"] = n_hvg',
      "",
      "            if n_hvg < 1000:",
      "                result.add_warning(",
      '                    f"Only {n_hvg} HVGs selected. Consider using 2000-4000 for best results."',
      "                )",
      "            elif n_hvg > 5000:",
      "                result.add_warning(",
      '                    f"{n_hvg} HVGs selected. Consider reducing to 2000-4000 "',
      '                    "for efficiency."',
      "                )",
      "",
      "    # Check gene count",
      "    if adata.n_vars > 30000:",
      "        result.add_recommendation(",
      '            f"Dataset has {adata.n_vars} genes. Subset to HVGs (2000-4000) "',
      '            "for faster training and better results."',
      "        )",
      "",
      "    # Check cell count",
      "    if adata.n_obs < 1000:",
      "        result.add_warning(",
      '            f"Dataset has only {adata.n_obs} cells. "',
      '            "Deep learning models work best with >5000 cells."',
      "        )",
      "",
      "    # Check for counts layer",
      "    if layer is None and 'counts' not in adata.layers:",
      "        result.add_recommendation(",
      "            \"Store raw counts in adata.layers['counts'] before any normalization. \"",
      '            "This preserves the original data for scvi-tools."',
      "        )",
      "",
      "    # Check for raw attribute",
      "    if adata.raw is not None:",
      '        result.info["has_raw"] = True',
      "        result.add_recommendation(",
      '            "adata.raw exists. If X is normalized, use adata.raw.to_adata() "',
      '            "to recover raw counts."',
      "        )",
      "    else:",
      '        result.info["has_raw"] = False',
      "",
      "    return result",
      "",
      "",
      "def suggest_model(adata, result: ValidationResult) -> str:",
      '    """',
      "    Suggest appropriate scvi-tools model based on data.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Data to analyze",
      "    result : ValidationResult",
      "        Validation result with info",
      "",
      "    Returns",
      "    -------",
      "    String with model suggestion",
      '    """',
      "    suggestions = []",
      "",
      "    # Check for multi-modal data",
      "    if 'protein_expression' in adata.obsm:",
      '        suggestions.append("totalVI: CITE-seq data detected (protein + RNA)")',
      "",
      "    if 'spliced' in adata.layers and 'unspliced' in adata.layers:",
      '        suggestions.append("veloVI: RNA velocity data detected (spliced + unspliced)")',
      "",
      "    # Check for labels",
      "    has_labels = result.info.get('n_labels', 0) > 0",
      "    has_batches = result.info.get('n_batches', 0) > 1",
      "",
      "    if has_batches:",
      "        if has_labels:",
      "            suggestions.append(",
      '                "scANVI: Integration with cell type labels (recommended for label transfer)"',
      "            )",
      "        else:",
      "            suggestions.append(",
      '                "scVI: Unsupervised batch integration"',
      "            )",
      "    else:",
      "        suggestions.append(",
      '            "scVI: Dimensionality reduction and differential expression"',
      "        )",
      "",
      "    if not suggestions:",
      '        suggestions.append("scVI: General-purpose single-cell analysis")',
      "",
      '    return "\\n".join([f"  - {s}" for s in suggestions])',
      "",
      "",
      "def main():",
      '    """Command-line interface."""',
      "    parser = argparse.ArgumentParser(",
      '        description="Validate AnnData for scvi-tools compatibility"',
      "    )",
      '    parser.add_argument("file", help="Path to h5ad file")',
      '    parser.add_argument("--layer", help="Layer to check (default: X)")',
      '    parser.add_argument("--batch-key", help="Batch column to check")',
      '    parser.add_argument("--labels-key", help="Labels column to check")',
      '    parser.add_argument("--suggest", action="store_true", help="Suggest model type")',
      "",
      "    args = parser.parse_args()",
      "",
      "    try:",
      "        import scanpy as sc",
      "    except ImportError:",
      '        print("Error: scanpy is required. Install with: pip install scanpy")',
      "        sys.exit(1)",
      "",
      "    # Load data",
      '    print(f"Loading {args.file}...")',
      "    try:",
      "        adata = sc.read_h5ad(args.file)",
      "    except Exception as e:",
      '        print(f"Error loading file: {e}")',
      "        sys.exit(1)",
      "",
      "    # Validate",
      "    result = validate_for_scvi(",
      "        adata,",
      "        layer=args.layer,",
      "        batch_key=args.batch_key,",
      "        labels_key=args.labels_key",
      "    )",
      "",
      "    # Print report",
      "    print(result.summary())",
      "",
      "    # Suggest model",
      "    if args.suggest:",
      '        print("\\nSuggested models:")',
      "        print(suggest_model(adata, result))",
      "",
      "    # Exit code",
      "    sys.exit(0 if result.is_valid else 1)",
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: bio-research-scvi-tools",
      "description: Deep learning for single-cell analysis using scvi-tools. This skill should be used when users need (1) data integration and batch correction with scVI/scANVI, (2) ATAC-seq analysis with PeakVI, (3) CITE-seq multi-modal analysis with totalVI, (4) multiome RNA+ATAC analysis with MultiVI, (5) spatial transcriptomics deconvolution with DestVI, (6) label transfer and reference mapping with scANVI/scArches, (7) RNA velocity with veloVI, or (8) any deep learning-based single-cell method. Triggers include mentions of scVI, scANVI, totalVI, PeakVI, MultiVI, DestVI, veloVI, sysVI, scArches, variational autoencoder, VAE, batch correction, data integration, multi-modal, CITE-seq, multiome, reference mapping, latent space.",
      "metadata:",
      "  plugin: bio-research",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# scvi-tools Deep Learning Skill",
      "",
      "This skill provides guidance for deep learning-based single-cell analysis using scvi-tools, the leading framework for probabilistic models in single-cell genomics.",
      "",
      "## How to Use This Skill",
      "",
      "1. Identify the appropriate workflow from the model/workflow tables below",
      "2. Read the corresponding reference file for detailed steps and code",
      "3. Use scripts in `scripts/` to avoid rewriting common code",
      "4. For installation or GPU issues, consult `references/environment_setup.md`",
      "5. For debugging, consult `references/troubleshooting.md`",
      "",
      "## When to Use This Skill",
      "",
      "- When scvi-tools, scVI, scANVI, or related models are mentioned",
      "- When deep learning-based batch correction or integration is needed",
      "- When working with multi-modal data (CITE-seq, multiome)",
      "- When reference mapping or label transfer is required",
      "- When analyzing ATAC-seq or spatial transcriptomics data",
      "- When learning latent representations of single-cell data",
      "",
      "## Model Selection Guide",
      "",
      "| Data Type | Model | Primary Use Case |",
      "|-----------|-------|------------------|",
      "| scRNA-seq | **scVI** | Unsupervised integration, DE, imputation |",
      "| scRNA-seq + labels | **scANVI** | Label transfer, semi-supervised integration |",
      "| CITE-seq (RNA+protein) | **totalVI** | Multi-modal integration, protein denoising |",
      "| scATAC-seq | **PeakVI** | Chromatin accessibility analysis |",
      "| Multiome (RNA+ATAC) | **MultiVI** | Joint modality analysis |",
      "| Spatial + scRNA reference | **DestVI** | Cell type deconvolution |",
      "| RNA velocity | **veloVI** | Transcriptional dynamics |",
      "| Cross-technology | **sysVI** | System-level batch correction |",
      "",
      "## Workflow Reference Files",
      "",
      "| Workflow | Reference File | Description |",
      "|----------|---------------|-------------|",
      "| Environment Setup | `references/environment_setup.md` | Installation, GPU, version info |",
      "| Data Preparation | `references/data_preparation.md` | Formatting data for any model |",
      "| scRNA Integration | `references/scrna_integration.md` | scVI/scANVI batch correction |",
      "| ATAC-seq Analysis | `references/atac_peakvi.md` | PeakVI for accessibility |",
      "| CITE-seq Analysis | `references/citeseq_totalvi.md` | totalVI for protein+RNA |",
      "| Multiome Analysis | `references/multiome_multivi.md` | MultiVI for RNA+ATAC |",
      "| Spatial Deconvolution | `references/spatial_deconvolution.md` | DestVI spatial analysis |",
      "| Label Transfer | `references/label_transfer.md` | scANVI reference mapping |",
      "| scArches Mapping | `references/scarches_mapping.md` | Query-to-reference mapping |",
      "| Batch Correction | `references/batch_correction_sysvi.md` | Advanced batch methods |",
      "| RNA Velocity | `references/rna_velocity_velovi.md` | veloVI dynamics |",
      "| Troubleshooting | `references/troubleshooting.md` | Common issues and solutions |",
      "",
      "## CLI Scripts",
      "",
      "Modular scripts for common workflows. Chain together or modify as needed.",
      "",
      "### Pipeline Scripts",
      "",
      "| Script | Purpose | Usage |",
      "|--------|---------|-------|",
      "| `prepare_data.py` | QC, filter, HVG selection | `python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch` |",
      "| `train_model.py` | Train any scvi-tools model | `python scripts/train_model.py prepared.h5ad results/ --model scvi` |",
      "| `cluster_embed.py` | Neighbors, UMAP, Leiden | `python scripts/cluster_embed.py adata.h5ad results/` |",
      "| `differential_expression.py` | DE analysis | `python scripts/differential_expression.py model/ adata.h5ad de.csv --groupby leiden` |",
      "| `transfer_labels.py` | Label transfer with scANVI | `python scripts/transfer_labels.py ref_model/ query.h5ad results/` |",
      "| `integrate_datasets.py` | Multi-dataset integration | `python scripts/integrate_datasets.py results/ data1.h5ad data2.h5ad` |",
      "| `validate_adata.py` | Check data compatibility | `python scripts/validate_adata.py data.h5ad --batch-key batch` |",
      "",
      "### Example Workflow",
      "",
      "```bash",
      "# 1. Validate input data",
      "python scripts/validate_adata.py raw.h5ad --batch-key batch --suggest",
      "",
      "# 2. Prepare data (QC, HVG selection)",
      "python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch --n-hvgs 2000",
      "",
      "# 3. Train model",
      "python scripts/train_model.py prepared.h5ad results/ --model scvi --batch-key batch",
      "",
      "# 4. Cluster and visualize",
      "python scripts/cluster_embed.py results/adata_trained.h5ad results/ --resolution 0.8",
      "",
      "# 5. Differential expression",
      "python scripts/differential_expression.py results/model results/adata_clustered.h5ad results/de.csv --groupby leiden",
      "```",
      "",
      "### Python Utilities",
      "",
      "The `scripts/model_utils.py` provides importable functions for custom workflows:",
      "",
      "| Function | Purpose |",
      "|----------|---------|",
      "| `prepare_adata()` | Data preparation (QC, HVG, layer setup) |",
      "| `train_scvi()` | Train scVI or scANVI |",
      "| `evaluate_integration()` | Compute integration metrics |",
      "| `get_marker_genes()` | Extract DE markers |",
      "| `save_results()` | Save model, data, plots |",
      "| `auto_select_model()` | Suggest best model |",
      "| `quick_clustering()` | Neighbors + UMAP + Leiden |",
      "",
      "## Critical Requirements",
      "",
      "1. **Raw counts required**: scvi-tools models require integer count data",
      "   ```python",
      '   adata.layers["counts"] = adata.X.copy()  # Before normalization',
      '   scvi.model.SCVI.setup_anndata(adata, layer="counts")',
      "   ```",
      "",
      "2. **HVG selection**: Use 2000-4000 highly variable genes",
      "   ```python",
      '   sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key="batch", layer="counts", flavor="seurat_v3")',
      "   adata = adata[:, adata.var['highly_variable']].copy()",
      "   ```",
      "",
      "3. **Batch information**: Specify batch_key for integration",
      "   ```python",
      '   scvi.model.SCVI.setup_anndata(adata, layer="counts", batch_key="batch")',
      "   ```",
      "",
      "## Quick Decision Tree",
      "",
      "```",
      "Need to integrate scRNA-seq data?",
      "├── Have cell type labels? → scANVI (references/label_transfer.md)",
      "└── No labels? → scVI (references/scrna_integration.md)",
      "",
      "Have multi-modal data?",
      "├── CITE-seq (RNA + protein)? → totalVI (references/citeseq_totalvi.md)",
      "├── Multiome (RNA + ATAC)? → MultiVI (references/multiome_multivi.md)",
      "└── scATAC-seq only? → PeakVI (references/atac_peakvi.md)",
      "",
      "Have spatial data?",
      "└── Need cell type deconvolution? → DestVI (references/spatial_deconvolution.md)",
      "",
      "Have pre-trained reference model?",
      "└── Map query to reference? → scArches (references/scarches_mapping.md)",
      "",
      "Need RNA velocity?",
      "└── veloVI (references/rna_velocity_velovi.md)",
      "",
      "Strong cross-technology batch effects?",
      "└── sysVI (references/batch_correction_sysvi.md)",
      "```",
      "",
      "## Key Resources",
      "",
      "- [scvi-tools Documentation](https://docs.scvi-tools.org/)",
      "- [scvi-tools Tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)",
      "- [Model Hub](https://huggingface.co/scvi-tools)",
      "- [GitHub Issues](https://github.com/scverse/scvi-tools/issues)",
      "",
    ].join("\n"),
  },
  "bio-research-single-cell-rna-qc": {
    "references/scverse_qc_guidelines.md": [
      "# scverse Quality Control Guidelines",
      "",
      "This document provides detailed information about quality control best practices for single-cell RNA-seq data, following the scverse ecosystem recommendations.",
      "",
      "## Quality Control Metrics",
      "",
      "### Count Depth (Total Counts)",
      "- **What it measures**: Total number of UMI/reads per cell",
      "- **Why it matters**: Low count cells may be empty droplets, debris, or poorly captured cells",
      "- **Typical range**: 500-50,000 counts per cell (varies by protocol)",
      "- **Red flags**: Bimodal distributions may indicate mixing of high and low-quality cells",
      "",
      "### Gene Detection (Genes per Cell)",
      "- **What it measures**: Number of genes with at least 1 count",
      "- **Why it matters**: Strongly correlates with count depth; low values indicate poor capture",
      "- **Typical range**: 200-5,000 genes per cell",
      "- **Red flags**: Very low values (<200) suggest technical failures",
      "",
      "### Mitochondrial Content",
      "- **What it measures**: Percentage of counts from mitochondrial genes",
      "- **Why it matters**: High MT% indicates cell stress, apoptosis, or lysed cells",
      "- **Typical range**: <5% for most tissues, up to 10-15% for metabolically active cells",
      "- **Species-specific patterns**:",
      "  - Mouse: Genes start with 'mt-' (e.g., mt-Nd1, mt-Co1)",
      "  - Human: Genes start with 'MT-' (e.g., MT-ND1, MT-CO1)",
      "- **Context matters**: Some cell types (cardiomyocytes, neurons) naturally have higher MT content",
      "",
      "### Ribosomal Content",
      "- **What it measures**: Percentage of counts from ribosomal protein genes",
      "- **Why it matters**: Can indicate cell state or contamination",
      "- **Patterns**: Genes start with 'Rpl'/'RPL' (large subunit) or 'Rps'/'RPS' (small subunit)",
      "- **Note**: High ribosomal content isn't always bad - metabolically active cells have more ribosomes",
      "",
      "### Hemoglobin Content",
      "- **What it measures**: Percentage of counts from hemoglobin genes",
      "- **Why it matters**: Indicates blood contamination in non-blood tissues",
      "- **Patterns**: Genes matching '^Hb[^(p)]' or '^HB[^(P)]' (excludes Hbp1/HBP1)",
      "- **When to use**: Particularly important for tissue samples (brain, liver, etc.)",
      "",
      "## MAD-Based Filtering Rationale",
      "",
      "### Why MAD Instead of Fixed Thresholds?",
      "",
      'Fixed thresholds (e.g., "remove cells with <500 genes") fail because:',
      "- Different protocols yield different ranges",
      "- Different tissues have different characteristics",
      "- Different species have different gene counts",
      "- Fixed thresholds are arbitrary and not data-driven",
      "",
      "MAD (Median Absolute Deviation) is robust to outliers and adapts to your dataset:",
      "```",
      "MAD = median(|X - median(X)|)",
      "Outlier bounds = median ± n_MADs × MAD",
      "```",
      "",
      "### Recommended MAD Thresholds",
      "",
      "Following scverse best practices (deliberately permissive):",
      "",
      "**5 MADs for count depth (log-transformed)**",
      "- Very permissive to retain rare cell populations",
      "- Catches extreme outliers (empty droplets, debris)",
      "- Log transformation handles the typical right-skewed distribution",
      "",
      "**5 MADs for gene counts (log-transformed)**",
      "- Parallels count depth filtering",
      "- Most informative when combined with count filtering",
      "- Log transformation normalizes the distribution",
      "",
      "**3 MADs for mitochondrial percentage**",
      "- More stringent because high MT% strongly indicates dying cells",
      "- Uses raw percentages (not log-transformed)",
      "- Combined with hard threshold for extra stringency",
      "",
      "**Hard threshold: 8% mitochondrial content**",
      "- Additional filter beyond MAD-based detection",
      "- Conservative cutoff that works across most tissues",
      "- Adjust higher (10-15%) for metabolically active cell types",
      "",
      "### Why Be Permissive?",
      "",
      "The default thresholds intentionally err on the side of keeping cells because:",
      "1. **Rare populations**: Stringent filtering may remove rare but viable cell types",
      "2. **Biological variation**: Some healthy cells naturally have extreme values",
      "3. **Reversibility**: Easier to filter more later than to recover lost cells",
      "4. **Downstream robustness**: Modern normalization methods handle moderate quality variation",
      "",
      "## Interpreting QC Visualizations",
      "",
      "### Histograms",
      "- **Bimodal distributions**: May indicate mixing of cell types or quality issues",
      "- **Long tails**: Common for count depth; MAD filtering handles this",
      "- **Sharp cutoffs**: May indicate prior filtering or technical artifacts",
      "",
      "### Violin Plots",
      "- Shows distribution shape and density",
      "- Median (line) and mean (diamond) should be similar for symmetric distributions",
      "- Wide distributions suggest high heterogeneity",
      "",
      "### Scatter Plots",
      "",
      "**Counts vs Genes (colored by MT%)**",
      "- Should show strong positive correlation (R² > 0.8 typical)",
      "- Points deviating from trend may be outliers",
      "- High MT% cells often cluster at low counts/genes",
      "",
      "**Counts vs MT%**",
      "- Negative correlation expected (dying cells have fewer counts)",
      "- Vertical stratification may indicate batch effects",
      "- Cells with high counts + high MT% are suspicious",
      "",
      "**Genes vs MT%**",
      "- Similar to counts vs MT%, but often weaker correlation",
      "- Useful for identifying cells with gene detection issues",
      "",
      "## Gene Filtering",
      "",
      "After filtering cells, remove genes detected in fewer than 20 cells:",
      "- **Why 20?**: Balances noise reduction with information retention",
      "- **Benefits**: Reduces dataset size, speeds up computation, removes noisy genes",
      "- **Trade-offs**: May lose very rare markers; adjust to 10 if studying rare populations",
      "",
      "## Species-Specific Considerations",
      "",
      "### Mouse (Mus musculus)",
      "- Mitochondrial genes: mt-* (lowercase)",
      "- Ribosomal genes: Rpl*, Rps* (capitalized first letter)",
      "- Hemoglobin genes: Hb* (but not Hbp1)",
      "",
      "### Human (Homo sapiens)",
      "- Mitochondrial genes: MT-* (uppercase)",
      "- Ribosomal genes: RPL*, RPS* (all uppercase)",
      "- Hemoglobin genes: HB* (but not HBP1)",
      "",
      "### Other Species",
      "Adjust gene name patterns in the script to match your organism's gene nomenclature. Consult Ensembl or your reference annotation for correct prefixes.",
      "",
      "## When to Adjust Parameters",
      "",
      "Consider adjusting filtering thresholds when:",
      "",
      "**More stringent (lower MADs)**",
      "- High ambient RNA contamination suspected",
      "- Many low-quality cells observed in visualizations",
      "- Downstream analysis shows quality-driven clustering",
      "",
      "**More permissive (higher MADs)**",
      "- Studying rare cell populations",
      "- Dataset has high technical quality",
      "- Cell types naturally have extreme values (e.g., neurons with high MT%)",
      "",
      "**Tissue-specific adjustments**",
      "- Brain/neurons: May need higher MT% threshold (10-15%)",
      "- Blood: Can be more stringent with MT% (5-8%)",
      "- Tumor samples: Often need more permissive thresholds due to biological variation",
      "",
      "## Advanced QC Considerations",
      "",
      "### Not Included in This Workflow",
      "",
      "**Ambient RNA correction**",
      "- Tool: SoupX, CellBender, DecontX",
      "- When: High background RNA in droplet-based data",
      "- Effect: Removes contamination from lysed cells",
      "",
      "**Doublet detection**",
      "- Tool: scDblFinder, scrublet, DoubletFinder",
      "- When: Always recommended for droplet-based data",
      "- Effect: Identifies and removes multiplets (2+ cells in one droplet)",
      "",
      "**Cell cycle scoring**",
      "- Tool: scanpy's score_genes_cell_cycle",
      "- When: Cell cycle effects confound biological signal",
      "- Effect: Allows regressing out or accounting for cell cycle phase",
      "",
      "**Batch correction**",
      "- Tool: Harmony, scVI, ComBat",
      "- When: Integrating data from multiple batches/experiments",
      "- Effect: Removes technical batch effects while preserving biology",
      "",
      "## References",
      "",
      "- scverse Best Practices: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html",
      "- Luecken & Theis (2019): Current best practices in single-cell RNA-seq analysis",
      "- Osorio & Cai (2021): Systematic determination of the mitochondrial proportion in human and mouse genomes",
      "- Germain et al. (2020): Doublet identification in single-cell sequencing data using scDblFinder",
      "",
    ].join("\n"),
    "scripts/qc_analysis.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Quality Control Analysis for Single-Cell RNA-seq Data",
      "Following scverse best practices from:",
      "https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html",
      "",
      "This is a convenience script that runs a complete QC workflow using the",
      "modular functions from qc_core.py and qc_plotting.py.",
      '"""',
      "",
      "import anndata as ad",
      "import scanpy as sc",
      "import sys",
      "import os",
      "import argparse",
      "",
      "# Import our modular utilities",
      "from qc_core import (",
      "    calculate_qc_metrics,",
      "    detect_outliers_mad,",
      "    apply_hard_threshold,",
      "    filter_cells,",
      "    filter_genes,",
      "    print_qc_summary",
      ")",
      "from qc_plotting import (",
      "    plot_qc_distributions,",
      "    plot_filtering_thresholds,",
      "    plot_qc_after_filtering",
      ")",
      "",
      'print("=" * 80)',
      'print("Single-Cell RNA-seq Quality Control Analysis")',
      'print("=" * 80)',
      "",
      "# Default parameters (single source of truth)",
      "DEFAULT_MAD_COUNTS = 5",
      "DEFAULT_MAD_GENES = 5",
      "DEFAULT_MAD_MT = 3",
      "DEFAULT_MT_THRESHOLD = 8",
      "DEFAULT_MIN_CELLS = 20",
      "DEFAULT_MT_PATTERN = 'mt-,MT-'",
      "DEFAULT_RIBO_PATTERN = 'Rpl,Rps,RPL,RPS'",
      "DEFAULT_HB_PATTERN = '^Hb[^(p)]|^HB[^(P)]'",
      "",
      "# Parse command-line arguments",
      "parser = argparse.ArgumentParser(",
      "    description='Quality Control Analysis for Single-Cell RNA-seq Data',",
      "    formatter_class=argparse.RawDescriptionHelpFormatter,",
      '    epilog="""',
      "Examples:",
      "  python3 qc_analysis.py data.h5ad",
      "  python3 qc_analysis.py raw_feature_bc_matrix.h5",
      "  python3 qc_analysis.py data.h5ad --mad-counts 4 --mad-genes 4 --mad-mt 2.5",
      "  python3 qc_analysis.py data.h5ad --mt-threshold 10 --min-cells 10",
      '  python3 qc_analysis.py data.h5ad --mt-pattern "^mt-" --ribo-pattern "^Rpl,^Rps"',
      '    """',
      ")",
      "",
      "parser.add_argument('input_file', help='Input .h5ad or .h5 file (10X Genomics format)')",
      "parser.add_argument('--output-dir', type=str, help='Output directory (default: <input_basename>_qc_results)')",
      "parser.add_argument('--mad-counts', type=float, default=DEFAULT_MAD_COUNTS, help=f'MAD threshold for total counts (default: {DEFAULT_MAD_COUNTS})')",
      "parser.add_argument('--mad-genes', type=float, default=DEFAULT_MAD_GENES, help=f'MAD threshold for gene counts (default: {DEFAULT_MAD_GENES})')",
      "parser.add_argument('--mad-mt', type=float, default=DEFAULT_MAD_MT, help=f'MAD threshold for mitochondrial percentage (default: {DEFAULT_MAD_MT})')",
      "parser.add_argument('--mt-threshold', type=float, default=DEFAULT_MT_THRESHOLD, help=f'Hard threshold for mitochondrial percentage (default: {DEFAULT_MT_THRESHOLD})')",
      "parser.add_argument('--min-cells', type=int, default=DEFAULT_MIN_CELLS, help=f'Minimum cells for gene filtering (default: {DEFAULT_MIN_CELLS})')",
      "parser.add_argument('--mt-pattern', type=str, default=DEFAULT_MT_PATTERN, help=f'Comma-separated mitochondrial gene prefixes (default: \"{DEFAULT_MT_PATTERN}\")')",
      "parser.add_argument('--ribo-pattern', type=str, default=DEFAULT_RIBO_PATTERN, help=f'Comma-separated ribosomal gene prefixes (default: \"{DEFAULT_RIBO_PATTERN}\")')",
      "parser.add_argument('--hb-pattern', type=str, default=DEFAULT_HB_PATTERN, help=f'Hemoglobin gene regex pattern (default: \"{DEFAULT_HB_PATTERN}\")')",
      "",
      "args = parser.parse_args()",
      "",
      "# Verify input file exists",
      "if not os.path.exists(args.input_file):",
      "    print(f\"\\nError: File '{args.input_file}' not found!\")",
      "    sys.exit(1)",
      "",
      "input_file = args.input_file",
      "base_name = os.path.splitext(os.path.basename(input_file))[0]",
      "",
      "# Set up output directory",
      "if args.output_dir:",
      "    output_dir = args.output_dir",
      "else:",
      '    output_dir = f"{base_name}_qc_results"',
      "",
      "os.makedirs(output_dir, exist_ok=True)",
      'print(f"\\nOutput directory: {output_dir}")',
      "",
      "# Display parameters",
      'print(f"\\nParameters:")',
      'print(f"  MAD thresholds: counts={args.mad_counts}, genes={args.mad_genes}, MT%={args.mad_mt}")',
      'print(f"  MT hard threshold: {args.mt_threshold}%")',
      'print(f"  Min cells for gene filtering: {args.min_cells}")',
      'print(f"  Gene patterns: MT={args.mt_pattern}, Ribo={args.ribo_pattern}")',
      "",
      "# Load the data",
      'print("\\n[1/5] Loading data...")',
      "file_ext = os.path.splitext(input_file)[1].lower()",
      "",
      "if file_ext == '.h5ad':",
      "    adata = ad.read_h5ad(input_file)",
      '    print(f"Loaded .h5ad file: {adata.n_obs} cells × {adata.n_vars} genes")',
      "elif file_ext == '.h5':",
      "    adata = sc.read_10x_h5(input_file)",
      '    print(f"Loaded 10X .h5 file: {adata.n_obs} cells × {adata.n_vars} genes")',
      "    # Make variable names unique (10X data sometimes has duplicate gene names)",
      "    adata.var_names_make_unique()",
      "else:",
      "    print(f\"\\nError: Unsupported file format '{file_ext}'. Expected .h5ad or .h5\")",
      "    sys.exit(1)",
      "",
      "# Store original counts for comparison",
      "n_cells_original = adata.n_obs",
      "n_genes_original = adata.n_vars",
      "",
      "# Calculate QC metrics",
      'print("\\n[2/5] Calculating QC metrics...")',
      "calculate_qc_metrics(adata, mt_pattern=args.mt_pattern,",
      "                     ribo_pattern=args.ribo_pattern,",
      "                     hb_pattern=args.hb_pattern,",
      "                     inplace=True)",
      "",
      "print(f\"  Found {adata.var['mt'].sum()} mitochondrial genes (pattern: {args.mt_pattern})\")",
      "print(f\"  Found {adata.var['ribo'].sum()} ribosomal genes (pattern: {args.ribo_pattern})\")",
      "print(f\"  Found {adata.var['hb'].sum()} hemoglobin genes (pattern: {args.hb_pattern})\")",
      "",
      "print_qc_summary(adata, label='QC Metrics Summary (before filtering)')",
      "",
      "# Create before-filtering visualizations",
      'print("\\n[3/5] Creating QC visualizations...")',
      "before_plot = os.path.join(output_dir, 'qc_metrics_before_filtering.png')",
      "plot_qc_distributions(adata, before_plot, title='Quality Control Metrics - Before Filtering')",
      'print(f"  Saved: {before_plot}")',
      "",
      "# Apply MAD-based filtering",
      'print("\\n[4/5] Applying MAD-based filtering thresholds...")',
      "",
      "# Detect outliers for each metric",
      "adata.obs['outlier_counts'] = detect_outliers_mad(adata, 'total_counts', args.mad_counts)",
      "adata.obs['outlier_genes'] = detect_outliers_mad(adata, 'n_genes_by_counts', args.mad_genes)",
      "adata.obs['outlier_mt'] = detect_outliers_mad(adata, 'pct_counts_mt', args.mad_mt)",
      "",
      "# Apply hard threshold for mitochondrial content",
      'print(f"\\n  Applying hard threshold for mitochondrial content (>{args.mt_threshold}%):")',
      "high_mt_mask = apply_hard_threshold(adata, 'pct_counts_mt', args.mt_threshold, operator='>')",
      "",
      "# Combine MT filters (MAD + hard threshold)",
      "adata.obs['outlier_mt'] = adata.obs['outlier_mt'] | high_mt_mask",
      "",
      "# Overall filtering decision",
      "adata.obs['pass_qc'] = ~(",
      "    adata.obs['outlier_counts'] |",
      "    adata.obs['outlier_genes'] |",
      "    adata.obs['outlier_mt']",
      ")",
      "",
      "print(f\"\\n  Total cells failing QC: {(~adata.obs['pass_qc']).sum()} ({(~adata.obs['pass_qc']).sum()/adata.n_obs*100:.2f}%)\")",
      "print(f\"  Cells passing QC: {adata.obs['pass_qc'].sum()} ({adata.obs['pass_qc'].sum()/adata.n_obs*100:.2f}%)\")",
      "",
      "# Visualize filtering thresholds",
      "outlier_masks = {",
      "    'total_counts': adata.obs['outlier_counts'].values,",
      "    'n_genes_by_counts': adata.obs['outlier_genes'].values,",
      "    'pct_counts_mt': adata.obs['outlier_mt'].values",
      "}",
      "",
      "thresholds = {",
      "    'total_counts': {'n_mads': args.mad_counts},",
      "    'n_genes_by_counts': {'n_mads': args.mad_genes},",
      "    'pct_counts_mt': {'n_mads': args.mad_mt, 'hard': args.mt_threshold}",
      "}",
      "",
      "threshold_plot = os.path.join(output_dir, 'qc_filtering_thresholds.png')",
      "plot_filtering_thresholds(adata, outlier_masks, thresholds, threshold_plot)",
      'print(f"\\n  Saved: {threshold_plot}")',
      "",
      "# Apply filtering",
      'print("\\n[5/5] Applying filters...")',
      "adata_filtered = filter_cells(adata, adata.obs['pass_qc'].values, inplace=False)",
      'print(f"  Cells after filtering: {adata_filtered.n_obs} (removed {n_cells_original - adata_filtered.n_obs})")',
      "",
      "# Filter genes",
      'print(f"\\n  Filtering genes detected in <{args.min_cells} cells...")',
      "filter_genes(adata_filtered, min_cells=args.min_cells, inplace=True)",
      'print(f"  Genes after filtering: {adata_filtered.n_vars} (removed {n_genes_original - adata_filtered.n_vars})")',
      "",
      "# Generate summary statistics",
      'print("\\n" + "=" * 80)',
      'print("QC Summary")',
      'print("=" * 80)',
      "",
      'print("\\nBefore filtering:")',
      'print(f"  Cells: {n_cells_original}")',
      'print(f"  Genes: {n_genes_original}")',
      "",
      'print("\\nAfter filtering:")',
      'print(f"  Cells: {adata_filtered.n_obs} ({adata_filtered.n_obs/n_cells_original*100:.1f}% retained)")',
      'print(f"  Genes: {adata_filtered.n_vars} ({adata_filtered.n_vars/n_genes_original*100:.1f}% retained)")',
      "",
      "print_qc_summary(adata_filtered, label='\\nFiltered data QC metrics')",
      "",
      "# Create after-filtering visualizations",
      "after_plot = os.path.join(output_dir, 'qc_metrics_after_filtering.png')",
      "plot_qc_after_filtering(adata_filtered, after_plot)",
      'print(f"\\n  Saved: {after_plot}")',
      "",
      "# Save filtered data",
      'print("\\nSaving filtered data...")',
      "output_filtered = os.path.join(output_dir, f'{base_name}_filtered.h5ad')",
      "output_with_qc = os.path.join(output_dir, f'{base_name}_with_qc.h5ad')",
      "adata_filtered.write(output_filtered)",
      'print(f"  Saved: {output_filtered}")',
      "",
      "# Also save the unfiltered data with QC annotations",
      "adata.write(output_with_qc)",
      'print(f"  Saved: {output_with_qc} (original data with QC annotations)")',
      "",
      'print("\\n" + "=" * 80)',
      'print("Quality Control Analysis Complete!")',
      'print("=" * 80)',
      'print(f"\\nAll results saved to: {output_dir}/")',
      'print("\\nGenerated files:")',
      'print("  1. qc_metrics_before_filtering.png - Initial QC visualizations")',
      'print("  2. qc_filtering_thresholds.png - MAD-based threshold visualization")',
      'print("  3. qc_metrics_after_filtering.png - Post-filtering QC visualizations")',
      'print(f"  4. {base_name}_filtered.h5ad - Filtered dataset")',
      'print(f"  5. {base_name}_with_qc.h5ad - Original dataset with QC annotations")',
      'print("\\nNext steps:")',
      'print("  - Consider ambient RNA correction (SoupX)")',
      'print("  - Consider doublet detection (scDblFinder)")',
      'print("  - Proceed with normalization and downstream analysis")',
      "",
    ].join("\n"),
    "scripts/qc_core.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Core utility functions for single-cell RNA-seq quality control.",
      "",
      "This module provides building blocks for metrics calculation and filtering",
      "while following scverse best practices from:",
      "https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html",
      '"""',
      "",
      "import anndata as ad",
      "import scanpy as sc",
      "import numpy as np",
      "from scipy.stats import median_abs_deviation",
      "",
      "",
      "def calculate_qc_metrics(adata, mt_pattern='mt-,MT-', ribo_pattern='Rpl,Rps,RPL,RPS',",
      "                        hb_pattern='^Hb[^(p)]|^HB[^(P)]', inplace=True):",
      '    """',
      "    Calculate QC metrics for single-cell RNA-seq data.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix",
      "    mt_pattern : str",
      "        Comma-separated mitochondrial gene prefixes (default: 'mt-,MT-')",
      "    ribo_pattern : str",
      "        Comma-separated ribosomal gene prefixes (default: 'Rpl,Rps,RPL,RPS')",
      "    hb_pattern : str",
      "        Regex pattern for hemoglobin genes (default: '^Hb[^(p)]|^HB[^(P)]')",
      "    inplace : bool",
      "        Modify adata in place (default: True)",
      "",
      "    Returns",
      "    -------",
      "    AnnData or None",
      "        If inplace=False, returns modified AnnData. Otherwise modifies in place.",
      '    """',
      "    if not inplace:",
      "        adata = adata.copy()",
      "",
      "    # Identify gene categories",
      "    mt_prefixes = tuple(mt_pattern.split(','))",
      "    adata.var['mt'] = adata.var_names.str.startswith(mt_prefixes)",
      "",
      "    ribo_prefixes = tuple(ribo_pattern.split(','))",
      "    adata.var['ribo'] = adata.var_names.str.startswith(ribo_prefixes)",
      "",
      "    adata.var['hb'] = adata.var_names.str.match(hb_pattern)",
      "",
      "    # Calculate QC metrics",
      "    sc.pp.calculate_qc_metrics(",
      "        adata,",
      "        qc_vars=['mt', 'ribo', 'hb'],",
      "        percent_top=None,",
      "        log1p=False,",
      "        inplace=True",
      "    )",
      "",
      "    if not inplace:",
      "        return adata",
      "",
      "",
      "def detect_outliers_mad(adata, metric, n_mads, verbose=True):",
      '    """',
      "    Detect outliers using Median Absolute Deviation (MAD).",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix with QC metrics",
      "    metric : str",
      "        Column name in adata.obs to use for outlier detection",
      "    n_mads : float",
      "        Number of MADs to use as threshold",
      "    verbose : bool",
      "        Print outlier statistics (default: True)",
      "",
      "    Returns",
      "    -------",
      "    np.ndarray",
      "        Boolean mask where True indicates outliers",
      '    """',
      "    metric_values = adata.obs[metric]",
      "    median = np.median(metric_values)",
      "    mad = median_abs_deviation(metric_values)",
      "",
      "    # Calculate bounds",
      "    lower = median - n_mads * mad",
      "    upper = median + n_mads * mad",
      "",
      "    # Identify outliers",
      "    outlier_mask = (metric_values < lower) | (metric_values > upper)",
      "",
      "    if verbose:",
      '        print(f"  {metric}:")',
      '        print(f"    Median: {median:.2f}, MAD: {mad:.2f}")',
      '        print(f"    Bounds: [{lower:.2f}, {upper:.2f}] ({n_mads} MADs)")',
      '        print(f"    Outliers: {outlier_mask.sum()} cells ({outlier_mask.sum()/len(metric_values)*100:.2f}%)")',
      "",
      "    return outlier_mask",
      "",
      "",
      "def apply_hard_threshold(adata, metric, threshold, operator='>', verbose=True):",
      '    """',
      "    Apply a hard threshold filter.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix",
      "    metric : str",
      "        Column name in adata.obs to filter on",
      "    threshold : float",
      "        Threshold value",
      "    operator : str",
      "        Comparison operator: '>', '<', '>=', '<=' (default: '>')",
      "    verbose : bool",
      "        Print filtering statistics (default: True)",
      "",
      "    Returns",
      "    -------",
      "    np.ndarray",
      "        Boolean mask where True indicates cells to filter out",
      '    """',
      "    metric_values = adata.obs[metric]",
      "",
      "    if operator == '>':",
      "        mask = metric_values > threshold",
      "    elif operator == '<':",
      "        mask = metric_values < threshold",
      "    elif operator == '>=':",
      "        mask = metric_values >= threshold",
      "    elif operator == '<=':",
      "        mask = metric_values <= threshold",
      "    else:",
      "        raise ValueError(f\"Invalid operator: {operator}. Use '>', '<', '>=', or '<='\")",
      "",
      "    if verbose:",
      '        print(f"  {metric} {operator} {threshold}:")',
      '        print(f"    Cells filtered: {mask.sum()} ({mask.sum()/len(metric_values)*100:.2f}%)")',
      "",
      "    return mask",
      "",
      "",
      "def filter_cells(adata, mask, inplace=False):",
      '    """',
      "    Filter cells based on a boolean mask.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix",
      "    mask : np.ndarray or pd.Series",
      "        Boolean mask where True indicates cells to KEEP",
      "    inplace : bool",
      "        Modify adata in place (default: False)",
      "",
      "    Returns",
      "    -------",
      "    AnnData",
      "        Filtered AnnData object",
      '    """',
      "    if inplace:",
      "        # This is actually a bit tricky - AnnData doesn't support true inplace filtering",
      "        # Return filtered copy which caller should reassign",
      "        return adata[mask].copy()",
      "    else:",
      "        return adata[mask].copy()",
      "",
      "",
      "def filter_genes(adata, min_cells=20, min_counts=None, inplace=True):",
      '    """',
      "    Filter genes based on detection thresholds.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix",
      "    min_cells : int",
      "        Minimum number of cells a gene must be detected in (default: 20)",
      "    min_counts : int, optional",
      "        Minimum total counts across all cells",
      "    inplace : bool",
      "        Modify adata in place (default: True)",
      "",
      "    Returns",
      "    -------",
      "    AnnData or None",
      "        If inplace=False, returns filtered AnnData",
      '    """',
      "    if not inplace:",
      "        adata = adata.copy()",
      "",
      "    if min_cells is not None:",
      "        sc.pp.filter_genes(adata, min_cells=min_cells)",
      "",
      "    if min_counts is not None:",
      "        sc.pp.filter_genes(adata, min_counts=min_counts)",
      "",
      "    if not inplace:",
      "        return adata",
      "",
      "",
      "def print_qc_summary(adata, label=''):",
      '    """',
      "    Print summary statistics for QC metrics.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix with QC metrics",
      "    label : str",
      "        Label to prepend to output (e.g., 'Before filtering', 'After filtering')",
      '    """',
      "    if label:",
      '        print(f"\\n{label}:")',
      '    print(f"  Cells: {adata.n_obs}")',
      '    print(f"  Genes: {adata.n_vars}")',
      "",
      "    if 'total_counts' in adata.obs:",
      "        print(f\"  Mean counts per cell: {adata.obs['total_counts'].mean():.0f}\")",
      "        print(f\"  Median counts per cell: {adata.obs['total_counts'].median():.0f}\")",
      "",
      "    if 'n_genes_by_counts' in adata.obs:",
      "        print(f\"  Mean genes per cell: {adata.obs['n_genes_by_counts'].mean():.0f}\")",
      "        print(f\"  Median genes per cell: {adata.obs['n_genes_by_counts'].median():.0f}\")",
      "",
      "    if 'pct_counts_mt' in adata.obs:",
      "        print(f\"  Mean mitochondrial %: {adata.obs['pct_counts_mt'].mean():.2f}%\")",
      "",
      "    if 'pct_counts_ribo' in adata.obs:",
      "        print(f\"  Mean ribosomal %: {adata.obs['pct_counts_ribo'].mean():.2f}%\")",
      "",
    ].join("\n"),
    "scripts/qc_plotting.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Visualization functions for single-cell RNA-seq quality control.",
      "",
      "This module provides plotting utilities for QC metrics and filtering thresholds.",
      '"""',
      "",
      "import numpy as np",
      "import matplotlib.pyplot as plt",
      "from scipy.stats import median_abs_deviation",
      "",
      "",
      "def plot_qc_distributions(adata, output_path, title='Quality Control Metrics'):",
      '    """',
      "    Create comprehensive QC distribution plots.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix with QC metrics",
      "    output_path : str",
      "        Path to save the figure",
      "    title : str",
      "        Figure title (default: 'Quality Control Metrics')",
      '    """',
      "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))",
      "    fig.suptitle(title, fontsize=16, y=0.995)",
      "",
      "    # Row 1: Histograms",
      "    axes[0, 0].hist(adata.obs['total_counts'], bins=100, color='steelblue', edgecolor='black')",
      "    axes[0, 0].set_xlabel('Total counts per cell')",
      "    axes[0, 0].set_ylabel('Number of cells')",
      "    axes[0, 0].set_title('Distribution of Total Counts')",
      "    axes[0, 0].axvline(adata.obs['total_counts'].median(), color='red', linestyle='--', label='Median')",
      "    axes[0, 0].legend()",
      "",
      "    axes[0, 1].hist(adata.obs['n_genes_by_counts'], bins=100, color='forestgreen', edgecolor='black')",
      "    axes[0, 1].set_xlabel('Genes per cell')",
      "    axes[0, 1].set_ylabel('Number of cells')",
      "    axes[0, 1].set_title('Distribution of Detected Genes')",
      "    axes[0, 1].axvline(adata.obs['n_genes_by_counts'].median(), color='red', linestyle='--', label='Median')",
      "    axes[0, 1].legend()",
      "",
      "    axes[0, 2].hist(adata.obs['pct_counts_mt'], bins=100, color='coral', edgecolor='black')",
      "    axes[0, 2].set_xlabel('Mitochondrial %')",
      "    axes[0, 2].set_ylabel('Number of cells')",
      "    axes[0, 2].set_title('Distribution of Mitochondrial Content')",
      "    axes[0, 2].axvline(adata.obs['pct_counts_mt'].median(), color='red', linestyle='--', label='Median')",
      "    axes[0, 2].legend()",
      "",
      "    # Row 2: Violin plots",
      "    axes[1, 0].violinplot([adata.obs['total_counts']], positions=[0], showmeans=True, showmedians=True)",
      "    axes[1, 0].set_ylabel('Total counts')",
      "    axes[1, 0].set_title('Total Counts per Cell')",
      "    axes[1, 0].set_xticks([])",
      "",
      "    axes[1, 1].violinplot([adata.obs['n_genes_by_counts']], positions=[0], showmeans=True, showmedians=True)",
      "    axes[1, 1].set_ylabel('Genes detected')",
      "    axes[1, 1].set_title('Genes per Cell')",
      "    axes[1, 1].set_xticks([])",
      "",
      "    axes[1, 2].violinplot([adata.obs['pct_counts_mt']], positions=[0], showmeans=True, showmedians=True)",
      "    axes[1, 2].set_ylabel('Mitochondrial %')",
      "    axes[1, 2].set_title('Mitochondrial Content')",
      "    axes[1, 2].set_xticks([])",
      "",
      "    # Row 3: Scatter plots",
      "    scatter1 = axes[2, 0].scatter(",
      "        adata.obs['total_counts'],",
      "        adata.obs['n_genes_by_counts'],",
      "        c=adata.obs['pct_counts_mt'],",
      "        cmap='viridis',",
      "        alpha=0.5,",
      "        s=10",
      "    )",
      "    axes[2, 0].set_xlabel('Total counts')",
      "    axes[2, 0].set_ylabel('Genes detected')",
      "    axes[2, 0].set_title('Counts vs Genes (colored by MT%)')",
      "    plt.colorbar(scatter1, ax=axes[2, 0], label='MT %')",
      "",
      "    axes[2, 1].scatter(",
      "        adata.obs['total_counts'],",
      "        adata.obs['pct_counts_mt'],",
      "        alpha=0.5,",
      "        s=10,",
      "        color='coral'",
      "    )",
      "    axes[2, 1].set_xlabel('Total counts')",
      "    axes[2, 1].set_ylabel('Mitochondrial %')",
      "    axes[2, 1].set_title('Total Counts vs Mitochondrial %')",
      "",
      "    axes[2, 2].scatter(",
      "        adata.obs['n_genes_by_counts'],",
      "        adata.obs['pct_counts_mt'],",
      "        alpha=0.5,",
      "        s=10,",
      "        color='forestgreen'",
      "    )",
      "    axes[2, 2].set_xlabel('Genes detected')",
      "    axes[2, 2].set_ylabel('Mitochondrial %')",
      "    axes[2, 2].set_title('Genes vs Mitochondrial %')",
      "",
      "    plt.tight_layout()",
      "    plt.savefig(output_path, dpi=300, bbox_inches='tight')",
      "    plt.close()",
      "",
      "",
      "def plot_filtering_thresholds(adata, outlier_masks, thresholds, output_path):",
      '    """',
      "    Visualize filtering thresholds overlaid on distributions.",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Annotated data matrix with QC metrics",
      "    outlier_masks : dict",
      "        Dictionary mapping metric names to boolean outlier masks",
      "        Example: {'total_counts': mask1, 'n_genes_by_counts': mask2, 'pct_counts_mt': mask3}",
      "    thresholds : dict",
      "        Dictionary with threshold information for each metric",
      "        Example: {'total_counts': {'n_mads': 5}, 'pct_counts_mt': {'n_mads': 3, 'hard': 8}}",
      "    output_path : str",
      "        Path to save the figure",
      '    """',
      "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))",
      "    fig.suptitle('MAD-Based Filtering Thresholds', fontsize=16)",
      "",
      "    # Helper function to plot with thresholds",
      "    def plot_with_threshold(ax, metric, outlier_mask, n_mads, hard_threshold=None):",
      "        data = adata.obs[metric]",
      "        median = np.median(data)",
      "        mad = median_abs_deviation(data)",
      "        lower = median - n_mads * mad",
      "        upper = median + n_mads * mad",
      "",
      "        ax.hist(data[~outlier_mask], bins=100, alpha=0.7, label='Pass QC', color='steelblue')",
      "        ax.hist(data[outlier_mask], bins=100, alpha=0.7, label='Fail QC', color='coral')",
      "        ax.axvline(lower, color='red', linestyle='--', linewidth=2, label=f'Thresholds ({n_mads} MADs)')",
      "        ax.axvline(upper, color='red', linestyle='--', linewidth=2)",
      "",
      "        if hard_threshold is not None:",
      "            ax.axvline(hard_threshold, color='darkred', linestyle=':', linewidth=2,",
      "                      label=f'Hard threshold ({hard_threshold})')",
      "",
      "        ax.set_xlabel(metric.replace('_', ' ').title())",
      "        ax.set_ylabel('Number of cells')",
      "        ax.legend()",
      "",
      "    # Plot each metric",
      "    metrics = [",
      "        ('total_counts', 'Total Counts'),",
      "        ('n_genes_by_counts', 'Genes Detected'),",
      "        ('pct_counts_mt', 'Mitochondrial %')",
      "    ]",
      "",
      "    for idx, (metric, label) in enumerate(metrics):",
      "        if metric in outlier_masks and metric in thresholds:",
      "            hard = thresholds[metric].get('hard', None)",
      "            plot_with_threshold(axes[idx], metric, outlier_masks[metric],",
      "                              thresholds[metric]['n_mads'], hard)",
      "",
      "    plt.tight_layout()",
      "    plt.savefig(output_path, dpi=300, bbox_inches='tight')",
      "    plt.close()",
      "",
      "",
      "def plot_qc_after_filtering(adata, output_path):",
      '    """',
      "    Create QC plots for filtered data (simplified version without outlier overlay).",
      "",
      "    Parameters",
      "    ----------",
      "    adata : AnnData",
      "        Filtered annotated data matrix with QC metrics",
      "    output_path : str",
      "        Path to save the figure",
      '    """',
      "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))",
      "    fig.suptitle('Quality Control Metrics - After Filtering', fontsize=16, y=0.995)",
      "",
      "    # Row 1: Histograms",
      "    axes[0, 0].hist(adata.obs['total_counts'], bins=100, color='steelblue', edgecolor='black')",
      "    axes[0, 0].set_xlabel('Total counts per cell')",
      "    axes[0, 0].set_ylabel('Number of cells')",
      "    axes[0, 0].set_title('Distribution of Total Counts')",
      "",
      "    axes[0, 1].hist(adata.obs['n_genes_by_counts'], bins=100, color='forestgreen', edgecolor='black')",
      "    axes[0, 1].set_xlabel('Genes per cell')",
      "    axes[0, 1].set_ylabel('Number of cells')",
      "    axes[0, 1].set_title('Distribution of Detected Genes')",
      "",
      "    axes[0, 2].hist(adata.obs['pct_counts_mt'], bins=100, color='coral', edgecolor='black')",
      "    axes[0, 2].set_xlabel('Mitochondrial %')",
      "    axes[0, 2].set_ylabel('Number of cells')",
      "    axes[0, 2].set_title('Distribution of Mitochondrial Content')",
      "",
      "    # Row 2: Scatter plots",
      "    scatter1 = axes[1, 0].scatter(",
      "        adata.obs['total_counts'],",
      "        adata.obs['n_genes_by_counts'],",
      "        c=adata.obs['pct_counts_mt'],",
      "        cmap='viridis',",
      "        alpha=0.5,",
      "        s=10",
      "    )",
      "    axes[1, 0].set_xlabel('Total counts')",
      "    axes[1, 0].set_ylabel('Genes detected')",
      "    axes[1, 0].set_title('Counts vs Genes (colored by MT%)')",
      "    plt.colorbar(scatter1, ax=axes[1, 0], label='MT %')",
      "",
      "    axes[1, 1].scatter(",
      "        adata.obs['total_counts'],",
      "        adata.obs['pct_counts_mt'],",
      "        alpha=0.5,",
      "        s=10,",
      "        color='coral'",
      "    )",
      "    axes[1, 1].set_xlabel('Total counts')",
      "    axes[1, 1].set_ylabel('Mitochondrial %')",
      "    axes[1, 1].set_title('Total Counts vs Mitochondrial %')",
      "",
      "    axes[1, 2].scatter(",
      "        adata.obs['n_genes_by_counts'],",
      "        adata.obs['pct_counts_mt'],",
      "        alpha=0.5,",
      "        s=10,",
      "        color='forestgreen'",
      "    )",
      "    axes[1, 2].set_xlabel('Genes detected')",
      "    axes[1, 2].set_ylabel('Mitochondrial %')",
      "    axes[1, 2].set_title('Genes vs Mitochondrial %')",
      "",
      "    plt.tight_layout()",
      "    plt.savefig(output_path, dpi=300, bbox_inches='tight')",
      "    plt.close()",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: bio-research-single-cell-rna-qc",
      "description: Performs quality control on single-cell RNA-seq data (.h5ad or .h5 files) using scverse best practices with MAD-based filtering and comprehensive visualizations. Use when users request QC analysis, filtering low-quality cells, assessing data quality, or following scverse/scanpy best practices for single-cell analysis.",
      "metadata:",
      "  plugin: bio-research",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Single-Cell RNA-seq Quality Control",
      "",
      "Automated QC workflow for single-cell RNA-seq data following scverse best practices.",
      "",
      "## When to Use This Skill",
      "",
      "Use when users:",
      "- Request quality control or QC on single-cell RNA-seq data",
      "- Want to filter low-quality cells or assess data quality",
      "- Need QC visualizations or metrics",
      "- Ask to follow scverse/scanpy best practices",
      "- Request MAD-based filtering or outlier detection",
      "",
      "**Supported input formats:**",
      "- `.h5ad` files (AnnData format from scanpy/Python workflows)",
      "- `.h5` files (10X Genomics Cell Ranger output)",
      "",
      "**Default recommendation**: Use Approach 1 (complete pipeline) unless the user has specific custom requirements or explicitly requests non-standard filtering logic.",
      "",
      "## Approach 1: Complete QC Pipeline (Recommended for Standard Workflows)",
      "",
      "For standard QC following scverse best practices, use the convenience script `scripts/qc_analysis.py`:",
      "",
      "```bash",
      "python3 scripts/qc_analysis.py input.h5ad",
      "# or for 10X Genomics .h5 files:",
      "python3 scripts/qc_analysis.py raw_feature_bc_matrix.h5",
      "```",
      "",
      "The script automatically detects the file format and loads it appropriately.",
      "",
      "**When to use this approach:**",
      "- Standard QC workflow with adjustable thresholds (all cells filtered the same way)",
      "- Batch processing multiple datasets",
      "- Quick exploratory analysis",
      '- User wants the "just works" solution',
      "",
      "**Requirements:** anndata, scanpy, scipy, matplotlib, seaborn, numpy",
      "",
      "**Parameters:**",
      "",
      "Customize filtering thresholds and gene patterns using command-line parameters:",
      "- `--output-dir` - Output directory",
      "- `--mad-counts`, `--mad-genes`, `--mad-mt` - MAD thresholds for counts/genes/MT%",
      "- `--mt-threshold` - Hard mitochondrial % cutoff",
      "- `--min-cells` - Gene filtering threshold",
      "- `--mt-pattern`, `--ribo-pattern`, `--hb-pattern` - Gene name patterns for different species",
      "",
      "Use `--help` to see current default values.",
      "",
      "**Outputs:**",
      "",
      "All files are saved to `<input_basename>_qc_results/` directory by default (or to the directory specified by `--output-dir`):",
      "- `qc_metrics_before_filtering.png` - Pre-filtering visualizations",
      "- `qc_filtering_thresholds.png` - MAD-based threshold overlays",
      "- `qc_metrics_after_filtering.png` - Post-filtering quality metrics",
      "- `<input_basename>_filtered.h5ad` - Clean, filtered dataset ready for downstream analysis",
      "- `<input_basename>_with_qc.h5ad` - Original data with QC annotations preserved",
      "",
      "If copying outputs for user access, copy individual files (not the entire directory) so users can preview them directly.",
      "",
      "### Workflow Steps",
      "",
      "The script performs the following steps:",
      "",
      "1. **Calculate QC metrics** - Count depth, gene detection, mitochondrial/ribosomal/hemoglobin content",
      "2. **Apply MAD-based filtering** - Permissive outlier detection using MAD thresholds for counts/genes/MT%",
      "3. **Filter genes** - Remove genes detected in few cells",
      "4. **Generate visualizations** - Comprehensive before/after plots with threshold overlays",
      "",
      "## Approach 2: Modular Building Blocks (For Custom Workflows)",
      "",
      "For custom analysis workflows or non-standard requirements, use the modular utility functions from `scripts/qc_core.py` and `scripts/qc_plotting.py`:",
      "",
      "```python",
      "# Run from scripts/ directory, or add scripts/ to sys.path if needed",
      "import anndata as ad",
      "from qc_core import calculate_qc_metrics, detect_outliers_mad, filter_cells",
      "from qc_plotting import plot_qc_distributions  # Only if visualization needed",
      "",
      "adata = ad.read_h5ad('input.h5ad')",
      "calculate_qc_metrics(adata, inplace=True)",
      "# ... custom analysis logic here",
      "```",
      "",
      "**When to use this approach:**",
      "- Different workflow needed (skip steps, change order, apply different thresholds to subsets)",
      "- Conditional logic (e.g., filter neurons differently than other cells)",
      "- Partial execution (only metrics/visualization, no filtering)",
      "- Integration with other analysis steps in a larger pipeline",
      "- Custom filtering criteria beyond what command-line params support",
      "",
      "**Available utility functions:**",
      "",
      "From `qc_core.py` (core QC operations):",
      "- `calculate_qc_metrics(adata, mt_pattern, ribo_pattern, hb_pattern, inplace=True)` - Calculate QC metrics and annotate adata",
      "- `detect_outliers_mad(adata, metric, n_mads, verbose=True)` - MAD-based outlier detection, returns boolean mask",
      "- `apply_hard_threshold(adata, metric, threshold, operator='>', verbose=True)` - Apply hard cutoffs, returns boolean mask",
      "- `filter_cells(adata, mask, inplace=False)` - Apply boolean mask to filter cells",
      "- `filter_genes(adata, min_cells=20, min_counts=None, inplace=True)` - Filter genes by detection",
      "- `print_qc_summary(adata, label='')` - Print summary statistics",
      "",
      "From `qc_plotting.py` (visualization):",
      "- `plot_qc_distributions(adata, output_path, title)` - Generate comprehensive QC plots",
      "- `plot_filtering_thresholds(adata, outlier_masks, thresholds, output_path)` - Visualize filtering thresholds",
      "- `plot_qc_after_filtering(adata, output_path)` - Generate post-filtering plots",
      "",
      "**Example custom workflows:**",
      "",
      "**Example 1: Only calculate metrics and visualize, don't filter yet**",
      "```python",
      "adata = ad.read_h5ad('input.h5ad')",
      "calculate_qc_metrics(adata, inplace=True)",
      "plot_qc_distributions(adata, 'qc_before.png', title='Initial QC')",
      "print_qc_summary(adata, label='Before filtering')",
      "```",
      "",
      "**Example 2: Apply only MT% filtering, keep other metrics permissive**",
      "```python",
      "adata = ad.read_h5ad('input.h5ad')",
      "calculate_qc_metrics(adata, inplace=True)",
      "",
      "# Only filter high MT% cells",
      "high_mt = apply_hard_threshold(adata, 'pct_counts_mt', 10, operator='>')",
      "adata_filtered = filter_cells(adata, ~high_mt)",
      "adata_filtered.write('filtered.h5ad')",
      "```",
      "",
      "**Example 3: Different thresholds for different subsets**",
      "```python",
      "adata = ad.read_h5ad('input.h5ad')",
      "calculate_qc_metrics(adata, inplace=True)",
      "",
      "# Apply type-specific QC (assumes cell_type metadata exists)",
      "neurons = adata.obs['cell_type'] == 'neuron'",
      "other_cells = ~neurons",
      "",
      "# Neurons tolerate higher MT%, other cells use stricter threshold",
      "neuron_qc = apply_hard_threshold(adata[neurons], 'pct_counts_mt', 15, operator='>')",
      "other_qc = apply_hard_threshold(adata[other_cells], 'pct_counts_mt', 8, operator='>')",
      "```",
      "",
      "## Best Practices",
      "",
      "1. **Be permissive with filtering** - Default thresholds intentionally retain most cells to avoid losing rare populations",
      "2. **Inspect visualizations** - Always review before/after plots to ensure filtering makes biological sense",
      "3. **Consider dataset-specific factors** - Some tissues naturally have higher mitochondrial content (e.g., neurons, cardiomyocytes)",
      "4. **Check gene annotations** - Mitochondrial gene prefixes vary by species (mt- for mouse, MT- for human)",
      "5. **Iterate if needed** - QC parameters may need adjustment based on the specific experiment or tissue type",
      "",
      "## Reference Materials",
      "",
      "For detailed QC methodology, parameter rationale, and troubleshooting guidance, see `references/scverse_qc_guidelines.md`. This reference provides:",
      "- Detailed explanations of each QC metric and why it matters",
      "- Rationale for MAD-based thresholds and why they're better than fixed cutoffs",
      "- Guidelines for interpreting QC visualizations (histograms, violin plots, scatter plots)",
      "- Species-specific considerations for gene annotations",
      "- When and how to adjust filtering parameters",
      "- Advanced QC considerations (ambient RNA correction, doublet detection)",
      "",
      "Load this reference when users need deeper understanding of the methodology or when troubleshooting QC issues.",
      "",
      "## Next Steps After QC",
      "",
      "Typical downstream analysis steps:",
      "- Ambient RNA correction (SoupX, CellBender)",
      "- Doublet detection (scDblFinder)",
      "- Normalization (log-normalize, scran)",
      "- Feature selection and dimensionality reduction",
      "- Clustering and cell type annotation",
      "",
    ].join("\n"),
  },
  "bio-research-start": {
    "SKILL.md": [
      "---",
      "name: bio-research-start",
      "description: Set up your bio-research environment and explore available tools",
      "metadata:",
      "  plugin: bio-research",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Bio-Research Start",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "You are helping a biological researcher get oriented with the bio-research plugin. Walk through the following steps in order.",
      "",
      "## Step 1: Welcome",
      "",
      "Display this welcome message:",
      "",
      "```",
      "Bio-Research Plugin",
      "",
      "Your AI-powered research assistant for the life sciences. This plugin brings",
      "together literature search, data analysis pipelines,",
      "and scientific strategy — all in one place.",
      "```",
      "",
      "## Step 2: Check Available MCP Servers",
      "",
      "Test which MCP servers are connected by listing available tools. Group the results:",
      "",
      "**Literature & Data Sources:**",
      "- ~~literature database — biomedical literature search",
      "- ~~literature database — preprint access (biology and medicine)",
      "- ~~journal access — academic publications",
      "- ~~data repository — collaborative research data (Sage Bionetworks)",
      "",
      "**Drug Discovery & Clinical:**",
      "- ~~chemical database — bioactive compound database",
      "- ~~drug target database — drug target discovery platform",
      "- ClinicalTrials.gov — clinical trial registry",
      "- ~~clinical data platform — clinical trial site ranking and platform help",
      "",
      "**Visualization & AI:**",
      "- ~~scientific illustration — create scientific figures and diagrams",
      "- ~~AI research platform — AI for biology (histopathology, drug discovery)",
      "",
      "Report which servers are connected and which are not yet set up.",
      "",
      "## Step 3: Survey Available Skills",
      "",
      "List the analysis skills available in this plugin:",
      "",
      "| Skill | What It Does |",
      "|-------|-------------|",
      "| **Single-Cell RNA QC** | Quality control for scRNA-seq data with MAD-based filtering |",
      "| **scvi-tools** | Deep learning for single-cell omics (scVI, scANVI, totalVI, PeakVI, etc.) |",
      "| **Nextflow Pipelines** | Run nf-core pipelines (RNA-seq, WGS/WES, ATAC-seq) |",
      "| **Instrument Data Converter** | Convert lab instrument output to Allotrope ASM format |",
      "| **Scientific Problem Selection** | Systematic framework for choosing research problems |",
      "",
      "## Step 4: Optional Setup — Binary MCP Servers",
      "",
      "Mention that two additional MCP servers are available as separate installations:",
      "",
      "- **~~genomics platform** — Access cloud analysis data and workflows",
      "  Install: Download `txg-node.mcpb` from https://github.com/10XGenomics/txg-mcp/releases",
      "- **~~tool database** (Harvard MIMS) — AI tools for scientific discovery",
      "  Install: Download `tooluniverse.mcpb` from https://github.com/mims-harvard/ToolUniverse/releases",
      "",
      "These require downloading binary files and are optional.",
      "",
      "## Step 5: Ask How to Help",
      "",
      "Ask the researcher what they're working on today. Suggest starting points based on common workflows:",
      "",
      '1. **Literature review** — "Search ~~literature database for recent papers on [topic]"',
      '2. **Analyze sequencing data** — "Run QC on my single-cell data" or "Set up an RNA-seq pipeline"',
      '3. **Drug discovery** — "Search ~~chemical database for compounds targeting [protein]" or "Find drug targets for [disease]"',
      '4. **Data standardization** — "Convert my instrument data to Allotrope format"',
      '5. **Research strategy** — "Help me evaluate a new project idea"',
      "",
      "Wait for the user's response and guide them to the appropriate tools and skills.",
      "",
    ].join("\n"),
  },
  "cowork-plugin-management-cowork-plugin-customizer": {
    "examples/customized-mcp.json": [
      "{",
      '  "mcpServers": {',
      '    "github": {',
      '      "type": "http",',
      '      "url": "https://api.githubcopilot.com/mcp/",',
      '      "headers": {',
      '        "Authorization": "Bearer ${GITHUB_TOKEN}"',
      "      }",
      "    },",
      '    "asana": {',
      '      "type": "sse",',
      '      "url": "https://mcp.asana.com/sse"',
      "    },",
      '    "slack": {',
      '      "type": "http",',
      '      "url": "https://slack.mcp.claude.com/mcp"',
      "    },",
      '    "figma": {',
      '      "type": "http",',
      '      "url": "https://mcp.figma.com/mcp"',
      "    },",
      '    "datadog": {',
      '      "type": "http",',
      '      "url": "https://api.datadoghq.com/mcp",',
      '      "headers": {',
      '        "DD-API-KEY": "${DATADOG_API_KEY}",',
      '        "DD-APPLICATION-KEY": "${DATADOG_APP_KEY}"',
      "      }",
      "    }",
      "  },",
      '  "recommendedCategories": [',
      '    "source-control",',
      '    "project-management",',
      '    "chat",',
      '    "documents",',
      '    "wiki-knowledge-base",',
      '    "design-graphics",',
      '    "analytics-bi"',
      "  ]",
      "}",
      "",
    ].join("\n"),
    "references/mcp-servers.md": [
      "# MCP Discovery and Connection",
      "",
      "How to find and connect MCPs during plugin customization.",
      "",
      "## Available Tools",
      "",
      "### `search_mcp_registry`",
      "Search the MCP directory for available connectors.",
      "",
      '**Input:** `{ "keywords": ["array", "of", "search", "terms"] }`',
      "",
      "**Output:** Up to 10 results, each with:",
      "- `name`: MCP display name",
      "- `description`: One-liner description",
      "- `tools`: List of tool names the MCP provides",
      "- `url`: MCP endpoint URL (use this in `.mcp.json`)",
      "- `directoryUuid`: UUID for use with suggest_connectors",
      "- `connected`: Boolean - whether user has this MCP connected",
      "",
      "### `suggest_connectors`",
      "Display Connect buttons to let users install/connect MCPs.",
      "",
      '**Input:** `{ "directoryUuids": ["uuid1", "uuid2"] }`',
      "",
      "**Output:** Renders UI with Connect buttons for each MCP",
      "",
      "## Category-to-Keywords Mapping",
      "",
      "| Category | Search Keywords |",
      "|----------|-----------------|",
      '| `project-management` | `["asana", "jira", "linear", "monday", "tasks"]` |',
      '| `software-coding` | `["github", "gitlab", "bitbucket", "code"]` |',
      '| `chat` | `["slack", "teams", "discord"]` |',
      '| `documents` | `["google docs", "notion", "confluence"]` |',
      '| `calendar` | `["google calendar", "calendar"]` |',
      '| `email` | `["gmail", "outlook", "email"]` |',
      '| `design-graphics` | `["figma", "sketch", "design"]` |',
      '| `analytics-bi` | `["datadog", "grafana", "analytics"]` |',
      '| `crm` | `["salesforce", "hubspot", "crm"]` |',
      '| `wiki-knowledge-base` | `["notion", "confluence", "outline", "wiki"]` |',
      '| `data-warehouse` | `["bigquery", "snowflake", "redshift"]` |',
      '| `conversation-intelligence` | `["gong", "chorus", "call recording"]` |',
      "",
      "## Workflow",
      "",
      "1. **Find customization point**: Look for `~~`-prefixed values (e.g., `~~Jira`)",
      "2. **Check earlier phase findings**: Did you already learn which tool they use?",
      "   - **Yes**: Search for that specific tool to get its `url`, skip to step 5",
      "   - **No**: Continue to step 3",
      "3. **Search**: Call `search_mcp_registry` with mapped keywords",
      "4. **Present choices and ask user**: Show all results, ask which they use",
      "5. **Connect if needed**: If not connected, call `suggest_connectors`",
      "6. **Update MCP config**: Add config using the `url` from search results",
      "",
      "## Updating Plugin MCP Configuration",
      "",
      "### Finding the Config File",
      "",
      "1. **Check `plugin.json`** for an `mcpServers` field:",
      "   ```json",
      "   {",
      '     "name": "my-plugin",',
      '     "mcpServers": "./config/servers.json"',
      "   }",
      "   ```",
      "   If present, edit the file at that path.",
      "",
      "2. **If no `mcpServers` field**, use `.mcp.json` at the plugin root (default).",
      "",
      "3. **If `mcpServers` points only to `.mcpb` files** (bundled servers), create a new `.mcp.json` at the plugin root.",
      "",
      "### Config File Format",
      "",
      "Both wrapped and unwrapped formats are supported:",
      "",
      "```json",
      "{",
      '  "mcpServers": {',
      '    "github": {',
      '      "type": "http",',
      '      "url": "https://api.githubcopilot.com/mcp/"',
      "    }",
      "  }",
      "}",
      "```",
      "",
      "Use the `url` field from `search_mcp_registry` results.",
      "",
      "**Note:** First-party integrations (Gmail, Google Calendar, Google Drive) are connected at the user level and don't need plugin `.mcp.json` entries.",
      "",
    ].join("\n"),
    "references/search-strategies.md": [
      "# Knowledge MCP Search Strategies",
      "",
      "Query patterns for gathering organizational context during plugin customization.",
      "",
      "## Finding Tool Names",
      "",
      "**Source control:**",
      '- Search: "GitHub" OR "GitLab" OR "Bitbucket"',
      '- Search: "pull request" OR "merge request"',
      "- Look for: repository links, CI/CD mentions",
      "",
      "**Headquarter management:**",
      '- Search: "Asana" OR "Jira" OR "Linear" OR "Monday"',
      '- Search: "sprint" AND "tickets"',
      "- Look for: task links, project board mentions",
      "",
      "**Chat:**",
      '- Search: "Slack" OR "Teams" OR "Discord"',
      "- Look for: channel mentions, integration discussions",
      "",
      "**Analytics:**",
      '- Search: "Datadog" OR "Grafana" OR "Mixpanel"',
      '- Search: "monitoring" OR "observability"',
      "- Look for: dashboard links, alert configurations",
      "",
      "**Design:**",
      '- Search: "Figma" OR "Sketch" OR "Adobe XD"',
      "- Look for: design file links, handoff discussions",
      "",
      "**CRM:**",
      '- Search: "Salesforce" OR "HubSpot"',
      "- Look for: deal mentions, customer record links",
      "",
      "## Finding Organization Values",
      "",
      "**Workspace/project IDs:**",
      "- Search for existing integrations or bookmarked links",
      "- Look for admin/setup documentation",
      "",
      "**Team conventions:**",
      '- Search: "story points" OR "estimation"',
      '- Search: "workflow" OR "ticket status"',
      "- Look for engineering process docs",
      "",
      "**Channel/team names:**",
      '- Search: "standup" OR "engineering" OR "releases"',
      "- Look for channel naming patterns",
      "",
      "## When Knowledge MCPs Are Unavailable",
      "",
      "If no knowledge MCPs are configured, skip automatic discovery and proceed directly to AskUserQuestion for all categories. Note: AskUserQuestion always includes a Skip button and a free-text input box for custom answers, so do not include `None` or `Other` as options.",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: cowork-plugin-management-cowork-plugin-customizer",
      'description: "Customize or personalize a Claude Code plugin for a specific organization\'s tools and workflows. Use when users want to customize a plugin, replace tool placeholders, or configure MCP servers for a plugin. This skill requires Cowork mode with mounted plugin directories and will not work in remote or standard CLI sessions."',
      "metadata:",
      "  plugin: cowork-plugin-management",
      "  type: skill",
      '  plugin-version: "0.1.0"',
      "---",
      "",
      "# Cowork Plugin Customization",
      "",
      "Adapt a generic plugin template to a specific organization by replacing customization points with actual tool names, configuring MCP servers, and applying organization-specific customizations.",
      "",
      '> **Finding the plugin**: To find the plugin\'s source files, run `find mnt/.local-plugins mnt/.plugins -type d -name "*<plugin-name>*"` to locate the plugin directory, then read its files to understand its structure before making changes. If you cannot find the plugin directory, the user is likely running this conversation in a remote container. Abort and let them know: "Customizing plugins is currently only available in the desktop app\'s Cowork mode."',
      "",
      "## Overview",
      "",
      "Generic plugins mark customization points with a `~~` prefix. Any line or value starting with `~~` is a placeholder that should be replaced during customization (e.g., `~~Jira` → `Asana`, `~~your-team-channel` → `#engineering`). To find all customization points in a plugin, use:",
      "",
      "```bash",
      "grep -rn '~~\\w' /path/to/plugin --include='*.md' --include='*.json'",
      "```",
      "",
      "> **Important**: Never change the name of the plugin or skill being customized. Only replace `~~`-prefixed placeholder values and update content — do not rename directories, files, or the plugin/skill name fields.",
      "",
      "> **Nontechnical output**: All user-facing output (todo list items, questions, summaries) must be written in plain, nontechnical language. Never mention `~~` prefixes, placeholders, or customization points to the user. Frame everything in terms of learning about the organization and its tools.",
      "",
      "The process:",
      "1. **Gather context** — use knowledge MCPs to learn what tools and processes the organization uses",
      "2. **Create todo list** — grep for `~~\\w` to find all customization points and build a todo list",
      "3. **Complete todo items** — apply gathered context, falling back to user questions when unclear",
      "4. **Search for useful MCPs** — find and connect MCPs for identified tools",
      "",
      "If an answer cannot be found via knowledge MCPs or user input, leave the customization point unchanged for a future customization cycle.",
      "",
      "## Customization Workflow",
      "",
      "### Phase 1: Gather Context from Knowledge MCPs",
      "",
      "Use company-internal knowledge MCPs to collect information. See `references/search-strategies.md` for detailed query patterns by category.",
      "",
      "**What to gather:**",
      "- Tool names for each `~~`-prefixed placeholder",
      "- Organizational processes and workflows",
      "- Team conventions (naming, statuses, estimation scales)",
      "- Configuration values (workspace IDs, project names, team identifiers)",
      "",
      "**Sources to search:**",
      "1. **Chat/Slack MCPs** — tool mentions, integrations, workflow discussions",
      "2. **Document MCPs** — onboarding docs, tool guides, setup instructions",
      "3. **Email MCPs** — license notifications, admin emails, setup invitations",
      "",
      "Record all findings for use in Phase 3.",
      "",
      "### Phase 2: Create Todo List from Customization Points",
      "",
      "Run `grep -rn '~~\\w' /path/to/plugin --include='*.md' --include='*.json'` to find all customization points. Group them by theme and create a todo list with user-friendly descriptions that focus on learning about the organization:",
      "",
      '- **Good**: "Learn how standup prep works at Company"',
      '- **Bad**: "Replace placeholders in commands/standup-prep.md"',
      "",
      "### Phase 3: Complete Todo Items",
      "",
      "Work through each item using Phase 1 context.",
      "",
      "**If knowledge MCPs provided a clear answer**: Apply directly without confirmation.",
      "",
      "**Otherwise**: Use AskUserQuestion. Don't assume \"industry standard\" defaults are correct — if knowledge MCPs didn't provide a specific answer, ask. Note: AskUserQuestion always includes a Skip button and a free-text input box for custom answers, so do not include `None` or `Other` as options.",
      "",
      "**Types of changes:**",
      "",
      "1. **Customization point replacements**: `~~Jira` → `Asana`, `~~your-org-channel` → `#engineering`",
      "2. **URL pattern updates**: `tickets.example.com/your-team/123` → `app.asana.com/0/PROJECT_ID/TASK_ID`",
      "3. **Organization-specific values**: Workspace IDs, project names, team identifiers",
      "",
      "If user doesn't know or skips, leave the `~~`-prefixed value unchanged.",
      "",
      "### Phase 4: Search for Useful MCPs",
      "",
      "After all customization points have been resolved, connect MCPs for the tools that were identified. See `references/mcp-servers.md` for the full workflow, category-to-keywords mapping, and config file format.",
      "",
      "For each tool identified during customization:",
      "1. Search the registry: `search_mcp_registry(keywords=[...])` using category keywords from `references/mcp-servers.md`, or search for the specific tool name if already known",
      '2. If unconnected: `suggest_connectors(directoryUuids=["chosen-uuid"])` — user completes OAuth',
      "3. Update the plugin's MCP config file (check `plugin.json` for custom location, otherwise `.mcp.json` at root)",
      "",
      "Collect all MCP results and present them together in the summary output (see below) — don't present MCPs one at a time during this phase.",
      "",
      "**Note:** First-party integrations (Gmail, Google Calendar, Google Drive) are connected at the user level and don't need plugin `.mcp.json` entries.",
      "",
      "## Packaging the Plugin",
      "",
      "After all customizations are applied, package the plugin as a `.plugin` file for the user:",
      "",
      "1. **Zip the plugin directory** (excluding `setup/` since it's no longer needed):",
      "   ```bash",
      '   cd /path/to/plugin && zip -r /tmp/plugin-name.plugin . -x "setup/*" && cp /tmp/plugin-name.plugin /path/to/outputs/plugin-name.plugin',
      "   ```",
      "2. **Present the file to the user** with the `.plugin` extension so they can install it directly. (Presenting the .plugin file will show to the user as a rich preview where they can look through the plugin files, and they can accept the customization by pressing a button.)",
      "",
      "> **Important**: Always create the zip in `/tmp/` first, then copy to the outputs folder. Writing directly to the outputs folder may fail due to permissions and leave behind temporary files.",
      "",
      "> **Naming**: Use the original plugin directory name for the `.plugin` file (e.g., if the plugin directory is `workbench`, the output file should be `workbench.plugin`). Do not rename the plugin or its files during customization — only replace placeholder values and update content.",
      "",
      "## Summary Output",
      "",
      "After customization, present the user with a summary of what was learned grouped by source. Always include the MCPs sections showing which MCPs were connected during setup and which ones the user should still connect:",
      "",
      "```markdown",
      "## From searching Slack",
      "- You use Asana for project management",
      "- Sprint cycles are 2 weeks",
      "",
      "## From searching documents",
      "- Story points use T-shirt sizes",
      "",
      "## From your answers",
      "- Ticket statuses are: Backlog, In Progress, In Review, Done",
      "```",
      "",
      "Then present the MCPs that were connected during setup and any that the user should still connect, with instructions on how to connect them.",
      "",
      "If no knowledge MCPs were available in Phase 1, and the user had to answer at least one question manually, include a note at the end:",
      "> By the way, connecting sources like Slack or Microsoft Teams would let me find answers automatically next time you customize a plugin.",
      "",
      "## Additional Resources",
      "",
      "- **`references/mcp-servers.md`** — MCP discovery workflow, category-to-keywords mapping, config file locations",
      "- **`references/search-strategies.md`** — Knowledge MCP query patterns for finding tool names and org values",
      "- **`examples/customized-mcp.json`** — Example fully configured `.mcp.json`",
      "",
    ].join("\n"),
  },
  "customer-support-customer-research": {
    "SKILL.md": [
      "---",
      "name: customer-support-customer-research",
      "description: Research customer questions by searching across documentation, knowledge bases, and connected sources, then synthesize a confidence-scored answer. Use when a customer asks a question you need to investigate, when building background on a customer situation, or when you need account context.",
      "metadata:",
      "  plugin: customer-support",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Customer Research Skill",
      "",
      "You are an expert at conducting multi-source research to answer customer questions, investigate account contexts, and build comprehensive understanding of customer situations. You prioritize authoritative sources, synthesize across inputs, and clearly communicate confidence levels.",
      "",
      "## Multi-Source Research Methodology",
      "",
      "### Research Process",
      "",
      "**Step 1: Understand the Question**",
      "Before searching, clarify what you're actually trying to find:",
      "- Is this a factual question with a definitive answer?",
      "- Is this a contextual question requiring multiple perspectives?",
      "- Is this an exploratory question where the scope is still being defined?",
      "- Who is the audience for the answer (internal team, customer, leadership)?",
      "",
      "**Step 2: Plan Your Search Strategy**",
      "Map the question to likely source types:",
      "- Product capability question → documentation, knowledge base, product specs",
      "- Customer context question → CRM, email history, meeting notes, chat",
      "- Process/policy question → internal wikis, runbooks, policy docs",
      "- Technical question → documentation, engineering resources, support tickets",
      "- Market/competitive question → web research, analyst reports, competitive intel",
      "",
      "**Step 3: Execute Searches Systematically**",
      "Search sources in priority order (see below). Don't stop at the first result — cross-reference across sources.",
      "",
      "**Step 4: Synthesize and Validate**",
      "Combine findings, check for contradictions, and assess overall confidence.",
      "",
      "**Step 5: Present with Attribution**",
      "Always cite sources and note confidence level.",
      "",
      "## Source Prioritization",
      "",
      "Search sources in this order, with decreasing authority:",
      "",
      "### Tier 1 — Official Internal Sources (Highest Confidence)",
      "These are authoritative and should be trusted unless outdated.",
      "",
      "- **Product documentation**: Official docs, specs, API references",
      "- **Knowledge base / wiki**: Internal articles, runbooks, FAQs",
      "- **Policy documents**: Official policies, terms, SLAs",
      "- **Product roadmap** (internal-facing): Feature timelines, priorities",
      "",
      "Confidence level: **High** (unless clearly outdated — check dates)",
      "",
      "### Tier 2 — Organizational Context",
      "These provide context but may reflect one perspective.",
      "",
      "- **CRM records**: Account notes, activity history, opportunity details",
      "- **Support tickets**: Previous resolutions, known issues, workarounds",
      "- **Internal documents** (Drive, shared folders): Specs, plans, analyses",
      "- **Meeting notes**: Previous discussions, decisions, commitments",
      "",
      "Confidence level: **Medium-High** (may be subjective or incomplete)",
      "",
      "### Tier 3 — Team Communications",
      "Informal but often contain the most recent information.",
      "",
      "- **Chat history**: Team discussions, quick answers, context",
      "- **Email threads**: Customer correspondence, internal discussions",
      "- **Calendar notes**: Meeting agendas and post-meeting notes",
      "",
      "Confidence level: **Medium** (informal, may be out of context, could be speculative)",
      "",
      "### Tier 4 — External Sources",
      "Useful for general knowledge but not authoritative for internal matters.",
      "",
      "- **Web search**: Official websites, blog posts, industry resources",
      "- **Community forums**: User discussions, workarounds, experiences",
      "- **Third-party documentation**: Integration partners, complementary tools",
      "- **News and analyst reports**: Market context, competitive intelligence",
      "",
      "Confidence level: **Low-Medium** (may not reflect your specific situation)",
      "",
      "### Tier 5 — Inferred or Analogical",
      "Use when direct sources don't yield answers.",
      "",
      "- **Similar situations**: How similar questions were handled before",
      "- **Analogous customers**: What worked for comparable accounts",
      "- **General best practices**: Industry standards and norms",
      "",
      "Confidence level: **Low** (clearly flag as inference, not fact)",
      "",
      "## Answer Synthesis",
      "",
      "### Confidence Levels",
      "",
      "Always assign and communicate a confidence level:",
      "",
      "**High Confidence:**",
      "- Answer confirmed by official documentation or authoritative source",
      "- Multiple sources corroborate the same answer",
      "- Information is current (verified within a reasonable timeframe)",
      '- "I\'m confident this is accurate based on [source]."',
      "",
      "**Medium Confidence:**",
      "- Answer found in informal sources (chat, email) but not official docs",
      "- Single source without corroboration",
      "- Information may be slightly outdated but likely still valid",
      '- "Based on [source], this appears to be the case, but I\'d recommend confirming with [team/person]."',
      "",
      "**Low Confidence:**",
      "- Answer is inferred from related information",
      "- Sources are outdated or potentially unreliable",
      "- Contradictory information found across sources",
      '- "I wasn\'t able to find a definitive answer. Based on [context], my best assessment is [answer], but this should be verified before sharing with the customer."',
      "",
      "**Unable to Determine:**",
      "- No relevant information found in any source",
      "- Question requires specialized knowledge not available in sources",
      '- "I couldn\'t find information about this. I recommend reaching out to [suggested expert/team] for a definitive answer."',
      "",
      "### Handling Contradictions",
      "",
      "When sources disagree:",
      "1. Note the contradiction explicitly",
      "2. Identify which source is more authoritative or more recent",
      "3. Present both perspectives with context",
      "4. Recommend how to resolve the discrepancy",
      "5. If going to a customer: use the most conservative/cautious answer until resolved",
      "",
      "### Synthesis Structure",
      "",
      "```",
      "**Direct Answer:** [Bottom-line answer — lead with this]",
      "",
      "**Confidence:** [High / Medium / Low]",
      "",
      "**Supporting Evidence:**",
      "- [Source 1]: [What it says]",
      "- [Source 2]: [What it says — corroborates or adds nuance]",
      "",
      "**Caveats:**",
      "- [Any limitations or conditions on the answer]",
      "- [Anything that might change the answer in specific contexts]",
      "",
      "**Recommendation:**",
      "- [Whether this is ready to share with customers]",
      "- [Any verification steps recommended]",
      "```",
      "",
      "## When to Escalate vs. Answer Directly",
      "",
      "### Answer Directly When:",
      "- Official documentation clearly addresses the question",
      "- Multiple reliable sources corroborate the answer",
      "- The question is factual and non-sensitive",
      "- The answer doesn't involve commitments, timelines, or pricing",
      "- You've answered similar questions before with confirmed accuracy",
      "",
      "### Escalate or Verify When:",
      "- The answer involves product roadmap commitments or timelines",
      "- Pricing, legal terms, or contract-specific questions",
      "- Security, compliance, or data handling questions",
      "- The answer could set a precedent or create expectations",
      "- You found contradictory information in sources",
      "- The question involves a specific customer's custom configuration",
      "- The answer requires specialized expertise you don't have",
      "- The customer is at risk and the wrong answer could exacerbate the situation",
      "",
      "### Escalation Path:",
      "1. **Subject matter expert**: For technical or domain-specific questions",
      "2. **Product team**: For roadmap, feature, or capability questions",
      "3. **Legal/compliance**: For terms, privacy, security, or regulatory questions",
      "4. **Billing/finance**: For pricing, invoice, or payment-related questions",
      "5. **Engineering**: For custom configurations, bugs, or technical root causes",
      "6. **Leadership**: For strategic decisions, exceptions, or high-stakes situations",
      "",
      "## Research Documentation for Team Knowledge Base",
      "",
      "After completing research, capture the knowledge for future use:",
      "",
      "### When to Document:",
      "- Question has come up before or likely will again",
      "- Research took significant effort to compile",
      "- Answer required synthesizing multiple sources",
      "- Answer corrects a common misunderstanding",
      "- Answer involves nuance that's easy to get wrong",
      "",
      "### Documentation Format:",
      "```",
      "## [Question/Topic]",
      "",
      "**Last Verified:** [date]",
      "**Confidence:** [level]",
      "",
      "### Answer",
      "[Clear, direct answer]",
      "",
      "### Details",
      "[Supporting detail, context, and nuance]",
      "",
      "### Sources",
      "[Where this information came from]",
      "",
      "### Related Questions",
      "[Other questions this might help answer]",
      "",
      "### Review Notes",
      "[When to re-verify, what might change this answer]",
      "```",
      "",
      "### Knowledge Base Hygiene:",
      "- Date-stamp all entries",
      "- Flag entries that reference specific product versions or features",
      "- Review and update entries quarterly",
      "- Archive entries that are no longer relevant",
      "- Tag entries for searchability (by topic, product area, customer segment)",
      "",
      "## Using This Skill",
      "",
      "When conducting customer research:",
      "",
      "1. Always start by clarifying what you're actually looking for",
      "2. Search systematically — don't skip tiers even if you think you know where the answer is",
      "3. Cross-reference findings across multiple sources",
      "4. Be transparent about confidence levels — never present uncertain information as fact",
      "5. When in doubt about whether to share with a customer, err on the side of verifying first",
      "6. Document your research for future team benefit",
      "7. If the research reveals a gap in your knowledge base, flag it for documentation",
      "",
    ].join("\n"),
  },
  "customer-support-draft-response": {
    "SKILL.md": [
      "---",
      "name: customer-support-draft-response",
      "description: Draft a professional customer-facing response tailored to the situation and relationship",
      "metadata:",
      "  plugin: customer-support",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <situation description>",
      "---",
      "",
      "# Draft Response",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Draft a professional, customer-facing response tailored to the situation, customer relationship, and communication context.",
      "",
      "## Usage",
      "",
      "```",
      "/draft-response <context about the customer question, issue, or request>",
      "```",
      "",
      "Examples:",
      "- `/draft-response Acme Corp is asking when the new dashboard feature will ship`",
      "- `/draft-response Customer escalation — their integration has been down for 2 days`",
      "- `/draft-response Responding to a feature request we won't be building`",
      "- `/draft-response Customer hit a billing error and wants a resolution ASAP`",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Context",
      "",
      "Parse the user's input to determine:",
      "",
      "- **Customer**: Who is the communication for? Look up account context if available.",
      "- **Situation type**: Question, issue, escalation, announcement, negotiation, bad news, good news, follow-up",
      "- **Urgency**: Is this time-sensitive? How long has the customer been waiting?",
      "- **Channel**: Email, support ticket, chat, or other (adjust formality accordingly)",
      "- **Relationship stage**: New customer, established, frustrated/escalated",
      "- **Stakeholder level**: End user, manager, executive, technical, business",
      "",
      "### 2. Research Context",
      "",
      "Gather relevant background from available sources:",
      "",
      "**~~email:**",
      "- Previous correspondence with this customer on this topic",
      "- Any commitments or timelines previously shared",
      "- Tone and style of the existing thread",
      "",
      "**~~chat:**",
      "- Internal discussions about this customer or topic",
      "- Any guidance from product, engineering, or leadership",
      "- Similar situations and how they were handled",
      "",
      "**~~CRM (if connected):**",
      "- Account details and plan level",
      "- Contact information and key stakeholders",
      "- Previous escalations or sensitive issues",
      "",
      "**~~support platform (if connected):**",
      "- Related tickets and their resolution",
      "- Known issues or workarounds",
      "- SLA status and response time commitments",
      "",
      "**~~knowledge base:**",
      "- Official documentation or help articles to reference",
      "- Product roadmap information (if shareable)",
      "- Policy or process documentation",
      "",
      "### 3. Generate the Draft",
      "",
      "Produce a response tailored to the situation:",
      "",
      "```",
      "## Draft Response",
      "",
      "**To:** [Customer contact name]",
      "**Re:** [Subject/topic]",
      "**Channel:** [Email / Ticket / Chat]",
      "**Tone:** [Empathetic / Professional / Technical / Celebratory / Candid]",
      "",
      "---",
      "",
      "[Draft response text]",
      "",
      "---",
      "",
      "### Notes for You (internal — do not send)",
      "- **Why this approach:** [Rationale for tone and content choices]",
      "- **Things to verify:** [Any facts or commitments to confirm before sending]",
      "- **Risk factors:** [Anything sensitive about this response]",
      "- **Follow-up needed:** [Actions to take after sending]",
      "- **Escalation note:** [If this should be reviewed by someone else first]",
      "```",
      "",
      "### 4. Situation-Specific Approaches",
      "",
      "**Answering a product question:**",
      "- Lead with the direct answer",
      "- Provide relevant documentation links",
      "- Offer to connect them with the right resource if needed",
      "- If you don't know the answer: say so honestly, commit to finding out, give a timeline",
      "",
      "**Responding to an issue or bug:**",
      "- Acknowledge the impact on their work",
      "- State what you know about the issue and its status",
      "- Provide workaround if available",
      "- Set expectations for resolution timeline",
      "- Commit to updates at regular intervals",
      "",
      "**Handling an escalation:**",
      "- Acknowledge the severity and their frustration",
      "- Take ownership (no deflecting or excuse-making)",
      "- Provide a clear action plan with timeline",
      "- Identify the person accountable for resolution",
      "- Offer a meeting or call if appropriate for the severity",
      "",
      "**Delivering bad news (feature sunset, delay, can't-fix):**",
      "- Be direct — don't bury the news",
      "- Explain the reasoning honestly",
      "- Acknowledge the impact on them specifically",
      "- Offer alternatives or mitigation",
      "- Provide a clear path forward",
      "",
      "**Sharing good news (feature launch, milestone, recognition):**",
      "- Lead with the positive outcome",
      "- Connect it to their specific goals or use case",
      "- Suggest next steps to capitalize on the good news",
      "- Express genuine enthusiasm",
      "",
      "**Declining a request (feature request, discount, exception):**",
      "- Acknowledge the request and its reasoning",
      "- Be honest about the decision",
      "- Explain the why without being dismissive",
      "- Offer alternatives when possible",
      "- Leave the door open for future conversation",
      "",
      "### 5. Response Quality Checks",
      "",
      "Before presenting the draft, verify:",
      "",
      "- [ ] Tone matches the situation and relationship",
      "- [ ] No commitments beyond what's authorized",
      "- [ ] No product roadmap details that shouldn't be shared externally",
      "- [ ] Accurate references to previous conversations",
      "- [ ] Clear next steps and ownership",
      "- [ ] Appropriate for the stakeholder level (not too technical for executives, not too vague for engineers)",
      "- [ ] Length is appropriate for the channel (shorter for chat, fuller for email)",
      "",
      "### 6. Offer Iterations",
      "",
      "After presenting the draft:",
      '- "Want me to adjust the tone? (more formal, more casual, more empathetic, more direct)"',
      '- "Should I add or remove any specific points?"',
      '- "Want me to make this shorter/longer?"',
      '- "Should I draft a version for a different stakeholder?"',
      '- "Want me to draft the internal escalation note as well?"',
      '- "Should I prepare a follow-up message to send after [X days] if no response?"',
      "",
    ].join("\n"),
  },
  "customer-support-escalate": {
    "SKILL.md": [
      "---",
      "name: customer-support-escalate",
      "description: Package an escalation for engineering, product, or leadership with full context",
      "metadata:",
      "  plugin: customer-support",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <issue summary> [customer name]",
      "---",
      "",
      "# Escalate",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Package a support issue into a structured escalation brief for engineering, product, or leadership. Gathers context, structures reproduction steps, assesses business impact, and identifies the right escalation target.",
      "",
      "## Usage",
      "",
      "```",
      "/escalate <issue description> [customer name or account]",
      "```",
      "",
      "Examples:",
      "- `/escalate API returning 500 errors intermittently for Acme Corp`",
      "- `/escalate Data export is missing rows — 3 customers reported this week`",
      "- `/escalate SSO login loop affecting all Enterprise customers`",
      "- `/escalate Customer threatening to churn over missing audit log feature`",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Issue",
      "",
      "Parse the input and determine:",
      "",
      "- **What's broken or needed**: The core technical or product issue",
      "- **Who's affected**: Specific customer(s), segment, or all users",
      "- **How long**: When did this start? How long has the customer been waiting?",
      "- **What's been tried**: Any troubleshooting or workarounds attempted",
      "- **Why escalate now**: What makes this need attention beyond normal support",
      "",
      'Use the "When to Escalate vs. Handle in Support" criteria from the **escalation** skill to confirm this warrants escalation.',
      "",
      "### 2. Gather Context",
      "",
      "Pull together relevant information from available sources:",
      "",
      "- **~~support platform**: Related tickets, timeline of communications, previous troubleshooting",
      "- **~~CRM** (if connected): Account details, key contacts, previous escalations",
      "- **~~chat**: Internal discussions about this issue, similar reports from other customers",
      "- **~~project tracker** (if connected): Related bug reports or feature requests, engineering status",
      "- **~~knowledge base**: Known issues or workarounds, relevant documentation",
      "",
      "### 3. Assess Business Impact",
      "",
      "Using the impact dimensions from the **escalation** skill, quantify:",
      "",
      "- **Breadth**: How many customers/users affected? Growing?",
      "- **Depth**: Blocked vs. inconvenienced?",
      "- **Duration**: How long has this been going on?",
      "- **Revenue**: ARR at risk? Pending deals affected?",
      "- **Time pressure**: Hard deadline?",
      "",
      "### 4. Determine Escalation Target",
      "",
      "Using the escalation tiers from the **escalation** skill, identify the right target: L2 Support, Engineering, Product, Security, or Leadership.",
      "",
      "### 5. Structure Reproduction Steps (for bugs)",
      "",
      "If the issue is a bug, follow the reproduction step best practices from the **escalation** skill to document clear repro steps with environment details and evidence.",
      "",
      "### 6. Generate Escalation Brief",
      "",
      "```",
      "## ESCALATION: [One-line summary]",
      "",
      "**Severity:** [Critical / High / Medium]",
      "**Target team:** [Engineering / Product / Security / Leadership]",
      "**Reported by:** [Your name/team]",
      "**Date:** [Today's date]",
      "",
      "### Impact",
      "- **Customers affected:** [Who and how many]",
      "- **Workflow impact:** [What they can't do]",
      "- **Revenue at risk:** [If applicable]",
      "- **Time in queue:** [How long this has been an issue]",
      "",
      "### Issue Description",
      "[Clear, concise description of the problem — 3-5 sentences]",
      "",
      "### What's Been Tried",
      "1. [Troubleshooting step and result]",
      "2. [Troubleshooting step and result]",
      "3. [Troubleshooting step and result]",
      "",
      "### Reproduction Steps",
      "[If applicable — follow the format from the escalation skill]",
      "",
      "### Customer Communication",
      "- **Last update to customer:** [Date and what was communicated]",
      "- **Customer expectation:** [What they're expecting and by when]",
      "- **Escalation risk:** [Will they escalate further if not resolved by X?]",
      "",
      "### What's Needed",
      '- [Specific ask — "investigate root cause", "prioritize fix",',
      '  "make product decision on X", "approve exception for Y"]',
      "- **Deadline:** [When this needs resolution or an update]",
      "",
      "### Supporting Context",
      "- [Related tickets or links]",
      "- [Internal discussion threads]",
      "- [Documentation or logs]",
      "```",
      "",
      "### 7. Offer Next Steps",
      "",
      "After generating the escalation:",
      '- "Want me to post this in a ~~chat channel for the target team?"',
      '- "Should I update the customer with an interim response?"',
      '- "Want me to set a follow-up reminder to check on this?"',
      '- "Should I draft a customer-facing update with the current status?"',
      "",
    ].join("\n"),
  },
  "customer-support-escalation": {
    "SKILL.md": [
      "---",
      "name: customer-support-escalation",
      "description: Structure and package support escalations for engineering, product, or leadership with full context, reproduction steps, and business impact. Use when an issue needs to go beyond support, when writing an escalation brief, or when assessing whether an issue warrants escalation.",
      "metadata:",
      "  plugin: customer-support",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Escalation Skill",
      "",
      "You are an expert at determining when and how to escalate support issues. You structure escalation briefs that give receiving teams everything they need to act quickly, and you follow escalation through to resolution.",
      "",
      "## When to Escalate vs. Handle in Support",
      "",
      "### Handle in Support When:",
      "- The issue has a documented solution or known workaround",
      "- It's a configuration or setup issue you can resolve",
      "- The customer needs guidance or training, not a fix",
      "- The issue is a known limitation with a documented alternative",
      "- Previous similar tickets were resolved at the support level",
      "",
      "### Escalate When:",
      "- **Technical**: Bug confirmed and needs a code fix, infrastructure investigation needed, data corruption or loss",
      "- **Complexity**: Issue is beyond support's ability to diagnose, requires access support doesn't have, involves custom implementation",
      "- **Impact**: Multiple customers affected, production system down, data integrity at risk, security concern",
      "- **Business**: High-value customer at risk, SLA breach imminent or occurred, customer requesting executive involvement",
      "- **Time**: Issue has been open beyond SLA, customer has been waiting unreasonably long, normal support channels aren't progressing",
      "- **Pattern**: Same issue reported by 3+ customers, recurring issue that was supposedly fixed, increasing severity over time",
      "",
      "## Escalation Tiers",
      "",
      "### L1 → L2 (Support Escalation)",
      "**From:** Frontline support",
      "**To:** Senior support / technical support specialists",
      "**When:** Issue requires deeper investigation, specialized product knowledge, or advanced troubleshooting",
      "**What to include:** Ticket summary, steps already tried, customer context",
      "",
      "### L2 → Engineering",
      "**From:** Senior support",
      "**To:** Engineering team (relevant product area)",
      "**When:** Confirmed bug, infrastructure issue, needs code change, requires system-level investigation",
      "**What to include:** Full reproduction steps, environment details, logs or error messages, business impact, customer timeline",
      "",
      "### L2 → Product",
      "**From:** Senior support",
      "**To:** Product management",
      "**When:** Feature gap causing customer pain, design decision needed, workflow doesn't match customer expectations, competing customer needs require prioritization",
      "**What to include:** Customer use case, business impact, frequency of request, competitive pressure (if known)",
      "",
      "### Any → Security",
      "**From:** Any support tier",
      "**To:** Security team",
      "**When:** Potential data exposure, unauthorized access, vulnerability report, compliance concern",
      "**What to include:** What was observed, who/what is potentially affected, immediate containment steps taken, urgency assessment",
      "**Note:** Security escalations bypass normal tier progression — escalate immediately regardless of your level",
      "",
      "### Any → Leadership",
      "**From:** Any tier (usually L2 or manager)",
      "**To:** Support leadership, executive team",
      "**When:** High-revenue customer threatening churn, SLA breach on critical account, cross-functional decision needed, exception to policy required, PR or legal risk",
      "**What to include:** Full business context, revenue at risk, what's been tried, specific decision or action needed, deadline",
      "",
      "## Structured Escalation Format",
      "",
      "Every escalation should follow this structure:",
      "",
      "```",
      "ESCALATION: [One-line summary]",
      "Severity: [Critical / High / Medium]",
      "Target: [Engineering / Product / Security / Leadership]",
      "",
      "IMPACT",
      "- Customers affected: [Number and names if relevant]",
      "- Workflow impact: [What's broken for them]",
      "- Revenue at risk: [If applicable]",
      "- SLA status: [Within SLA / At risk / Breached]",
      "",
      "ISSUE DESCRIPTION",
      "[3-5 sentences: what's happening, when it started,",
      "how it manifests, scope of impact]",
      "",
      "REPRODUCTION STEPS (for bugs)",
      "1. [Step]",
      "2. [Step]",
      "3. [Step]",
      "Expected: [X]",
      "Actual: [Y]",
      "Environment: [Details]",
      "",
      "WHAT'S BEEN TRIED",
      "1. [Action] → [Result]",
      "2. [Action] → [Result]",
      "3. [Action] → [Result]",
      "",
      "CUSTOMER COMMUNICATION",
      "- Last update: [Date — what was said]",
      "- Customer expectation: [What they expect and by when]",
      "- Escalation risk: [Will they escalate further?]",
      "",
      "WHAT'S NEEDED",
      "- [Specific ask: investigate, fix, decide, approve]",
      "- Deadline: [Date/time]",
      "",
      "SUPPORTING CONTEXT",
      "- [Ticket links]",
      "- [Internal threads]",
      "- [Logs or screenshots]",
      "```",
      "",
      "## Business Impact Assessment",
      "",
      "When escalating, quantify impact where possible:",
      "",
      "### Impact Dimensions",
      "",
      "| Dimension | Questions to Answer |",
      "|-----------|-------------------|",
      "| **Breadth** | How many customers/users are affected? Is it growing? |",
      "| **Depth** | How severely are they impacted? Blocked vs. inconvenienced? |",
      "| **Duration** | How long has this been going on? How long until it's critical? |",
      "| **Revenue** | What's the ARR at risk? Are there pending deals affected? |",
      "| **Reputation** | Could this become public? Is it a reference customer? |",
      "| **Contractual** | Are SLAs being breached? Are there contractual obligations? |",
      "",
      "### Severity Shorthand",
      "",
      "- **Critical**: Production down, data at risk, security breach, or multiple high-value customers affected. Needs immediate attention.",
      "- **High**: Major functionality broken, key customer blocked, SLA at risk. Needs same-day attention.",
      "- **Medium**: Significant issue with workaround, important but not urgent business impact. Needs attention this week.",
      "",
      "## Writing Reproduction Steps",
      "",
      "Good reproduction steps are the single most valuable thing in a bug escalation. Follow these practices:",
      "",
      "1. **Start from a clean state**: Describe the starting point (account type, configuration, permissions)",
      '2. **Be specific**: "Click the Export button in the top-right of the Dashboard page" not "try to export"',
      '3. **Include exact values**: Use specific inputs, dates, IDs — not "enter some data"',
      "4. **Note the environment**: Browser, OS, account type, feature flags, plan level",
      "5. **Capture the frequency**: Always reproducible? Intermittent? Only under certain conditions?",
      "6. **Include evidence**: Screenshots, error messages (exact text), network logs, console output",
      '7. **Note what you\'ve ruled out**: "Tested in Chrome and Firefox — same behavior" "Not account-specific — reproduced on test account"',
      "",
      "## Follow-up Cadence After Escalation",
      "",
      "Don't escalate and forget. Maintain ownership of the customer relationship.",
      "",
      "| Severity | Internal Follow-up | Customer Update |",
      "|----------|-------------------|-----------------|",
      "| **Critical** | Every 2 hours | Every 2-4 hours (or per SLA) |",
      "| **High** | Every 4 hours | Every 4-8 hours |",
      "| **Medium** | Daily | Every 1-2 business days |",
      "",
      "### Follow-up Actions",
      "- Check with the receiving team for progress",
      "- Update the customer even if there's no new information (\"We're still investigating — here's what we know so far\")",
      "- Adjust severity if the situation changes (better or worse)",
      "- Document all updates in the ticket for audit trail",
      "- Close the loop when resolved: confirm with customer, update internal tracking, capture learnings",
      "",
      "## De-escalation",
      "",
      "Not every escalation stays escalated. De-escalate when:",
      "- Root cause is found and it's a support-resolvable issue",
      "- A workaround is found that unblocks the customer",
      "- The issue resolves itself (but still document root cause)",
      "- New information changes the severity assessment",
      "",
      "When de-escalating:",
      "- Notify the team you escalated to",
      "- Update the ticket with the resolution",
      "- Inform the customer of the resolution",
      "- Document what was learned for future reference",
      "",
      "## Using This Skill",
      "",
      "When handling escalations:",
      "",
      "1. Always quantify impact — vague escalations get deprioritized",
      "2. Include reproduction steps for bugs — this is the #1 thing engineering needs",
      '3. Be clear about what you need — "investigate" vs. "fix" vs. "decide" are different asks',
      "4. Set and communicate a deadline — urgency without a deadline is ambiguous",
      "5. Maintain ownership of the customer relationship even after escalating the technical issue",
      "6. Follow up proactively — don't wait for the receiving team to come to you",
      "7. Document everything — the escalation trail is valuable for pattern detection and process improvement",
      "",
    ].join("\n"),
  },
  "customer-support-kb-article": {
    "SKILL.md": [
      "---",
      "name: customer-support-kb-article",
      "description: Draft a knowledge base article from a resolved issue or common question",
      "metadata:",
      "  plugin: customer-support",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <resolved issue or ticket>",
      "---",
      "",
      "# KB Article",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Draft a publish-ready knowledge base article from a resolved support issue, common question, or documented workaround. Structures the content for searchability and self-service.",
      "",
      "## Usage",
      "",
      "```",
      "/kb-article <resolved issue, ticket reference, or topic description>",
      "```",
      "",
      "Examples:",
      "- `/kb-article How to configure SSO with Okta — resolved this for 3 customers last month`",
      "- `/kb-article Ticket #4521 — customer couldn't export data over 10k rows`",
      "- `/kb-article Common question: how to set up webhook notifications`",
      "- `/kb-article Known issue: dashboard charts not loading on Safari 16`",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Source Material",
      "",
      "Parse the input to identify:",
      "",
      "- **What was the problem?** The original issue, question, or error",
      "- **What was the solution?** The resolution, workaround, or answer",
      "- **Who does this affect?** User type, plan level, or configuration",
      "- **How common is this?** One-off or recurring issue",
      "- **What article type fits best?** Use the article types from the **knowledge-management** skill (how-to, troubleshooting, FAQ, known issue, reference)",
      "",
      "If a ticket reference is provided, look up the full context:",
      "",
      "- **~~support platform**: Pull the ticket thread, resolution, and any internal notes",
      "- **~~knowledge base**: Check if a similar article already exists (update vs. create new)",
      "- **~~project tracker**: Check if there's a related bug or feature request",
      "",
      "### 2. Draft the Article",
      "",
      "Using the article structure and formatting standards from the **knowledge-management** skill:",
      "",
      "- Follow the template for the chosen article type (how-to, troubleshooting, FAQ, known issue, or reference)",
      "- Apply the searchability best practices: customer-language title, plain-language opening sentence, exact error messages, common synonyms",
      "- Keep it scannable: headers, numbered steps, short paragraphs",
      "",
      "### 3. Generate the Article",
      "",
      "Present the draft with metadata:",
      "",
      "```",
      "## KB Article Draft",
      "",
      "**Title:** [Article title]",
      "**Type:** [How-to / Troubleshooting / FAQ / Known Issue / Reference]",
      "**Category:** [Product area or topic]",
      "**Tags:** [Searchable tags]",
      "**Audience:** [All users / Admins / Developers / Specific plan]",
      "",
      "---",
      "",
      "[Full article content — using the appropriate template",
      "from the knowledge-management skill]",
      "",
      "---",
      "",
      "### Publishing Notes",
      "- **Source:** [Ticket #, customer conversation, or internal discussion]",
      "- **Existing articles to update:** [If this overlaps with existing content]",
      "- **Review needed from:** [SME or team if technical accuracy needs verification]",
      "- **Suggested review date:** [When to revisit for accuracy]",
      "```",
      "",
      "### 4. Offer Next Steps",
      "",
      "After generating the article:",
      '- "Want me to check if a similar article already exists in your ~~knowledge base?"',
      '- "Should I adjust the technical depth for a different audience?"',
      '- "Want me to draft a companion article (e.g., a how-to to go with this troubleshooting guide)?"',
      '- "Should I create an internal-only version with additional technical detail?"',
      "",
    ].join("\n"),
  },
  "customer-support-knowledge-management": {
    "SKILL.md": [
      "---",
      "name: customer-support-knowledge-management",
      "description: Write and maintain knowledge base articles from resolved support issues. Use when a ticket has been resolved and the solution should be documented, when updating existing KB articles, or when creating how-to guides, troubleshooting docs, or FAQ entries.",
      "metadata:",
      "  plugin: customer-support",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Knowledge Management Skill",
      "",
      "You are an expert at creating, organizing, and maintaining support knowledge base content. You write articles that are searchable, scannable, and solve customer problems on the first read. You understand that every good KB article reduces future ticket volume.",
      "",
      "## Article Structure and Formatting Standards",
      "",
      "### Universal Article Elements",
      "",
      "Every KB article should include:",
      "",
      "1. **Title**: Clear, searchable, describes the outcome or problem (not internal jargon)",
      "2. **Overview**: 1-2 sentences explaining what this article covers and who it's for",
      "3. **Body**: Structured content appropriate to the article type",
      "4. **Related articles**: Links to relevant companion content",
      "5. **Metadata**: Category, tags, audience, last updated date",
      "",
      "### Formatting Rules",
      "",
      "- **Use headers (H2, H3)** to break content into scannable sections",
      "- **Use numbered lists** for sequential steps",
      "- **Use bullet lists** for non-sequential items",
      "- **Use bold** for UI element names, key terms, and emphasis",
      "- **Use code blocks** for commands, API calls, error messages, and configuration values",
      "- **Use tables** for comparisons, options, or reference data",
      "- **Use callouts/notes** for warnings, tips, and important caveats",
      "- **Keep paragraphs short** — 2-4 sentences max",
      "- **One idea per section** — if a section covers two topics, split it",
      "",
      "## Writing for Searchability",
      "",
      "Articles are useless if customers can't find them. Optimize every article for search:",
      "",
      "### Title Best Practices",
      "",
      "| Good Title | Bad Title | Why |",
      "|------------|-----------|-----|",
      '| "How to configure SSO with Okta" | "SSO Setup" | Specific, includes the tool name customers search for |',
      '| "Fix: Dashboard shows blank page" | "Dashboard Issue" | Includes the symptom customers experience |',
      '| "API rate limits and quotas" | "API Information" | Includes the specific terms customers search for |',
      '| "Error: \'Connection refused\' when importing data" | "Import Problems" | Includes the exact error message |',
      "",
      "### Keyword Optimization",
      "",
      "- **Include exact error messages** — customers copy-paste error text into search",
      '- **Use customer language**, not internal terminology — "can\'t log in" not "authentication failure"',
      '- **Include common synonyms** — "delete/remove", "dashboard/home page", "export/download"',
      "- **Add alternate phrasings** — address the same issue from different angles in the overview",
      "- **Tag with product areas** — make sure category and tags match how customers think about the product",
      "",
      "### Opening Sentence Formula",
      "",
      "Start every article with a sentence that restates the problem or task in plain language:",
      "",
      '- **How-to**: "This guide shows you how to [accomplish X]."',
      '- **Troubleshooting**: "If you\'re seeing [symptom], this article explains how to fix it."',
      "- **FAQ**: \"[Question in the customer's words]? Here's the answer.\"",
      '- **Known issue**: "Some users are experiencing [symptom]. Here\'s what we know and how to work around it."',
      "",
      "## Common Article Types",
      "",
      "### How-to Articles",
      "",
      "**Purpose**: Step-by-step instructions for accomplishing a task.",
      "",
      "**Structure**:",
      "```",
      "# How to [accomplish task]",
      "",
      "[Overview — what this guide covers and when you'd use it]",
      "",
      "## Prerequisites",
      "- [What's needed before starting]",
      "",
      "## Steps",
      "### 1. [Action]",
      "[Instruction with specific details]",
      "",
      "### 2. [Action]",
      "[Instruction]",
      "",
      "## Verify It Worked",
      "[How to confirm success]",
      "",
      "## Common Issues",
      "- [Issue]: [Fix]",
      "",
      "## Related Articles",
      "- [Links]",
      "```",
      "",
      "**Best practices**:",
      "- Start each step with a verb",
      '- Include the specific path: "Go to Settings > Integrations > API Keys"',
      '- Mention what the user should see after each step ("You should see a green confirmation banner")',
      "- Test the steps yourself or verify with a recent ticket resolution",
      "",
      "### Troubleshooting Articles",
      "",
      "**Purpose**: Diagnose and resolve a specific problem.",
      "",
      "**Structure**:",
      "```",
      "# [Problem description — what the user sees]",
      "",
      "## Symptoms",
      "- [What the user observes]",
      "",
      "## Cause",
      "[Why this happens — brief, non-jargon explanation]",
      "",
      "## Solution",
      "### Option 1: [Primary fix]",
      "[Steps]",
      "",
      "### Option 2: [Alternative if Option 1 doesn't work]",
      "[Steps]",
      "",
      "## Prevention",
      "[How to avoid this in the future]",
      "",
      "## Still Having Issues?",
      "[How to get help]",
      "```",
      "",
      "**Best practices**:",
      "- Lead with symptoms, not causes — customers search for what they see",
      "- Provide multiple solutions when possible (most likely fix first)",
      '- Include a "Still having issues?" section that points to support',
      "- If the root cause is complex, keep the customer-facing explanation simple",
      "",
      "### FAQ Articles",
      "",
      "**Purpose**: Quick answer to a common question.",
      "",
      "**Structure**:",
      "```",
      "# [Question — in the customer's words]",
      "",
      "[Direct answer — 1-3 sentences]",
      "",
      "## Details",
      "[Additional context, nuance, or explanation if needed]",
      "",
      "## Related Questions",
      "- [Link to related FAQ]",
      "- [Link to related FAQ]",
      "```",
      "",
      "**Best practices**:",
      "- Answer the question in the first sentence",
      "- Keep it concise — if the answer needs a walkthrough, it's a how-to, not an FAQ",
      "- Group related FAQs and link between them",
      "",
      "### Known Issue Articles",
      "",
      "**Purpose**: Document a known bug or limitation with a workaround.",
      "",
      "**Structure**:",
      "```",
      "# [Known Issue]: [Brief description]",
      "",
      "**Status:** [Investigating / Workaround Available / Fix In Progress / Resolved]",
      "**Affected:** [Who/what is affected]",
      "**Last updated:** [Date]",
      "",
      "## Symptoms",
      "[What users experience]",
      "",
      "## Workaround",
      '[Steps to work around the issue, or "No workaround available"]',
      "",
      "## Fix Timeline",
      "[Expected fix date or current status]",
      "",
      "## Updates",
      "- [Date]: [Update]",
      "```",
      "",
      "**Best practices**:",
      "- Keep the status current — nothing erodes trust faster than a stale known issue article",
      "- Update the article when the fix ships and mark as resolved",
      "- If resolved, keep the article live for 30 days for customers still searching the old symptoms",
      "",
      "## Review and Maintenance Cadence",
      "",
      "Knowledge bases decay without maintenance. Follow this schedule:",
      "",
      "| Activity | Frequency | Who |",
      "|----------|-----------|-----|",
      "| **New article review** | Before publishing | Peer review + SME for technical content |",
      "| **Accuracy audit** | Quarterly | Support team reviews top-traffic articles |",
      "| **Stale content check** | Monthly | Flag articles not updated in 6+ months |",
      "| **Known issue updates** | Weekly | Update status on all open known issues |",
      "| **Analytics review** | Monthly | Check which articles have low helpfulness ratings or high bounce rates |",
      "| **Gap analysis** | Quarterly | Identify top ticket topics without KB articles |",
      "",
      "### Article Lifecycle",
      "",
      "1. **Draft**: Written, needs review",
      "2. **Published**: Live and available to customers",
      "3. **Needs update**: Flagged for revision (product change, feedback, or age)",
      "4. **Archived**: No longer relevant but preserved for reference",
      "5. **Retired**: Removed from the knowledge base",
      "",
      "### When to Update vs. Create New",
      "",
      "**Update existing** when:",
      "- The product changed and steps need refreshing",
      "- The article is mostly right but missing a detail",
      "- Feedback indicates customers are confused by a specific section",
      "- A better workaround or solution was found",
      "",
      "**Create new** when:",
      "- A new feature or product area needs documentation",
      "- A resolved ticket reveals a gap — no article exists for this topic",
      "- The existing article covers too many topics and should be split",
      "- A different audience needs the same information explained differently",
      "",
      "## Linking and Categorization Taxonomy",
      "",
      "### Category Structure",
      "",
      "Organize articles into a hierarchy that matches how customers think:",
      "",
      "```",
      "Getting Started",
      "├── Account setup",
      "├── First-time configuration",
      "└── Quick start guides",
      "",
      "Features & How-tos",
      "├── [Feature area 1]",
      "├── [Feature area 2]",
      "└── [Feature area 3]",
      "",
      "Integrations",
      "├── [Integration 1]",
      "├── [Integration 2]",
      "└── API reference",
      "",
      "Troubleshooting",
      "├── Common errors",
      "├── Performance issues",
      "└── Known issues",
      "",
      "Billing & Account",
      "├── Plans and pricing",
      "├── Billing questions",
      "└── Account management",
      "```",
      "",
      "### Linking Best Practices",
      "",
      '- **Link from troubleshooting to how-to**: "For setup instructions, see [How to configure X]"',
      '- **Link from how-to to troubleshooting**: "If you encounter errors, see [Troubleshooting X]"',
      '- **Link from FAQ to detailed articles**: "For a full walkthrough, see [Guide to X]"',
      "- **Link from known issues to workarounds**: Keep the chain from problem to solution short",
      "- **Use relative links** within the KB — they survive restructuring better than absolute URLs",
      "- **Avoid circular links** — if A links to B, B shouldn't link back to A unless both are genuinely useful entry points",
      "",
      "## Using This Skill",
      "",
      "When creating and maintaining KB content:",
      "",
      "1. Write for the customer who is frustrated and searching for an answer — be clear, direct, and helpful",
      "2. Every article should be findable through search using the words a customer would type",
      "3. Test your articles — follow the steps yourself or have someone unfamiliar with the topic follow them",
      "4. Keep articles focused — one problem, one solution. Split if an article is growing too long",
      "5. Maintain aggressively — a wrong article is worse than no article",
      "6. Track what's missing — every ticket that could have been a KB article is a content gap",
      "7. Measure impact — articles that don't get traffic or don't reduce tickets need to be improved or retired",
      "",
    ].join("\n"),
  },
  "customer-support-research": {
    "SKILL.md": [
      "---",
      "name: customer-support-research",
      "description: Multi-source research on a customer question or topic with source attribution",
      "metadata:",
      "  plugin: customer-support",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <question or topic>",
      "---",
      "",
      "# Research",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Multi-source research on a customer question, product topic, or account-related inquiry. Synthesizes findings from all available sources with clear attribution.",
      "",
      "## Usage",
      "",
      "```",
      "/research <question or topic>",
      "```",
      "",
      "## Workflow",
      "",
      "### 1. Parse the Research Request",
      "",
      "Identify what type of research is needed:",
      '- **Customer question**: Something a customer has asked that needs an answer (e.g., "Does our product support SSO with Okta?")',
      '- **Issue investigation**: Background on a reported problem (e.g., "Has this bug been reported before? What\'s the known workaround?")',
      '- **Account context**: History with a specific customer (e.g., "What did we tell Acme Corp last time they asked about this?")',
      '- **Topic research**: General topic relevant to support work (e.g., "Best practices for webhook retry logic")',
      "",
      "### 2. Search Available Sources",
      "",
      "Search in priority order, adapting to what is connected:",
      "",
      "**Tier 1 — Internal Documentation (highest confidence):**",
      "- ~~knowledge base (if connected): product docs, runbooks, FAQs",
      "- ~~cloud storage: internal documents, specs, guides, past research",
      "- ~~CRM notes: previous answers to similar questions, account context",
      "",
      "**Tier 2 — Team Communications:**",
      "- ~~chat: search for the topic in relevant channels; check if teammates have discussed or answered this before",
      "- ~~email: search for previous correspondence on this topic",
      "- ~~support platform (if connected): check if this has been asked/resolved before",
      "",
      "**Tier 3 — External Sources:**",
      "- Web search: official documentation, blog posts, community forums",
      "- Public knowledge bases, help centers, release notes",
      "",
      "### 3. Synthesize Findings",
      "",
      "Compile results into a structured research brief:",
      "",
      "```",
      "## Research: [Question/Topic]",
      "",
      "### Answer",
      "[Clear, direct answer to the question — lead with the bottom line]",
      "",
      "**Confidence:** [High / Medium / Low]",
      "[Explain what drives the confidence level]",
      "",
      "### Key Findings",
      "",
      "**From [Source 1]:**",
      "- [Finding with specific detail]",
      "- [Finding with specific detail]",
      "",
      "**From [Source 2]:**",
      "- [Finding with specific detail]",
      "",
      "### Context & Nuance",
      "[Any caveats, edge cases, or additional context that matters]",
      "",
      "### Sources",
      "1. [Source name/link] — [what it contributed]",
      "2. [Source name/link] — [what it contributed]",
      "3. [Source name/link] — [what it contributed]",
      "",
      "### Gaps & Unknowns",
      "- [What couldn't be confirmed]",
      "- [What might need verification from a subject matter expert]",
      "",
      "### Recommended Next Steps",
      "- [Action if the answer needs to go to a customer]",
      "- [Action if further research is needed]",
      "- [Who to consult for verification if needed]",
      "```",
      "",
      "### 4. Handle Insufficient Sources",
      "",
      "If no connected sources yield results:",
      "",
      "- Perform web research on the topic",
      "- Ask the user for internal context:",
      '  - "I couldn\'t find this in connected sources. Do you have internal docs or knowledge base articles about this?"',
      '  - "Has your team discussed this topic before? Any ~~chat channels I should check?"',
      '  - "Is there a subject matter expert who would know the answer?"',
      "- Be transparent about limitations:",
      '  - "This answer is based on web research only — please verify against your internal documentation before sharing with the customer."',
      '  - "I found a possible answer but couldn\'t confirm it from an authoritative internal source."',
      "",
      "### 5. Customer-Facing Considerations",
      "",
      "If the research is to answer a customer question:",
      "",
      "- Flag if the answer involves product roadmap, pricing, legal, or security topics that may need review",
      "- Note if the answer differs from what may have been communicated previously",
      "- Suggest appropriate caveats for the customer-facing response",
      '- Offer to draft the customer response: "Want me to draft a response to the customer based on these findings?"',
      "",
      "### 6. Knowledge Capture",
      "",
      "After research is complete, suggest capturing the knowledge:",
      "",
      '- "Should I save these findings to your knowledge base for future reference?"',
      '- "Want me to create a FAQ entry based on this research?"',
      '- "This might be worth documenting — should I draft a runbook entry?"',
      "",
      "This helps build institutional knowledge and reduces duplicate research effort across the team.",
      "",
    ].join("\n"),
  },
  "customer-support-response-drafting": {
    "SKILL.md": [
      "---",
      "name: customer-support-response-drafting",
      "description: Draft professional, empathetic customer-facing responses adapted to the situation, urgency, and channel. Use when responding to customer tickets, escalations, outage notifications, bug reports, feature requests, or any customer-facing communication.",
      "metadata:",
      "  plugin: customer-support",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Response Drafting Skill",
      "",
      "You are an expert at drafting professional, empathetic, and effective customer-facing communications. You adapt tone, structure, and content based on the situation, relationship stage, stakeholder level, and communication channel.",
      "",
      "## Customer Communication Best Practices",
      "",
      "### Core Principles",
      "",
      "1. **Lead with empathy**: Acknowledge the customer's situation before jumping to solutions",
      "2. **Be direct**: Get to the point — customers are busy. Bottom-line-up-front.",
      "3. **Be honest**: Never overpromise, never mislead, never hide bad news in jargon",
      "4. **Be specific**: Use concrete details, timelines, and names — avoid vague language",
      '5. **Own it**: Take responsibility when appropriate. "We" not "the system" or "the process"',
      "6. **Close the loop**: Every response should have a clear next step or call to action",
      "7. **Match their energy**: If they're frustrated, be empathetic first. If they're excited, be enthusiastic.",
      "",
      "### Response Structure",
      "",
      "**For most customer communications, follow this structure:**",
      "",
      "```",
      "1. Acknowledgment / Context (1-2 sentences)",
      "   - Acknowledge what they said, asked, or are experiencing",
      "   - Show you understand their situation",
      "",
      "2. Core Message (1-3 paragraphs)",
      "   - Deliver the main information, answer, or update",
      "   - Be specific and concrete",
      "   - Include relevant details they need",
      "",
      "3. Next Steps (1-3 bullets)",
      "   - What YOU will do and by when",
      "   - What THEY need to do (if anything)",
      "   - When they'll hear from you next",
      "",
      "4. Closing (1 sentence)",
      "   - Warm but professional sign-off",
      "   - Reinforce you're available if needed",
      "```",
      "",
      "### Length Guidelines",
      "",
      "- **Chat/IM**: 1-4 sentences. Get to the point immediately.",
      "- **Support ticket response**: 1-3 short paragraphs. Structured and scannable.",
      "- **Email**: 3-5 paragraphs max. Respect their inbox.",
      "- **Escalation response**: As long as needed to be thorough, but well-structured with headers.",
      "- **Executive communication**: Shorter is better. 2-3 paragraphs max. Data-driven.",
      "",
      "## Tone and Style Guidelines",
      "",
      "### Tone Spectrum",
      "",
      "| Situation | Tone | Characteristics |",
      "|-----------|------|----------------|",
      "| Good news / wins | Celebratory | Enthusiastic, warm, congratulatory, forward-looking |",
      "| Routine update | Professional | Clear, concise, informative, friendly |",
      "| Technical response | Precise | Accurate, detailed, structured, patient |",
      "| Delayed delivery | Accountable | Honest, apologetic, action-oriented, specific |",
      "| Bad news | Candid | Direct, empathetic, solution-oriented, respectful |",
      "| Issue / outage | Urgent | Immediate, transparent, actionable, reassuring |",
      "| Escalation | Executive | Composed, ownership-taking, plan-presenting, confident |",
      "| Billing / account | Precise | Clear, factual, empathetic, resolution-focused |",
      "",
      "### Tone Adjustments by Relationship Stage",
      "",
      "**New Customer (0-3 months):**",
      "- More formal and professional",
      "- Extra context and explanation (don't assume knowledge)",
      "- Proactively offer help and resources",
      "- Build trust through reliability and responsiveness",
      "",
      "**Established Customer (3+ months):**",
      "- Warm and collaborative",
      "- Can reference shared history and previous conversations",
      "- More direct and efficient communication",
      "- Show awareness of their goals and priorities",
      "",
      "**Frustrated or Escalated Customer:**",
      "- Extra empathy and acknowledgment",
      "- Urgency in response times",
      "- Concrete action plans with specific commitments",
      "- Shorter feedback loops",
      "",
      "### Writing Style Rules",
      "",
      "**DO:**",
      '- Use active voice ("We\'ll investigate" not "This will be investigated")',
      '- Use "I" for personal commitments and "we" for team commitments',
      '- Name specific people when assigning actions ("Sarah from our engineering team will...")',
      "- Use the customer's terminology, not your internal jargon",
      '- Include specific dates and times, not relative terms ("by Friday January 24" not "in a few days")',
      "- Break up long responses with headers or bullet points",
      "",
      "**DON'T:**",
      '- Use corporate jargon or buzzwords ("synergy", "leverage", "paradigm shift")',
      "- Deflect blame to other teams, systems, or processes",
      '- Use passive voice to avoid ownership ("Mistakes were made")',
      "- Include unnecessary caveats or hedging that undermines confidence",
      "- CC people unnecessarily — only include those who need to be in the conversation",
      "- Use exclamation marks excessively (one per email max, if any)",
      "",
      "## Response Templates for Common Scenarios",
      "",
      "### Acknowledging a Bug Report",
      "",
      "```",
      "Hi [Name],",
      "",
      "Thank you for reporting this — I can see how [specific impact] would be",
      "frustrating for your team.",
      "",
      "I've confirmed the issue and escalated it to our engineering team as a",
      "[priority level]. Here's what we know so far:",
      "- [What's happening]",
      "- [What's causing it, if known]",
      "- [Workaround, if available]",
      "",
      "I'll update you by [specific date/time] with a resolution timeline.",
      "In the meantime, [workaround details if applicable].",
      "",
      "Let me know if you have any questions or if this is impacting you in",
      "other ways I should know about.",
      "",
      "Best,",
      "[Your name]",
      "```",
      "",
      "### Acknowledging a Billing or Account Issue",
      "",
      "```",
      "Hi [Name],",
      "",
      "Thank you for reaching out about this — I understand billing issues",
      "need prompt attention, and I want to make sure this gets resolved",
      "quickly.",
      "",
      "I've looked into your account and here's what I'm seeing:",
      "- [What happened — clear factual explanation]",
      "- [Impact on their account — charges, access, etc.]",
      "",
      "Here's what I'm doing to fix this:",
      "- [Action 1 — with timeline]",
      "- [Action 2 — if applicable]",
      "",
      '[If resolution is immediate: "This has been corrected and you should',
      'see the change reflected within [timeframe]."]',
      "[If needs investigation: \"I'm escalating this to our billing team",
      'and will have an update for you by [specific date]."]',
      "",
      "I'm sorry for the inconvenience. Let me know if you have any",
      "questions about your account.",
      "",
      "Best,",
      "[Your name]",
      "```",
      "",
      "### Responding to a Feature Request You Won't Build",
      "",
      "```",
      "Hi [Name],",
      "",
      "Thank you for sharing this request — I can see why [capability] would",
      "be valuable for [their use case].",
      "",
      "I discussed this with our product team, and this isn't something we're",
      "planning to build in the near term. The primary reason is [honest,",
      "respectful explanation — e.g., it serves a narrow use case, it conflicts",
      "with our architecture direction, etc.].",
      "",
      "That said, I want to make sure you can accomplish your goal. Here are",
      "some alternatives:",
      "- [Alternative approach 1]",
      "- [Alternative approach 2]",
      "- [Integration or workaround if applicable]",
      "",
      "I've also documented your request in our feedback system, and if our",
      "direction changes, I'll let you know.",
      "",
      "Would any of these alternatives work for your team? Happy to dig",
      "deeper into any of them.",
      "",
      "Best,",
      "[Your name]",
      "```",
      "",
      "### Outage or Incident Communication",
      "",
      "```",
      "Hi [Name],",
      "",
      "I wanted to reach out directly to let you know about an issue affecting",
      "[service/feature] that I know your team relies on.",
      "",
      "**What happened:** [Clear, non-technical explanation]",
      "**Impact:** [How it affects them specifically]",
      "**Status:** [Current status — investigating / identified / fixing / resolved]",
      '**ETA for resolution:** [Specific time if known, or "we\'ll update every X hours"]',
      "",
      '[If applicable: "In the meantime, you can [workaround]."]',
      "",
      "I'm personally tracking this and will update you as soon as we have a",
      "resolution. You can also check [status page URL] for real-time updates.",
      "",
      "I'm sorry for the disruption to your team's work. We take this seriously",
      "and [what you're doing to prevent recurrence if known].",
      "",
      "[Your name]",
      "```",
      "",
      "### Following Up After Silence",
      "",
      "```",
      "Hi [Name],",
      "",
      "I wanted to check in — I sent over [what you sent] on [date] and",
      "wanted to make sure it didn't get lost in the shuffle.",
      "",
      "[Brief reminder of what you need from them or what you're offering]",
      "",
      "If now isn't a good time, no worries — just let me know when would be",
      "better, and I'm happy to reconnect then.",
      "",
      "Best,",
      "[Your name]",
      "```",
      "",
      "## Personalization Based on Customer Context",
      "",
      "### New Customer",
      "- Include more context and explanation",
      "- Reference onboarding milestones and goals",
      "- Proactively share resources and best practices",
      "- Introduce relevant self-service resources",
      "",
      "### Established Customer",
      "- Reference their history and previous interactions",
      "- Skip introductory explanations they already know",
      "- Acknowledge their experience with the product",
      "- Be more direct and efficient",
      "",
      "### Frustrated or Escalated Customer",
      "- Increase empathy and acknowledgment",
      "- Focus on solving their problem, not deflecting",
      "- Provide concrete action plans with timelines",
      "- Offer direct escalation paths if needed",
      "",
      "## Follow-up and Escalation Guidance",
      "",
      "### Follow-up Cadence",
      "",
      "| Situation | Follow-up Timing |",
      "|-----------|-----------------|",
      "| Unanswered question | 2-3 business days |",
      "| Open support issue | Daily until resolved for critical, 2-3 days for standard |",
      "| Post-meeting action items | Within 24 hours (send notes), then check at deadline |",
      "| General check-in | As needed for ongoing issues |",
      "| After delivering bad news | 1 week to check on impact and sentiment |",
      "",
      "### When to Escalate",
      "",
      "**Escalate to your manager when:**",
      "- Customer threatens to cancel or significantly downsell",
      "- Customer requests exception to policy you can't authorize",
      "- An issue has been unresolved for longer than SLA allows",
      "- Customer requests direct contact with leadership",
      "- You've made an error that needs senior involvement to resolve",
      "",
      "**Escalate to product/engineering when:**",
      "- Bug is critical and blocking the customer's business",
      "- Feature gap is causing a competitive loss",
      "- Customer has unique technical requirements beyond standard support",
      "- Integration issues require engineering investigation",
      "",
      "**Escalation format:**",
      "```",
      "ESCALATION: [Customer Name] — [One-line summary]",
      "",
      "Urgency: [Critical / High / Medium]",
      "Customer impact: [What's broken for them]",
      "History: [Brief background — 2-3 sentences]",
      "What I've tried: [Actions taken so far]",
      "What I need: [Specific help or decision needed]",
      "Deadline: [When this needs to be resolved by]",
      "```",
      "",
      "## Using This Skill",
      "",
      "When drafting customer responses:",
      "",
      "1. Identify the situation type first (good news, bad news, technical, etc.)",
      "2. Consider the customer's relationship stage and stakeholder level",
      "3. Match your tone to the situation — empathy first for problems, enthusiasm for wins",
      "4. Be specific with dates, names, and commitments",
      "5. Always include a clear next step",
      "6. Read the draft from the customer's perspective before finalizing",
      "7. If the response involves commitments or sensitive topics, get internal alignment first",
      "8. Keep it concise — every sentence should earn its place",
      "",
    ].join("\n"),
  },
  "customer-support-ticket-triage": {
    "SKILL.md": [
      "---",
      "name: customer-support-ticket-triage",
      "description: Triage incoming support tickets by categorizing issues, assigning priority (P1-P4), and recommending routing. Use when a new ticket or customer issue comes in, when assessing severity, or when deciding which team should handle an issue.",
      "metadata:",
      "  plugin: customer-support",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Ticket Triage Skill",
      "",
      "You are an expert at rapidly categorizing, prioritizing, and routing customer support tickets. You assess issues systematically, identify urgency and impact, and ensure tickets reach the right team with the right context.",
      "",
      "## Category Taxonomy",
      "",
      "Assign every ticket a **primary category** and optionally a **secondary category**:",
      "",
      "| Category | Description | Signal Words |",
      "|----------|-------------|-------------|",
      "| **Bug** | Product is behaving incorrectly or unexpectedly | Error, broken, crash, not working, unexpected, wrong, failing |",
      "| **How-to** | Customer needs guidance on using the product | How do I, can I, where is, setting up, configure, help with |",
      "| **Feature request** | Customer wants a capability that doesn't exist | Would be great if, wish I could, any plans to, requesting |",
      "| **Billing** | Payment, subscription, invoice, or pricing issues | Charge, invoice, payment, subscription, refund, upgrade, downgrade |",
      "| **Account** | Account access, permissions, settings, or user management | Login, password, access, permission, SSO, locked out, can't sign in |",
      "| **Integration** | Issues connecting to third-party tools or APIs | API, webhook, integration, connect, OAuth, sync, third-party |",
      "| **Security** | Security concerns, data access, or compliance questions | Data breach, unauthorized, compliance, GDPR, SOC 2, vulnerability |",
      "| **Data** | Data quality, migration, import/export issues | Missing data, export, import, migration, incorrect data, duplicates |",
      "| **Performance** | Speed, reliability, or availability issues | Slow, timeout, latency, down, unavailable, degraded |",
      "",
      "### Category Determination Tips",
      "",
      "- If the customer reports **both** a bug and a feature request, the bug is primary",
      "- If they can't log in due to a bug, category is **Bug** (not Account) — root cause drives the category",
      '- "It used to work and now it doesn\'t" = **Bug**',
      '- "I want it to work differently" = **Feature request**',
      '- "How do I make it work?" = **How-to**',
      "- When in doubt, lean toward **Bug** — it's better to investigate than dismiss",
      "",
      "## Priority Framework",
      "",
      "### P1 — Critical",
      "**Criteria:** Production system down, data loss or corruption, security breach, all or most users affected.",
      "",
      "- The customer cannot use the product at all",
      "- Data is being lost, corrupted, or exposed",
      "- A security incident is in progress",
      "- The issue is worsening or expanding in scope",
      "",
      "**SLA expectation:** Respond within 1 hour. Continuous work until resolved or mitigated. Updates every 1-2 hours.",
      "",
      "### P2 — High",
      "**Criteria:** Major feature broken, significant workflow blocked, many users affected, no workaround.",
      "",
      "- A core workflow is broken but the product is partially usable",
      "- Multiple users are affected or a key account is impacted",
      "- The issue is blocking time-sensitive work",
      "- No reasonable workaround exists",
      "",
      "**SLA expectation:** Respond within 4 hours. Active investigation same day. Updates every 4 hours.",
      "",
      "### P3 — Medium",
      "**Criteria:** Feature partially broken, workaround available, single user or small team affected.",
      "",
      "- A feature isn't working correctly but a workaround exists",
      "- The issue is inconvenient but not blocking critical work",
      "- A single user or small team is affected",
      "- The customer is not escalating urgently",
      "",
      "**SLA expectation:** Respond within 1 business day. Resolution or update within 3 business days.",
      "",
      "### P4 — Low",
      "**Criteria:** Minor inconvenience, cosmetic issue, general question, feature request.",
      "",
      "- Cosmetic or UI issues that don't affect functionality",
      "- Feature requests and enhancement ideas",
      "- General questions or how-to inquiries",
      "- Issues with simple, documented solutions",
      "",
      "**SLA expectation:** Respond within 2 business days. Resolution at normal pace.",
      "",
      "### Priority Escalation Triggers",
      "",
      "Automatically bump priority up when:",
      "- Customer has been waiting longer than the SLA allows",
      "- Multiple customers report the same issue (pattern detected)",
      "- The customer explicitly escalates or mentions executive involvement",
      "- A workaround that was in place stops working",
      "- The issue expands in scope (more users, more data, new symptoms)",
      "",
      "## Routing Rules",
      "",
      "Route tickets based on category and complexity:",
      "",
      "| Route to | When |",
      "|----------|------|",
      "| **Tier 1 (frontline support)** | How-to questions, known issues with documented solutions, billing inquiries, password resets |",
      "| **Tier 2 (senior support)** | Bugs requiring investigation, complex configuration, integration troubleshooting, account issues |",
      "| **Engineering** | Confirmed bugs needing code fixes, infrastructure issues, performance degradation |",
      "| **Product** | Feature requests with significant demand, design decisions, workflow gaps |",
      "| **Security** | Data access concerns, vulnerability reports, compliance questions |",
      "| **Billing/Finance** | Refund requests, contract disputes, complex billing adjustments |",
      "",
      "## Duplicate Detection",
      "",
      "Before creating a new ticket or routing, check for duplicates:",
      "",
      "1. **Search by symptom**: Look for tickets with similar error messages or descriptions",
      "2. **Search by customer**: Check if this customer has an open ticket for the same issue",
      "3. **Search by product area**: Look for recent tickets in the same feature area",
      "4. **Check known issues**: Compare against documented known issues",
      "",
      "**If a duplicate is found:**",
      "- Link the new ticket to the existing one",
      "- Notify the customer that this is a known issue being tracked",
      "- Add any new information from the new report to the existing ticket",
      "- Bump priority if the new report adds urgency (more customers affected, etc.)",
      "",
      "## Auto-Response Templates by Category",
      "",
      "### Bug — Initial Response",
      "```",
      "Thank you for reporting this. I can see how [specific impact]",
      "would be disruptive for your work.",
      "",
      "I've logged this as a [priority] issue and our team is",
      'investigating. [If workaround exists: "In the meantime, you',
      'can [workaround]."]',
      "",
      "I'll update you within [SLA timeframe] with what we find.",
      "```",
      "",
      "### How-to — Initial Response",
      "```",
      "Great question! [Direct answer or link to documentation]",
      "",
      '[If more complex: "Let me walk you through the steps:"]',
      "[Steps or guidance]",
      "",
      "Let me know if that helps, or if you have any follow-up",
      "questions.",
      "```",
      "",
      "### Feature Request — Initial Response",
      "```",
      "Thank you for this suggestion — I can see why [capability]",
      "would be valuable for your workflow.",
      "",
      "I've documented this and shared it with our product team.",
      "While I can't commit to a specific timeline, your feedback",
      "directly informs our roadmap priorities.",
      "",
      '[If alternative exists: "In the meantime, you might find',
      '[alternative] helpful for achieving something similar."]',
      "```",
      "",
      "### Billing — Initial Response",
      "```",
      "I understand billing issues need prompt attention. Let me",
      "look into this for you.",
      "",
      "[If straightforward: resolution details]",
      "[If complex: \"I'm reviewing your account now and will have",
      'an answer for you within [timeframe]."]',
      "```",
      "",
      "### Security — Initial Response",
      "```",
      "Thank you for flagging this — we take security concerns",
      "seriously and are reviewing this immediately.",
      "",
      "I've escalated this to our security team for investigation.",
      "We'll follow up with you within [timeframe] with our findings.",
      "",
      '[If action is needed: "In the meantime, we recommend',
      '[protective action]."]',
      "```",
      "",
      "## Using This Skill",
      "",
      "When triaging tickets:",
      "",
      "1. Read the full ticket before categorizing — context in later messages often changes the assessment",
      "2. Categorize by **root cause**, not just the symptom described",
      "3. When in doubt on priority, err on the side of higher — it's easier to de-escalate than to recover from a missed SLA",
      "4. Always check for duplicates and known issues before routing",
      "5. Write internal notes that help the next person pick up context quickly",
      "6. Include what you've already checked or ruled out to avoid duplicate investigation",
      "7. Flag patterns — if you're seeing the same issue repeatedly, escalate the pattern even if individual tickets are low priority",
      "",
    ].join("\n"),
  },
  "customer-support-triage": {
    "SKILL.md": [
      "---",
      "name: customer-support-triage",
      "description: Triage and prioritize a support ticket or customer issue",
      "metadata:",
      "  plugin: customer-support",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <ticket or issue description>",
      "---",
      "",
      "# Triage",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Categorize, prioritize, and route an incoming support ticket or customer issue. Produces a structured triage assessment with a suggested initial response.",
      "",
      "## Usage",
      "",
      "```",
      "/triage <ticket text, customer message, or issue description>",
      "```",
      "",
      "Examples:",
      "- `/triage Customer says their dashboard has been showing a blank page since this morning`",
      '- `/triage "I was charged twice for my subscription this month"`',
      "- `/triage User can't connect their SSO — getting a 403 error on the callback URL`",
      "- `/triage Feature request: they want to export reports as PDF`",
      "",
      "## Workflow",
      "",
      "### 1. Parse the Issue",
      "",
      "Read the input and extract:",
      "",
      "- **Core problem**: What is the customer actually experiencing?",
      "- **Symptoms**: What specific behavior or error are they seeing?",
      "- **Customer context**: Who is this? Any account details, plan level, or history available?",
      "- **Urgency signals**: Are they blocked? Is this production? How many users affected?",
      "- **Emotional state**: Frustrated, confused, matter-of-fact, escalating?",
      "",
      "### 2. Categorize and Prioritize",
      "",
      "Using the category taxonomy and priority framework from the **ticket-triage** skill:",
      "",
      "- Assign a **primary category** (bug, how-to, feature request, billing, account, integration, security, data, performance) and an optional secondary category",
      "- Assign a **priority** (P1–P4) based on impact and urgency",
      "- Identify the **product area** the issue maps to",
      "",
      "### 3. Check for Duplicates and Known Issues",
      "",
      "Before routing, check available sources:",
      "",
      "- **~~support platform**: Search for similar open or recently resolved tickets",
      "- **~~knowledge base**: Check for known issues or existing documentation",
      "- **~~project tracker**: Check if there's an existing bug report or feature request",
      "",
      "### 4. Determine Routing",
      "",
      "Using the routing rules from the **ticket-triage** skill, recommend which team or queue should handle this based on category and complexity.",
      "",
      "### 5. Generate Triage Output",
      "",
      "```",
      "## Triage: [One-line issue summary]",
      "",
      "**Category:** [Primary] / [Secondary if applicable]",
      "**Priority:** [P1-P4] — [Brief justification]",
      "**Product area:** [Area/team]",
      "",
      "### Issue Summary",
      "[2-3 sentence summary of what the customer is experiencing]",
      "",
      "### Key Details",
      "- **Customer:** [Name/account if known]",
      "- **Impact:** [Who and what is affected]",
      "- **Workaround:** [Available / Not available / Unknown]",
      "- **Related tickets:** [Links to similar issues if found]",
      "- **Known issue:** [Yes — link / No / Checking]",
      "",
      "### Routing Recommendation",
      "**Route to:** [Team or queue]",
      "**Why:** [Brief reasoning]",
      "",
      "### Suggested Initial Response",
      "[Draft first response to the customer — acknowledge the issue,",
      "set expectations, provide workaround if available.",
      "Use the auto-response templates from the ticket-triage skill",
      "as a starting point.]",
      "",
      "### Internal Notes",
      "- [Any additional context for the agent picking this up]",
      "- [Reproduction hints if it's a bug]",
      "- [Escalation triggers to watch for]",
      "```",
      "",
      "### 6. Offer Next Steps",
      "",
      "After presenting the triage:",
      '- "Want me to draft a full response to the customer?"',
      '- "Should I search for more context on this issue?"',
      '- "Want me to check if this is a known bug in the tracker?"',
      '- "Should I escalate this? I can package it with /escalate."',
      "",
    ].join("\n"),
  },
  "data-analyze": {
    "SKILL.md": [
      "---",
      "name: data-analyze",
      "description: Answer data questions -- from quick lookups to full analyses",
      "metadata:",
      "  plugin: data",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <question>",
      "---",
      "",
      "# /analyze - Answer Data Questions",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Answer a data question, from a quick lookup to a full analysis to a formal report.",
      "",
      "## Usage",
      "",
      "```",
      "/analyze <natural language question>",
      "```",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Question",
      "",
      "Parse the user's question and determine:",
      "",
      "- **Complexity level**:",
      '  - **Quick answer**: Single metric, simple filter, factual lookup (e.g., "How many users signed up last week?")',
      '  - **Full analysis**: Multi-dimensional exploration, trend analysis, comparison (e.g., "What\'s driving the drop in conversion rate?")',
      '  - **Formal report**: Comprehensive investigation with methodology, caveats, and recommendations (e.g., "Prepare a quarterly business review of our subscription metrics")',
      "- **Data requirements**: Which tables, metrics, dimensions, and time ranges are needed",
      "- **Output format**: Number, table, chart, narrative, or combination",
      "",
      "### 2. Gather Data",
      "",
      "**If a data warehouse MCP server is connected:**",
      "",
      "1. Explore the schema to find relevant tables and columns",
      "2. Write SQL query(ies) to extract the needed data",
      "3. Execute the query and retrieve results",
      "4. If the query fails, debug and retry (check column names, table references, syntax for the specific dialect)",
      "5. If results look unexpected, run sanity checks before proceeding",
      "",
      "**If no data warehouse is connected:**",
      "",
      "1. Ask the user to provide data in one of these ways:",
      "   - Paste query results directly",
      "   - Upload a CSV or Excel file",
      "   - Describe the schema so you can write queries for them to run",
      "2. If writing queries for manual execution, use the `sql-queries` skill for dialect-specific best practices",
      "3. Once data is provided, proceed with analysis",
      "",
      "### 3. Analyze",
      "",
      "- Calculate relevant metrics, aggregations, and comparisons",
      "- Identify patterns, trends, outliers, and anomalies",
      "- Compare across dimensions (time periods, segments, categories)",
      "- For complex analyses, break the problem into sub-questions and address each",
      "",
      "### 4. Validate Before Presenting",
      "",
      "Before sharing results, run through validation checks:",
      "",
      "- **Row count sanity**: Does the number of records make sense?",
      "- **Null check**: Are there unexpected nulls that could skew results?",
      "- **Magnitude check**: Are the numbers in a reasonable range?",
      "- **Trend continuity**: Do time series have unexpected gaps?",
      "- **Aggregation logic**: Do subtotals sum to totals correctly?",
      "",
      "If any check raises concerns, investigate and note caveats.",
      "",
      "### 5. Present Findings",
      "",
      "**For quick answers:**",
      "- State the answer directly with relevant context",
      "- Include the query used (collapsed or in a code block) for reproducibility",
      "",
      "**For full analyses:**",
      "- Lead with the key finding or insight",
      "- Support with data tables and/or visualizations",
      "- Note methodology and any caveats",
      "- Suggest follow-up questions",
      "",
      "**For formal reports:**",
      "- Executive summary with key takeaways",
      "- Methodology section explaining approach and data sources",
      "- Detailed findings with supporting evidence",
      "- Caveats, limitations, and data quality notes",
      "- Recommendations and suggested next steps",
      "",
      "### 6. Visualize Where Helpful",
      "",
      "When a chart would communicate results more effectively than a table:",
      "",
      "- Use the `data-visualization` skill to select the right chart type",
      "- Generate a Python visualization or build it into an HTML dashboard",
      "- Follow visualization best practices for clarity and accuracy",
      "",
      "## Examples",
      "",
      "**Quick answer:**",
      "```",
      "/analyze How many new users signed up in December?",
      "```",
      "",
      "**Full analysis:**",
      "```",
      "/analyze What's causing the increase in support ticket volume over the past 3 months? Break down by category and priority.",
      "```",
      "",
      "**Formal report:**",
      "```",
      "/analyze Prepare a data quality assessment of our customer table -- completeness, consistency, and any issues we should address.",
      "```",
      "",
      "## Tips",
      "",
      "- Be specific about time ranges, segments, or metrics when possible",
      "- If you know the table names, mention them to speed up the process",
      "- For complex questions, Claude may break them into multiple queries",
      "- Results are always validated before presentation -- if something looks off, Claude will flag it",
      "",
    ].join("\n"),
  },
  "data-build-dashboard": {
    "SKILL.md": [
      "---",
      "name: data-build-dashboard",
      "description: Build an interactive HTML dashboard with charts, filters, and tables",
      "metadata:",
      "  plugin: data",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <description> [data source]",
      "---",
      "",
      "# /build-dashboard - Build Interactive Dashboards",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Build a self-contained interactive HTML dashboard with charts, filters, tables, and professional styling. Opens directly in a browser -- no server or dependencies required.",
      "",
      "## Usage",
      "",
      "```",
      "/build-dashboard <description of dashboard> [data source]",
      "```",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Dashboard Requirements",
      "",
      "Determine:",
      "",
      "- **Purpose**: Executive overview, operational monitoring, deep-dive analysis, team reporting",
      "- **Audience**: Who will use this dashboard?",
      "- **Key metrics**: What numbers matter most?",
      "- **Dimensions**: What should users be able to filter or slice by?",
      "- **Data source**: Live query, pasted data, CSV file, or sample data",
      "",
      "### 2. Gather the Data",
      "",
      "**If data warehouse is connected:**",
      "1. Query the necessary data",
      "2. Embed the results as JSON within the HTML file",
      "",
      "**If data is pasted or uploaded:**",
      "1. Parse and clean the data",
      "2. Embed as JSON in the dashboard",
      "",
      "**If working from a description without data:**",
      "1. Create a realistic sample dataset matching the described schema",
      "2. Note in the dashboard that it uses sample data",
      "3. Provide instructions for swapping in real data",
      "",
      "### 3. Design the Dashboard Layout",
      "",
      "Follow a standard dashboard layout pattern:",
      "",
      "```",
      "┌──────────────────────────────────────────────────┐",
      "│  Dashboard Title                    [Filters ▼]  │",
      "├────────────┬────────────┬────────────┬───────────┤",
      "│  KPI Card  │  KPI Card  │  KPI Card  │ KPI Card  │",
      "├────────────┴────────────┼────────────┴───────────┤",
      "│                         │                        │",
      "│    Primary Chart        │   Secondary Chart      │",
      "│    (largest area)       │                        │",
      "│                         │                        │",
      "├─────────────────────────┴────────────────────────┤",
      "│                                                  │",
      "│    Detail Table (sortable, scrollable)           │",
      "│                                                  │",
      "└──────────────────────────────────────────────────┘",
      "```",
      "",
      "**Adapt the layout to the content:**",
      "- 2-4 KPI cards at the top for headline numbers",
      "- 1-3 charts in the middle section for trends and breakdowns",
      "- Optional detail table at the bottom for drill-down data",
      "- Filters in the header or sidebar depending on complexity",
      "",
      "### 4. Build the HTML Dashboard",
      "",
      "Generate a single self-contained HTML file that includes:",
      "",
      "**Structure (HTML):**",
      "- Semantic HTML5 layout",
      "- Responsive grid using CSS Grid or Flexbox",
      "- Filter controls (dropdowns, date pickers, toggles)",
      "- KPI cards with values and labels",
      "- Chart containers",
      "- Data table with sortable headers",
      "",
      "**Styling (CSS):**",
      "- Professional color scheme (clean whites, grays, with accent colors for data)",
      "- Card-based layout with subtle shadows",
      "- Consistent typography (system fonts for fast loading)",
      "- Responsive design that works on different screen sizes",
      "- Print-friendly styles",
      "",
      "**Interactivity (JavaScript):**",
      "- Chart.js for interactive charts (included via CDN)",
      "- Filter dropdowns that update all charts and tables simultaneously",
      "- Sortable table columns",
      "- Hover tooltips on charts",
      "- Number formatting (commas, currency, percentages)",
      "",
      "**Data (embedded JSON):**",
      "- All data embedded directly in the HTML as JavaScript variables",
      "- No external data fetches required",
      "- Dashboard works completely offline",
      "",
      "### 5. Implement Chart Types",
      "",
      "Use Chart.js for all charts. Common dashboard chart patterns:",
      "",
      "- **Line chart**: Time series trends",
      "- **Bar chart**: Category comparisons",
      "- **Doughnut chart**: Composition (when <6 categories)",
      "- **Stacked bar**: Composition over time",
      "- **Mixed (bar + line)**: Volume with rate overlay",
      "",
      "### 6. Add Interactivity",
      "",
      "**Filters:**",
      "```javascript",
      "// All filters update a central filter state",
      "// Charts and tables re-render when filters change",
      "function applyFilters() {",
      "    const filtered = data.filter(row => matchesFilters(row));",
      "    updateKPIs(filtered);",
      "    updateCharts(filtered);",
      "    updateTable(filtered);",
      "}",
      "```",
      "",
      "**Table sorting:**",
      "- Click column headers to sort ascending/descending",
      "- Visual indicator for current sort column and direction",
      "",
      "**Tooltips:**",
      "- Charts show detailed values on hover",
      "- KPI cards show comparison to previous period",
      "",
      "### 7. Save and Open",
      "",
      "1. Save the dashboard as an HTML file with a descriptive name (e.g., `sales_dashboard.html`)",
      "2. Open it in the user's default browser",
      "3. Confirm it renders correctly",
      "4. Provide instructions for updating data or customizing",
      "",
      "## Output Template",
      "",
      "The generated HTML file follows this structure:",
      "",
      "```html",
      "<!DOCTYPE html>",
      '<html lang="en">',
      "<head>",
      '    <meta charset="UTF-8">',
      '    <meta name="viewport" content="width=device-width, initial-scale=1.0">',
      "    <title>[Dashboard Title]</title>",
      '    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.5.1" integrity="sha384-jb8JQMbMoBUzgWatfe6COACi2ljcDdZQ2OxczGA3bGNeWe+6DChMTBJemed7ZnvJ" crossorigin="anonymous"></script>',
      "    <style>",
      "        /* Professional dashboard CSS */",
      "    </style>",
      "</head>",
      "<body>",
      '    <div class="dashboard">',
      "        <header><!-- Title and filters --></header>",
      '        <section class="kpis"><!-- KPI cards --></section>',
      '        <section class="charts"><!-- Chart containers --></section>',
      '        <section class="details"><!-- Data table --></section>',
      "    </div>",
      "    <script>",
      "        const DATA = [/* embedded JSON data */];",
      "        // Dashboard initialization and interactivity",
      "    </script>",
      "</body>",
      "</html>",
      "```",
      "",
      "## Examples",
      "",
      "```",
      "/build-dashboard Monthly sales dashboard with revenue trend, top products, and regional breakdown. Data is in the orders table.",
      "```",
      "",
      "```",
      "/build-dashboard Here's our support ticket data [pastes CSV]. Build a dashboard showing volume by priority, response time trends, and resolution rates.",
      "```",
      "",
      "```",
      "/build-dashboard Create a template executive dashboard for a SaaS company showing MRR, churn, new customers, and NPS. Use sample data.",
      "```",
      "",
      "## Tips",
      "",
      "- Dashboards are fully self-contained HTML files -- share them with anyone by sending the file",
      "- For real-time dashboards, consider connecting to a BI tool instead. These dashboards are point-in-time snapshots",
      '- Request "dark mode" or "presentation mode" for different styling',
      "- You can request a specific color scheme to match your brand",
      "",
    ].join("\n"),
  },
  "data-create-viz": {
    "SKILL.md": [
      "---",
      "name: data-create-viz",
      "description: Create publication-quality visualizations with Python",
      "metadata:",
      "  plugin: data",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <data source> [chart type]",
      "---",
      "",
      "# /create-viz - Create Visualizations",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Create publication-quality data visualizations using Python. Generates charts from data with best practices for clarity, accuracy, and design.",
      "",
      "## Usage",
      "",
      "```",
      "/create-viz <data source> [chart type] [additional instructions]",
      "```",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Request",
      "",
      "Determine:",
      "",
      "- **Data source**: Query results, pasted data, CSV/Excel file, or data to be queried",
      "- **Chart type**: Explicitly requested or needs to be recommended",
      "- **Purpose**: Exploration, presentation, report, dashboard component",
      "- **Audience**: Technical team, executives, external stakeholders",
      "",
      "### 2. Get the Data",
      "",
      "**If data warehouse is connected and data needs querying:**",
      "1. Write and execute the query",
      "2. Load results into a pandas DataFrame",
      "",
      "**If data is pasted or uploaded:**",
      "1. Parse the data into a pandas DataFrame",
      "2. Clean and prepare as needed (type conversions, null handling)",
      "",
      "**If data is from a previous analysis in the conversation:**",
      "1. Reference the existing data",
      "",
      "### 3. Select Chart Type",
      "",
      "If the user didn't specify a chart type, recommend one based on the data and question:",
      "",
      "| Data Relationship | Recommended Chart |",
      "|---|---|",
      "| Trend over time | Line chart |",
      "| Comparison across categories | Bar chart (horizontal if many categories) |",
      "| Part-to-whole composition | Stacked bar or area chart (avoid pie charts unless <6 categories) |",
      "| Distribution of values | Histogram or box plot |",
      "| Correlation between two variables | Scatter plot |",
      "| Two-variable comparison over time | Dual-axis line or grouped bar |",
      "| Geographic data | Choropleth map |",
      "| Ranking | Horizontal bar chart |",
      "| Flow or process | Sankey diagram |",
      "| Matrix of relationships | Heatmap |",
      "",
      "Explain the recommendation briefly if the user didn't specify.",
      "",
      "### 4. Generate the Visualization",
      "",
      "Write Python code using one of these libraries based on the need:",
      "",
      "- **matplotlib + seaborn**: Best for static, publication-quality charts. Default choice.",
      "- **plotly**: Best for interactive charts or when the user requests interactivity.",
      "",
      "**Code requirements:**",
      "",
      "```python",
      "import matplotlib.pyplot as plt",
      "import seaborn as sns",
      "import pandas as pd",
      "",
      "# Set professional style",
      "plt.style.use('seaborn-v0_8-whitegrid')",
      'sns.set_palette("husl")',
      "",
      "# Create figure with appropriate size",
      "fig, ax = plt.subplots(figsize=(10, 6))",
      "",
      "# [chart-specific code]",
      "",
      "# Always include:",
      "ax.set_title('Clear, Descriptive Title', fontsize=14, fontweight='bold')",
      "ax.set_xlabel('X-Axis Label', fontsize=11)",
      "ax.set_ylabel('Y-Axis Label', fontsize=11)",
      "",
      "# Format numbers appropriately",
      "# - Percentages: '45.2%' not '0.452'",
      "# - Currency: '$1.2M' not '1200000'",
      "# - Large numbers: '2.3K' or '1.5M' not '2300' or '1500000'",
      "",
      "# Remove chart junk",
      "ax.spines['top'].set_visible(False)",
      "ax.spines['right'].set_visible(False)",
      "",
      "plt.tight_layout()",
      "plt.savefig('chart_name.png', dpi=150, bbox_inches='tight')",
      "plt.show()",
      "```",
      "",
      "### 5. Apply Design Best Practices",
      "",
      "**Color:**",
      "- Use a consistent, colorblind-friendly palette",
      "- Use color meaningfully (not decoratively)",
      "- Highlight the key data point or trend with a contrasting color",
      "- Grey out less important reference data",
      "",
      "**Typography:**",
      '- Descriptive title that states the insight, not just the metric (e.g., "Revenue grew 23% YoY" not "Revenue by Month")',
      "- Readable axis labels (not rotated 90 degrees if avoidable)",
      "- Data labels on key points when they add clarity",
      "",
      "**Layout:**",
      "- Appropriate whitespace and margins",
      "- Legend placement that doesn't obscure data",
      "- Sorted categories by value (not alphabetically) unless there's a natural order",
      "",
      "**Accuracy:**",
      "- Y-axis starts at zero for bar charts",
      "- No misleading axis breaks without clear notation",
      "- Consistent scales when comparing panels",
      "- Appropriate precision (don't show 10 decimal places)",
      "",
      "### 6. Save and Present",
      "",
      "1. Save the chart as a PNG file with descriptive name",
      "2. Display the chart to the user",
      "3. Provide the code used so they can modify it",
      "4. Suggest variations (different chart type, different grouping, zoomed time range)",
      "",
      "## Examples",
      "",
      "```",
      "/create-viz Show monthly revenue for the last 12 months as a line chart with the trend highlighted",
      "```",
      "",
      "```",
      "/create-viz Here's our NPS data by product: [pastes data]. Create a horizontal bar chart ranking products by score.",
      "```",
      "",
      "```",
      "/create-viz Query the orders table and create a heatmap of order volume by day-of-week and hour",
      "```",
      "",
      "## Tips",
      "",
      '- If you want interactive charts (hover, zoom, filter), mention "interactive" and Claude will use plotly',
      '- Specify "presentation" if you need larger fonts and higher contrast',
      '- You can request multiple charts at once (e.g., "create a 2x2 grid of charts showing...")',
      "- Charts are saved to your current directory as PNG files",
      "",
    ].join("\n"),
  },
  "data-data-context-extractor": {
    "references/domain-template.md": [
      "# Domain Reference File Template",
      "",
      "Use this template when creating reference files for specific data domains (e.g., revenue, users, marketing).",
      "",
      "---",
      "",
      "```markdown",
      "# [DOMAIN_NAME] Tables",
      "",
      "This document contains [domain]-related tables, metrics, and query patterns.",
      "",
      "---",
      "",
      "## Quick Reference",
      "",
      "### Business Context",
      "",
      "[2-3 sentences explaining what this domain covers and key concepts]",
      "",
      "### Entity Clarification",
      "",
      '**"[AMBIGUOUS_TERM]" can mean:**',
      "- **[MEANING_1]**: [DEFINITION] ([TABLE]: [ID_FIELD])",
      "- **[MEANING_2]**: [DEFINITION] ([TABLE]: [ID_FIELD])",
      "",
      "Always clarify which one before querying.",
      "",
      "### Standard Filters",
      "",
      "For [domain] queries, always:",
      "```sql",
      "WHERE [STANDARD_FILTER_1]",
      "  AND [STANDARD_FILTER_2]",
      "```",
      "",
      "---",
      "",
      "## Key Tables",
      "",
      "### [TABLE_1_NAME]",
      "**Location**: `[project.dataset.table]` or `[schema.table]`",
      "**Description**: [What this table contains, when to use it]",
      "**Primary Key**: [COLUMN(S)]",
      "**Update Frequency**: [Daily/Hourly/Real-time] ([LAG] lag)",
      "**Partitioned By**: [PARTITION_COLUMN] (if applicable)",
      "",
      "| Column | Type | Description | Notes |",
      "|--------|------|-------------|-------|",
      "| **[column_1]** | [TYPE] | [DESCRIPTION] | [GOTCHA_OR_CONTEXT] |",
      "| **[column_2]** | [TYPE] | [DESCRIPTION] | |",
      "| **[column_3]** | [TYPE] | [DESCRIPTION] | Nullable |",
      "",
      "**Relationships**:",
      "- Joins to `[OTHER_TABLE]` on `[JOIN_KEY]`",
      "- Parent of `[CHILD_TABLE]` via `[FOREIGN_KEY]`",
      "",
      "**Nested/Struct Fields** (if applicable):",
      "- `[struct_name].[field_1]`: [DESCRIPTION]",
      "- `[struct_name].[field_2]`: [DESCRIPTION]",
      "",
      "---",
      "",
      "### [TABLE_2_NAME]",
      "[REPEAT FORMAT]",
      "",
      "---",
      "",
      "## Key Metrics",
      "",
      "| Metric | Definition | Table | Formula | Notes |",
      "|--------|------------|-------|---------|-------|",
      "| [METRIC_1] | [DEFINITION] | [TABLE] | `[FORMULA]` | [CAVEATS] |",
      "| [METRIC_2] | [DEFINITION] | [TABLE] | `[FORMULA]` | |",
      "",
      "---",
      "",
      "## Sample Queries",
      "",
      "### [QUERY_PURPOSE_1]",
      "```sql",
      "-- [Brief description of what this query does]",
      "SELECT",
      "    [columns]",
      "FROM [table]",
      "WHERE [standard_filters]",
      "GROUP BY [grouping]",
      "ORDER BY [ordering]",
      "```",
      "",
      "### [QUERY_PURPOSE_2]",
      "```sql",
      "[ANOTHER_COMMON_QUERY]",
      "```",
      "",
      "### [QUERY_PURPOSE_3]: [More Complex Pattern]",
      "```sql",
      "WITH [cte_name] AS (",
      "    [CTE_LOGIC]",
      ")",
      "SELECT",
      "    [final_columns]",
      "FROM [cte_name]",
      "[joins_and_filters]",
      "```",
      "",
      "---",
      "",
      "## Common Gotchas",
      "",
      "1. **[GOTCHA_1]**: [EXPLANATION]",
      "   - Wrong: `[INCORRECT_APPROACH]`",
      "   - Right: `[CORRECT_APPROACH]`",
      "",
      "2. **[GOTCHA_2]**: [EXPLANATION]",
      "",
      "---",
      "",
      "## Related Dashboards (if applicable)",
      "",
      "| Dashboard | Link | Use For |",
      "|-----------|------|---------|",
      "| [DASHBOARD_1] | [URL] | [DESCRIPTION] |",
      "| [DASHBOARD_2] | [URL] | [DESCRIPTION] |",
      "```",
      "",
      "---",
      "",
      "## Tips for Creating Domain Files",
      "",
      "1. **Start with the most-queried tables** - Don't try to document everything",
      "2. **Include column-level detail only for important columns** - Skip obvious ones like `created_at`",
      "3. **Real query examples > abstract descriptions** - Show don't tell",
      "4. **Document the gotchas prominently** - These save the most time",
      "5. **Keep sample queries runnable** - Use real table/column names",
      "6. **Note nested/struct fields explicitly** - These trip people up",
      "",
      "## Suggested Domain Files",
      "",
      "Common domains to document (create separate files for each):",
      "",
      "- `revenue.md` - Billing, subscriptions, ARR, transactions",
      "- `users.md` - Accounts, authentication, user attributes",
      "- `product.md` - Feature usage, events, sessions",
      "- `growth.md` - DAU/WAU/MAU, retention, activation",
      "- `sales.md` - CRM, pipeline, opportunities",
      "- `marketing.md` - Campaigns, attribution, leads",
      "- `support.md` - Tickets, CSAT, response times",
      "",
    ].join("\n"),
    "references/example-output.md": [
      "# Example: Generated Skill",
      "",
      'This is an example of what a generated skill looks like after the bootstrap process. This example is for a fictional e-commerce company called "ShopCo" using Snowflake.',
      "",
      "---",
      "",
      "## Example SKILL.md",
      "",
      "```markdown",
      "---",
      "name: shopco-data-analyst",
      'description: "ShopCo data analysis skill for Snowflake. Provides context for querying e-commerce data including customer, order, and product analytics. Use when analyzing ShopCo data for: (1) Revenue and order metrics, (2) Customer behavior and retention, (3) Product performance, or any data questions requiring ShopCo-specific context."',
      "---",
      "",
      "# ShopCo Data Analysis",
      "",
      "## SQL Dialect: Snowflake",
      "",
      '- **Table references**: `SHOPCO_DW.SCHEMA.TABLE` or with quotes for case-sensitive: `"Column_Name"`',
      "- **Safe division**: `DIV0(a, b)` returns 0, `DIV0NULL(a, b)` returns NULL",
      "- **Date functions**:",
      "  - `DATE_TRUNC('MONTH', date_col)`",
      "  - `DATEADD(DAY, -1, date_col)`",
      "  - `DATEDIFF(DAY, start_date, end_date)`",
      "- **Column exclusion**: `SELECT * EXCLUDE (column_to_exclude)`",
      "",
      "---",
      "",
      "## Entity Disambiguation",
      "",
      '**"Customer" can mean:**',
      "- **User**: A login account that can browse and save items (CORE.DIM_USERS: user_id)",
      "- **Customer**: A user who has made at least one purchase (CORE.DIM_CUSTOMERS: customer_id)",
      "- **Account**: A billing entity, can have multiple users in B2B (CORE.DIM_ACCOUNTS: account_id)",
      "",
      "**Relationships:**",
      "- User → Customer: 1:1 (customer_id = user_id for purchasers)",
      "- Account → User: 1:many (join on account_id)",
      "",
      "---",
      "",
      "## Business Terminology",
      "",
      "| Term | Definition | Notes |",
      "|------|------------|-------|",
      "| GMV | Gross Merchandise Value - total order value before returns/discounts | Use for top-line reporting |",
      "| NMV | Net Merchandise Value - GMV minus returns and discounts | Use for actual revenue |",
      "| AOV | Average Order Value - NMV / order count | Exclude $0 orders |",
      "| LTV | Lifetime Value - total NMV per customer since first order | Rolling calc, updates daily |",
      "| CAC | Customer Acquisition Cost - marketing spend / new customers | By cohort month |",
      "",
      "---",
      "",
      "## Standard Filters",
      "",
      "Always apply these filters unless explicitly told otherwise:",
      "",
      "```sql",
      "-- Exclude test and internal orders",
      "WHERE order_status != 'TEST'",
      "  AND customer_type != 'INTERNAL'",
      "  AND is_employee_order = FALSE",
      "",
      "-- Exclude cancelled orders for revenue metrics",
      "  AND order_status NOT IN ('CANCELLED', 'FRAUDULENT')",
      "```",
      "",
      "---",
      "",
      "## Key Metrics",
      "",
      "### Gross Merchandise Value (GMV)",
      "- **Definition**: Total value of all orders placed",
      "- **Formula**: `SUM(order_total_gross)`",
      "- **Source**: `CORE.FCT_ORDERS.order_total_gross`",
      "- **Time grain**: Daily, aggregated to weekly/monthly",
      "- **Caveats**: Includes orders that may later be cancelled or returned",
      "",
      "### Net Revenue",
      "- **Definition**: Actual revenue after returns and discounts",
      "- **Formula**: `SUM(order_total_gross - return_amount - discount_amount)`",
      "- **Source**: `CORE.FCT_ORDERS`",
      "- **Caveats**: Returns can occur up to 90 days post-order; use settled_revenue for finalized numbers",
      "",
      "---",
      "",
      "## Knowledge Base Navigation",
      "",
      "| Domain | Reference File | Use For |",
      "|--------|----------------|---------|",
      "| Orders | `references/orders.md` | Order tables, GMV/NMV calculations |",
      "| Customers | `references/customers.md` | User/customer entities, LTV, cohorts |",
      "| Products | `references/products.md` | Catalog, inventory, categories |",
      "",
      "---",
      "",
      "## Common Query Patterns",
      "",
      "### Daily GMV by Channel",
      "```sql",
      "SELECT",
      "    DATE_TRUNC('DAY', order_timestamp) AS order_date,",
      "    channel,",
      "    SUM(order_total_gross) AS gmv,",
      "    COUNT(DISTINCT order_id) AS order_count",
      "FROM SHOPCO_DW.CORE.FCT_ORDERS",
      "WHERE order_status NOT IN ('TEST', 'CANCELLED', 'FRAUDULENT')",
      "  AND order_timestamp >= DATEADD(DAY, -30, CURRENT_DATE())",
      "GROUP BY 1, 2",
      "ORDER BY 1 DESC, 3 DESC",
      "```",
      "",
      "### Customer Cohort Retention",
      "```sql",
      "WITH cohorts AS (",
      "    SELECT",
      "        customer_id,",
      "        DATE_TRUNC('MONTH', first_order_date) AS cohort_month",
      "    FROM SHOPCO_DW.CORE.DIM_CUSTOMERS",
      ")",
      "SELECT",
      "    c.cohort_month,",
      "    DATEDIFF(MONTH, c.cohort_month, DATE_TRUNC('MONTH', o.order_timestamp)) AS months_since_first,",
      "    COUNT(DISTINCT c.customer_id) AS active_customers",
      "FROM cohorts c",
      "JOIN SHOPCO_DW.CORE.FCT_ORDERS o ON c.customer_id = o.customer_id",
      "WHERE o.order_status NOT IN ('TEST', 'CANCELLED')",
      "GROUP BY 1, 2",
      "ORDER BY 1, 2",
      "```",
      "```",
      "",
      "---",
      "",
      "## Example references/orders.md",
      "",
      "```markdown",
      "# Orders Tables",
      "",
      "Order and transaction data for ShopCo.",
      "",
      "---",
      "",
      "## Key Tables",
      "",
      "### FCT_ORDERS",
      "**Location**: `SHOPCO_DW.CORE.FCT_ORDERS`",
      "**Description**: Fact table of all orders. One row per order.",
      "**Primary Key**: `order_id`",
      "**Update Frequency**: Hourly (15 min lag)",
      "**Partitioned By**: `order_date`",
      "",
      "| Column | Type | Description | Notes |",
      "|--------|------|-------------|-------|",
      "| **order_id** | VARCHAR | Unique order identifier | |",
      "| **customer_id** | VARCHAR | FK to DIM_CUSTOMERS | NULL for guest checkout |",
      "| **order_timestamp** | TIMESTAMP_NTZ | When order was placed | UTC |",
      "| **order_date** | DATE | Date portion of order_timestamp | Partition column |",
      "| **order_status** | VARCHAR | Current status | PENDING, SHIPPED, DELIVERED, CANCELLED, RETURNED |",
      "| **channel** | VARCHAR | Acquisition channel | WEB, APP, MARKETPLACE |",
      "| **order_total_gross** | DECIMAL(12,2) | Pre-discount total | |",
      "| **discount_amount** | DECIMAL(12,2) | Total discounts applied | |",
      "| **return_amount** | DECIMAL(12,2) | Value of returned items | Updates async |",
      "",
      "**Relationships**:",
      "- Joins to `DIM_CUSTOMERS` on `customer_id`",
      "- Parent of `FCT_ORDER_ITEMS` via `order_id`",
      "",
      "---",
      "",
      "## Sample Queries",
      "",
      "### Orders with Returns Rate",
      "```sql",
      "SELECT",
      "    DATE_TRUNC('WEEK', order_date) AS week,",
      "    COUNT(*) AS total_orders,",
      "    SUM(CASE WHEN return_amount > 0 THEN 1 ELSE 0 END) AS orders_with_returns,",
      "    DIV0(SUM(CASE WHEN return_amount > 0 THEN 1 ELSE 0 END), COUNT(*)) AS return_rate",
      "FROM SHOPCO_DW.CORE.FCT_ORDERS",
      "WHERE order_status NOT IN ('TEST', 'CANCELLED')",
      "  AND order_date >= DATEADD(MONTH, -3, CURRENT_DATE())",
      "GROUP BY 1",
      "ORDER BY 1",
      "```",
      "```",
      "",
      "---",
      "",
      "This example demonstrates:",
      "- Complete frontmatter with triggering description",
      "- Dialect-specific SQL notes",
      "- Clear entity disambiguation",
      "- Terminology glossary",
      "- Standard filters as copy-paste SQL",
      "- Metric definitions with formulas",
      "- Navigation to reference files",
      "- Real, runnable query examples",
      "",
    ].join("\n"),
    "references/skill-template.md": [
      "# Generated Skill Template",
      "",
      "Use this template when generating a new data analysis skill. Replace all `[PLACEHOLDER]` values.",
      "",
      "---",
      "",
      "```markdown",
      "---",
      "name: [company]-data-analyst",
      'description: "[COMPANY] data analysis skill. Provides context for querying [WAREHOUSE_TYPE] including entity definitions, metric calculations, and common query patterns. Use when analyzing [COMPANY] data for: (1) [PRIMARY_USE_CASE_1], (2) [PRIMARY_USE_CASE_2], (3) [PRIMARY_USE_CASE_3], or any data questions requiring [COMPANY]-specific context."',
      "---",
      "",
      "# [COMPANY] Data Analysis",
      "",
      "## SQL Dialect: [WAREHOUSE_TYPE]",
      "",
      "[INSERT APPROPRIATE DIALECT SECTION FROM sql-dialects.md]",
      "",
      "---",
      "",
      "## Entity Disambiguation",
      "",
      "When users mention these terms, clarify which entity they mean:",
      "",
      "[EXAMPLE FORMAT - customize based on discovery:]",
      "",
      '**"User" can mean:**',
      "- **Account**: An individual login/profile ([PRIMARY_TABLE]: [ID_FIELD])",
      "- **Organization**: A billing entity that can have multiple accounts ([ORG_TABLE]: [ORG_ID])",
      "- **[OTHER_TYPE]**: [DEFINITION] ([TABLE]: [ID])",
      "",
      "**Relationships:**",
      "- [ENTITY_1] → [ENTITY_2]: [RELATIONSHIP_TYPE] (join on [JOIN_KEY])",
      "",
      "---",
      "",
      "## Business Terminology",
      "",
      "| Term | Definition | Notes |",
      "|------|------------|-------|",
      "| [TERM_1] | [DEFINITION] | [CONTEXT/GOTCHA] |",
      "| [TERM_2] | [DEFINITION] | [CONTEXT/GOTCHA] |",
      "| [ACRONYM] | [FULL_NAME] - [EXPLANATION] | |",
      "",
      "---",
      "",
      "## Standard Filters",
      "",
      "Always apply these filters unless explicitly told otherwise:",
      "",
      "```sql",
      "-- Exclude test/internal data",
      "WHERE [TEST_FLAG_COLUMN] = FALSE",
      "  AND [INTERNAL_FLAG_COLUMN] = FALSE",
      "",
      "-- Exclude invalid/fraud",
      "  AND [STATUS_COLUMN] != '[EXCLUDED_STATUS]'",
      "",
      "-- [OTHER STANDARD EXCLUSIONS]",
      "```",
      "",
      "**When to override:**",
      "- [SCENARIO_1]: Include [NORMALLY_EXCLUDED] when [CONDITION]",
      "",
      "---",
      "",
      "## Key Metrics",
      "",
      "### [METRIC_1_NAME]",
      "- **Definition**: [PLAIN_ENGLISH_EXPLANATION]",
      "- **Formula**: `[EXACT_CALCULATION]`",
      "- **Source**: `[TABLE_NAME].[COLUMN_NAME]`",
      "- **Time grain**: [DAILY/WEEKLY/MONTHLY]",
      "- **Caveats**: [EDGE_CASES_OR_GOTCHAS]",
      "",
      "### [METRIC_2_NAME]",
      "[REPEAT FORMAT]",
      "",
      "---",
      "",
      "## Data Freshness",
      "",
      "| Table | Update Frequency | Typical Lag |",
      "|-------|------------------|-------------|",
      "| [TABLE_1] | [FREQUENCY] | [LAG] |",
      "| [TABLE_2] | [FREQUENCY] | [LAG] |",
      "",
      "To check data freshness:",
      "```sql",
      "SELECT MAX([DATE_COLUMN]) as latest_data FROM [TABLE]",
      "```",
      "",
      "---",
      "",
      "## Knowledge Base Navigation",
      "",
      "Use these reference files for detailed table documentation:",
      "",
      "| Domain | Reference File | Use For |",
      "|--------|----------------|---------|",
      "| [DOMAIN_1] | `references/[domain1].md` | [BRIEF_DESCRIPTION] |",
      "| [DOMAIN_2] | `references/[domain2].md` | [BRIEF_DESCRIPTION] |",
      "| Entities | `references/entities.md` | Entity definitions and relationships |",
      "| Metrics | `references/metrics.md` | KPI calculations and formulas |",
      "",
      "---",
      "",
      "## Common Query Patterns",
      "",
      "### [PATTERN_1_NAME]",
      "```sql",
      "[SAMPLE_QUERY]",
      "```",
      "",
      "### [PATTERN_2_NAME]",
      "```sql",
      "[SAMPLE_QUERY]",
      "```",
      "",
      "---",
      "",
      "## Troubleshooting",
      "",
      "### Common Mistakes",
      "- **[MISTAKE_1]**: [EXPLANATION] → [CORRECT_APPROACH]",
      "- **[MISTAKE_2]**: [EXPLANATION] → [CORRECT_APPROACH]",
      "",
      "### Access Issues",
      "- If you encounter permission errors on `[TABLE]`: [WORKAROUND]",
      "- For PII-restricted columns: [ALTERNATIVE_APPROACH]",
      "",
      "### Performance Tips",
      "- Filter by `[PARTITION_COLUMN]` first to reduce data scanned",
      "- For large tables, use `LIMIT` during exploration",
      "- Prefer `[AGGREGATED_TABLE]` over `[RAW_TABLE]` when possible",
      "```",
      "",
      "---",
      "",
      "## Customization Notes",
      "",
      "When generating a skill:",
      "",
      "1. **Fill all placeholders** - Don't leave any `[PLACEHOLDER]` text",
      "2. **Remove unused sections** - If they don't have dashboards, remove that section",
      "3. **Add specificity** - Generic advice is less useful than specific column names and values",
      "4. **Include real examples** - Sample queries should use actual table/column names",
      "5. **Keep it scannable** - Use tables and code blocks liberally",
      "",
    ].join("\n"),
    "references/sql-dialects.md": [
      "# SQL Dialect Reference",
      "",
      "Include the appropriate section in generated skills based on the user's data warehouse.",
      "",
      "---",
      "",
      "## BigQuery",
      "",
      "```markdown",
      "## SQL Dialect: BigQuery",
      "",
      "- **Table references**: Use backticks: \\`project.dataset.table\\`",
      "- **Safe division**: `SAFE_DIVIDE(a, b)` returns NULL instead of error",
      "- **Date functions**:",
      "  - `DATE_TRUNC(date_col, MONTH)`",
      "  - `DATE_SUB(date_col, INTERVAL 1 DAY)`",
      "  - `DATE_DIFF(end_date, start_date, DAY)`",
      "- **Column exclusion**: `SELECT * EXCEPT(column_to_exclude)`",
      "- **Arrays**: `UNNEST(array_column)` to flatten",
      "- **Structs**: Access with dot notation `struct_col.field_name`",
      "- **Timestamps**: `TIMESTAMP_TRUNC()`, times in UTC by default",
      "- **String matching**: `LIKE`, `REGEXP_CONTAINS(col, r'pattern')`",
      "- **NULLs in aggregations**: Most functions ignore NULLs; use `IFNULL()` or `COALESCE()`",
      "```",
      "",
      "---",
      "",
      "## Snowflake",
      "",
      "```markdown",
      "## SQL Dialect: Snowflake",
      "",
      '- **Table references**: `DATABASE.SCHEMA.TABLE` or with quotes for case-sensitive: `"Column_Name"`',
      "- **Safe division**: `DIV0(a, b)` returns 0, `DIV0NULL(a, b)` returns NULL",
      "- **Date functions**:",
      "  - `DATE_TRUNC('MONTH', date_col)`",
      "  - `DATEADD(DAY, -1, date_col)`",
      "  - `DATEDIFF(DAY, start_date, end_date)`",
      "- **Column exclusion**: `SELECT * EXCLUDE (column_to_exclude)`",
      "- **Arrays**: `FLATTEN(array_column)` to flatten, access with `value`",
      "- **Variants/JSON**: Access with colon notation `variant_col:field_name`",
      "- **Timestamps**: `TIMESTAMP_NTZ` (no timezone), `TIMESTAMP_TZ` (with timezone)",
      "- **String matching**: `LIKE`, `REGEXP_LIKE(col, 'pattern')`",
      "- **Case sensitivity**: Identifiers are uppercase by default unless quoted",
      "```",
      "",
      "---",
      "",
      "## PostgreSQL / Redshift",
      "",
      "```markdown",
      "## SQL Dialect: PostgreSQL/Redshift",
      "",
      "- **Table references**: `schema.table` (lowercase convention)",
      "- **Safe division**: `NULLIF(b, 0)` pattern: `a / NULLIF(b, 0)`",
      "- **Date functions**:",
      "  - `DATE_TRUNC('month', date_col)`",
      "  - `date_col - INTERVAL '1 day'`",
      "  - `DATE_PART('day', end_date - start_date)`",
      "- **Column selection**: No EXCEPT; must list columns explicitly",
      "- **Arrays**: `UNNEST(array_column)` (PostgreSQL), limited in Redshift",
      "- **JSON**: `json_col->>'field_name'` for text, `json_col->'field_name'` for JSON",
      "- **Timestamps**: `AT TIME ZONE 'UTC'` for timezone conversion",
      "- **String matching**: `LIKE`, `col ~ 'pattern'` for regex",
      "- **Boolean**: Native BOOLEAN type; use `TRUE`/`FALSE`",
      "```",
      "",
      "---",
      "",
      "## Databricks / Spark SQL",
      "",
      "```markdown",
      "## SQL Dialect: Databricks/Spark SQL",
      "",
      "- **Table references**: `catalog.schema.table` (Unity Catalog) or `schema.table`",
      "- **Safe division**: Use `NULLIF`: `a / NULLIF(b, 0)` or `TRY_DIVIDE(a, b)`",
      "- **Date functions**:",
      "  - `DATE_TRUNC('MONTH', date_col)`",
      "  - `DATE_SUB(date_col, 1)`",
      "  - `DATEDIFF(end_date, start_date)`",
      "- **Column exclusion**: `SELECT * EXCEPT (column_to_exclude)` (Databricks SQL)",
      "- **Arrays**: `EXPLODE(array_column)` to flatten",
      "- **Structs**: Access with dot notation `struct_col.field_name`",
      "- **JSON**: `json_col:field_name` or `GET_JSON_OBJECT()`",
      "- **String matching**: `LIKE`, `RLIKE` for regex",
      "- **Delta features**: `DESCRIBE HISTORY`, time travel with `VERSION AS OF`",
      "```",
      "",
      "---",
      "",
      "## MySQL",
      "",
      "```markdown",
      "## SQL Dialect: MySQL",
      "",
      "- **Table references**: \\`database\\`.\\`table\\` with backticks",
      "- **Safe division**: Manual: `IF(b = 0, NULL, a / b)` or `a / NULLIF(b, 0)`",
      "- **Date functions**:",
      "  - `DATE_FORMAT(date_col, '%Y-%m-01')` for truncation",
      "  - `DATE_SUB(date_col, INTERVAL 1 DAY)`",
      "  - `DATEDIFF(end_date, start_date)`",
      "- **Column selection**: No EXCEPT; must list columns explicitly",
      "- **Arrays**: Limited native support; often stored as JSON",
      "- **JSON**: `JSON_EXTRACT(col, '$.field')` or `col->>'$.field'`",
      "- **Timestamps**: `CONVERT_TZ()` for timezone conversion",
      "- **String matching**: `LIKE`, `REGEXP` for regex",
      "- **Case sensitivity**: Table names case-sensitive on Linux, not on Windows",
      "```",
      "",
      "---",
      "",
      "## Common Patterns Across Dialects",
      "",
      "| Operation | BigQuery | Snowflake | PostgreSQL | Databricks |",
      "|-----------|----------|-----------|------------|------------|",
      "| Current date | `CURRENT_DATE()` | `CURRENT_DATE()` | `CURRENT_DATE` | `CURRENT_DATE()` |",
      "| Current timestamp | `CURRENT_TIMESTAMP()` | `CURRENT_TIMESTAMP()` | `NOW()` | `CURRENT_TIMESTAMP()` |",
      "| String concat | `CONCAT()` or `\\|\\|` | `CONCAT()` or `\\|\\|` | `CONCAT()` or `\\|\\|` | `CONCAT()` or `\\|\\|` |",
      "| Coalesce | `COALESCE()` | `COALESCE()` | `COALESCE()` | `COALESCE()` |",
      "| Case when | `CASE WHEN` | `CASE WHEN` | `CASE WHEN` | `CASE WHEN` |",
      "| Count distinct | `COUNT(DISTINCT x)` | `COUNT(DISTINCT x)` | `COUNT(DISTINCT x)` | `COUNT(DISTINCT x)` |",
      "",
    ].join("\n"),
    "scripts/package_data_skill.py": [
      "#!/usr/bin/env python3",
      '"""',
      "Package a generated data analysis skill into a distributable .skill file (zip format).",
      "",
      "Usage:",
      "    python package_data_skill.py <path/to/skill-folder> [output-directory]",
      "",
      "Example:",
      "    python package_data_skill.py /home/claude/acme-data-analyst",
      "    python package_data_skill.py /home/claude/acme-data-analyst /tmp/outputs",
      '"""',
      "",
      "import sys",
      "import zipfile",
      "from pathlib import Path",
      "",
      "",
      "def validate_skill(skill_path: Path) -> tuple[bool, str]:",
      '    """Basic validation of skill structure."""',
      "",
      "    # Check SKILL.md exists",
      '    skill_md = skill_path / "SKILL.md"',
      "    if not skill_md.exists():",
      '        return False, "Missing SKILL.md"',
      "",
      "    # Check SKILL.md has frontmatter",
      "    content = skill_md.read_text()",
      '    if not content.startswith("---"):',
      '        return False, "SKILL.md missing YAML frontmatter"',
      "",
      "    # Check for required frontmatter fields",
      '    if "name:" not in content[:500]:',
      "        return False, \"SKILL.md missing 'name' in frontmatter\"",
      '    if "description:" not in content[:1000]:',
      "        return False, \"SKILL.md missing 'description' in frontmatter\"",
      "",
      "    # Check for placeholder text that wasn't filled in",
      '    if "[PLACEHOLDER]" in content or "[COMPANY]" in content:',
      '        return False, "SKILL.md contains unfilled placeholder text"',
      "",
      '    return True, "Validation passed"',
      "",
      "",
      "def package_skill(skill_path: str, output_dir: str = None) -> Path | None:",
      '    """',
      "    Package a skill folder into a .skill file.",
      "",
      "    Args:",
      "        skill_path: Path to the skill folder",
      "        output_dir: Optional output directory",
      "",
      "    Returns:",
      "        Path to the created .skill file, or None if error",
      '    """',
      "    skill_path = Path(skill_path).resolve()",
      "",
      "    # Validate folder exists",
      "    if not skill_path.exists():",
      '        print(f"Error: Skill folder not found: {skill_path}")',
      "        return None",
      "",
      "    if not skill_path.is_dir():",
      '        print(f"Error: Path is not a directory: {skill_path}")',
      "        return None",
      "",
      "    # Run validation",
      '    print("Validating skill...")',
      "    valid, message = validate_skill(skill_path)",
      "    if not valid:",
      '        print(f"Validation failed: {message}")',
      "        return None",
      '    print(f"{message}\\n")',
      "",
      "    # Determine output location",
      "    skill_name = skill_path.name",
      "    if output_dir:",
      "        output_path = Path(output_dir).resolve()",
      "    else:",
      "        output_path = Path.cwd()",
      "",
      "    output_path.mkdir(parents=True, exist_ok=True)",
      '    skill_filename = output_path / f"{skill_name}.zip"',
      "",
      "    # Create the zip file",
      "    try:",
      "        with zipfile.ZipFile(skill_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:",
      "            for file_path in skill_path.rglob('*'):",
      "                if file_path.is_file():",
      "                    # Skip hidden files and common junk",
      "                    if any(part.startswith('.') for part in file_path.parts):",
      "                        continue",
      "                    if file_path.name in ['__pycache__', '.DS_Store', 'Thumbs.db']:",
      "                        continue",
      "",
      "                    # Calculate relative path within the zip",
      "                    arcname = file_path.relative_to(skill_path.parent)",
      "                    zipf.write(file_path, arcname)",
      '                    print(f"  Added: {arcname}")',
      "",
      '        print(f"\\nSuccessfully packaged skill to: {skill_filename}")',
      "        return skill_filename",
      "",
      "    except Exception as e:",
      '        print(f"Error creating zip file: {e}")',
      "        return None",
      "",
      "",
      "def main():",
      "    if len(sys.argv) < 2:",
      "        print(__doc__)",
      "        sys.exit(1)",
      "",
      "    skill_path = sys.argv[1]",
      "    output_dir = sys.argv[2] if len(sys.argv) > 2 else None",
      "",
      '    print(f"Packaging skill: {skill_path}")',
      "    if output_dir:",
      '        print(f"   Output directory: {output_dir}")',
      "    print()",
      "",
      "    result = package_skill(skill_path, output_dir)",
      "    sys.exit(0 if result else 1)",
      "",
      "",
      'if __name__ == "__main__":',
      "    main()",
      "",
    ].join("\n"),
    "SKILL.md": [
      "---",
      "name: data-data-context-extractor",
      "description: |",
      "  Generate or improve a company-specific data analysis skill by extracting tribal knowledge from analysts.",
      '  BOOTSTRAP MODE - Triggers: "Create a data context skill", "Set up data analysis for our warehouse", "Help me create a skill for our database", "Generate a data skill for [company]" → Discovers schemas, asks key questions, generates initial skill with reference files',
      '  ITERATION MODE - Triggers: "Add context about [domain]", "The skill needs more info about [topic]", "Update the data skill with [metrics/tables/terminology]", "Improve the [domain] reference" → Loads existing skill, asks targeted questions, appends/updates reference files',
      "  Use when data analysts want Claude to understand their company's specific data warehouse, terminology, metrics definitions, and common query patterns.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Data Context Extractor",
      "",
      "A meta-skill that extracts company-specific data knowledge from analysts and generates tailored data analysis skills.",
      "",
      "## How It Works",
      "",
      "This skill has two modes:",
      "",
      "1. **Bootstrap Mode**: Create a new data analysis skill from scratch",
      "2. **Iteration Mode**: Improve an existing skill by adding domain-specific reference files",
      "",
      "---",
      "",
      "## Bootstrap Mode",
      "",
      "Use when: User wants to create a new data context skill for their warehouse.",
      "",
      "### Phase 1: Database Connection & Discovery",
      "",
      "**Step 1: Identify the database type**",
      "",
      'Ask: "What data warehouse are you using?"',
      "",
      "Common options:",
      "- **BigQuery**",
      "- **Snowflake**",
      "- **PostgreSQL/Redshift**",
      "- **Databricks**",
      "",
      "Use `~~data warehouse` tools (query and schema) to connect. If unclear, check available MCP tools in the current session.",
      "",
      "**Step 2: Explore the schema**",
      "",
      "Use `~~data warehouse` schema tools to:",
      "1. List available datasets/schemas",
      '2. Identify the most important tables (ask user: "Which 3-5 tables do analysts query most often?")',
      "3. Pull schema details for those key tables",
      "",
      "Sample exploration queries by dialect:",
      "```sql",
      "-- BigQuery: List datasets",
      "SELECT schema_name FROM INFORMATION_SCHEMA.SCHEMATA",
      "",
      "-- BigQuery: List tables in a dataset",
      "SELECT table_name FROM `project.dataset.INFORMATION_SCHEMA.TABLES`",
      "",
      "-- Snowflake: List schemas",
      "SHOW SCHEMAS IN DATABASE my_database",
      "",
      "-- Snowflake: List tables",
      "SHOW TABLES IN SCHEMA my_schema",
      "```",
      "",
      "### Phase 2: Core Questions (Ask These)",
      "",
      "After schema discovery, ask these questions conversationally (not all at once):",
      "",
      "**Entity Disambiguation (Critical)**",
      "> \"When people here say 'user' or 'customer', what exactly do they mean? Are there different types?\"",
      "",
      "Listen for:",
      "- Multiple entity types (user vs account vs organization)",
      "- Relationships between them (1:1, 1:many, many:many)",
      "- Which ID fields link them together",
      "",
      "**Primary Identifiers**",
      '> "What\'s the main identifier for a [customer/user/account]? Are there multiple IDs for the same entity?"',
      "",
      "Listen for:",
      "- Primary keys vs business keys",
      "- UUID vs integer IDs",
      "- Legacy ID systems",
      "",
      "**Key Metrics**",
      '> "What are the 2-3 metrics people ask about most? How is each one calculated?"',
      "",
      "Listen for:",
      "- Exact formulas (ARR = monthly_revenue × 12)",
      "- Which tables/columns feed each metric",
      "- Time period conventions (trailing 7 days, calendar month, etc.)",
      "",
      "**Data Hygiene**",
      '> "What should ALWAYS be filtered out of queries? (test data, fraud, internal users, etc.)"',
      "",
      "Listen for:",
      "- Standard WHERE clauses to always include",
      "- Flag columns that indicate exclusions (is_test, is_internal, is_fraud)",
      "- Specific values to exclude (status = 'deleted')",
      "",
      "**Common Gotchas**",
      '> "What mistakes do new analysts typically make with this data?"',
      "",
      "Listen for:",
      "- Confusing column names",
      "- Timezone issues",
      "- NULL handling quirks",
      "- Historical vs current state tables",
      "",
      "### Phase 3: Generate the Skill",
      "",
      "Create a skill with this structure:",
      "",
      "```",
      "[company]-data-analyst/",
      "├── SKILL.md",
      "└── references/",
      "    ├── entities.md          # Entity definitions and relationships",
      "    ├── metrics.md           # KPI calculations",
      "    ├── tables/              # One file per domain",
      "    │   ├── [domain1].md",
      "    │   └── [domain2].md",
      "    └── dashboards.json      # Optional: existing dashboards catalog",
      "```",
      "",
      "**SKILL.md Template**: See `references/skill-template.md`",
      "",
      "**SQL Dialect Section**: See `references/sql-dialects.md` and include the appropriate dialect notes.",
      "",
      "**Reference File Template**: See `references/domain-template.md`",
      "",
      "### Phase 4: Package and Deliver",
      "",
      "1. Create all files in the skill directory",
      "2. Package as a zip file",
      "3. Present to user with summary of what was captured",
      "",
      "---",
      "",
      "## Iteration Mode",
      "",
      "Use when: User has an existing skill but needs to add more context.",
      "",
      "### Step 1: Load Existing Skill",
      "",
      "Ask user to upload their existing skill (zip or folder), or locate it if already in the session.",
      "",
      "Read the current SKILL.md and reference files to understand what's already documented.",
      "",
      "### Step 2: Identify the Gap",
      "",
      'Ask: "What domain or topic needs more context? What queries are failing or producing wrong results?"',
      "",
      "Common gaps:",
      "- A new data domain (marketing, finance, product, etc.)",
      "- Missing metric definitions",
      "- Undocumented table relationships",
      "- New terminology",
      "",
      "### Step 3: Targeted Discovery",
      "",
      "For the identified domain:",
      "",
      "1. **Explore relevant tables**: Use `~~data warehouse` schema tools to find tables in that domain",
      "2. **Ask domain-specific questions**:",
      '   - "What tables are used for [domain] analysis?"',
      '   - "What are the key metrics for [domain]?"',
      '   - "Any special filters or gotchas for [domain] data?"',
      "",
      "3. **Generate new reference file**: Create `references/[domain].md` using the domain template",
      "",
      "### Step 4: Update and Repackage",
      "",
      "1. Add the new reference file",
      '2. Update SKILL.md\'s "Knowledge Base Navigation" section to include the new domain',
      "3. Repackage the skill",
      "4. Present the updated skill to user",
      "",
      "---",
      "",
      "## Reference File Standards",
      "",
      "Each reference file should include:",
      "",
      "### For Table Documentation",
      "- **Location**: Full table path",
      "- **Description**: What this table contains, when to use it",
      "- **Primary Key**: How to uniquely identify rows",
      "- **Update Frequency**: How often data refreshes",
      "- **Key Columns**: Table with column name, type, description, notes",
      "- **Relationships**: How this table joins to others",
      "- **Sample Queries**: 2-3 common query patterns",
      "",
      "### For Metrics Documentation",
      "- **Metric Name**: Human-readable name",
      "- **Definition**: Plain English explanation",
      "- **Formula**: Exact calculation with column references",
      "- **Source Table(s)**: Where the data comes from",
      "- **Caveats**: Edge cases, exclusions, gotchas",
      "",
      "### For Entity Documentation",
      "- **Entity Name**: What it's called",
      "- **Definition**: What it represents in the business",
      "- **Primary Table**: Where to find this entity",
      "- **ID Field(s)**: How to identify it",
      "- **Relationships**: How it relates to other entities",
      "- **Common Filters**: Standard exclusions (internal, test, etc.)",
      "",
      "---",
      "",
      "## Quality Checklist",
      "",
      "Before delivering a generated skill, verify:",
      "",
      "- [ ] SKILL.md has complete frontmatter (name, description)",
      "- [ ] Entity disambiguation section is clear",
      "- [ ] Key terminology is defined",
      "- [ ] Standard filters/exclusions are documented",
      "- [ ] At least 2-3 sample queries per domain",
      "- [ ] SQL uses correct dialect syntax",
      "- [ ] Reference files are linked from SKILL.md navigation section",
      "",
    ].join("\n"),
  },
  "data-data-exploration": {
    "SKILL.md": [
      "---",
      "name: data-data-exploration",
      "description: Profile and explore datasets to understand their shape, quality, and patterns before analysis. Use when encountering a new dataset, assessing data quality, discovering column distributions, identifying nulls and outliers, or deciding which dimensions to analyze.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Data Exploration Skill",
      "",
      "Systematic methodology for profiling datasets, assessing data quality, discovering patterns, and understanding schemas.",
      "",
      "## Data Profiling Methodology",
      "",
      "### Phase 1: Structural Understanding",
      "",
      "Before analyzing any data, understand its structure:",
      "",
      "**Table-level questions:**",
      "- How many rows and columns?",
      "- What is the grain (one row per what)?",
      "- What is the primary key? Is it unique?",
      "- When was the data last updated?",
      "- How far back does the data go?",
      "",
      "**Column classification:**",
      "Categorize each column as one of:",
      "- **Identifier**: Unique keys, foreign keys, entity IDs",
      "- **Dimension**: Categorical attributes for grouping/filtering (status, type, region, category)",
      "- **Metric**: Quantitative values for measurement (revenue, count, duration, score)",
      "- **Temporal**: Dates and timestamps (created_at, updated_at, event_date)",
      "- **Text**: Free-form text fields (description, notes, name)",
      "- **Boolean**: True/false flags",
      "- **Structural**: JSON, arrays, nested structures",
      "",
      "### Phase 2: Column-Level Profiling",
      "",
      "For each column, compute:",
      "",
      "**All columns:**",
      "- Null count and null rate",
      "- Distinct count and cardinality ratio (distinct / total)",
      "- Most common values (top 5-10 with frequencies)",
      "- Least common values (bottom 5 to spot anomalies)",
      "",
      "**Numeric columns (metrics):**",
      "```",
      "min, max, mean, median (p50)",
      "standard deviation",
      "percentiles: p1, p5, p25, p75, p95, p99",
      "zero count",
      "negative count (if unexpected)",
      "```",
      "",
      "**String columns (dimensions, text):**",
      "```",
      "min length, max length, avg length",
      "empty string count",
      "pattern analysis (do values follow a format?)",
      "case consistency (all upper, all lower, mixed?)",
      "leading/trailing whitespace count",
      "```",
      "",
      "**Date/timestamp columns:**",
      "```",
      "min date, max date",
      "null dates",
      "future dates (if unexpected)",
      "distribution by month/week",
      "gaps in time series",
      "```",
      "",
      "**Boolean columns:**",
      "```",
      "true count, false count, null count",
      "true rate",
      "```",
      "",
      "### Phase 3: Relationship Discovery",
      "",
      "After profiling individual columns:",
      "",
      "- **Foreign key candidates**: ID columns that might link to other tables",
      "- **Hierarchies**: Columns that form natural drill-down paths (country > state > city)",
      "- **Correlations**: Numeric columns that move together",
      "- **Derived columns**: Columns that appear to be computed from others",
      "- **Redundant columns**: Columns with identical or near-identical information",
      "",
      "## Quality Assessment Framework",
      "",
      "### Completeness Score",
      "",
      "Rate each column:",
      "- **Complete** (>99% non-null): Green",
      "- **Mostly complete** (95-99%): Yellow -- investigate the nulls",
      "- **Incomplete** (80-95%): Orange -- understand why and whether it matters",
      "- **Sparse** (<80%): Red -- may not be usable without imputation",
      "",
      "### Consistency Checks",
      "",
      "Look for:",
      '- **Value format inconsistency**: Same concept represented differently ("USA", "US", "United States", "us")',
      "- **Type inconsistency**: Numbers stored as strings, dates in various formats",
      "- **Referential integrity**: Foreign keys that don't match any parent record",
      "- **Business rule violations**: Negative quantities, end dates before start dates, percentages > 100",
      '- **Cross-column consistency**: Status = "completed" but completed_at is null',
      "",
      "### Accuracy Indicators",
      "",
      "Red flags that suggest accuracy issues:",
      '- **Placeholder values**: 0, -1, 999999, "N/A", "TBD", "test", "xxx"',
      "- **Default values**: Suspiciously high frequency of a single value",
      "- **Stale data**: Updated_at shows no recent changes in an active system",
      "- **Impossible values**: Ages > 150, dates in the far future, negative durations",
      "- **Round number bias**: All values ending in 0 or 5 (suggests estimation, not measurement)",
      "",
      "### Timeliness Assessment",
      "",
      "- When was the table last updated?",
      "- What is the expected update frequency?",
      "- Is there a lag between event time and load time?",
      "- Are there gaps in the time series?",
      "",
      "## Pattern Discovery Techniques",
      "",
      "### Distribution Analysis",
      "",
      "For numeric columns, characterize the distribution:",
      "- **Normal**: Mean and median are close, bell-shaped",
      "- **Skewed right**: Long tail of high values (common for revenue, session duration)",
      "- **Skewed left**: Long tail of low values (less common)",
      "- **Bimodal**: Two peaks (suggests two distinct populations)",
      "- **Power law**: Few very large values, many small ones (common for user activity)",
      "- **Uniform**: Roughly equal frequency across range (often synthetic or random)",
      "",
      "### Temporal Patterns",
      "",
      "For time series data, look for:",
      "- **Trend**: Sustained upward or downward movement",
      "- **Seasonality**: Repeating patterns (weekly, monthly, quarterly, annual)",
      "- **Day-of-week effects**: Weekday vs. weekend differences",
      "- **Holiday effects**: Drops or spikes around known holidays",
      "- **Change points**: Sudden shifts in level or trend",
      "- **Anomalies**: Individual data points that break the pattern",
      "",
      "### Segmentation Discovery",
      "",
      "Identify natural segments by:",
      "- Finding categorical columns with 3-20 distinct values",
      "- Comparing metric distributions across segment values",
      "- Looking for segments with significantly different behavior",
      "- Testing whether segments are homogeneous or contain sub-segments",
      "",
      "### Correlation Exploration",
      "",
      "Between numeric columns:",
      "- Compute correlation matrix for all metric pairs",
      "- Flag strong correlations (|r| > 0.7) for investigation",
      "- Note: Correlation does not imply causation -- flag this explicitly",
      "- Check for non-linear relationships (e.g., quadratic, logarithmic)",
      "",
      "## Schema Understanding and Documentation",
      "",
      "### Schema Documentation Template",
      "",
      "When documenting a dataset for team use:",
      "",
      "```markdown",
      "## Table: [schema.table_name]",
      "",
      "**Description**: [What this table represents]",
      "**Grain**: [One row per...]",
      "**Primary Key**: [column(s)]",
      "**Row Count**: [approximate, with date]",
      "**Update Frequency**: [real-time / hourly / daily / weekly]",
      "**Owner**: [team or person responsible]",
      "",
      "### Key Columns",
      "",
      "| Column | Type | Description | Example Values | Notes |",
      "|--------|------|-------------|----------------|-------|",
      '| user_id | STRING | Unique user identifier | "usr_abc123" | FK to users.id |',
      '| event_type | STRING | Type of event | "click", "view", "purchase" | 15 distinct values |',
      "| revenue | DECIMAL | Transaction revenue in USD | 29.99, 149.00 | Null for non-purchase events |",
      "| created_at | TIMESTAMP | When the event occurred | 2024-01-15 14:23:01 | Partitioned on this column |",
      "",
      "### Relationships",
      "- Joins to `users` on `user_id`",
      "- Joins to `products` on `product_id`",
      "- Parent of `event_details` (1:many on event_id)",
      "",
      "### Known Issues",
      "- [List any known data quality issues]",
      "- [Note any gotchas for analysts]",
      "",
      "### Common Query Patterns",
      "- [Typical use cases for this table]",
      "```",
      "",
      "### Schema Exploration Queries",
      "",
      "When connected to a data warehouse, use these patterns to discover schema:",
      "",
      "```sql",
      "-- List all tables in a schema (PostgreSQL)",
      "SELECT table_name, table_type",
      "FROM information_schema.tables",
      "WHERE table_schema = 'public'",
      "ORDER BY table_name;",
      "",
      "-- Column details (PostgreSQL)",
      "SELECT column_name, data_type, is_nullable, column_default",
      "FROM information_schema.columns",
      "WHERE table_name = 'my_table'",
      "ORDER BY ordinal_position;",
      "",
      "-- Table sizes (PostgreSQL)",
      "SELECT relname, pg_size_pretty(pg_total_relation_size(relid))",
      "FROM pg_catalog.pg_statio_user_tables",
      "ORDER BY pg_total_relation_size(relid) DESC;",
      "",
      "-- Row counts for all tables (general pattern)",
      "-- Run per-table: SELECT COUNT(*) FROM table_name",
      "```",
      "",
      "### Lineage and Dependencies",
      "",
      "When exploring an unfamiliar data environment:",
      "",
      '1. Start with the "output" tables (what reports or dashboards consume)',
      "2. Trace upstream: What tables feed into them?",
      "3. Identify raw/staging/mart layers",
      "4. Map the transformation chain from raw data to analytical tables",
      "5. Note where data is enriched, filtered, or aggregated",
      "",
    ].join("\n"),
  },
  "data-data-validation": {
    "SKILL.md": [
      "---",
      "name: data-data-validation",
      "description: QA an analysis before sharing with stakeholders — methodology checks, accuracy verification, and bias detection. Use when reviewing an analysis for errors, checking for survivorship bias, validating aggregation logic, or preparing documentation for reproducibility.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Data Validation Skill",
      "",
      "Pre-delivery QA checklist, common data analysis pitfalls, result sanity checking, and documentation standards for reproducibility.",
      "",
      "## Pre-Delivery QA Checklist",
      "",
      "Run through this checklist before sharing any analysis with stakeholders.",
      "",
      "### Data Quality Checks",
      "",
      "- [ ] **Source verification**: Confirmed which tables/data sources were used. Are they the right ones for this question?",
      '- [ ] **Freshness**: Data is current enough for the analysis. Noted the "as of" date.',
      "- [ ] **Completeness**: No unexpected gaps in time series or missing segments.",
      "- [ ] **Null handling**: Checked null rates in key columns. Nulls are handled appropriately (excluded, imputed, or flagged).",
      "- [ ] **Deduplication**: Confirmed no double-counting from bad joins or duplicate source records.",
      "- [ ] **Filter verification**: All WHERE clauses and filters are correct. No unintended exclusions.",
      "",
      "### Calculation Checks",
      "",
      "- [ ] **Aggregation logic**: GROUP BY includes all non-aggregated columns. Aggregation level matches the analysis grain.",
      "- [ ] **Denominator correctness**: Rate and percentage calculations use the right denominator. Denominators are non-zero.",
      "- [ ] **Date alignment**: Comparisons use the same time period length. Partial periods are excluded or noted.",
      "- [ ] **Join correctness**: JOIN types are appropriate (INNER vs LEFT). Many-to-many joins haven't inflated counts.",
      "- [ ] **Metric definitions**: Metrics match how stakeholders define them. Any deviations are noted.",
      "- [ ] **Subtotals sum**: Parts add up to the whole where expected. If they don't, explain why (e.g., overlap).",
      "",
      "### Reasonableness Checks",
      "",
      "- [ ] **Magnitude**: Numbers are in a plausible range. Revenue isn't negative. Percentages are between 0-100%.",
      "- [ ] **Trend continuity**: No unexplained jumps or drops in time series.",
      "- [ ] **Cross-reference**: Key numbers match other known sources (dashboards, previous reports, finance data).",
      "- [ ] **Order of magnitude**: Total revenue is in the right ballpark. User counts match known figures.",
      "- [ ] **Edge cases**: What happens at the boundaries? Empty segments, zero-activity periods, new entities.",
      "",
      "### Presentation Checks",
      "",
      "- [ ] **Chart accuracy**: Bar charts start at zero. Axes are labeled. Scales are consistent across panels.",
      "- [ ] **Number formatting**: Appropriate precision. Consistent currency/percentage formatting. Thousands separators where needed.",
      "- [ ] **Title clarity**: Titles state the insight, not just the metric. Date ranges are specified.",
      "- [ ] **Caveat transparency**: Known limitations and assumptions are stated explicitly.",
      "- [ ] **Reproducibility**: Someone else could recreate this analysis from the documentation provided.",
      "",
      "## Common Data Analysis Pitfalls",
      "",
      "### Join Explosion",
      "",
      "**The problem**: A many-to-many join silently multiplies rows, inflating counts and sums.",
      "",
      "**How to detect**:",
      "```sql",
      "-- Check row count before and after join",
      "SELECT COUNT(*) FROM table_a;  -- 1,000",
      "SELECT COUNT(*) FROM table_a a JOIN table_b b ON a.id = b.a_id;  -- 3,500 (uh oh)",
      "```",
      "",
      "**How to prevent**:",
      "- Always check row counts after joins",
      "- If counts increase, investigate the join relationship (is it really 1:1 or 1:many?)",
      "- Use `COUNT(DISTINCT a.id)` instead of `COUNT(*)` when counting entities through joins",
      "",
      "### Survivorship Bias",
      "",
      "**The problem**: Analyzing only entities that exist today, ignoring those that were deleted, churned, or failed.",
      "",
      "**Examples**:",
      '- Analyzing user behavior of "current users" misses churned users',
      '- Looking at "companies using our product" ignores those who evaluated and left',
      '- Studying properties of "successful" outcomes without "unsuccessful" ones',
      "",
      '**How to prevent**: Ask "who is NOT in this dataset?" before drawing conclusions.',
      "",
      "### Incomplete Period Comparison",
      "",
      "**The problem**: Comparing a partial period to a full period.",
      "",
      "**Examples**:",
      "- \"January revenue is $500K vs. December's $800K\" -- but January isn't over yet",
      '- "This week\'s signups are down" -- checked on Wednesday, comparing to a full prior week',
      "",
      "**How to prevent**: Always filter to complete periods, or compare same-day-of-month / same-number-of-days.",
      "",
      "### Denominator Shifting",
      "",
      "**The problem**: The denominator changes between periods, making rates incomparable.",
      "",
      "**Examples**:",
      '- Conversion rate improves because you changed how you count "eligible" users',
      '- Churn rate changes because the definition of "active" was updated',
      "",
      "**How to prevent**: Use consistent definitions across all compared periods. Note any definition changes.",
      "",
      "### Average of Averages",
      "",
      "**The problem**: Averaging pre-computed averages gives wrong results when group sizes differ.",
      "",
      "**Example**:",
      "- Group A: 100 users, average revenue $50",
      "- Group B: 10 users, average revenue $200",
      "- Wrong: Average of averages = ($50 + $200) / 2 = $125",
      "- Right: Weighted average = (100*$50 + 10*$200) / 110 = $63.64",
      "",
      "**How to prevent**: Always aggregate from raw data. Never average pre-aggregated averages.",
      "",
      "### Timezone Mismatches",
      "",
      "**The problem**: Different data sources use different timezones, causing misalignment.",
      "",
      "**Examples**:",
      "- Event timestamps in UTC vs. user-facing dates in local time",
      "- Daily rollups that use different cutoff times",
      "",
      "**How to prevent**: Standardize all timestamps to a single timezone (UTC recommended) before analysis. Document the timezone used.",
      "",
      "### Selection Bias in Segmentation",
      "",
      "**The problem**: Segments are defined by the outcome you're measuring, creating circular logic.",
      "",
      "**Examples**:",
      '- "Users who completed onboarding have higher retention" -- obviously, they self-selected',
      '- "Power users generate more revenue" -- they became power users BY generating revenue',
      "",
      "**How to prevent**: Define segments based on pre-treatment characteristics, not outcomes.",
      "",
      "## Result Sanity Checking",
      "",
      "### Magnitude Checks",
      "",
      'For any key number in your analysis, verify it passes the "smell test":',
      "",
      "| Metric Type | Sanity Check |",
      "|---|---|",
      "| User counts | Does this match known MAU/DAU figures? |",
      "| Revenue | Is this in the right order of magnitude vs. known ARR? |",
      "| Conversion rates | Is this between 0% and 100%? Does it match dashboard figures? |",
      "| Growth rates | Is 50%+ MoM growth realistic, or is there a data issue? |",
      "| Averages | Is the average reasonable given what you know about the distribution? |",
      "| Percentages | Do segment percentages sum to ~100%? |",
      "",
      "### Cross-Validation Techniques",
      "",
      "1. **Calculate the same metric two different ways** and verify they match",
      "2. **Spot-check individual records** -- pick a few specific entities and trace their data manually",
      "3. **Compare to known benchmarks** -- match against published dashboards, finance reports, or prior analyses",
      "4. **Reverse engineer** -- if total revenue is X, does per-user revenue times user count approximately equal X?",
      "5. **Boundary checks** -- what happens when you filter to a single day, a single user, or a single category? Are those micro-results sensible?",
      "",
      "### Red Flags That Warrant Investigation",
      "",
      "- Any metric that changed by more than 50% period-over-period without an obvious cause",
      "- Counts or sums that are exact round numbers (suggests a filter or default value issue)",
      "- Rates exactly at 0% or 100% (may indicate incomplete data)",
      "- Results that perfectly confirm the hypothesis (reality is usually messier)",
      "- Identical values across time periods or segments (suggests the query is ignoring a dimension)",
      "",
      "## Documentation Standards for Reproducibility",
      "",
      "### Analysis Documentation Template",
      "",
      "Every non-trivial analysis should include:",
      "",
      "```markdown",
      "## Analysis: [Title]",
      "",
      "### Question",
      "[The specific question being answered]",
      "",
      "### Data Sources",
      "- Table: [schema.table_name] (as of [date])",
      "- Table: [schema.other_table] (as of [date])",
      "- File: [filename] (source: [where it came from])",
      "",
      "### Definitions",
      "- [Metric A]: [Exactly how it's calculated]",
      "- [Segment X]: [Exactly how membership is determined]",
      "- [Time period]: [Start date] to [end date], [timezone]",
      "",
      "### Methodology",
      "1. [Step 1 of the analysis approach]",
      "2. [Step 2]",
      "3. [Step 3]",
      "",
      "### Assumptions and Limitations",
      "- [Assumption 1 and why it's reasonable]",
      "- [Limitation 1 and its potential impact on conclusions]",
      "",
      "### Key Findings",
      "1. [Finding 1 with supporting evidence]",
      "2. [Finding 2 with supporting evidence]",
      "",
      "### SQL Queries",
      "[All queries used, with comments]",
      "",
      "### Caveats",
      "- [Things the reader should know before acting on this]",
      "```",
      "",
      "### Code Documentation",
      "",
      "For any code (SQL, Python) that may be reused:",
      "",
      "```python",
      '"""',
      "Analysis: Monthly Cohort Retention",
      "Author: [Name]",
      "Date: [Date]",
      "Data Source: events table, users table",
      "Last Validated: [Date] -- results matched dashboard within 2%",
      "",
      "Purpose:",
      "    Calculate monthly user retention cohorts based on first activity date.",
      "",
      "Assumptions:",
      '    - "Active" means at least one event in the month',
      "    - Excludes test/internal accounts (user_type != 'internal')",
      "    - Uses UTC dates throughout",
      "",
      "Output:",
      "    Cohort retention matrix with cohort_month rows and months_since_signup columns.",
      "    Values are retention rates (0-100%).",
      '"""',
      "```",
      "",
      "### Version Control for Analyses",
      "",
      "- Save queries and code in version control (git) or a shared docs system",
      "- Note the date of the data snapshot used",
      "- If an analysis is re-run with updated data, document what changed and why",
      "- Link to prior versions of recurring analyses for trend comparison",
      "",
    ].join("\n"),
  },
  "data-data-visualization": {
    "SKILL.md": [
      "---",
      "name: data-data-visualization",
      "description: Create effective data visualizations with Python (matplotlib, seaborn, plotly). Use when building charts, choosing the right chart type for a dataset, creating publication-quality figures, or applying design principles like accessibility and color theory.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Data Visualization Skill",
      "",
      "Chart selection guidance, Python visualization code patterns, design principles, and accessibility considerations for creating effective data visualizations.",
      "",
      "## Chart Selection Guide",
      "",
      "### Choose by Data Relationship",
      "",
      "| What You're Showing | Best Chart | Alternatives |",
      "|---|---|---|",
      "| **Trend over time** | Line chart | Area chart (if showing cumulative or composition) |",
      "| **Comparison across categories** | Vertical bar chart | Horizontal bar (many categories), lollipop chart |",
      "| **Ranking** | Horizontal bar chart | Dot plot, slope chart (comparing two periods) |",
      "| **Part-to-whole composition** | Stacked bar chart | Treemap (hierarchical), waffle chart |",
      "| **Composition over time** | Stacked area chart | 100% stacked bar (for proportion focus) |",
      "| **Distribution** | Histogram | Box plot (comparing groups), violin plot, strip plot |",
      "| **Correlation (2 variables)** | Scatter plot | Bubble chart (add 3rd variable as size) |",
      "| **Correlation (many variables)** | Heatmap (correlation matrix) | Pair plot |",
      "| **Geographic patterns** | Choropleth map | Bubble map, hex map |",
      "| **Flow / process** | Sankey diagram | Funnel chart (sequential stages) |",
      "| **Relationship network** | Network graph | Chord diagram |",
      "| **Performance vs. target** | Bullet chart | Gauge (single KPI only) |",
      "| **Multiple KPIs at once** | Small multiples | Dashboard with separate charts |",
      "",
      "### When NOT to Use Certain Charts",
      "",
      "- **Pie charts**: Avoid unless <6 categories and exact proportions matter less than rough comparison. Humans are bad at comparing angles. Use bar charts instead.",
      "- **3D charts**: Never. They distort perception and add no information.",
      "- **Dual-axis charts**: Use cautiously. They can mislead by implying correlation. Clearly label both axes if used.",
      "- **Stacked bar (many categories)**: Hard to compare middle segments. Use small multiples or grouped bars instead.",
      "- **Donut charts**: Slightly better than pie charts but same fundamental issues. Use for single KPI display at most.",
      "",
      "## Python Visualization Code Patterns",
      "",
      "### Setup and Style",
      "",
      "```python",
      "import matplotlib.pyplot as plt",
      "import matplotlib.ticker as mticker",
      "import seaborn as sns",
      "import pandas as pd",
      "import numpy as np",
      "",
      "# Professional style setup",
      "plt.style.use('seaborn-v0_8-whitegrid')",
      "plt.rcParams.update({",
      "    'figure.figsize': (10, 6),",
      "    'figure.dpi': 150,",
      "    'font.size': 11,",
      "    'axes.titlesize': 14,",
      "    'axes.titleweight': 'bold',",
      "    'axes.labelsize': 11,",
      "    'xtick.labelsize': 10,",
      "    'ytick.labelsize': 10,",
      "    'legend.fontsize': 10,",
      "    'figure.titlesize': 16,",
      "})",
      "",
      "# Colorblind-friendly palettes",
      "PALETTE_CATEGORICAL = ['#4C72B0', '#DD8452', '#55A868', '#C44E52', '#8172B3', '#937860']",
      "PALETTE_SEQUENTIAL = 'YlOrRd'",
      "PALETTE_DIVERGING = 'RdBu_r'",
      "```",
      "",
      "### Line Chart (Time Series)",
      "",
      "```python",
      "fig, ax = plt.subplots(figsize=(10, 6))",
      "",
      "for label, group in df.groupby('category'):",
      "    ax.plot(group['date'], group['value'], label=label, linewidth=2)",
      "",
      "ax.set_title('Metric Trend by Category', fontweight='bold')",
      "ax.set_xlabel('Date')",
      "ax.set_ylabel('Value')",
      "ax.legend(loc='upper left', frameon=True)",
      "ax.spines['top'].set_visible(False)",
      "ax.spines['right'].set_visible(False)",
      "",
      "# Format dates on x-axis",
      "fig.autofmt_xdate()",
      "",
      "plt.tight_layout()",
      "plt.savefig('trend_chart.png', dpi=150, bbox_inches='tight')",
      "```",
      "",
      "### Bar Chart (Comparison)",
      "",
      "```python",
      "fig, ax = plt.subplots(figsize=(10, 6))",
      "",
      "# Sort by value for easy reading",
      "df_sorted = df.sort_values('metric', ascending=True)",
      "",
      "bars = ax.barh(df_sorted['category'], df_sorted['metric'], color=PALETTE_CATEGORICAL[0])",
      "",
      "# Add value labels",
      "for bar in bars:",
      "    width = bar.get_width()",
      "    ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,",
      "            f'{width:,.0f}', ha='left', va='center', fontsize=10)",
      "",
      "ax.set_title('Metric by Category (Ranked)', fontweight='bold')",
      "ax.set_xlabel('Metric Value')",
      "ax.spines['top'].set_visible(False)",
      "ax.spines['right'].set_visible(False)",
      "",
      "plt.tight_layout()",
      "plt.savefig('bar_chart.png', dpi=150, bbox_inches='tight')",
      "```",
      "",
      "### Histogram (Distribution)",
      "",
      "```python",
      "fig, ax = plt.subplots(figsize=(10, 6))",
      "",
      "ax.hist(df['value'], bins=30, color=PALETTE_CATEGORICAL[0], edgecolor='white', alpha=0.8)",
      "",
      "# Add mean and median lines",
      "mean_val = df['value'].mean()",
      "median_val = df['value'].median()",
      "ax.axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:,.1f}')",
      "ax.axvline(median_val, color='green', linestyle='--', linewidth=1.5, label=f'Median: {median_val:,.1f}')",
      "",
      "ax.set_title('Distribution of Values', fontweight='bold')",
      "ax.set_xlabel('Value')",
      "ax.set_ylabel('Frequency')",
      "ax.legend()",
      "ax.spines['top'].set_visible(False)",
      "ax.spines['right'].set_visible(False)",
      "",
      "plt.tight_layout()",
      "plt.savefig('histogram.png', dpi=150, bbox_inches='tight')",
      "```",
      "",
      "### Heatmap",
      "",
      "```python",
      "fig, ax = plt.subplots(figsize=(10, 8))",
      "",
      "# Pivot data for heatmap format",
      "pivot = df.pivot_table(index='row_dim', columns='col_dim', values='metric', aggfunc='sum')",
      "",
      "sns.heatmap(pivot, annot=True, fmt=',.0f', cmap='YlOrRd',",
      "            linewidths=0.5, ax=ax, cbar_kws={'label': 'Metric Value'})",
      "",
      "ax.set_title('Metric by Row Dimension and Column Dimension', fontweight='bold')",
      "ax.set_xlabel('Column Dimension')",
      "ax.set_ylabel('Row Dimension')",
      "",
      "plt.tight_layout()",
      "plt.savefig('heatmap.png', dpi=150, bbox_inches='tight')",
      "```",
      "",
      "### Small Multiples",
      "",
      "```python",
      "categories = df['category'].unique()",
      "n_cats = len(categories)",
      "n_cols = min(3, n_cats)",
      "n_rows = (n_cats + n_cols - 1) // n_cols",
      "",
      "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows), sharex=True, sharey=True)",
      "axes = axes.flatten() if n_cats > 1 else [axes]",
      "",
      "for i, cat in enumerate(categories):",
      "    ax = axes[i]",
      "    subset = df[df['category'] == cat]",
      "    ax.plot(subset['date'], subset['value'], color=PALETTE_CATEGORICAL[i % len(PALETTE_CATEGORICAL)])",
      "    ax.set_title(cat, fontsize=12)",
      "    ax.spines['top'].set_visible(False)",
      "    ax.spines['right'].set_visible(False)",
      "",
      "# Hide empty subplots",
      "for j in range(i+1, len(axes)):",
      "    axes[j].set_visible(False)",
      "",
      "fig.suptitle('Trends by Category', fontsize=14, fontweight='bold', y=1.02)",
      "plt.tight_layout()",
      "plt.savefig('small_multiples.png', dpi=150, bbox_inches='tight')",
      "```",
      "",
      "### Number Formatting Helpers",
      "",
      "```python",
      "def format_number(val, format_type='number'):",
      '    """Format numbers for chart labels."""',
      "    if format_type == 'currency':",
      "        if abs(val) >= 1e9:",
      "            return f'${val/1e9:.1f}B'",
      "        elif abs(val) >= 1e6:",
      "            return f'${val/1e6:.1f}M'",
      "        elif abs(val) >= 1e3:",
      "            return f'${val/1e3:.1f}K'",
      "        else:",
      "            return f'${val:,.0f}'",
      "    elif format_type == 'percent':",
      "        return f'{val:.1f}%'",
      "    elif format_type == 'number':",
      "        if abs(val) >= 1e9:",
      "            return f'{val/1e9:.1f}B'",
      "        elif abs(val) >= 1e6:",
      "            return f'{val/1e6:.1f}M'",
      "        elif abs(val) >= 1e3:",
      "            return f'{val/1e3:.1f}K'",
      "        else:",
      "            return f'{val:,.0f}'",
      "    return str(val)",
      "",
      "# Usage with axis formatter",
      "ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, p: format_number(x, 'currency')))",
      "```",
      "",
      "### Interactive Charts with Plotly",
      "",
      "```python",
      "import plotly.express as px",
      "import plotly.graph_objects as go",
      "",
      "# Simple interactive line chart",
      "fig = px.line(df, x='date', y='value', color='category',",
      "              title='Interactive Metric Trend',",
      "              labels={'value': 'Metric Value', 'date': 'Date'})",
      "fig.update_layout(hovermode='x unified')",
      "fig.write_html('interactive_chart.html')",
      "fig.show()",
      "",
      "# Interactive scatter with hover data",
      "fig = px.scatter(df, x='metric_a', y='metric_b', color='category',",
      "                 size='size_metric', hover_data=['name', 'detail_field'],",
      "                 title='Correlation Analysis')",
      "fig.show()",
      "```",
      "",
      "## Design Principles",
      "",
      "### Color",
      "",
      "- **Use color purposefully**: Color should encode data, not decorate",
      "- **Highlight the story**: Use a bright accent color for the key insight; grey everything else",
      "- **Sequential data**: Use a single-hue gradient (light to dark) for ordered values",
      "- **Diverging data**: Use a two-hue gradient with neutral midpoint for data with a meaningful center",
      "- **Categorical data**: Use distinct hues, maximum 6-8 before it gets confusing",
      "- **Avoid red/green only**: 8% of men are red-green colorblind. Use blue/orange as primary pair",
      "",
      "### Typography",
      "",
      '- **Title states the insight**: "Revenue grew 23% YoY" beats "Revenue by Month"',
      "- **Subtitle adds context**: Date range, filters applied, data source",
      "- **Axis labels are readable**: Never rotated 90 degrees if avoidable. Shorten or wrap instead",
      "- **Data labels add precision**: Use on key points, not every single bar",
      "- **Annotation highlights**: Call out specific points with text annotations",
      "",
      "### Layout",
      "",
      "- **Reduce chart junk**: Remove gridlines, borders, backgrounds that don't carry information",
      "- **Sort meaningfully**: Categories sorted by value (not alphabetically) unless there's a natural order (months, stages)",
      "- **Appropriate aspect ratio**: Time series wider than tall (3:1 to 2:1); comparisons can be squarer",
      "- **White space is good**: Don't cram charts together. Give each visualization room to breathe",
      "",
      "### Accuracy",
      "",
      "- **Bar charts start at zero**: Always. A bar from 95 to 100 exaggerates a 5% difference",
      "- **Line charts can have non-zero baselines**: When the range of variation is meaningful",
      "- **Consistent scales across panels**: When comparing multiple charts, use the same axis range",
      "- **Show uncertainty**: Error bars, confidence intervals, or ranges when data is uncertain",
      "- **Label your axes**: Never make the reader guess what the numbers mean",
      "",
      "## Accessibility Considerations",
      "",
      "### Color Blindness",
      "",
      "- Never rely on color alone to distinguish data series",
      "- Add pattern fills, different line styles (solid, dashed, dotted), or direct labels",
      "- Test with a colorblind simulator (e.g., Coblis, Sim Daltonism)",
      '- Use the colorblind-friendly palette: `sns.color_palette("colorblind")`',
      "",
      "### Screen Readers",
      "",
      "- Include alt text describing the chart's key finding",
      "- Provide a data table alternative alongside the visualization",
      "- Use semantic titles and labels",
      "",
      "### General Accessibility",
      "",
      "- Sufficient contrast between data elements and background",
      "- Text size minimum 10pt for labels, 12pt for titles",
      "- Avoid conveying information only through spatial position (add labels)",
      "- Consider printing: does the chart work in black and white?",
      "",
      "### Accessibility Checklist",
      "",
      "Before sharing a visualization:",
      "- [ ] Chart works without color (patterns, labels, or line styles differentiate series)",
      "- [ ] Text is readable at standard zoom level",
      "- [ ] Title describes the insight, not just the data",
      "- [ ] Axes are labeled with units",
      "- [ ] Legend is clear and positioned without obscuring data",
      "- [ ] Data source and date range are noted",
      "",
    ].join("\n"),
  },
  "data-explore-data": {
    "SKILL.md": [
      "---",
      "name: data-explore-data",
      "description: Profile and explore a dataset to understand its shape, quality, and patterns",
      "metadata:",
      "  plugin: data",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <table or file>",
      "---",
      "",
      "# /explore-data - Profile and Explore a Dataset",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate a comprehensive data profile for a table or uploaded file. Understand its shape, quality, and patterns before diving into analysis.",
      "",
      "## Usage",
      "",
      "```",
      "/explore-data <table_name or file>",
      "```",
      "",
      "## Workflow",
      "",
      "### 1. Access the Data",
      "",
      "**If a data warehouse MCP server is connected:**",
      "",
      "1. Resolve the table name (handle schema prefixes, suggest matches if ambiguous)",
      "2. Query table metadata: column names, types, descriptions if available",
      "3. Run profiling queries against the live data",
      "",
      "**If a file is provided (CSV, Excel, Parquet, JSON):**",
      "",
      "1. Read the file and load into a working dataset",
      "2. Infer column types from the data",
      "",
      "**If neither:**",
      "",
      "1. Ask the user to provide a table name (with their warehouse connected) or upload a file",
      "2. If they describe a table schema, provide guidance on what profiling queries to run",
      "",
      "### 2. Generate Data Profile",
      "",
      "Run the following profiling checks:",
      "",
      "**Table-level metrics:**",
      "- Total row count",
      "- Column count and types breakdown",
      "- Approximate table size (if available from metadata)",
      "- Date range coverage (min/max of date columns)",
      "",
      "**Column-level metrics for each column:**",
      "- Data type (and whether it matches expected type)",
      "- Null count and null rate (%)",
      "- Distinct count and cardinality (distinct / total)",
      "- For numeric columns: min, max, mean, median, stddev, percentiles (p25, p50, p75, p95, p99)",
      "- For string columns: min/max length, most common values (top 10), empty string count",
      "- For date/timestamp columns: min, max, distribution by time period",
      "- For boolean columns: true/false/null distribution",
      "",
      "**Present the profile as a clean summary table**, grouped by column type (dimensions, metrics, dates, IDs).",
      "",
      "### 3. Identify Data Quality Issues",
      "",
      "Flag potential problems:",
      "",
      "- **High null rates**: Columns with >5% nulls (warn), >20% nulls (alert)",
      '- **Low cardinality surprises**: Columns that should be high-cardinality but aren\'t (e.g., a "user_id" with only 50 distinct values)',
      "- **High cardinality surprises**: Columns that should be categorical but have too many distinct values",
      '- **Suspicious values**: Negative amounts where only positive expected, future dates in historical data, obviously placeholder values (e.g., "N/A", "TBD", "test", "999999")',
      "- **Duplicate detection**: Check if there's a natural key and whether it has duplicates",
      "- **Distribution skew**: Extremely skewed numeric distributions that could affect averages",
      "- **Encoding issues**: Mixed case in categorical fields, trailing whitespace, inconsistent formats",
      "",
      "### 4. Suggest Interesting Dimensions and Metrics",
      "",
      "Based on the column profile, recommend:",
      "",
      "- **Best dimension columns** for slicing data (categorical columns with reasonable cardinality, 3-50 values)",
      "- **Key metric columns** for measurement (numeric columns with meaningful distributions)",
      "- **Time columns** suitable for trend analysis",
      "- **Natural groupings** or hierarchies apparent in the data",
      "- **Potential join keys** linking to other tables (ID columns, foreign keys)",
      "",
      "### 5. Recommend Follow-Up Analyses",
      "",
      "Suggest 3-5 specific analyses the user could run next:",
      "",
      '- "Trend analysis on [metric] by [time_column] grouped by [dimension]"',
      '- "Distribution deep-dive on [skewed_column] to understand outliers"',
      '- "Data quality investigation on [problematic_column]"',
      '- "Correlation analysis between [metric_a] and [metric_b]"',
      '- "Cohort analysis using [date_column] and [status_column]"',
      "",
      "## Output Format",
      "",
      "```",
      "## Data Profile: [table_name]",
      "",
      "### Overview",
      "- Rows: 2,340,891",
      "- Columns: 23 (8 dimensions, 6 metrics, 4 dates, 5 IDs)",
      "- Date range: 2021-03-15 to 2024-01-22",
      "",
      "### Column Details",
      "[summary table]",
      "",
      "### Data Quality Issues",
      "[flagged issues with severity]",
      "",
      "### Recommended Explorations",
      "[numbered list of suggested follow-up analyses]",
      "```",
      "",
      "## Tips",
      "",
      "- For very large tables (100M+ rows), profiling queries use sampling by default -- mention if you need exact counts",
      "- If exploring a new dataset for the first time, this command gives you the lay of the land before writing specific queries",
      "- The quality flags are heuristic -- not every flag is a real problem, but each is worth a quick look",
      "",
    ].join("\n"),
  },
  "data-interactive-dashboard-builder": {
    "SKILL.md": [
      "---",
      "name: data-interactive-dashboard-builder",
      "description: Build self-contained interactive HTML dashboards with Chart.js, dropdown filters, and professional styling. Use when creating dashboards, building interactive reports, or generating shareable HTML files with charts and filters that work without a server.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Interactive Dashboard Builder Skill",
      "",
      "Patterns and techniques for building self-contained HTML/JS dashboards with Chart.js, filters, interactivity, and professional styling.",
      "",
      "## HTML/JS Dashboard Patterns",
      "",
      "### Base Template",
      "",
      "Every dashboard follows this structure:",
      "",
      "```html",
      "<!DOCTYPE html>",
      '<html lang="en">',
      "<head>",
      '    <meta charset="UTF-8">',
      '    <meta name="viewport" content="width=device-width, initial-scale=1.0">',
      "    <title>Dashboard Title</title>",
      '    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.5.1" integrity="sha384-jb8JQMbMoBUzgWatfe6COACi2ljcDdZQ2OxczGA3bGNeWe+6DChMTBJemed7ZnvJ" crossorigin="anonymous"></script>',
      '    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@3.0.0" integrity="sha384-cVMg8E3QFwTvGCDuK+ET4PD341jF3W8nO1auiXfuZNQkzbUUiBGLsIQUE+b1mxws" crossorigin="anonymous"></script>',
      "    <style>",
      "        /* Dashboard styles go here */",
      "    </style>",
      "</head>",
      "<body>",
      '    <div class="dashboard-container">',
      '        <header class="dashboard-header">',
      "            <h1>Dashboard Title</h1>",
      '            <div class="filters">',
      "                <!-- Filter controls -->",
      "            </div>",
      "        </header>",
      "",
      '        <section class="kpi-row">',
      "            <!-- KPI cards -->",
      "        </section>",
      "",
      '        <section class="chart-row">',
      "            <!-- Chart containers -->",
      "        </section>",
      "",
      '        <section class="table-section">',
      "            <!-- Data table -->",
      "        </section>",
      "",
      '        <footer class="dashboard-footer">',
      '            <span>Data as of: <span id="data-date"></span></span>',
      "        </footer>",
      "    </div>",
      "",
      "    <script>",
      "        // Embedded data",
      "        const DATA = [];",
      "",
      "        // Dashboard logic",
      "        class Dashboard {",
      "            constructor(data) {",
      "                this.rawData = data;",
      "                this.filteredData = data;",
      "                this.charts = {};",
      "                this.init();",
      "            }",
      "",
      "            init() {",
      "                this.setupFilters();",
      "                this.renderKPIs();",
      "                this.renderCharts();",
      "                this.renderTable();",
      "            }",
      "",
      "            applyFilters() {",
      "                // Filter logic",
      "                this.filteredData = this.rawData.filter(row => {",
      "                    // Apply each active filter",
      "                    return true; // placeholder",
      "                });",
      "                this.renderKPIs();",
      "                this.updateCharts();",
      "                this.renderTable();",
      "            }",
      "",
      "            // ... methods for each section",
      "        }",
      "",
      "        const dashboard = new Dashboard(DATA);",
      "    </script>",
      "</body>",
      "</html>",
      "```",
      "",
      "### KPI Card Pattern",
      "",
      "```html",
      '<div class="kpi-card">',
      '    <div class="kpi-label">Total Revenue</div>',
      '    <div class="kpi-value" id="kpi-revenue">$0</div>',
      '    <div class="kpi-change positive" id="kpi-revenue-change">+0%</div>',
      "</div>",
      "```",
      "",
      "```javascript",
      "function renderKPI(elementId, value, previousValue, format = 'number') {",
      "    const el = document.getElementById(elementId);",
      "    const changeEl = document.getElementById(elementId + '-change');",
      "",
      "    // Format the value",
      "    el.textContent = formatValue(value, format);",
      "",
      "    // Calculate and display change",
      "    if (previousValue && previousValue !== 0) {",
      "        const pctChange = ((value - previousValue) / previousValue) * 100;",
      "        const sign = pctChange >= 0 ? '+' : '';",
      "        changeEl.textContent = `${sign}${pctChange.toFixed(1)}% vs prior period`;",
      "        changeEl.className = `kpi-change ${pctChange >= 0 ? 'positive' : 'negative'}`;",
      "    }",
      "}",
      "",
      "function formatValue(value, format) {",
      "    switch (format) {",
      "        case 'currency':",
      "            if (value >= 1e6) return `$${(value / 1e6).toFixed(1)}M`;",
      "            if (value >= 1e3) return `$${(value / 1e3).toFixed(1)}K`;",
      "            return `$${value.toFixed(0)}`;",
      "        case 'percent':",
      "            return `${value.toFixed(1)}%`;",
      "        case 'number':",
      "            if (value >= 1e6) return `${(value / 1e6).toFixed(1)}M`;",
      "            if (value >= 1e3) return `${(value / 1e3).toFixed(1)}K`;",
      "            return value.toLocaleString();",
      "        default:",
      "            return value.toString();",
      "    }",
      "}",
      "```",
      "",
      "### Chart Container Pattern",
      "",
      "```html",
      '<div class="chart-container">',
      '    <h3 class="chart-title">Monthly Revenue Trend</h3>',
      '    <canvas id="revenue-chart"></canvas>',
      "</div>",
      "```",
      "",
      "## Chart.js Integration",
      "",
      "### Line Chart",
      "",
      "```javascript",
      "function createLineChart(canvasId, labels, datasets) {",
      "    const ctx = document.getElementById(canvasId).getContext('2d');",
      "    return new Chart(ctx, {",
      "        type: 'line',",
      "        data: {",
      "            labels: labels,",
      "            datasets: datasets.map((ds, i) => ({",
      "                label: ds.label,",
      "                data: ds.data,",
      "                borderColor: COLORS[i % COLORS.length],",
      "                backgroundColor: COLORS[i % COLORS.length] + '20',",
      "                borderWidth: 2,",
      "                fill: ds.fill || false,",
      "                tension: 0.3,",
      "                pointRadius: 3,",
      "                pointHoverRadius: 6,",
      "            }))",
      "        },",
      "        options: {",
      "            responsive: true,",
      "            maintainAspectRatio: false,",
      "            interaction: {",
      "                mode: 'index',",
      "                intersect: false,",
      "            },",
      "            plugins: {",
      "                legend: {",
      "                    position: 'top',",
      "                    labels: { usePointStyle: true, padding: 20 }",
      "                },",
      "                tooltip: {",
      "                    callbacks: {",
      "                        label: function(context) {",
      "                            return `${context.dataset.label}: ${formatValue(context.parsed.y, 'currency')}`;",
      "                        }",
      "                    }",
      "                }",
      "            },",
      "            scales: {",
      "                x: {",
      "                    grid: { display: false }",
      "                },",
      "                y: {",
      "                    beginAtZero: true,",
      "                    ticks: {",
      "                        callback: function(value) {",
      "                            return formatValue(value, 'currency');",
      "                        }",
      "                    }",
      "                }",
      "            }",
      "        }",
      "    });",
      "}",
      "```",
      "",
      "### Bar Chart",
      "",
      "```javascript",
      "function createBarChart(canvasId, labels, data, options = {}) {",
      "    const ctx = document.getElementById(canvasId).getContext('2d');",
      "    const isHorizontal = options.horizontal || labels.length > 8;",
      "",
      "    return new Chart(ctx, {",
      "        type: 'bar',",
      "        data: {",
      "            labels: labels,",
      "            datasets: [{",
      "                label: options.label || 'Value',",
      "                data: data,",
      "                backgroundColor: options.colors || COLORS.map(c => c + 'CC'),",
      "                borderColor: options.colors || COLORS,",
      "                borderWidth: 1,",
      "                borderRadius: 4,",
      "            }]",
      "        },",
      "        options: {",
      "            responsive: true,",
      "            maintainAspectRatio: false,",
      "            indexAxis: isHorizontal ? 'y' : 'x',",
      "            plugins: {",
      "                legend: { display: false },",
      "                tooltip: {",
      "                    callbacks: {",
      "                        label: function(context) {",
      "                            return formatValue(context.parsed[isHorizontal ? 'x' : 'y'], options.format || 'number');",
      "                        }",
      "                    }",
      "                }",
      "            },",
      "            scales: {",
      "                x: {",
      "                    beginAtZero: true,",
      "                    grid: { display: isHorizontal },",
      "                    ticks: isHorizontal ? {",
      "                        callback: function(value) {",
      "                            return formatValue(value, options.format || 'number');",
      "                        }",
      "                    } : {}",
      "                },",
      "                y: {",
      "                    beginAtZero: !isHorizontal,",
      "                    grid: { display: !isHorizontal },",
      "                    ticks: !isHorizontal ? {",
      "                        callback: function(value) {",
      "                            return formatValue(value, options.format || 'number');",
      "                        }",
      "                    } : {}",
      "                }",
      "            }",
      "        }",
      "    });",
      "}",
      "```",
      "",
      "### Doughnut Chart",
      "",
      "```javascript",
      "function createDoughnutChart(canvasId, labels, data) {",
      "    const ctx = document.getElementById(canvasId).getContext('2d');",
      "    return new Chart(ctx, {",
      "        type: 'doughnut',",
      "        data: {",
      "            labels: labels,",
      "            datasets: [{",
      "                data: data,",
      "                backgroundColor: COLORS.map(c => c + 'CC'),",
      "                borderColor: '#ffffff',",
      "                borderWidth: 2,",
      "            }]",
      "        },",
      "        options: {",
      "            responsive: true,",
      "            maintainAspectRatio: false,",
      "            cutout: '60%',",
      "            plugins: {",
      "                legend: {",
      "                    position: 'right',",
      "                    labels: { usePointStyle: true, padding: 15 }",
      "                },",
      "                tooltip: {",
      "                    callbacks: {",
      "                        label: function(context) {",
      "                            const total = context.dataset.data.reduce((a, b) => a + b, 0);",
      "                            const pct = ((context.parsed / total) * 100).toFixed(1);",
      "                            return `${context.label}: ${formatValue(context.parsed, 'number')} (${pct}%)`;",
      "                        }",
      "                    }",
      "                }",
      "            }",
      "        }",
      "    });",
      "}",
      "```",
      "",
      "### Updating Charts on Filter Change",
      "",
      "```javascript",
      "function updateChart(chart, newLabels, newData) {",
      "    chart.data.labels = newLabels;",
      "",
      "    if (Array.isArray(newData[0])) {",
      "        // Multiple datasets",
      "        newData.forEach((data, i) => {",
      "            chart.data.datasets[i].data = data;",
      "        });",
      "    } else {",
      "        chart.data.datasets[0].data = newData;",
      "    }",
      "",
      "    chart.update('none'); // 'none' disables animation for instant update",
      "}",
      "```",
      "",
      "## Filter and Interactivity Implementation",
      "",
      "### Dropdown Filter",
      "",
      "```html",
      '<div class="filter-group">',
      '    <label for="filter-region">Region</label>',
      '    <select id="filter-region" onchange="dashboard.applyFilters()">',
      '        <option value="all">All Regions</option>',
      "    </select>",
      "</div>",
      "```",
      "",
      "```javascript",
      "function populateFilter(selectId, data, field) {",
      "    const select = document.getElementById(selectId);",
      "    const values = [...new Set(data.map(d => d[field]))].sort();",
      "",
      '    // Keep the "All" option, add unique values',
      "    values.forEach(val => {",
      "        const option = document.createElement('option');",
      "        option.value = val;",
      "        option.textContent = val;",
      "        select.appendChild(option);",
      "    });",
      "}",
      "",
      "function getFilterValue(selectId) {",
      "    const val = document.getElementById(selectId).value;",
      "    return val === 'all' ? null : val;",
      "}",
      "```",
      "",
      "### Date Range Filter",
      "",
      "```html",
      '<div class="filter-group">',
      "    <label>Date Range</label>",
      '    <input type="date" id="filter-date-start" onchange="dashboard.applyFilters()">',
      "    <span>to</span>",
      '    <input type="date" id="filter-date-end" onchange="dashboard.applyFilters()">',
      "</div>",
      "```",
      "",
      "```javascript",
      "function filterByDateRange(data, dateField, startDate, endDate) {",
      "    return data.filter(row => {",
      "        const rowDate = new Date(row[dateField]);",
      "        if (startDate && rowDate < new Date(startDate)) return false;",
      "        if (endDate && rowDate > new Date(endDate)) return false;",
      "        return true;",
      "    });",
      "}",
      "```",
      "",
      "### Combined Filter Logic",
      "",
      "```javascript",
      "applyFilters() {",
      "    const region = getFilterValue('filter-region');",
      "    const category = getFilterValue('filter-category');",
      "    const startDate = document.getElementById('filter-date-start').value;",
      "    const endDate = document.getElementById('filter-date-end').value;",
      "",
      "    this.filteredData = this.rawData.filter(row => {",
      "        if (region && row.region !== region) return false;",
      "        if (category && row.category !== category) return false;",
      "        if (startDate && row.date < startDate) return false;",
      "        if (endDate && row.date > endDate) return false;",
      "        return true;",
      "    });",
      "",
      "    this.renderKPIs();",
      "    this.updateCharts();",
      "    this.renderTable();",
      "}",
      "```",
      "",
      "### Sortable Table",
      "",
      "```javascript",
      "function renderTable(containerId, data, columns) {",
      "    const container = document.getElementById(containerId);",
      "    let sortCol = null;",
      "    let sortDir = 'desc';",
      "",
      "    function render(sortedData) {",
      "        let html = '<table class=\"data-table\">';",
      "",
      "        // Header",
      "        html += '<thead><tr>';",
      "        columns.forEach(col => {",
      "            const arrow = sortCol === col.field",
      "                ? (sortDir === 'asc' ? ' ▲' : ' ▼')",
      "                : '';",
      '            html += `<th onclick="sortTable(\'${col.field}\')" style="cursor:pointer">${col.label}${arrow}</th>`;',
      "        });",
      "        html += '</tr></thead>';",
      "",
      "        // Body",
      "        html += '<tbody>';",
      "        sortedData.forEach(row => {",
      "            html += '<tr>';",
      "            columns.forEach(col => {",
      "                const value = col.format ? formatValue(row[col.field], col.format) : row[col.field];",
      "                html += `<td>${value}</td>`;",
      "            });",
      "            html += '</tr>';",
      "        });",
      "        html += '</tbody></table>';",
      "",
      "        container.innerHTML = html;",
      "    }",
      "",
      "    window.sortTable = function(field) {",
      "        if (sortCol === field) {",
      "            sortDir = sortDir === 'asc' ? 'desc' : 'asc';",
      "        } else {",
      "            sortCol = field;",
      "            sortDir = 'desc';",
      "        }",
      "        const sorted = [...data].sort((a, b) => {",
      "            const aVal = a[field], bVal = b[field];",
      "            const cmp = aVal < bVal ? -1 : aVal > bVal ? 1 : 0;",
      "            return sortDir === 'asc' ? cmp : -cmp;",
      "        });",
      "        render(sorted);",
      "    };",
      "",
      "    render(data);",
      "}",
      "```",
      "",
      "## CSS Styling for Dashboards",
      "",
      "### Color System",
      "",
      "```css",
      ":root {",
      "    /* Background layers */",
      "    --bg-primary: #f8f9fa;",
      "    --bg-card: #ffffff;",
      "    --bg-header: #1a1a2e;",
      "",
      "    /* Text */",
      "    --text-primary: #212529;",
      "    --text-secondary: #6c757d;",
      "    --text-on-dark: #ffffff;",
      "",
      "    /* Accent colors for data */",
      "    --color-1: #4C72B0;",
      "    --color-2: #DD8452;",
      "    --color-3: #55A868;",
      "    --color-4: #C44E52;",
      "    --color-5: #8172B3;",
      "    --color-6: #937860;",
      "",
      "    /* Status colors */",
      "    --positive: #28a745;",
      "    --negative: #dc3545;",
      "    --neutral: #6c757d;",
      "",
      "    /* Spacing */",
      "    --gap: 16px;",
      "    --radius: 8px;",
      "}",
      "```",
      "",
      "### Layout",
      "",
      "```css",
      "* {",
      "    margin: 0;",
      "    padding: 0;",
      "    box-sizing: border-box;",
      "}",
      "",
      "body {",
      "    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;",
      "    background: var(--bg-primary);",
      "    color: var(--text-primary);",
      "    line-height: 1.5;",
      "}",
      "",
      ".dashboard-container {",
      "    max-width: 1400px;",
      "    margin: 0 auto;",
      "    padding: var(--gap);",
      "}",
      "",
      ".dashboard-header {",
      "    background: var(--bg-header);",
      "    color: var(--text-on-dark);",
      "    padding: 20px 24px;",
      "    border-radius: var(--radius);",
      "    margin-bottom: var(--gap);",
      "    display: flex;",
      "    justify-content: space-between;",
      "    align-items: center;",
      "    flex-wrap: wrap;",
      "    gap: 12px;",
      "}",
      "",
      ".dashboard-header h1 {",
      "    font-size: 20px;",
      "    font-weight: 600;",
      "}",
      "```",
      "",
      "### KPI Cards",
      "",
      "```css",
      ".kpi-row {",
      "    display: grid;",
      "    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));",
      "    gap: var(--gap);",
      "    margin-bottom: var(--gap);",
      "}",
      "",
      ".kpi-card {",
      "    background: var(--bg-card);",
      "    border-radius: var(--radius);",
      "    padding: 20px 24px;",
      "    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);",
      "}",
      "",
      ".kpi-label {",
      "    font-size: 13px;",
      "    color: var(--text-secondary);",
      "    text-transform: uppercase;",
      "    letter-spacing: 0.5px;",
      "    margin-bottom: 4px;",
      "}",
      "",
      ".kpi-value {",
      "    font-size: 28px;",
      "    font-weight: 700;",
      "    color: var(--text-primary);",
      "    margin-bottom: 4px;",
      "}",
      "",
      ".kpi-change {",
      "    font-size: 13px;",
      "    font-weight: 500;",
      "}",
      "",
      ".kpi-change.positive { color: var(--positive); }",
      ".kpi-change.negative { color: var(--negative); }",
      "```",
      "",
      "### Chart Containers",
      "",
      "```css",
      ".chart-row {",
      "    display: grid;",
      "    grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));",
      "    gap: var(--gap);",
      "    margin-bottom: var(--gap);",
      "}",
      "",
      ".chart-container {",
      "    background: var(--bg-card);",
      "    border-radius: var(--radius);",
      "    padding: 20px 24px;",
      "    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);",
      "}",
      "",
      ".chart-container h3 {",
      "    font-size: 14px;",
      "    font-weight: 600;",
      "    color: var(--text-primary);",
      "    margin-bottom: 16px;",
      "}",
      "",
      ".chart-container canvas {",
      "    max-height: 300px;",
      "}",
      "```",
      "",
      "### Filters",
      "",
      "```css",
      ".filters {",
      "    display: flex;",
      "    gap: 12px;",
      "    align-items: center;",
      "    flex-wrap: wrap;",
      "}",
      "",
      ".filter-group {",
      "    display: flex;",
      "    align-items: center;",
      "    gap: 6px;",
      "}",
      "",
      ".filter-group label {",
      "    font-size: 12px;",
      "    color: rgba(255, 255, 255, 0.7);",
      "}",
      "",
      ".filter-group select,",
      '.filter-group input[type="date"] {',
      "    padding: 6px 10px;",
      "    border: 1px solid rgba(255, 255, 255, 0.2);",
      "    border-radius: 4px;",
      "    background: rgba(255, 255, 255, 0.1);",
      "    color: var(--text-on-dark);",
      "    font-size: 13px;",
      "}",
      "",
      ".filter-group select option {",
      "    background: var(--bg-header);",
      "    color: var(--text-on-dark);",
      "}",
      "```",
      "",
      "### Data Table",
      "",
      "```css",
      ".table-section {",
      "    background: var(--bg-card);",
      "    border-radius: var(--radius);",
      "    padding: 20px 24px;",
      "    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);",
      "    overflow-x: auto;",
      "}",
      "",
      ".data-table {",
      "    width: 100%;",
      "    border-collapse: collapse;",
      "    font-size: 13px;",
      "}",
      "",
      ".data-table thead th {",
      "    text-align: left;",
      "    padding: 10px 12px;",
      "    border-bottom: 2px solid #dee2e6;",
      "    color: var(--text-secondary);",
      "    font-weight: 600;",
      "    font-size: 12px;",
      "    text-transform: uppercase;",
      "    letter-spacing: 0.5px;",
      "    white-space: nowrap;",
      "    user-select: none;",
      "}",
      "",
      ".data-table thead th:hover {",
      "    color: var(--text-primary);",
      "    background: #f8f9fa;",
      "}",
      "",
      ".data-table tbody td {",
      "    padding: 10px 12px;",
      "    border-bottom: 1px solid #f0f0f0;",
      "}",
      "",
      ".data-table tbody tr:hover {",
      "    background: #f8f9fa;",
      "}",
      "",
      ".data-table tbody tr:last-child td {",
      "    border-bottom: none;",
      "}",
      "```",
      "",
      "### Responsive Design",
      "",
      "```css",
      "@media (max-width: 768px) {",
      "    .dashboard-header {",
      "        flex-direction: column;",
      "        align-items: flex-start;",
      "    }",
      "",
      "    .kpi-row {",
      "        grid-template-columns: repeat(2, 1fr);",
      "    }",
      "",
      "    .chart-row {",
      "        grid-template-columns: 1fr;",
      "    }",
      "",
      "    .filters {",
      "        flex-direction: column;",
      "        align-items: flex-start;",
      "    }",
      "}",
      "",
      "@media print {",
      "    body { background: white; }",
      "    .dashboard-container { max-width: none; }",
      "    .filters { display: none; }",
      "    .chart-container { break-inside: avoid; }",
      "    .kpi-card { border: 1px solid #dee2e6; box-shadow: none; }",
      "}",
      "```",
      "",
      "## Performance Considerations for Large Datasets",
      "",
      "### Data Size Guidelines",
      "",
      "| Data Size | Approach |",
      "|---|---|",
      "| <1,000 rows | Embed directly in HTML. Full interactivity. |",
      "| 1,000 - 10,000 rows | Embed in HTML. May need to pre-aggregate for charts. |",
      "| 10,000 - 100,000 rows | Pre-aggregate server-side. Embed only aggregated data. |",
      "| >100,000 rows | Not suitable for client-side dashboard. Use a BI tool or paginate. |",
      "",
      "### Pre-Aggregation Pattern",
      "",
      "Instead of embedding raw data and aggregating in the browser:",
      "",
      "```javascript",
      "// DON'T: embed 50,000 raw rows",
      "const RAW_DATA = [/* 50,000 rows */];",
      "",
      "// DO: pre-aggregate before embedding",
      "const CHART_DATA = {",
      "    monthly_revenue: [",
      "        { month: '2024-01', revenue: 150000, orders: 1200 },",
      "        { month: '2024-02', revenue: 165000, orders: 1350 },",
      "        // ... 12 rows instead of 50,000",
      "    ],",
      "    top_products: [",
      "        { product: 'Widget A', revenue: 45000 },",
      "        // ... 10 rows",
      "    ],",
      "    kpis: {",
      "        total_revenue: 1980000,",
      "        total_orders: 15600,",
      "        avg_order_value: 127,",
      "    }",
      "};",
      "```",
      "",
      "### Chart Performance",
      "",
      "- Limit line charts to <500 data points per series (downsample if needed)",
      "- Limit bar charts to <50 categories",
      "- For scatter plots, cap at 1,000 points (use sampling for larger datasets)",
      "- Disable animations for dashboards with many charts: `animation: false` in Chart.js options",
      "- Use `Chart.update('none')` instead of `Chart.update()` for filter-triggered updates",
      "",
      "### DOM Performance",
      "",
      "- Limit data tables to 100-200 visible rows. Add pagination for more.",
      "- Use `requestAnimationFrame` for coordinated chart updates",
      "- Avoid rebuilding the entire DOM on filter change -- update only changed elements",
      "",
      "```javascript",
      "// Efficient table pagination",
      "function renderTablePage(data, page, pageSize = 50) {",
      "    const start = page * pageSize;",
      "    const end = Math.min(start + pageSize, data.length);",
      "    const pageData = data.slice(start, end);",
      "    // Render only pageData",
      '    // Show pagination controls: "Showing 1-50 of 2,340"',
      "}",
      "```",
      "",
    ].join("\n"),
  },
  "data-sql-queries": {
    "SKILL.md": [
      "---",
      "name: data-sql-queries",
      "description: Write correct, performant SQL across all major data warehouse dialects (Snowflake, BigQuery, Databricks, PostgreSQL, etc.). Use when writing queries, optimizing slow SQL, translating between dialects, or building complex analytical queries with CTEs, window functions, or aggregations.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# SQL Queries Skill",
      "",
      "Write correct, performant, readable SQL across all major data warehouse dialects.",
      "",
      "## Dialect-Specific Reference",
      "",
      "### PostgreSQL (including Aurora, RDS, Supabase, Neon)",
      "",
      "**Date/time:**",
      "```sql",
      "-- Current date/time",
      "CURRENT_DATE, CURRENT_TIMESTAMP, NOW()",
      "",
      "-- Date arithmetic",
      "date_column + INTERVAL '7 days'",
      "date_column - INTERVAL '1 month'",
      "",
      "-- Truncate to period",
      "DATE_TRUNC('month', created_at)",
      "",
      "-- Extract parts",
      "EXTRACT(YEAR FROM created_at)",
      "EXTRACT(DOW FROM created_at)  -- 0=Sunday",
      "",
      "-- Format",
      "TO_CHAR(created_at, 'YYYY-MM-DD')",
      "```",
      "",
      "**String functions:**",
      "```sql",
      "-- Concatenation",
      "first_name || ' ' || last_name",
      "CONCAT(first_name, ' ', last_name)",
      "",
      "-- Pattern matching",
      "column ILIKE '%pattern%'  -- case-insensitive",
      "column ~ '^regex_pattern$'  -- regex",
      "",
      "-- String manipulation",
      "LEFT(str, n), RIGHT(str, n)",
      "SPLIT_PART(str, delimiter, position)",
      "REGEXP_REPLACE(str, pattern, replacement)",
      "```",
      "",
      "**Arrays and JSON:**",
      "```sql",
      "-- JSON access",
      "data->>'key'  -- text",
      "data->'nested'->'key'  -- json",
      "data#>>'{path,to,key}'  -- nested text",
      "",
      "-- Array operations",
      "ARRAY_AGG(column)",
      "ANY(array_column)",
      "array_column @> ARRAY['value']",
      "```",
      "",
      "**Performance tips:**",
      "- Use `EXPLAIN ANALYZE` to profile queries",
      "- Create indexes on frequently filtered/joined columns",
      "- Use `EXISTS` over `IN` for correlated subqueries",
      "- Partial indexes for common filter conditions",
      "- Use connection pooling for concurrent access",
      "",
      "---",
      "",
      "### Snowflake",
      "",
      "**Date/time:**",
      "```sql",
      "-- Current date/time",
      "CURRENT_DATE(), CURRENT_TIMESTAMP(), SYSDATE()",
      "",
      "-- Date arithmetic",
      "DATEADD(day, 7, date_column)",
      "DATEDIFF(day, start_date, end_date)",
      "",
      "-- Truncate to period",
      "DATE_TRUNC('month', created_at)",
      "",
      "-- Extract parts",
      "YEAR(created_at), MONTH(created_at), DAY(created_at)",
      "DAYOFWEEK(created_at)",
      "",
      "-- Format",
      "TO_CHAR(created_at, 'YYYY-MM-DD')",
      "```",
      "",
      "**String functions:**",
      "```sql",
      "-- Case-insensitive by default (depends on collation)",
      "column ILIKE '%pattern%'",
      "REGEXP_LIKE(column, 'pattern')",
      "",
      "-- Parse JSON",
      "column:key::string  -- dot notation for VARIANT",
      'PARSE_JSON(\'{"key": "value"}\')',
      "GET_PATH(variant_col, 'path.to.key')",
      "",
      "-- Flatten arrays/objects",
      "SELECT f.value FROM table, LATERAL FLATTEN(input => array_col) f",
      "```",
      "",
      "**Semi-structured data:**",
      "```sql",
      "-- VARIANT type access",
      "data:customer:name::STRING",
      "data:items[0]:price::NUMBER",
      "",
      "-- Flatten nested structures",
      "SELECT",
      "    t.id,",
      "    item.value:name::STRING as item_name,",
      "    item.value:qty::NUMBER as quantity",
      "FROM my_table t,",
      "LATERAL FLATTEN(input => t.data:items) item",
      "```",
      "",
      "**Performance tips:**",
      "- Use clustering keys on large tables (not traditional indexes)",
      "- Filter on clustering key columns for partition pruning",
      "- Set appropriate warehouse size for query complexity",
      "- Use `RESULT_SCAN(LAST_QUERY_ID())` to avoid re-running expensive queries",
      "- Use transient tables for staging/temp data",
      "",
      "---",
      "",
      "### BigQuery (Google Cloud)",
      "",
      "**Date/time:**",
      "```sql",
      "-- Current date/time",
      "CURRENT_DATE(), CURRENT_TIMESTAMP()",
      "",
      "-- Date arithmetic",
      "DATE_ADD(date_column, INTERVAL 7 DAY)",
      "DATE_SUB(date_column, INTERVAL 1 MONTH)",
      "DATE_DIFF(end_date, start_date, DAY)",
      "TIMESTAMP_DIFF(end_ts, start_ts, HOUR)",
      "",
      "-- Truncate to period",
      "DATE_TRUNC(created_at, MONTH)",
      "TIMESTAMP_TRUNC(created_at, HOUR)",
      "",
      "-- Extract parts",
      "EXTRACT(YEAR FROM created_at)",
      "EXTRACT(DAYOFWEEK FROM created_at)  -- 1=Sunday",
      "",
      "-- Format",
      "FORMAT_DATE('%Y-%m-%d', date_column)",
      "FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', ts_column)",
      "```",
      "",
      "**String functions:**",
      "```sql",
      "-- No ILIKE, use LOWER()",
      "LOWER(column) LIKE '%pattern%'",
      "REGEXP_CONTAINS(column, r'pattern')",
      "REGEXP_EXTRACT(column, r'pattern')",
      "",
      "-- String manipulation",
      "SPLIT(str, delimiter)  -- returns ARRAY",
      "ARRAY_TO_STRING(array, delimiter)",
      "```",
      "",
      "**Arrays and structs:**",
      "```sql",
      "-- Array operations",
      "ARRAY_AGG(column)",
      "UNNEST(array_column)",
      "ARRAY_LENGTH(array_column)",
      "value IN UNNEST(array_column)",
      "",
      "-- Struct access",
      "struct_column.field_name",
      "```",
      "",
      "**Performance tips:**",
      "- Always filter on partition columns (usually date) to reduce bytes scanned",
      "- Use clustering for frequently filtered columns within partitions",
      "- Use `APPROX_COUNT_DISTINCT()` for large-scale cardinality estimates",
      "- Avoid `SELECT *` -- billing is per-byte scanned",
      "- Use `DECLARE` and `SET` for parameterized scripts",
      "- Preview query cost with dry run before executing large queries",
      "",
      "---",
      "",
      "### Redshift (Amazon)",
      "",
      "**Date/time:**",
      "```sql",
      "-- Current date/time",
      "CURRENT_DATE, GETDATE(), SYSDATE",
      "",
      "-- Date arithmetic",
      "DATEADD(day, 7, date_column)",
      "DATEDIFF(day, start_date, end_date)",
      "",
      "-- Truncate to period",
      "DATE_TRUNC('month', created_at)",
      "",
      "-- Extract parts",
      "EXTRACT(YEAR FROM created_at)",
      "DATE_PART('dow', created_at)",
      "```",
      "",
      "**String functions:**",
      "```sql",
      "-- Case-insensitive",
      "column ILIKE '%pattern%'",
      "REGEXP_INSTR(column, 'pattern') > 0",
      "",
      "-- String manipulation",
      "SPLIT_PART(str, delimiter, position)",
      "LISTAGG(column, ', ') WITHIN GROUP (ORDER BY column)",
      "```",
      "",
      "**Performance tips:**",
      "- Design distribution keys for collocated joins (DISTKEY)",
      "- Use sort keys for frequently filtered columns (SORTKEY)",
      "- Use `EXPLAIN` to check query plan",
      "- Avoid cross-node data movement (watch for DS_BCAST and DS_DIST)",
      "- `ANALYZE` and `VACUUM` regularly",
      "- Use late-binding views for schema flexibility",
      "",
      "---",
      "",
      "### Databricks SQL",
      "",
      "**Date/time:**",
      "```sql",
      "-- Current date/time",
      "CURRENT_DATE(), CURRENT_TIMESTAMP()",
      "",
      "-- Date arithmetic",
      "DATE_ADD(date_column, 7)",
      "DATEDIFF(end_date, start_date)",
      "ADD_MONTHS(date_column, 1)",
      "",
      "-- Truncate to period",
      "DATE_TRUNC('MONTH', created_at)",
      "TRUNC(date_column, 'MM')",
      "",
      "-- Extract parts",
      "YEAR(created_at), MONTH(created_at)",
      "DAYOFWEEK(created_at)",
      "```",
      "",
      "**Delta Lake features:**",
      "```sql",
      "-- Time travel",
      "SELECT * FROM my_table TIMESTAMP AS OF '2024-01-15'",
      "SELECT * FROM my_table VERSION AS OF 42",
      "",
      "-- Describe history",
      "DESCRIBE HISTORY my_table",
      "",
      "-- Merge (upsert)",
      "MERGE INTO target USING source",
      "ON target.id = source.id",
      "WHEN MATCHED THEN UPDATE SET *",
      "WHEN NOT MATCHED THEN INSERT *",
      "```",
      "",
      "**Performance tips:**",
      "- Use Delta Lake's `OPTIMIZE` and `ZORDER` for query performance",
      "- Leverage Photon engine for compute-intensive queries",
      "- Use `CACHE TABLE` for frequently accessed datasets",
      "- Partition by low-cardinality date columns",
      "",
      "---",
      "",
      "## Common SQL Patterns",
      "",
      "### Window Functions",
      "",
      "```sql",
      "-- Ranking",
      "ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY created_at DESC)",
      "RANK() OVER (PARTITION BY category ORDER BY revenue DESC)",
      "DENSE_RANK() OVER (ORDER BY score DESC)",
      "",
      "-- Running totals / moving averages",
      "SUM(revenue) OVER (ORDER BY date_col ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total",
      "AVG(revenue) OVER (ORDER BY date_col ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7d",
      "",
      "-- Lag / Lead",
      "LAG(value, 1) OVER (PARTITION BY entity ORDER BY date_col) as prev_value",
      "LEAD(value, 1) OVER (PARTITION BY entity ORDER BY date_col) as next_value",
      "",
      "-- First / Last value",
      "FIRST_VALUE(status) OVER (PARTITION BY user_id ORDER BY created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)",
      "LAST_VALUE(status) OVER (PARTITION BY user_id ORDER BY created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)",
      "",
      "-- Percent of total",
      "revenue / SUM(revenue) OVER () as pct_of_total",
      "revenue / SUM(revenue) OVER (PARTITION BY category) as pct_of_category",
      "```",
      "",
      "### CTEs for Readability",
      "",
      "```sql",
      "WITH",
      "-- Step 1: Define the base population",
      "base_users AS (",
      "    SELECT user_id, created_at, plan_type",
      "    FROM users",
      "    WHERE created_at >= DATE '2024-01-01'",
      "      AND status = 'active'",
      "),",
      "",
      "-- Step 2: Calculate user-level metrics",
      "user_metrics AS (",
      "    SELECT",
      "        u.user_id,",
      "        u.plan_type,",
      "        COUNT(DISTINCT e.session_id) as session_count,",
      "        SUM(e.revenue) as total_revenue",
      "    FROM base_users u",
      "    LEFT JOIN events e ON u.user_id = e.user_id",
      "    GROUP BY u.user_id, u.plan_type",
      "),",
      "",
      "-- Step 3: Aggregate to summary level",
      "summary AS (",
      "    SELECT",
      "        plan_type,",
      "        COUNT(*) as user_count,",
      "        AVG(session_count) as avg_sessions,",
      "        SUM(total_revenue) as total_revenue",
      "    FROM user_metrics",
      "    GROUP BY plan_type",
      ")",
      "",
      "SELECT * FROM summary ORDER BY total_revenue DESC;",
      "```",
      "",
      "### Cohort Retention",
      "",
      "```sql",
      "WITH cohorts AS (",
      "    SELECT",
      "        user_id,",
      "        DATE_TRUNC('month', first_activity_date) as cohort_month",
      "    FROM users",
      "),",
      "activity AS (",
      "    SELECT",
      "        user_id,",
      "        DATE_TRUNC('month', activity_date) as activity_month",
      "    FROM user_activity",
      ")",
      "SELECT",
      "    c.cohort_month,",
      "    COUNT(DISTINCT c.user_id) as cohort_size,",
      "    COUNT(DISTINCT CASE",
      "        WHEN a.activity_month = c.cohort_month THEN a.user_id",
      "    END) as month_0,",
      "    COUNT(DISTINCT CASE",
      "        WHEN a.activity_month = c.cohort_month + INTERVAL '1 month' THEN a.user_id",
      "    END) as month_1,",
      "    COUNT(DISTINCT CASE",
      "        WHEN a.activity_month = c.cohort_month + INTERVAL '3 months' THEN a.user_id",
      "    END) as month_3",
      "FROM cohorts c",
      "LEFT JOIN activity a ON c.user_id = a.user_id",
      "GROUP BY c.cohort_month",
      "ORDER BY c.cohort_month;",
      "```",
      "",
      "### Funnel Analysis",
      "",
      "```sql",
      "WITH funnel AS (",
      "    SELECT",
      "        user_id,",
      "        MAX(CASE WHEN event = 'page_view' THEN 1 ELSE 0 END) as step_1_view,",
      "        MAX(CASE WHEN event = 'signup_start' THEN 1 ELSE 0 END) as step_2_start,",
      "        MAX(CASE WHEN event = 'signup_complete' THEN 1 ELSE 0 END) as step_3_complete,",
      "        MAX(CASE WHEN event = 'first_purchase' THEN 1 ELSE 0 END) as step_4_purchase",
      "    FROM events",
      "    WHERE event_date >= CURRENT_DATE - INTERVAL '30 days'",
      "    GROUP BY user_id",
      ")",
      "SELECT",
      "    COUNT(*) as total_users,",
      "    SUM(step_1_view) as viewed,",
      "    SUM(step_2_start) as started_signup,",
      "    SUM(step_3_complete) as completed_signup,",
      "    SUM(step_4_purchase) as purchased,",
      "    ROUND(100.0 * SUM(step_2_start) / NULLIF(SUM(step_1_view), 0), 1) as view_to_start_pct,",
      "    ROUND(100.0 * SUM(step_3_complete) / NULLIF(SUM(step_2_start), 0), 1) as start_to_complete_pct,",
      "    ROUND(100.0 * SUM(step_4_purchase) / NULLIF(SUM(step_3_complete), 0), 1) as complete_to_purchase_pct",
      "FROM funnel;",
      "```",
      "",
      "### Deduplication",
      "",
      "```sql",
      "-- Keep the most recent record per key",
      "WITH ranked AS (",
      "    SELECT",
      "        *,",
      "        ROW_NUMBER() OVER (",
      "            PARTITION BY entity_id",
      "            ORDER BY updated_at DESC",
      "        ) as rn",
      "    FROM source_table",
      ")",
      "SELECT * FROM ranked WHERE rn = 1;",
      "```",
      "",
      "## Error Handling and Debugging",
      "",
      "When a query fails:",
      "",
      "1. **Syntax errors**: Check for dialect-specific syntax (e.g., `ILIKE` not available in BigQuery, `SAFE_DIVIDE` only in BigQuery)",
      "2. **Column not found**: Verify column names against schema -- check for typos, case sensitivity (PostgreSQL is case-sensitive for quoted identifiers)",
      "3. **Type mismatches**: Cast explicitly when comparing different types (`CAST(col AS DATE)`, `col::DATE`)",
      "4. **Division by zero**: Use `NULLIF(denominator, 0)` or dialect-specific safe division",
      "5. **Ambiguous columns**: Always qualify column names with table alias in JOINs",
      "6. **Group by errors**: All non-aggregated columns must be in GROUP BY (except in BigQuery which allows grouping by alias)",
      "",
    ].join("\n"),
  },
  "data-statistical-analysis": {
    "SKILL.md": [
      "---",
      "name: data-statistical-analysis",
      "description: Apply statistical methods including descriptive stats, trend analysis, outlier detection, and hypothesis testing. Use when analyzing distributions, testing for significance, detecting anomalies, computing correlations, or interpreting statistical results.",
      "metadata:",
      "  plugin: data",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Statistical Analysis Skill",
      "",
      "Descriptive statistics, trend analysis, outlier detection, hypothesis testing, and guidance on when to be cautious about statistical claims.",
      "",
      "## Descriptive Statistics Methodology",
      "",
      "### Central Tendency",
      "",
      "Choose the right measure of center based on the data:",
      "",
      "| Situation | Use | Why |",
      "|---|---|---|",
      "| Symmetric distribution, no outliers | Mean | Most efficient estimator |",
      "| Skewed distribution | Median | Robust to outliers |",
      "| Categorical or ordinal data | Mode | Only option for non-numeric |",
      "| Highly skewed with outliers (e.g., revenue per user) | Median + mean | Report both; the gap shows skew |",
      "",
      "**Always report mean and median together for business metrics.** If they diverge significantly, the data is skewed and the mean alone is misleading.",
      "",
      "### Spread and Variability",
      "",
      "- **Standard deviation**: How far values typically fall from the mean. Use with normally distributed data.",
      "- **Interquartile range (IQR)**: Distance from p25 to p75. Robust to outliers. Use with skewed data.",
      "- **Coefficient of variation (CV)**: StdDev / Mean. Use to compare variability across metrics with different scales.",
      "- **Range**: Max minus min. Sensitive to outliers but gives a quick sense of data extent.",
      "",
      "### Percentiles for Business Context",
      "",
      "Report key percentiles to tell a richer story than mean alone:",
      "",
      "```",
      "p1:   Bottom 1% (floor / minimum typical value)",
      "p5:   Low end of normal range",
      "p25:  First quartile",
      "p50:  Median (typical user)",
      "p75:  Third quartile",
      "p90:  Top 10% / power users",
      "p95:  High end of normal range",
      "p99:  Top 1% / extreme users",
      "```",
      "",
      '**Example narrative**: "The median session duration is 4.2 minutes, but the top 10% of users spend over 22 minutes per session, pulling the mean up to 7.8 minutes."',
      "",
      "### Describing Distributions",
      "",
      "Characterize every numeric distribution you analyze:",
      "",
      "- **Shape**: Normal, right-skewed, left-skewed, bimodal, uniform, heavy-tailed",
      "- **Center**: Mean and median (and the gap between them)",
      "- **Spread**: Standard deviation or IQR",
      "- **Outliers**: How many and how extreme",
      "- **Bounds**: Is there a natural floor (zero) or ceiling (100%)?",
      "",
      "## Trend Analysis and Forecasting",
      "",
      "### Identifying Trends",
      "",
      "**Moving averages** to smooth noise:",
      "```python",
      "# 7-day moving average (good for daily data with weekly seasonality)",
      "df['ma_7d'] = df['metric'].rolling(window=7, min_periods=1).mean()",
      "",
      "# 28-day moving average (smooths weekly AND monthly patterns)",
      "df['ma_28d'] = df['metric'].rolling(window=28, min_periods=1).mean()",
      "```",
      "",
      "**Period-over-period comparison**:",
      "- Week-over-week (WoW): Compare to same day last week",
      "- Month-over-month (MoM): Compare to same month prior",
      "- Year-over-year (YoY): Gold standard for seasonal businesses",
      "- Same-day-last-year: Compare specific calendar day",
      "",
      "**Growth rates**:",
      "```",
      "Simple growth: (current - previous) / previous",
      "CAGR: (ending / beginning) ^ (1 / years) - 1",
      "Log growth: ln(current / previous)  -- better for volatile series",
      "```",
      "",
      "### Seasonality Detection",
      "",
      "Check for periodic patterns:",
      "1. Plot the raw time series -- visual inspection first",
      "2. Compute day-of-week averages: is there a clear weekly pattern?",
      "3. Compute month-of-year averages: is there an annual cycle?",
      "4. When comparing periods, always use YoY or same-period comparisons to avoid conflating trend with seasonality",
      "",
      "### Forecasting (Simple Methods)",
      "",
      "For business analysts (not data scientists), use straightforward methods:",
      "",
      "- **Naive forecast**: Tomorrow = today. Use as a baseline.",
      "- **Seasonal naive**: Tomorrow = same day last week/year.",
      "- **Linear trend**: Fit a line to historical data. Only for clearly linear trends.",
      "- **Moving average forecast**: Use trailing average as the forecast.",
      "",
      "**Always communicate uncertainty**. Provide a range, not a point estimate:",
      '- "We expect 10K-12K signups next month based on the 3-month trend"',
      '- NOT "We will get exactly 11,234 signups next month"',
      "",
      "**When to escalate to a data scientist**: Non-linear trends, multiple seasonalities, external factors (marketing spend, holidays), or when forecast accuracy matters for resource allocation.",
      "",
      "## Outlier and Anomaly Detection",
      "",
      "### Statistical Methods",
      "",
      "**Z-score method** (for normally distributed data):",
      "```python",
      "z_scores = (df['value'] - df['value'].mean()) / df['value'].std()",
      "outliers = df[abs(z_scores) > 3]  # More than 3 standard deviations",
      "```",
      "",
      "**IQR method** (robust to non-normal distributions):",
      "```python",
      "Q1 = df['value'].quantile(0.25)",
      "Q3 = df['value'].quantile(0.75)",
      "IQR = Q3 - Q1",
      "lower_bound = Q1 - 1.5 * IQR",
      "upper_bound = Q3 + 1.5 * IQR",
      "outliers = df[(df['value'] < lower_bound) | (df['value'] > upper_bound)]",
      "```",
      "",
      "**Percentile method** (simplest):",
      "```python",
      "outliers = df[(df['value'] < df['value'].quantile(0.01)) |",
      "              (df['value'] > df['value'].quantile(0.99))]",
      "```",
      "",
      "### Handling Outliers",
      "",
      "Do NOT automatically remove outliers. Instead:",
      "",
      "1. **Investigate**: Is this a data error, a genuine extreme value, or a different population?",
      "2. **Data errors**: Fix or remove (e.g., negative ages, timestamps in year 1970)",
      "3. **Genuine extremes**: Keep them but consider using robust statistics (median instead of mean)",
      "4. **Different population**: Segment them out for separate analysis (e.g., enterprise vs. SMB customers)",
      "",
      '**Report what you did**: "We excluded 47 records (0.3%) with transaction amounts >$50K, which represent bulk enterprise orders analyzed separately."',
      "",
      "### Time Series Anomaly Detection",
      "",
      "For detecting unusual values in a time series:",
      "",
      "1. Compute expected value (moving average or same-period-last-year)",
      "2. Compute deviation from expected",
      "3. Flag deviations beyond a threshold (typically 2-3 standard deviations of the residuals)",
      "4. Distinguish between point anomalies (single unusual value) and change points (sustained shift)",
      "",
      "## Hypothesis Testing Basics",
      "",
      "### When to Use",
      "",
      "Use hypothesis testing when you need to determine whether an observed difference is likely real or could be due to random chance. Common scenarios:",
      "",
      "- A/B test results: Is variant B actually better than A?",
      "- Before/after comparison: Did the product change actually move the metric?",
      "- Segment comparison: Do enterprise customers really have higher retention?",
      "",
      "### The Framework",
      "",
      "1. **Null hypothesis (H0)**: There is no difference (the default assumption)",
      "2. **Alternative hypothesis (H1)**: There is a difference",
      "3. **Choose significance level (alpha)**: Typically 0.05 (5% chance of false positive)",
      "4. **Compute test statistic and p-value**",
      "5. **Interpret**: If p < alpha, reject H0 (evidence of a real difference)",
      "",
      "### Common Tests",
      "",
      "| Scenario | Test | When to Use |",
      "|---|---|---|",
      "| Compare two group means | t-test (independent) | Normal data, two groups |",
      "| Compare two group proportions | z-test for proportions | Conversion rates, binary outcomes |",
      "| Compare paired measurements | Paired t-test | Before/after on same entities |",
      "| Compare 3+ group means | ANOVA | Multiple segments or variants |",
      "| Non-normal data, two groups | Mann-Whitney U test | Skewed metrics, ordinal data |",
      "| Association between categories | Chi-squared test | Two categorical variables |",
      "",
      "### Practical Significance vs. Statistical Significance",
      "",
      "**Statistical significance** means the difference is unlikely due to chance.",
      "",
      "**Practical significance** means the difference is large enough to matter for business decisions.",
      "",
      "A difference can be statistically significant but practically meaningless (common with large samples). Always report:",
      '- **Effect size**: How big is the difference? (e.g., "Variant B improved conversion by 0.3 percentage points")',
      "- **Confidence interval**: What's the range of plausible true effects?",
      "- **Business impact**: What does this translate to in revenue, users, or other business terms?",
      "",
      "### Sample Size Considerations",
      "",
      "- Small samples produce unreliable results, even with significant p-values",
      "- Rule of thumb for proportions: Need at least 30 events per group for basic reliability",
      "- For detecting small effects (e.g., 1% conversion rate change), you may need thousands of observations per group",
      '- If your sample is small, say so: "With only 200 observations per group, we have limited power to detect effects smaller than X%"',
      "",
      "## When to Be Cautious About Statistical Claims",
      "",
      "### Correlation Is Not Causation",
      "",
      "When you find a correlation, explicitly consider:",
      "- **Reverse causation**: Maybe B causes A, not A causes B",
      "- **Confounding variables**: Maybe C causes both A and B",
      "- **Coincidence**: With enough variables, spurious correlations are inevitable",
      "",
      '**What you can say**: "Users who use feature X have 30% higher retention"',
      '**What you cannot say without more evidence**: "Feature X causes 30% higher retention"',
      "",
      "### Multiple Comparisons Problem",
      "",
      'When you test many hypotheses, some will be "significant" by chance:',
      "- Testing 20 metrics at p=0.05 means ~1 will be falsely significant",
      "- If you looked at many segments before finding one that's different, note that",
      "- Adjust for multiple comparisons with Bonferroni correction (divide alpha by number of tests) or report how many tests were run",
      "",
      "### Simpson's Paradox",
      "",
      "A trend in aggregated data can reverse when data is segmented:",
      "- Always check whether the conclusion holds across key segments",
      "- Example: Overall conversion goes up, but conversion goes down in every segment -- because the mix shifted toward a higher-converting segment",
      "",
      "### Survivorship Bias",
      "",
      'You can only analyze entities that "survived" to be in your dataset:',
      "- Analyzing active users ignores those who churned",
      "- Analyzing successful companies ignores those that failed",
      '- Always ask: "Who is missing from this dataset, and would their inclusion change the conclusion?"',
      "",
      "### Ecological Fallacy",
      "",
      "Aggregate trends may not apply to individuals:",
      '- "Countries with higher X have higher Y" does NOT mean "individuals with higher X have higher Y"',
      "- Be careful about applying group-level findings to individual cases",
      "",
      "### Anchoring on Specific Numbers",
      "",
      "Be wary of false precision:",
      '- "Churn will be 4.73% next quarter" implies more certainty than is warranted',
      '- Prefer ranges: "We expect churn between 4-6% based on historical patterns"',
      '- Round appropriately: "About 5%" is often more honest than "4.73%"',
      "",
    ].join("\n"),
  },
  "data-validate": {
    "SKILL.md": [
      "---",
      "name: data-validate",
      "description: QA an analysis before sharing -- methodology, accuracy, and bias checks",
      "metadata:",
      "  plugin: data",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <analysis to review>",
      "---",
      "",
      "# /validate - Validate Analysis Before Sharing",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Review an analysis for accuracy, methodology, and potential biases before sharing with stakeholders. Generates a confidence assessment and improvement suggestions.",
      "",
      "## Usage",
      "",
      "```",
      "/validate <analysis to review>",
      "```",
      "",
      "The analysis can be:",
      "- A document or report in the conversation",
      "- A file (markdown, notebook, spreadsheet)",
      "- SQL queries and their results",
      "- Charts and their underlying data",
      "- A description of methodology and findings",
      "",
      "## Workflow",
      "",
      "### 1. Review Methodology and Assumptions",
      "",
      "Examine:",
      "",
      "- **Question framing**: Is the analysis answering the right question? Could the question be interpreted differently?",
      "- **Data selection**: Are the right tables/datasets being used? Is the time range appropriate?",
      "- **Population definition**: Is the analysis population correctly defined? Are there unintended exclusions?",
      "- **Metric definitions**: Are metrics defined clearly and consistently? Do they match how stakeholders understand them?",
      "- **Baseline and comparison**: Is the comparison fair? Are time periods, cohort sizes, and contexts comparable?",
      "",
      "### 2. Check for Common Analytical Errors",
      "",
      "Systematically review for:",
      "",
      "**Data completeness:**",
      "- Missing data that could skew results (e.g., nulls in key fields, missing time periods)",
      "- Data freshness issues (is the most recent data actually complete or still loading?)",
      '- Survivorship bias (are you only looking at entities that "survived" to the analysis date?)',
      "",
      "**Statistical issues:**",
      "- Simpson's paradox (trend reverses when data is aggregated vs. segmented)",
      "- Correlation presented as causation without supporting evidence",
      "- Small sample sizes leading to unreliable conclusions",
      "- Outliers disproportionately affecting averages (should medians be used instead?)",
      "- Multiple testing / cherry-picking significant results",
      "",
      "**Aggregation errors:**",
      "- Double-counting from improper joins (many-to-many explosions)",
      "- Incorrect denominators in rate calculations",
      "- Mixing granularity levels (e.g., user-level metrics averaged with account-level)",
      "- Revenue recognized vs. billed vs. collected confusion",
      "",
      "**Time-related issues:**",
      "- Seasonality not accounted for in comparisons",
      "- Incomplete periods included in averages (e.g., partial month compared to full months)",
      "- Timezone inconsistencies between data sources",
      "- Look-ahead bias (using future information to explain past events)",
      "",
      "**Selection and scope:**",
      "- Cherry-picked time ranges that favor a particular narrative",
      "- Excluded segments without justification",
      "- Changing definitions mid-analysis",
      "",
      "### 3. Verify Calculations and Aggregations",
      "",
      "Where possible, spot-check:",
      "",
      "- Recalculate a few key numbers independently",
      "- Verify that subtotals sum to totals",
      "- Check that percentages sum to 100% (or close to it) where expected",
      "- Confirm that YoY/MoM comparisons use the correct base periods",
      "- Validate that filters are applied consistently across all metrics",
      "",
      "### 4. Assess Visualizations",
      "",
      "If the analysis includes charts:",
      "",
      "- Do axes start at appropriate values (zero for bar charts)?",
      "- Are scales consistent across comparison charts?",
      "- Do chart titles accurately describe what's shown?",
      "- Could the visualization mislead a quick reader?",
      "- Are there truncated axes, inconsistent intervals, or 3D effects that distort perception?",
      "",
      "### 5. Evaluate Narrative and Conclusions",
      "",
      "Review whether:",
      "",
      "- Conclusions are supported by the data shown",
      "- Alternative explanations are acknowledged",
      "- Uncertainty is communicated appropriately",
      "- Recommendations follow logically from findings",
      "- The level of confidence matches the strength of evidence",
      "",
      "### 6. Suggest Improvements",
      "",
      "Provide specific, actionable suggestions:",
      "",
      "- Additional analyses that would strengthen the conclusions",
      "- Caveats or limitations that should be noted",
      "- Better visualizations or framings for key points",
      "- Missing context that stakeholders would want",
      "",
      "### 7. Generate Confidence Assessment",
      "",
      "Rate the analysis on a 3-level scale:",
      "",
      "**Ready to share** -- Analysis is methodologically sound, calculations verified, caveats noted. Minor suggestions for improvement but nothing blocking.",
      "",
      "**Share with noted caveats** -- Analysis is largely correct but has specific limitations or assumptions that must be communicated to stakeholders. List the required caveats.",
      "",
      "**Needs revision** -- Found specific errors, methodological issues, or missing analyses that should be addressed before sharing. List the required changes with priority order.",
      "",
      "## Output Format",
      "",
      "```",
      "## Validation Report",
      "",
      "### Overall Assessment: [Ready to share | Share with caveats | Needs revision]",
      "",
      "### Methodology Review",
      "[Findings about approach, data selection, definitions]",
      "",
      "### Issues Found",
      "1. [Severity: High/Medium/Low] [Issue description and impact]",
      "2. ...",
      "",
      "### Calculation Spot-Checks",
      "- [Metric]: [Verified / Discrepancy found]",
      "- ...",
      "",
      "### Visualization Review",
      "[Any issues with charts or visual presentation]",
      "",
      "### Suggested Improvements",
      "1. [Improvement and why it matters]",
      "2. ...",
      "",
      "### Required Caveats for Stakeholders",
      "- [Caveat that must be communicated]",
      "- ...",
      "```",
      "",
      "## Examples",
      "",
      "```",
      "/validate Review this quarterly revenue analysis before I send it to the exec team: [analysis]",
      "```",
      "",
      "```",
      "/validate Check my churn analysis -- I'm comparing Q4 churn rates to Q3 but Q4 has a shorter measurement window",
      "```",
      "",
      "```",
      "/validate Here's a SQL query and its results for our conversion funnel. Does the logic look right? [query + results]",
      "```",
      "",
      "## Tips",
      "",
      "- Run /validate before any high-stakes presentation or decision",
      "- Even quick analyses benefit from a sanity check -- it takes a minute and can save your credibility",
      "- If the validation finds issues, fix them and re-validate",
      "- Share the validation output alongside your analysis to build stakeholder confidence",
      "",
    ].join("\n"),
  },
  "data-write-query": {
    "SKILL.md": [
      "---",
      "name: data-write-query",
      "description: Write optimized SQL for your dialect with best practices",
      "metadata:",
      "  plugin: data",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <description of what data you need>",
      "---",
      "",
      "# /write-query - Write Optimized SQL",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Write a SQL query from a natural language description, optimized for your specific SQL dialect and following best practices.",
      "",
      "## Usage",
      "",
      "```",
      "/write-query <description of what data you need>",
      "```",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Request",
      "",
      "Parse the user's description to identify:",
      "",
      "- **Output columns**: What fields should the result include?",
      "- **Filters**: What conditions limit the data (time ranges, segments, statuses)?",
      "- **Aggregations**: Are there GROUP BY operations, counts, sums, averages?",
      "- **Joins**: Does this require combining multiple tables?",
      "- **Ordering**: How should results be sorted?",
      "- **Limits**: Is there a top-N or sample requirement?",
      "",
      "### 2. Determine SQL Dialect",
      "",
      "If the user's SQL dialect is not already known, ask which they use:",
      "",
      "- **PostgreSQL** (including Aurora, RDS, Supabase, Neon)",
      "- **Snowflake**",
      "- **BigQuery** (Google Cloud)",
      "- **Redshift** (Amazon)",
      "- **Databricks SQL**",
      "- **MySQL** (including Aurora MySQL, PlanetScale)",
      "- **SQL Server** (Microsoft)",
      "- **DuckDB**",
      "- **SQLite**",
      "- **Other** (ask for specifics)",
      "",
      "Remember the dialect for future queries in the same session.",
      "",
      "### 3. Discover Schema (If Warehouse Connected)",
      "",
      "If a data warehouse MCP server is connected:",
      "",
      "1. Search for relevant tables based on the user's description",
      "2. Inspect column names, types, and relationships",
      "3. Check for partitioning or clustering keys that affect performance",
      "4. Look for pre-built views or materialized views that might simplify the query",
      "",
      "### 4. Write the Query",
      "",
      "Follow these best practices:",
      "",
      "**Structure:**",
      "- Use CTEs (WITH clauses) for readability when queries have multiple logical steps",
      "- One CTE per logical transformation or data source",
      "- Name CTEs descriptively (e.g., `daily_signups`, `active_users`, `revenue_by_product`)",
      "",
      "**Performance:**",
      "- Never use `SELECT *` in production queries -- specify only needed columns",
      "- Filter early (push WHERE clauses as close to the base tables as possible)",
      "- Use partition filters when available (especially date partitions)",
      "- Prefer `EXISTS` over `IN` for subqueries with large result sets",
      "- Use appropriate JOIN types (don't use LEFT JOIN when INNER JOIN is correct)",
      "- Avoid correlated subqueries when a JOIN or window function works",
      "- Be mindful of exploding joins (many-to-many)",
      "",
      "**Readability:**",
      '- Add comments explaining the "why" for non-obvious logic',
      "- Use consistent indentation and formatting",
      "- Alias tables with meaningful short names (not just `a`, `b`, `c`)",
      "- Put each major clause on its own line",
      "",
      "**Dialect-specific optimizations:**",
      "- Apply dialect-specific syntax and functions (see `sql-queries` skill for details)",
      "- Use dialect-appropriate date functions, string functions, and window syntax",
      "- Note any dialect-specific performance features (e.g., Snowflake clustering, BigQuery partitioning)",
      "",
      "### 5. Present the Query",
      "",
      "Provide:",
      "",
      "1. **The complete query** in a SQL code block with syntax highlighting",
      "2. **Brief explanation** of what each CTE or section does",
      "3. **Performance notes** if relevant (expected cost, partition usage, potential bottlenecks)",
      "4. **Modification suggestions** -- how to adjust for common variations (different time range, different granularity, additional filters)",
      "",
      "### 6. Offer to Execute",
      "",
      "If a data warehouse is connected, offer to run the query and analyze the results. If the user wants to run it themselves, the query is ready to copy-paste.",
      "",
      "## Examples",
      "",
      "**Simple aggregation:**",
      "```",
      "/write-query Count of orders by status for the last 30 days",
      "```",
      "",
      "**Complex analysis:**",
      "```",
      "/write-query Cohort retention analysis -- group users by their signup month, then show what percentage are still active (had at least one event) at 1, 3, 6, and 12 months after signup",
      "```",
      "",
      "**Performance-critical:**",
      "```",
      "/write-query We have a 500M row events table partitioned by date. Find the top 100 users by event count in the last 7 days with their most recent event type.",
      "```",
      "",
      "## Tips",
      "",
      "- Mention your SQL dialect upfront to get the right syntax immediately",
      "- If you know the table names, include them -- otherwise Claude will help you find them",
      "- Specify if you need the query to be idempotent (safe to re-run) or one-time",
      "- For recurring queries, mention if it should be parameterized for date ranges",
      "",
    ].join("\n"),
  },
  docx: {
    "SKILL.md": [
      "---",
      "name: docx",
      'description: "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks"',
      "license: Proprietary. LICENSE.txt has complete terms",
      "---",
      "",
      "# DOCX creation, editing, and analysis",
      "",
      "## Overview",
      "",
      "A user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.",
      "",
      "## Workflow Decision Tree",
      "",
      "### Reading/Analyzing Content",
      'Use "Text extraction" or "Raw XML access" sections below',
      "",
      "### Creating New Document",
      'Use "Creating a new Word document" workflow',
      "",
      "### Editing Existing Document",
      "- **Your own document + simple changes**",
      '  Use "Basic OOXML editing" workflow',
      "",
      "- **Someone else's document**",
      '  Use **"Redlining workflow"** (recommended default)',
      "",
      "- **Legal, academic, business, or government docs**",
      '  Use **"Redlining workflow"** (required)',
      "",
      "## Reading and analyzing content",
      "",
      "### Text extraction",
      "If you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:",
      "",
      "```bash",
      "# Convert document to markdown with tracked changes",
      "pandoc --track-changes=all path-to-file.docx -o output.md",
      "# Options: --track-changes=accept/reject/all",
      "```",
      "",
      "### Raw XML access",
      "You need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.",
      "",
      "#### Unpacking a file",
      "`python ooxml/scripts/unpack.py <office_file> <output_directory>`",
      "",
      "#### Key file structures",
      "* `word/document.xml` - Main document contents",
      "* `word/comments.xml` - Comments referenced in document.xml",
      "* `word/media/` - Embedded images and media files",
      "* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags",
      "",
      "## Creating a new Word document",
      "",
      "When creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.",
      "",
      "### Workflow",
      "1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.",
      "2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)",
      "3. Export as .docx using Packer.toBuffer()",
      "",
      "## Editing an existing Word document",
      "",
      "When editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.",
      "",
      "### Workflow",
      "1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.",
      "2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`",
      '3. Create and run a Python script using the Document library (see "Document Library" section in ooxml.md)',
      "4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`",
      "",
      "The Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.",
      "",
      "## Redlining workflow for document review",
      "",
      "This workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.",
      "",
      "**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.",
      "",
      "**Principle: Minimal, Precise Edits**",
      "When implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.",
      "",
      'Example - Changing "30 days" to "60 days" in a sentence:',
      "```python",
      "# BAD - Replaces entire sentence",
      "'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'",
      "",
      "# GOOD - Only marks what changed, preserves original <w:r> for unchanged text",
      '\'<w:r w:rsidR="00AB12CD"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR="00AB12CD"><w:t> days.</w:t></w:r>\'',
      "```",
      "",
      "### Tracked changes workflow",
      "",
      "1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:",
      "   ```bash",
      "   pandoc --track-changes=all path-to-file.docx -o current.md",
      "   ```",
      "",
      "2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:",
      "",
      "   **Location methods** (for finding changes in XML):",
      '   - Section/heading numbers (e.g., "Section 3.2", "Article IV")',
      "   - Paragraph identifiers if numbered",
      "   - Grep patterns with unique surrounding text",
      '   - Document structure (e.g., "first paragraph", "signature block")',
      "   - **DO NOT use markdown line numbers** - they don't map to XML structure",
      "",
      "   **Batch organization** (group 3-10 related changes per batch):",
      '   - By section: "Batch 1: Section 2 amendments", "Batch 2: Section 5 updates"',
      '   - By type: "Batch 1: Date corrections", "Batch 2: Party name changes"',
      "   - By complexity: Start with simple text replacements, then tackle complex structural changes",
      '   - Sequential: "Batch 1: Pages 1-3", "Batch 2: Pages 4-6"',
      "",
      "3. **Read documentation and unpack**:",
      '   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the "Document Library" and "Tracked Change Patterns" sections.',
      "   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`",
      "   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.",
      "",
      "4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:",
      "   - Makes debugging easier (smaller batch = easier to isolate errors)",
      "   - Allows incremental progress",
      "   - Maintains efficiency (batch size of 3-10 changes works well)",
      "",
      "   **Suggested batch groupings:**",
      '   - By document section (e.g., "Section 3 changes", "Definitions", "Termination clause")',
      '   - By change type (e.g., "Date changes", "Party name updates", "Legal term replacements")',
      '   - By proximity (e.g., "Changes on pages 1-3", "Changes in first half of document")',
      "",
      "   For each batch of related changes:",
      "",
      "   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.",
      "",
      '   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **"Document Library"** section in ooxml.md for patterns.',
      "",
      "   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.",
      "",
      "5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:",
      "   ```bash",
      "   python ooxml/scripts/pack.py unpacked reviewed-document.docx",
      "   ```",
      "",
      "6. **Final verification**: Do a comprehensive check of the complete document:",
      "   - Convert final document to markdown:",
      "     ```bash",
      "     pandoc --track-changes=all reviewed-document.docx -o verification.md",
      "     ```",
      "   - Verify ALL changes were applied correctly:",
      "     ```bash",
      '     grep "original phrase" verification.md  # Should NOT find it',
      '     grep "replacement phrase" verification.md  # Should find it',
      "     ```",
      "   - Check that no unintended changes were introduced",
      "",
      "",
      "## Converting Documents to Images",
      "",
      "To visually analyze Word documents, convert them to images using a two-step process:",
      "",
      "1. **Convert DOCX to PDF**:",
      "   ```bash",
      "   soffice --headless --convert-to pdf document.docx",
      "   ```",
      "",
      "2. **Convert PDF pages to JPEG images**:",
      "   ```bash",
      "   pdftoppm -jpeg -r 150 document.pdf page",
      "   ```",
      "   This creates files like `page-1.jpg`, `page-2.jpg`, etc.",
      "",
      "Options:",
      "- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)",
      "- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)",
      "- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)",
      "- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)",
      "- `page`: Prefix for output files",
      "",
      "Example for specific range:",
      "```bash",
      "pdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5",
      "```",
      "",
      "## Code Style Guidelines",
      "**IMPORTANT**: When generating code for DOCX operations:",
      "- Write concise code",
      "- Avoid verbose variable names and redundant operations",
      "- Avoid unnecessary print statements",
      "",
      "## Dependencies",
      "",
      "Required dependencies (install if not available):",
      "",
      "- **pandoc**: `sudo apt-get install pandoc` (for text extraction)",
      "- **docx**: `npm install -g docx` (for creating new documents)",
      "- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)",
      "- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)",
      "- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
    ].join("\n"),
  },
  "enterprise-search-digest": {
    "SKILL.md": [
      "---",
      "name: enterprise-search-digest",
      "description: Generate a daily or weekly digest of activity across all connected sources",
      "metadata:",
      "  plugin: enterprise-search",
      "  type: command",
      '  plugin-version: "1.0.0"',
      '  argument-hint: "[--daily | --weekly | --since <date>]"',
      "---",
      "",
      "# Digest Command",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Scan recent activity across all connected sources and generate a structured digest highlighting what matters.",
      "",
      "## Instructions",
      "",
      "### 1. Parse Flags",
      "",
      "Determine the time window from the user's input:",
      "",
      "- `--daily` — Last 24 hours (default if no flag specified)",
      "- `--weekly` — Last 7 days",
      "",
      "The user may also specify a custom range:",
      "- `--since yesterday`",
      "- `--since Monday`",
      "- `--since 2025-01-20`",
      "",
      "### 2. Check Available Sources",
      "",
      "Identify which MCP sources are connected (same approach as the search command):",
      "",
      "- **~~chat** — channels, DMs, mentions",
      "- **~~email** — inbox, sent, threads",
      "- **~~cloud storage** — recently modified docs shared with user",
      "- **~~project tracker** — tasks assigned, completed, commented on",
      "- **~~CRM** — opportunity updates, account activity",
      "- **~~knowledge base** — recently updated wiki pages",
      "",
      "If no sources are connected, guide the user:",
      "```",
      "To generate a digest, you'll need at least one source connected.",
      "Check your MCP settings to add ~~chat, ~~email, ~~cloud storage, or other tools.",
      "```",
      "",
      "### 3. Gather Activity from Each Source",
      "",
      "**~~chat:**",
      "- Search for messages mentioning the user (`to:me`)",
      "- Check channels the user is in for recent activity",
      "- Look for threads the user participated in",
      "- Identify new messages in key channels",
      "",
      "**~~email:**",
      "- Search recent inbox messages",
      "- Identify threads with new replies",
      "- Flag emails with action items or questions directed at the user",
      "",
      "**~~cloud storage:**",
      "- Find documents recently modified or shared with the user",
      "- Note new comments on docs the user owns or collaborates on",
      "",
      "**~~project tracker:**",
      "- Tasks assigned to the user (new or updated)",
      "- Tasks completed by others that the user follows",
      "- Comments on tasks the user is involved with",
      "",
      "**~~CRM:**",
      "- Opportunity stage changes",
      "- New activities logged on accounts the user owns",
      "- Updated contacts or accounts",
      "",
      "**~~knowledge base:**",
      "- Recently updated documents in relevant collections",
      "- New documents created in watched areas",
      "",
      "### 4. Identify Key Items",
      "",
      "From all gathered activity, extract and categorize:",
      "",
      "**Action Items:**",
      '- Direct requests made to the user ("Can you...", "Please...", "@user")',
      "- Tasks assigned or due soon",
      "- Questions awaiting the user's response",
      "- Review requests",
      "",
      "**Decisions:**",
      "- Conclusions reached in threads or emails",
      "- Approvals or rejections",
      "- Policy or direction changes",
      "",
      "**Mentions:**",
      "- Times the user was mentioned or referenced",
      "- Discussions about the user's projects or areas",
      "",
      "**Updates:**",
      "- Status changes on projects the user follows",
      "- Document updates in the user's domain",
      "- Completed items the user was waiting on",
      "",
      "### 5. Group by Topic",
      "",
      "Organize the digest by topic, project, or theme rather than by source. Merge related activity across sources:",
      "",
      "```",
      "## Headquarter Aurora",
      "- ~~chat: Design review thread concluded — team chose Option B (#design, Tuesday)",
      "- ~~email: Sarah sent updated spec incorporating feedback (Wednesday)",
      '- ~~cloud storage: "Aurora API Spec v3" updated by Sarah (Wednesday)',
      "- ~~project tracker: 3 tasks moved to In Progress, 2 completed",
      "",
      "## Budget Planning",
      "- ~~email: Finance team requesting Q2 projections by Friday",
      "- ~~chat: Todd shared template in #finance (Monday)",
      '- ~~cloud storage: "Q2 Budget Template" shared with you (Monday)',
      "```",
      "",
      "### 6. Format the Digest",
      "",
      "Structure the output clearly:",
      "",
      "```",
      "# [Daily/Weekly] Digest — [Date or Date Range]",
      "",
      "Sources scanned: ~~chat, ~~email, ~~cloud storage, [others]",
      "",
      "## Action Items (X items)",
      "- [ ] [Action item 1] — from [person], [source] ([date])",
      "- [ ] [Action item 2] — from [person], [source] ([date])",
      "",
      "## Decisions Made",
      "- [Decision 1] — [context] ([source], [date])",
      "- [Decision 2] — [context] ([source], [date])",
      "",
      "## [Topic/Headquarter Group 1]",
      "[Activity summary with source attribution]",
      "",
      "## [Topic/Headquarter Group 2]",
      "[Activity summary with source attribution]",
      "",
      "## Mentions",
      "- [Mention context] — [source] ([date])",
      "",
      "## Documents Updated",
      "- [Doc name] — [who modified, what changed] ([date])",
      "```",
      "",
      "### 7. Handle Unavailable Sources",
      "",
      "If any source fails or is unreachable:",
      "```",
      "Note: Could not reach [source name] for this digest.",
      "The following sources were included: [list of successful sources].",
      "```",
      "",
      "Do not let one failed source prevent the digest from being generated. Produce the best digest possible from available sources.",
      "",
      "### 8. Summary Stats",
      "",
      "End with a quick summary:",
      "```",
      "---",
      "[X] action items · [Y] decisions · [Z] mentions · [W] doc updates",
      "Across [N] sources · Covering [time range]",
      "```",
      "",
      "## Notes",
      "",
      "- Default to `--daily` if no flag is specified",
      "- Group by topic/project, not by source — users care about what happened, not where it happened",
      "- Action items should always be listed first — they are the most actionable part of a digest",
      "- Deduplicate cross-source activity (same decision in ~~chat and email = one entry)",
      "- For weekly digests, prioritize significance over completeness — highlight what matters, skip noise",
      "- If the user has a memory system (CLAUDE.md), use it to decode people names and project references",
      "- Include enough context in each item that the user can decide whether to dig deeper without clicking through",
      "",
    ].join("\n"),
  },
  "enterprise-search-knowledge-synthesis": {
    "SKILL.md": [
      "---",
      "name: enterprise-search-knowledge-synthesis",
      "description: Combines search results from multiple sources into coherent, deduplicated answers with source attribution. Handles confidence scoring based on freshness and authority, and summarizes large result sets effectively.",
      "metadata:",
      "  plugin: enterprise-search",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Knowledge Synthesis",
      "",
      "The last mile of enterprise search. Takes raw results from multiple sources and produces a coherent, trustworthy answer.",
      "",
      "## The Goal",
      "",
      "Transform this:",
      "```",
      "~~chat result: \"Sarah said in #eng: 'let's go with REST, GraphQL is overkill for our use case'\"",
      '~~email result: "Subject: API Decision — Sarah\'s email confirming REST approach with rationale"',
      '~~cloud storage result: "API Design Doc v3 — updated section 2 to reflect REST decision"',
      '~~project tracker result: "Task: Finalize API approach — marked complete by Sarah"',
      "```",
      "",
      "Into this:",
      "```",
      "The team decided to go with REST over GraphQL for the API redesign. Sarah made the",
      "call, noting that GraphQL was overkill for the current use case. This was discussed",
      "in #engineering on Tuesday, confirmed via email Wednesday, and the design doc has",
      "been updated to reflect the decision. The related ~~project tracker task is marked complete.",
      "",
      "Sources:",
      "- ~~chat: #engineering thread (Jan 14)",
      '- ~~email: "API Decision" from Sarah (Jan 15)',
      '- ~~cloud storage: "API Design Doc v3" (updated Jan 15)',
      '- ~~project tracker: "Finalize API approach" (completed Jan 15)',
      "```",
      "",
      "## Deduplication",
      "",
      "### Cross-Source Deduplication",
      "",
      "The same information often appears in multiple places. Identify and merge duplicates:",
      "",
      "**Signals that results are about the same thing:**",
      "- Same or very similar text content",
      "- Same author/sender",
      "- Timestamps within a short window (same day or adjacent days)",
      "- References to the same entity (project name, document, decision)",
      '- One source references another ("as discussed in ~~chat", "per the email", "see the doc")',
      "",
      "**How to merge:**",
      "- Combine into a single narrative item",
      "- Cite all sources where it appeared",
      "- Use the most complete version as the primary text",
      "- Add unique details from each source",
      "",
      "### Deduplication Priority",
      "",
      "When the same information exists in multiple sources, prefer:",
      "```",
      "1. The most complete version (fullest context)",
      "2. The most authoritative source (official doc > chat)",
      "3. The most recent version (latest update wins for evolving info)",
      "```",
      "",
      "### What NOT to Deduplicate",
      "",
      "Keep as separate items when:",
      "- The same topic is discussed but with different conclusions",
      "- Different people express different viewpoints",
      "- The information evolved meaningfully between sources (v1 vs v2 of a decision)",
      "- Different time periods are represented",
      "",
      "## Citation and Source Attribution",
      "",
      "Every claim in the synthesized answer must be attributable to a source.",
      "",
      "### Attribution Format",
      "",
      "Inline for direct references:",
      "```",
      "Sarah confirmed the REST approach in her email on Wednesday.",
      'The design doc was updated to reflect this (~~cloud storage: "API Design Doc v3").',
      "```",
      "",
      "Source list at the end for completeness:",
      "```",
      "Sources:",
      "- ~~chat: #engineering discussion (Jan 14) — initial decision thread",
      '- ~~email: "API Decision" from Sarah Chen (Jan 15) — formal confirmation',
      '- ~~cloud storage: "API Design Doc v3" last modified Jan 15 — updated specification',
      "```",
      "",
      "### Attribution Rules",
      "",
      "- Always name the source type (~~chat, ~~email, ~~cloud storage, etc.)",
      "- Include the specific location (channel, folder, thread)",
      "- Include the date or relative time",
      "- Include the author when relevant",
      "- Include document/thread titles when available",
      "- For ~~chat, note the channel name",
      "- For ~~email, note the subject line and sender",
      "- For ~~cloud storage, note the document title",
      "",
      "## Confidence Levels",
      "",
      "Not all results are equally trustworthy. Assess confidence based on:",
      "",
      "### Freshness",
      "",
      "| Recency | Confidence impact |",
      "|---------|------------------|",
      "| Today / yesterday | High confidence for current state |",
      "| This week | Good confidence |",
      "| This month | Moderate — things may have changed |",
      "| Older than a month | Lower confidence — flag as potentially outdated |",
      "",
      "For status queries, heavily weight freshness. For policy/factual queries, freshness matters less.",
      "",
      "### Authority",
      "",
      "| Source type | Authority level |",
      "|-------------|----------------|",
      "| Official wiki / knowledge base | Highest — curated, maintained |",
      "| Shared documents (final versions) | High — intentionally published |",
      "| Email announcements | High — formal communication |",
      "| Meeting notes | Moderate-high — may be incomplete |",
      "| Chat messages (thread conclusions) | Moderate — informal but real-time |",
      "| Chat messages (mid-thread) | Lower — may not reflect final position |",
      "| Draft documents | Low — not finalized |",
      "| Task comments | Contextual — depends on commenter |",
      "",
      "### Expressing Confidence",
      "",
      "When confidence is high (multiple fresh, authoritative sources agree):",
      "```",
      "The team decided to use REST for the API redesign. [direct statement]",
      "```",
      "",
      "When confidence is moderate (single source or somewhat dated):",
      "```",
      "Based on the discussion in #engineering last month, the team was leaning",
      "toward REST for the API redesign. This may have evolved since then.",
      "```",
      "",
      "When confidence is low (old data, informal source, or conflicting signals):",
      "```",
      "I found a reference to an API migration discussion from three months ago",
      "in ~~chat, but I couldn't find a formal decision document. The information",
      "may be outdated. You might want to check with the team for current status.",
      "```",
      "",
      "### Conflicting Information",
      "",
      "When sources disagree:",
      "```",
      "I found conflicting information about the API approach:",
      "- The ~~chat discussion on Jan 10 suggested GraphQL",
      "- But Sarah's email on Jan 15 confirmed REST",
      "- The design doc (updated Jan 15) reflects REST",
      "",
      "The most recent sources indicate REST was the final decision,",
      "but the earlier ~~chat discussion explored GraphQL first.",
      "```",
      "",
      "Always surface conflicts rather than silently picking one version.",
      "",
      "## Summarization Strategies",
      "",
      "### For Small Result Sets (1-5 results)",
      "",
      "Present each result with context. No summarization needed — give the user everything:",
      "```",
      "[Direct answer synthesized from results]",
      "",
      "[Detail from source 1]",
      "[Detail from source 2]",
      "",
      "Sources: [full attribution]",
      "```",
      "",
      "### For Medium Result Sets (5-15 results)",
      "",
      "Group by theme and summarize each group:",
      "```",
      "[Overall answer]",
      "",
      "Theme 1: [summary of related results]",
      "Theme 2: [summary of related results]",
      "",
      "Key sources: [top 3-5 most relevant sources]",
      "Full results: [count] items found across [sources]",
      "```",
      "",
      "### For Large Result Sets (15+ results)",
      "",
      "Provide a high-level synthesis with the option to drill down:",
      "```",
      "[Overall answer based on most relevant results]",
      "",
      "Summary:",
      "- [Key finding 1] (supported by N sources)",
      "- [Key finding 2] (supported by N sources)",
      "- [Key finding 3] (supported by N sources)",
      "",
      "Top sources:",
      "- [Most authoritative/relevant source]",
      "- [Second most relevant]",
      "- [Third most relevant]",
      "",
      "Found [total count] results across [source list].",
      "Want me to dig deeper into any specific aspect?",
      "```",
      "",
      "### Summarization Rules",
      "",
      "- Lead with the answer, not the search process",
      "- Do not list raw results — synthesize them into narrative",
      "- Group related items from different sources together",
      "- Preserve important nuance and caveats",
      "- Include enough detail that the user can decide whether to dig deeper",
      "- Always offer to provide more detail if the result set was large",
      "",
      "## Synthesis Workflow",
      "",
      "```",
      "[Raw results from all sources]",
      "          ↓",
      "[1. Deduplicate — merge same info from different sources]",
      "          ↓",
      "[2. Cluster — group related results by theme/topic]",
      "          ↓",
      "[3. Rank — order clusters and items by relevance to query]",
      "          ↓",
      "[4. Assess confidence — freshness × authority × agreement]",
      "          ↓",
      "[5. Synthesize — produce narrative answer with attribution]",
      "          ↓",
      "[6. Format — choose appropriate detail level for result count]",
      "          ↓",
      "[Coherent answer with sources]",
      "```",
      "",
      "## Anti-Patterns",
      "",
      "**Do not:**",
      '- List results source by source ("From ~~chat: ... From ~~email: ... From ~~cloud storage: ...")',
      "- Include irrelevant results just because they matched a keyword",
      "- Bury the answer under methodology explanation",
      "- Present conflicting info without flagging the conflict",
      "- Omit source attribution",
      "- Present uncertain information with the same confidence as well-supported facts",
      "- Summarize so aggressively that useful detail is lost",
      "",
      "**Do:**",
      "- Lead with the answer",
      "- Group by topic, not by source",
      "- Flag confidence levels when appropriate",
      "- Surface conflicts explicitly",
      "- Attribute all claims to sources",
      "- Offer to go deeper when result sets are large",
      "",
    ].join("\n"),
  },
  "enterprise-search-search": {
    "SKILL.md": [
      "---",
      "name: enterprise-search-search",
      "description: Search across all connected sources in one query",
      "metadata:",
      "  plugin: enterprise-search",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <query>",
      "---",
      "",
      "# Search Command",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Search across all connected MCP sources in a single query. Decompose the user's question, run parallel searches, and synthesize results.",
      "",
      "## Instructions",
      "",
      "### 1. Check Available Sources",
      "",
      "Before searching, determine which MCP sources are available. Attempt to identify connected tools from the available tool list. Common sources:",
      "",
      "- **~~chat** — chat platform tools",
      "- **~~email** — email tools",
      "- **~~cloud storage** — cloud storage tools",
      "- **~~project tracker** — project tracking tools",
      "- **~~CRM** — CRM tools",
      "- **~~knowledge base** — knowledge base tools",
      "",
      "If no MCP sources are connected:",
      "```",
      "To search across your tools, you'll need to connect at least one source.",
      "Check your MCP settings to add ~~chat, ~~email, ~~cloud storage, or other tools.",
      "",
      "Supported sources: ~~chat, ~~email, ~~cloud storage, ~~project tracker, ~~CRM, ~~knowledge base,",
      "and any other MCP-connected service.",
      "```",
      "",
      "### 2. Parse the User's Query",
      "",
      "Analyze the search query to understand:",
      "",
      "- **Intent**: What is the user looking for? (a decision, a document, a person, a status update, a conversation)",
      "- **Entities**: People, projects, teams, tools mentioned",
      '- **Time constraints**: Recency signals ("this week", "last month", specific dates)',
      '- **Source hints**: References to specific tools ("in ~~chat", "that email", "the doc")',
      "- **Filters**: Extract explicit filters from the query:",
      "  - `from:` — Filter by sender/author",
      "  - `in:` — Filter by channel, folder, or location",
      "  - `after:` — Only results after this date",
      "  - `before:` — Only results before this date",
      "  - `type:` — Filter by content type (message, email, doc, thread, file)",
      "",
      "### 3. Decompose into Sub-Queries",
      "",
      "For each available source, create a targeted sub-query using that source's native search syntax:",
      "",
      "**~~chat:**",
      "- Use available search and read tools for your chat platform",
      "- Translate filters: `from:` maps to sender, `in:` maps to channel/room, dates map to time range filters",
      "- Use natural language queries for semantic search when appropriate",
      "- Use keyword queries for exact matches",
      "",
      "**~~email:**",
      "- Use available email search tools",
      "- Translate filters: `from:` maps to sender, dates map to time range filters",
      "- Map `type:` to attachment filters or subject-line searches as appropriate",
      "",
      "**~~cloud storage:**",
      "- Use available file search tools",
      "- Translate to file query syntax: name contains, full text contains, modified date, file type",
      "- Consider both file names and content",
      "",
      "**~~project tracker:**",
      "- Use available task search or typeahead tools",
      "- Map to task text search, assignee filters, date filters, project filters",
      "",
      "**~~CRM:**",
      "- Use available CRM query tools",
      "- Search across Account, Contact, Opportunity, and other relevant objects",
      "",
      "**~~knowledge base:**",
      "- Use semantic search for conceptual questions",
      "- Use keyword search for exact matches",
      "",
      "### 4. Execute Searches in Parallel",
      "",
      "Run all sub-queries simultaneously across available sources. Do not wait for one source before searching another.",
      "",
      "For each source:",
      "- Execute the translated query",
      "- Capture results with metadata (timestamps, authors, links, source type)",
      "- Note any sources that fail or return errors — do not let one failure block others",
      "",
      "### 5. Rank and Deduplicate Results",
      "",
      "**Deduplication:**",
      "- Identify the same information appearing across sources (e.g., a decision discussed in ~~chat AND confirmed via email)",
      "- Group related results together rather than showing duplicates",
      "- Prefer the most authoritative or complete version",
      "",
      "**Ranking factors:**",
      "- **Relevance**: How well does the result match the query intent?",
      "- **Freshness**: More recent results rank higher for status/decision queries",
      '- **Authority**: Official docs > wiki > chat messages for factual questions; conversations > docs for "what did we discuss" queries',
      "- **Completeness**: Results with more context rank higher",
      "",
      "### 6. Present Unified Results",
      "",
      "Format the response as a synthesized answer, not a raw list of results:",
      "",
      "**For factual/decision queries:**",
      "```",
      "[Direct answer to the question]",
      "",
      "Sources:",
      "- [Source 1: brief description] (~~chat, #channel, date)",
      "- [Source 2: brief description] (~~email, from person, date)",
      "- [Source 3: brief description] (~~cloud storage, doc name, last modified)",
      "```",
      "",
      '**For exploratory queries ("what do we know about X"):**',
      "```",
      "[Synthesized summary combining information from all sources]",
      "",
      "Found across:",
      "- ~~chat: X relevant messages in Y channels",
      "- ~~email: X relevant threads",
      "- ~~cloud storage: X related documents",
      "- [Other sources as applicable]",
      "",
      "Key sources:",
      "- [Most important source with link/reference]",
      "- [Second most important source]",
      "```",
      "",
      '**For "find" queries (looking for a specific thing):**',
      "```",
      "[The thing they're looking for, with direct reference]",
      "",
      "Also found:",
      "- [Related items from other sources]",
      "```",
      "",
      "### 7. Handle Edge Cases",
      "",
      "**Ambiguous queries:**",
      "If the query could mean multiple things, ask one clarifying question before searching:",
      "```",
      '"API redesign" could refer to a few things. Are you looking for:',
      "1. The REST API v2 redesign (Headquarter Aurora)",
      "2. The internal SDK API changes",
      "3. Something else?",
      "```",
      "",
      "**No results:**",
      "```",
      'I couldn\'t find anything matching "[query]" across [list of sources searched].',
      "",
      "Try:",
      '- Broader terms (e.g., "database" instead of "PostgreSQL migration")',
      "- Different time range (currently searching [time range])",
      "- Checking if the relevant source is connected (currently searching: [sources])",
      "```",
      "",
      "**Partial results (some sources failed):**",
      "```",
      "[Results from successful sources]",
      "",
      "Note: I couldn't reach [failed source(s)] during this search.",
      "Results above are from [successful sources] only.",
      "```",
      "",
      "## Notes",
      "",
      "- Always search multiple sources in parallel — never sequentially",
      "- Synthesize results into answers, do not just list raw search results",
      "- Include source attribution so users can dig deeper",
      "- Respect the user's filter syntax and apply it appropriately per source",
      "- When a query mentions a specific person, search for their messages/docs/mentions across all sources",
      "- For time-sensitive queries, prioritize recency in ranking",
      "- If only one source is connected, still provide useful results from that source",
      "",
    ].join("\n"),
  },
  "enterprise-search-search-strategy": {
    "SKILL.md": [
      "---",
      "name: enterprise-search-search-strategy",
      "description: Query decomposition and multi-source search orchestration. Breaks natural language questions into targeted searches per source, translates queries into source-specific syntax, ranks results by relevance, and handles ambiguity and fallback strategies.",
      "metadata:",
      "  plugin: enterprise-search",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Search Strategy",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../../CONNECTORS.md).",
      "",
      "The core intelligence behind enterprise search. Transforms a single natural language question into parallel, source-specific searches and produces ranked, deduplicated results.",
      "",
      "## The Goal",
      "",
      "Turn this:",
      "```",
      '"What did we decide about the API migration timeline?"',
      "```",
      "",
      "Into targeted searches across every connected source:",
      "```",
      '~~chat:  "API migration timeline decision" (semantic) + "API migration" in:#engineering after:2025-01-01',
      '~~knowledge base: semantic search "API migration timeline decision"',
      '~~project tracker:  text search "API migration" in relevant workspace',
      "```",
      "",
      "Then synthesize the results into a single coherent answer.",
      "",
      "## Query Decomposition",
      "",
      "### Step 1: Identify Query Type",
      "",
      "Classify the user's question to determine search strategy:",
      "",
      "| Query Type | Example | Strategy |",
      "|-----------|---------|----------|",
      '| **Decision** | "What did we decide about X?" | Prioritize conversations (~~chat, email), look for conclusion signals |',
      '| **Status** | "What\'s the status of Headquarter Y?" | Prioritize recent activity, task trackers, status updates |',
      '| **Document** | "Where\'s the spec for Z?" | Prioritize Drive, wiki, shared docs |',
      '| **Person** | "Who\'s working on X?" | Search task assignments, message authors, doc collaborators |',
      '| **Factual** | "What\'s our policy on X?" | Prioritize wiki, official docs, then confirmatory conversations |',
      '| **Temporal** | "When did X happen?" | Search with broad date range, look for timestamps |',
      '| **Exploratory** | "What do we know about X?" | Broad search across all sources, synthesize |',
      "",
      "### Step 2: Extract Search Components",
      "",
      "From the query, extract:",
      "",
      "- **Keywords**: Core terms that must appear in results",
      "- **Entities**: People, projects, teams, tools (use memory system if available)",
      "- **Intent signals**: Decision words, status words, temporal markers",
      "- **Constraints**: Time ranges, source hints, author filters",
      "- **Negations**: Things to exclude",
      "",
      "### Step 3: Generate Sub-Queries Per Source",
      "",
      "For each available source, create one or more targeted queries:",
      "",
      "**Prefer semantic search** for:",
      '- Conceptual questions ("What do we think about...")',
      "- Questions where exact keywords are unknown",
      "- Exploratory queries",
      "",
      "**Prefer keyword search** for:",
      "- Known terms, project names, acronyms",
      "- Exact phrases the user quoted",
      "- Filter-heavy queries (from:, in:, after:)",
      "",
      "**Generate multiple query variants** when the topic might be referred to differently:",
      "```",
      'User: "Kubernetes setup"',
      'Queries: "Kubernetes", "k8s", "cluster", "container orchestration"',
      "```",
      "",
      "## Source-Specific Query Translation",
      "",
      "### ~~chat",
      "",
      "**Semantic search** (natural language questions):",
      "```",
      'query: "What is the status of project aurora?"',
      "```",
      "",
      "**Keyword search:**",
      "```",
      'query: "project aurora status update"',
      'query: "aurora in:#engineering after:2025-01-15"',
      'query: "from:<@UserID> aurora"',
      "```",
      "",
      "**Filter mapping:**",
      "| Enterprise filter | ~~chat syntax |",
      "|------------------|--------------|",
      "| `from:sarah` | `from:sarah` or `from:<@USERID>` |",
      "| `in:engineering` | `in:engineering` |",
      "| `after:2025-01-01` | `after:2025-01-01` |",
      "| `before:2025-02-01` | `before:2025-02-01` |",
      "| `type:thread` | `is:thread` |",
      "| `type:file` | `has:file` |",
      "",
      "### ~~knowledge base (Wiki)",
      "",
      "**Semantic search** — Use for conceptual queries:",
      "```",
      'descriptive_query: "API migration timeline and decision rationale"',
      "```",
      "",
      "**Keyword search** — Use for exact terms:",
      "```",
      'query: "API migration"',
      'query: "\\"API migration timeline\\""  (exact phrase)',
      "```",
      "",
      "### ~~project tracker",
      "",
      "**Task search:**",
      "```",
      'text: "API migration"',
      "workspace: [workspace_id]",
      "completed: false  (for status queries)",
      'assignee_any: "me"  (for "my tasks" queries)',
      "```",
      "",
      "**Filter mapping:**",
      "| Enterprise filter | ~~project tracker parameter |",
      "|------------------|----------------|",
      "| `from:sarah` | `assignee_any` or `created_by_any` |",
      '| `after:2025-01-01` | `modified_on_after: "2025-01-01"` |',
      '| `type:milestone` | `resource_subtype: "milestone"` |',
      "",
      "## Result Ranking",
      "",
      "### Relevance Scoring",
      "",
      "Score each result on these factors (weighted by query type):",
      "",
      "| Factor | Weight (Decision) | Weight (Status) | Weight (Document) | Weight (Factual) |",
      "|--------|-------------------|------------------|--------------------|-------------------|",
      "| Keyword match | 0.3 | 0.2 | 0.4 | 0.3 |",
      "| Freshness | 0.3 | 0.4 | 0.2 | 0.1 |",
      "| Authority | 0.2 | 0.1 | 0.3 | 0.4 |",
      "| Completeness | 0.2 | 0.3 | 0.1 | 0.2 |",
      "",
      "### Authority Hierarchy",
      "",
      "Depends on query type:",
      "",
      "**For factual/policy questions:**",
      "```",
      "Wiki/Official docs > Shared documents > Email announcements > Chat messages",
      "```",
      "",
      '**For "what happened" / decision questions:**',
      "```",
      "Meeting notes > Thread conclusions > Email confirmations > Chat messages",
      "```",
      "",
      "**For status questions:**",
      "```",
      "Task tracker > Recent chat > Status docs > Email updates",
      "```",
      "",
      "## Handling Ambiguity",
      "",
      "When a query is ambiguous, prefer asking one focused clarifying question over guessing:",
      "",
      "```",
      'Ambiguous: "search for the migration"',
      '→ "I found references to a few migrations. Are you looking for:',
      "   1. The database migration (Headquarter Phoenix)",
      "   2. The cloud migration (AWS → GCP)",
      '   3. The email migration (Exchange → O365)"',
      "```",
      "",
      "Only ask for clarification when:",
      "- There are genuinely distinct interpretations that would produce very different results",
      "- The ambiguity would significantly affect which sources to search",
      "",
      "Do NOT ask for clarification when:",
      "- The query is clear enough to produce useful results",
      "- Minor ambiguity can be resolved by returning results from multiple interpretations",
      "",
      "## Fallback Strategies",
      "",
      "When a source is unavailable or returns no results:",
      "",
      "1. **Source unavailable**: Skip it, search remaining sources, note the gap",
      "2. **No results from a source**: Try broader query terms, remove date filters, try alternate keywords",
      "3. **All sources return nothing**: Suggest query modifications to the user",
      "4. **Rate limited**: Note the limitation, return results from other sources, suggest retrying later",
      "",
      "### Query Broadening",
      "",
      "If initial queries return too few results:",
      "```",
      'Original: "PostgreSQL migration Q2 timeline decision"',
      'Broader:  "PostgreSQL migration"',
      'Broader:  "database migration"',
      'Broadest: "migration"',
      "```",
      "",
      "Remove constraints in this order:",
      "1. Date filters (search all time)",
      "2. Source/location filters",
      "3. Less important keywords",
      "4. Keep only core entity/topic terms",
      "",
      "## Parallel Execution",
      "",
      "Always execute searches across sources in parallel, never sequentially. The total search time should be roughly equal to the slowest single source, not the sum of all sources.",
      "",
      "```",
      "[User query]",
      "     ↓ decompose",
      "[~~chat query] [~~email query] [~~cloud storage query] [Wiki query] [~~project tracker query]",
      "     ↓            ↓            ↓              ↓            ↓",
      "  (parallel execution)",
      "     ↓",
      "[Merge + Rank + Deduplicate]",
      "     ↓",
      "[Synthesized answer]",
      "```",
      "",
    ].join("\n"),
  },
  "enterprise-search-source-management": {
    "SKILL.md": [
      "---",
      "name: enterprise-search-source-management",
      "description: Manages connected MCP sources for enterprise search. Detects available sources, guides users to connect new ones, handles source priority ordering, and manages rate limiting awareness.",
      "metadata:",
      "  plugin: enterprise-search",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Source Management",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../../CONNECTORS.md).",
      "",
      "Knows what sources are available, helps connect new ones, and manages how sources are queried.",
      "",
      "## Checking Available Sources",
      "",
      "Determine which MCP sources are connected by checking available tools. Each source corresponds to a set of MCP tools:",
      "",
      "| Source | Key capabilities |",
      "|--------|-----------------|",
      "| **~~chat** | Search messages, read channels and threads |",
      "| **~~email** | Search messages, read individual emails |",
      "| **~~cloud storage** | Search files, fetch document contents |",
      "| **~~project tracker** | Search tasks, typeahead search |",
      "| **~~CRM** | Query records (accounts, contacts, opportunities) |",
      "| **~~knowledge base** | Semantic search, keyword search |",
      "",
      "If a tool prefix is available, the source is connected and searchable.",
      "",
      "## Guiding Users to Connect Sources",
      "",
      "When a user searches but has few or no sources connected:",
      "",
      "```",
      "You currently have [N] source(s) connected: [list].",
      "",
      "To expand your search, you can connect additional sources in your MCP settings:",
      "- ~~chat — messages, threads, channels",
      "- ~~email — emails, conversations, attachments",
      "- ~~cloud storage — docs, sheets, slides",
      "- ~~project tracker — tasks, projects, milestones",
      "- ~~CRM — accounts, contacts, opportunities",
      "- ~~knowledge base — wiki pages, knowledge base articles",
      "",
      "The more sources you connect, the more complete your search results.",
      "```",
      "",
      "When a user asks about a specific tool that is not connected:",
      "",
      "```",
      "[Tool name] isn't currently connected. To add it:",
      "1. Open your MCP settings",
      "2. Add the [tool] MCP server configuration",
      "3. Authenticate when prompted",
      "",
      "Once connected, it will be automatically included in future searches.",
      "```",
      "",
      "## Source Priority Ordering",
      "",
      "Different query types benefit from searching certain sources first. Use these priorities to weight results, not to skip sources:",
      "",
      "### By Query Type",
      "",
      '**Decision queries** ("What did we decide..."):',
      "```",
      "1. ~~chat (conversations where decisions happen)",
      "2. ~~email (decision confirmations, announcements)",
      "3. ~~cloud storage (meeting notes, decision logs)",
      "4. Wiki (if decisions are documented)",
      "5. Task tracker (if decisions are captured in tasks)",
      "```",
      "",
      '**Status queries** ("What\'s the status of..."):',
      "```",
      "1. Task tracker (~~project tracker — authoritative status)",
      "2. ~~chat (real-time discussion)",
      "3. ~~cloud storage (status docs, reports)",
      "4. ~~email (status update emails)",
      "5. Wiki (project pages)",
      "```",
      "",
      '**Document queries** ("Where\'s the doc for..."):',
      "```",
      "1. ~~cloud storage (primary doc storage)",
      "2. Wiki / ~~knowledge base (knowledge base)",
      "3. ~~email (docs shared via email)",
      "4. ~~chat (docs shared in channels)",
      "5. Task tracker (docs linked to tasks)",
      "```",
      "",
      '**People queries** ("Who works on..." / "Who knows about..."):',
      "```",
      "1. ~~chat (message authors, channel members)",
      "2. Task tracker (task assignees)",
      "3. ~~cloud storage (doc authors, collaborators)",
      "4. ~~CRM (account owners, contacts)",
      "5. ~~email (email participants)",
      "```",
      "",
      '**Factual/Policy queries** ("What\'s our policy on..."):',
      "```",
      "1. Wiki / ~~knowledge base (official documentation)",
      "2. ~~cloud storage (policy docs, handbooks)",
      "3. ~~email (policy announcements)",
      "4. ~~chat (policy discussions)",
      "```",
      "",
      "### Default Priority (General Queries)",
      "",
      "When query type is unclear:",
      "```",
      "1. ~~chat (highest volume, most real-time)",
      "2. ~~email (formal communications)",
      "3. ~~cloud storage (documents and files)",
      "4. Wiki / ~~knowledge base (structured knowledge)",
      "5. Task tracker (work items)",
      "6. CRM (customer data)",
      "```",
      "",
      "## Rate Limiting Awareness",
      "",
      "MCP sources may have rate limits. Handle them gracefully:",
      "",
      "### Detection",
      "",
      "Rate limit responses typically appear as:",
      "- HTTP 429 responses",
      '- Error messages mentioning "rate limit", "too many requests", or "quota exceeded"',
      "- Throttled or delayed responses",
      "",
      "### Handling",
      "",
      "When a source is rate limited:",
      "",
      "1. **Do not retry immediately** — respect the limit",
      "2. **Continue with other sources** — do not block the entire search",
      "3. **Inform the user**:",
      "```",
      "Note: [Source] is temporarily rate limited. Results below are from",
      "[other sources]. You can retry in a few minutes to include [source].",
      "```",
      "4. **For digests** — if rate limited mid-scan, note which time range was covered before the limit hit",
      "",
      "### Prevention",
      "",
      "- Avoid unnecessary API calls — check if the source is likely to have relevant results before querying",
      "- Use targeted queries over broad scans when possible",
      "- For digests, batch requests where the API supports it",
      "- Cache awareness: if a search was just run, avoid re-running the same query immediately",
      "",
      "## Source Health",
      "",
      "Track source availability during a session:",
      "",
      "```",
      "Source Status:",
      "  ~~chat:        ✓ Available",
      "  ~~email:        ✓ Available",
      "  ~~cloud storage:  ✓ Available",
      "  ~~project tracker:        ✗ Not connected",
      "  ~~CRM:   ✗ Not connected",
      "  ~~knowledge base:      ⚠ Rate limited (retry in 2 min)",
      "```",
      "",
      "When reporting search results, include which sources were searched so the user knows the scope of the answer.",
      "",
      "## Adding Custom Sources",
      "",
      "The enterprise search plugin works with any MCP-connected source. As new MCP servers become available, they can be added to the `.mcp.json` configuration. The search and digest commands will automatically detect and include new sources based on available tools.",
      "",
      "To add a new source:",
      "1. Add the MCP server configuration to `.mcp.json`",
      "2. Authenticate if required",
      "3. The source will be included in subsequent searches automatically",
      "",
    ].join("\n"),
  },
  "finance-audit-support": {
    "SKILL.md": [
      "---",
      "name: finance-audit-support",
      "description: Support SOX 404 compliance with control testing methodology, sample selection, and documentation standards. Use when generating testing workpapers, selecting audit samples, classifying control deficiencies, or preparing for internal or external audits.",
      "metadata:",
      "  plugin: finance",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Audit Support",
      "",
      '**Important**: This skill assists with SOX compliance workflows but does not provide audit or legal advice. All testing workpapers and assessments should be reviewed by qualified financial professionals. While "significance" and "materiality" are context-specific concepts that are ultimately assessed by auditors, this skill is intended to assist professionals in the creation and evaluation of effective internal controls and documentation for audits.',
      "",
      "SOX 404 control testing methodology, sample selection approaches, testing documentation standards, control deficiency classification, and common control types.",
      "",
      "## SOX 404 Control Testing Methodology",
      "",
      "### Overview",
      "",
      "SOX Section 404 requires management to assess the effectiveness of internal controls over financial reporting (ICFR). This involves:",
      "",
      "1. **Scoping:** Identify significant accounts and relevant assertions",
      "2. **Risk assessment:** Evaluate the risk of material misstatement for each significant account",
      "3. **Control identification:** Document the controls that address each risk",
      "4. **Testing:** Test the design and operating effectiveness of key controls",
      "5. **Evaluation:** Assess whether any deficiencies exist and their severity",
      "6. **Reporting:** Document the assessment and any material weaknesses",
      "",
      "### Scoping Significant Accounts",
      "",
      "An account is significant if there is more than a remote likelihood that it could contain a misstatement that is material (individually or in aggregate).",
      "",
      "**Quantitative factors:**",
      "- Account balance exceeds materiality threshold (typically 3-5% of a key benchmark)",
      "- Transaction volume is high, increasing the risk of error",
      "- Account is subject to significant estimates or judgment",
      "",
      "**Qualitative factors:**",
      "- Account involves complex accounting (revenue recognition, derivatives, pensions)",
      "- Account is susceptible to fraud (cash, revenue, related-party transactions)",
      "- Account has had prior misstatements or audit adjustments",
      "- Account involves significant management judgment or estimates",
      "- New account or significantly changed process",
      "",
      "### Relevant Assertions by Account Type",
      "",
      "| Account Type | Key Assertions |",
      "|-------------|---------------|",
      "| Revenue | Occurrence, Completeness, Accuracy, Cut-off |",
      "| Accounts Receivable | Existence, Valuation (allowance), Rights |",
      "| Inventory | Existence, Valuation, Completeness |",
      "| Fixed Assets | Existence, Valuation, Completeness, Rights |",
      "| Accounts Payable | Completeness, Accuracy, Existence |",
      "| Accrued Liabilities | Completeness, Valuation, Accuracy |",
      "| Equity | Completeness, Accuracy, Presentation |",
      "| Financial Close/Reporting | Presentation, Accuracy, Completeness |",
      "",
      "### Design Effectiveness vs Operating Effectiveness",
      "",
      "**Design effectiveness:** Is the control properly designed to prevent or detect a material misstatement in the relevant assertion?",
      "- Evaluated through walkthroughs (trace a transaction end-to-end through the process)",
      "- Confirm the control is placed at the right point in the process",
      "- Confirm the control addresses the identified risk",
      "- Performed at least annually, or when processes change",
      "",
      "**Operating effectiveness:** Did the control actually operate as designed throughout the testing period?",
      "- Evaluated through testing (inspection, observation, re-performance, inquiry)",
      "- Requires sufficient sample sizes to support a conclusion",
      "- Must cover the full period of reliance",
      "",
      "## Sample Selection Approaches",
      "",
      "### Random Selection",
      "",
      "**When to use:** Default method for transaction-level controls with large populations.",
      "",
      "**Method:**",
      "1. Define the population (all transactions subject to the control during the period)",
      "2. Number each item in the population sequentially",
      "3. Use a random number generator to select sample items",
      "4. Ensure no bias in selection (all items have equal probability)",
      "",
      "**Advantages:** Statistically valid, defensible, no selection bias",
      "**Disadvantages:** May miss high-risk items, requires complete population listing",
      "",
      "### Targeted (Judgmental) Selection",
      "",
      "**When to use:** Supplement to random selection for risk-based testing; primary method when population is small or highly varied.",
      "",
      "**Method:**",
      "1. Identify items with specific risk characteristics:",
      "   - High dollar amount (above a defined threshold)",
      "   - Unusual or non-standard transactions",
      "   - Period-end transactions (cut-off risk)",
      "   - Related-party transactions",
      "   - Manual or override transactions",
      "   - New vendor/customer transactions",
      "2. Select items matching risk criteria",
      "3. Document rationale for each targeted selection",
      "",
      "**Advantages:** Focuses on highest-risk items, efficient use of testing effort",
      "**Disadvantages:** Not statistically representative, may over-represent certain risks",
      "",
      "### Haphazard Selection",
      "",
      "**When to use:** When random selection is impractical (no sequential population listing) and population is relatively homogeneous.",
      "",
      "**Method:**",
      "1. Select items without any specific pattern or bias",
      "2. Ensure selections are spread across the full population period",
      "3. Avoid unconscious bias (don't always pick items at the top, round numbers, etc.)",
      "",
      "**Advantages:** Simple, no technology required",
      "**Disadvantages:** Not statistically valid, susceptible to unconscious bias",
      "",
      "### Systematic Selection",
      "",
      "**When to use:** When population is sequential and you want even coverage across the period.",
      "",
      "**Method:**",
      "1. Calculate the sampling interval: Population size / Sample size",
      "2. Select a random starting point within the first interval",
      "3. Select every Nth item from the starting point",
      "",
      "**Example:** Population of 1,000, sample of 25 → interval of 40. Random start: item 17. Select items 17, 57, 97, 137, ...",
      "",
      "**Advantages:** Even coverage across population, simple to execute",
      "**Disadvantages:** Periodic patterns in the population could bias results",
      "",
      "### Sample Size Guidance",
      "",
      "| Control Frequency | Expected Population | Low Risk Sample | Moderate Risk Sample | High Risk Sample |",
      "|------------------|--------------------|-----------------|--------------------|-----------------|",
      "| Annual | 1 | 1 | 1 | 1 |",
      "| Quarterly | 4 | 2 | 2 | 3 |",
      "| Monthly | 12 | 2 | 3 | 4 |",
      "| Weekly | 52 | 5 | 8 | 15 |",
      "| Daily | ~250 | 20 | 30 | 40 |",
      "| Per-transaction (small pop.) | < 250 | 20 | 30 | 40 |",
      "| Per-transaction (large pop.) | 250+ | 25 | 40 | 60 |",
      "",
      "**Factors increasing sample size:**",
      "- Higher inherent risk in the account/process",
      "- Control is the sole control addressing a significant risk (no redundancy)",
      "- Prior period control deficiency identified",
      "- New control (not tested in prior periods)",
      "- External auditor reliance on management testing",
      "",
      "## Testing Documentation Standards",
      "",
      "### Workpaper Requirements",
      "",
      "Every control test should be documented with:",
      "",
      "1. **Control identification:**",
      "   - Control number/ID",
      "   - Control description (what is done, by whom, how often)",
      "   - Control type (manual, automated, IT-dependent manual)",
      "   - Control frequency",
      "   - Risk and assertion addressed",
      "",
      "2. **Test design:**",
      "   - Test objective (what you are trying to determine)",
      "   - Test procedures (step-by-step instructions)",
      "   - Expected evidence (what you expect to see if the control is effective)",
      "   - Sample selection methodology and rationale",
      "",
      "3. **Test execution:**",
      "   - Population description and size",
      "   - Sample selection details (method, items selected)",
      "   - Results for each sample item (pass/fail with specific evidence examined)",
      "   - Exceptions noted with full description",
      "",
      "4. **Conclusion:**",
      "   - Overall assessment (effective / deficiency / significant deficiency / material weakness)",
      "   - Basis for conclusion",
      "   - Impact assessment for any exceptions",
      "   - Compensating controls considered (if applicable)",
      "",
      "5. **Sign-off:**",
      "   - Tester name and date",
      "   - Reviewer name and date",
      "",
      "### Evidence Standards",
      "",
      "**Sufficient evidence includes:**",
      "- Screenshots showing system-enforced controls",
      "- Signed/initialed approval documents",
      "- Email approvals with identifiable approver and date",
      "- System audit logs showing who performed the action and when",
      "- Re-performed calculations with matching results",
      "- Observation notes (with date, location, observer)",
      "",
      "**Insufficient evidence:**",
      "- Verbal confirmations alone (must be corroborated)",
      "- Undated documents",
      "- Evidence without identifiable performer/approver",
      "- Generic system reports without date/time stamps",
      '- "Per discussion with [name]" without corroborating documentation',
      "",
      "### Working Paper Organization",
      "",
      "Organize testing files by control area:",
      "",
      "```",
      "SOX Testing/",
      "├── [Year]/",
      "│   ├── Scoping and Risk Assessment/",
      "│   ├── Revenue Cycle/",
      "│   │   ├── Control Matrix",
      "│   │   ├── Walkthrough Documentation",
      "│   │   ├── Test Workpapers (one per control)",
      "│   │   └── Supporting Evidence",
      "│   ├── Procure to Pay/",
      "│   ├── Payroll/",
      "│   ├── Financial Close/",
      "│   ├── Treasury/",
      "│   ├── Fixed Assets/",
      "│   ├── IT General Controls/",
      "│   ├── Entity Level Controls/",
      "│   └── Summary and Conclusions/",
      "│       ├── Deficiency Evaluation",
      "│       └── Management Assessment",
      "```",
      "",
      "## Control Deficiency Classification",
      "",
      "### Deficiency",
      "",
      "A deficiency in internal control exists when the design or operation of a control does not allow management or employees, in the normal course of performing their assigned functions, to prevent or detect misstatements on a timely basis.",
      "",
      "**Evaluation factors:**",
      "- What is the likelihood that the control failure could result in a misstatement?",
      "- What is the magnitude of the potential misstatement?",
      "- Is there a compensating control that mitigates the deficiency?",
      "",
      "### Significant Deficiency",
      "",
      "A deficiency, or combination of deficiencies, that is less severe than a material weakness yet important enough to merit attention by those charged with governance.",
      "",
      "**Indicators:**",
      "- The deficiency could result in a misstatement that is more than inconsequential but less than material",
      "- There is more than a remote (but less than reasonably possible) likelihood of a material misstatement",
      "- The control is a key control and the deficiency is not fully mitigated by compensating controls",
      "- Combination of individually minor deficiencies that together represent a significant concern",
      "",
      "### Material Weakness",
      "",
      "A deficiency, or combination of deficiencies, such that there is a reasonable possibility that a material misstatement of the financial statements will not be prevented or detected on a timely basis.",
      "",
      "**Indicators:**",
      "- Identification of fraud by senior management (any magnitude)",
      "- Restatement of previously issued financial statements to correct a material error",
      "- Identification by the auditor of a material misstatement that would not have been detected by the company's controls",
      "- Ineffective oversight of financial reporting by the audit committee",
      "- Deficiency in a pervasive control (entity-level, IT general control) affecting multiple processes",
      "",
      "### Deficiency Aggregation",
      "",
      "Individual deficiencies that are not significant individually may be significant in combination:",
      "",
      "1. Identify all deficiencies in the same process or affecting the same assertion",
      "2. Evaluate whether the combined effect could result in a material misstatement",
      "3. Consider whether deficiencies in compensating controls exacerbate other deficiencies",
      "4. Document the aggregation analysis and conclusion",
      "",
      "### Remediation",
      "",
      "For each identified deficiency:",
      "",
      "1. **Root cause analysis:** Why did the control fail? (design gap, execution failure, staffing, training, system issue)",
      "2. **Remediation plan:** Specific actions to fix the control (redesign, additional training, system enhancement, added review)",
      "3. **Timeline:** Target date for remediation completion",
      "4. **Owner:** Person responsible for implementing the remediation",
      "5. **Validation:** How and when the remediated control will be re-tested to confirm effectiveness",
      "",
      "## Common Control Types",
      "",
      "### IT General Controls (ITGCs)",
      "",
      "Controls over the IT environment that support the reliable functioning of application controls and automated processes.",
      "",
      "**Access Controls:**",
      "- User access provisioning (new access requests require approval)",
      "- User access de-provisioning (terminated users removed timely)",
      "- Privileged access management (admin/superuser access restricted and monitored)",
      "- Periodic access reviews (user access recertified on a defined schedule)",
      "- Password policies (complexity, rotation, lockout)",
      "- Segregation of duties enforcement (conflicting access prevented)",
      "",
      "**Change Management:**",
      "- Change requests documented and approved before implementation",
      "- Changes tested in a non-production environment before promotion",
      "- Separation of development and production environments",
      "- Emergency change procedures (documented, approved post-implementation)",
      "- Change review and post-implementation validation",
      "",
      "**IT Operations:**",
      "- Batch job monitoring and exception handling",
      "- Backup and recovery procedures (regular backups, tested restores)",
      "- System availability and performance monitoring",
      "- Incident management and escalation procedures",
      "- Disaster recovery planning and testing",
      "",
      "### Manual Controls",
      "",
      "Controls performed by people using judgment, typically involving review and approval.",
      "",
      "**Examples:**",
      "- Management review of financial statements and key metrics",
      "- Supervisory approval of journal entries above a threshold",
      "- Three-way match verification (PO, receipt, invoice)",
      "- Account reconciliation preparation and review",
      "- Physical inventory observation and count",
      "- Vendor master data change approval",
      "- Customer credit approval",
      "",
      "**Key attributes to test:**",
      "- Was the control performed by the right person (proper authority)?",
      "- Was it performed timely (within the required timeframe)?",
      "- Is there evidence of the review (signature, initials, email, system log)?",
      "- Did the reviewer have sufficient information to perform an effective review?",
      "- Were exceptions identified and appropriately addressed?",
      "",
      "### Automated Controls",
      "",
      "Controls enforced by IT systems without human intervention.",
      "",
      "**Examples:**",
      "- System-enforced approval workflows (cannot proceed without required approvals)",
      "- Three-way match automation (system blocks payment if PO/receipt/invoice don't match)",
      "- Duplicate payment detection (system flags or blocks duplicate invoices)",
      "- Credit limit enforcement (system prevents orders exceeding credit limit)",
      "- Automated calculations (depreciation, amortization, interest, tax)",
      "- System-enforced segregation of duties (conflicting roles prevented)",
      "- Input validation controls (required fields, format checks, range checks)",
      "- Automated reconciliation matching",
      "",
      "**Testing approach:**",
      "- Test design: Confirm the system configuration enforces the control as intended",
      "- Test operating effectiveness: For automated controls, if the system configuration has not changed, one test of the control is typically sufficient for the period (supplemented by ITGC testing of change management)",
      "- Verify change management ITGCs are effective (if system changed, re-test the control)",
      "",
      "### IT-Dependent Manual Controls",
      "",
      "Manual controls that rely on the completeness and accuracy of system-generated information.",
      "",
      "**Examples:**",
      "- Management review of a system-generated exception report",
      "- Supervisor review of a system-generated aging report to assess reserves",
      "- Reconciliation using system-generated trial balance data",
      "- Approval of transactions identified by a system-generated workflow",
      "",
      "**Testing approach:**",
      "- Test the manual control (review, approval, follow-up on exceptions)",
      "- AND test the completeness and accuracy of the underlying report/data (IPE — Information Produced by the Entity)",
      "- IPE testing confirms the data the reviewer relied on was complete and accurate",
      "",
      "### Entity-Level Controls",
      "",
      "Broad controls that operate at the organizational level and affect multiple processes.",
      "",
      "**Examples:**",
      "- Tone at the top / code of conduct",
      "- Risk assessment process",
      "- Audit committee oversight of financial reporting",
      "- Internal audit function and activities",
      "- Fraud risk assessment and anti-fraud programs",
      "- Whistleblower/ethics hotline",
      "- Management monitoring of control effectiveness",
      "- Financial reporting competence (staffing, training, qualifications)",
      "- Period-end financial reporting process (close procedures, GAAP compliance reviews)",
      "",
      "**Significance:**",
      "- Entity-level controls can mitigate but typically cannot replace process-level controls",
      "- Ineffective entity-level controls (especially audit committee oversight and tone at the top) are strong indicators of a material weakness",
      "- Effective entity-level controls may reduce the extent of testing needed for process-level controls",
      "",
    ].join("\n"),
  },
  "finance-close-management": {
    "SKILL.md": [
      "---",
      "name: finance-close-management",
      "description: Manage the month-end close process with task sequencing, dependencies, and status tracking. Use when planning the close calendar, tracking close progress, identifying blockers, or sequencing close activities by day.",
      "metadata:",
      "  plugin: finance",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Close Management",
      "",
      "**Important**: This skill assists with close management workflows but does not provide financial advice. All close activities should be reviewed by qualified financial professionals.",
      "",
      "Month-end close checklist, task sequencing and dependencies, status tracking, and common close activities organized by day.",
      "",
      "## Month-End Close Checklist",
      "",
      "### Pre-Close (Last 2-3 Business Days of the Month)",
      "",
      "- [ ] Send close calendar and deadline reminders to all contributors",
      "- [ ] Confirm cut-off procedures with AP, AR, payroll, and treasury",
      "- [ ] Verify all sub-systems are processing normally (ERP, payroll, banking)",
      "- [ ] Complete preliminary bank reconciliation (all but last-day activity)",
      "- [ ] Review open purchase orders for potential accrual needs",
      "- [ ] Confirm payroll processing schedule aligns with close timeline",
      "- [ ] Collect information for any known unusual transactions",
      "",
      "### Close Day 1 (T+1: First Business Day After Month-End)",
      "",
      "- [ ] Confirm all sub-ledger modules have completed period-end processing",
      "- [ ] Run AP accruals for goods/services received but not invoiced",
      "- [ ] Post payroll entries and payroll accrual (if pay period straddles month-end)",
      "- [ ] Record cash receipts and disbursements through month-end",
      "- [ ] Post intercompany transactions and confirm with counterparties",
      "- [ ] Complete bank reconciliation with final bank statement",
      "- [ ] Run fixed asset depreciation",
      "- [ ] Post prepaid expense amortization",
      "",
      "### Close Day 2 (T+2)",
      "",
      "- [ ] Complete revenue recognition entries and deferred revenue adjustments",
      "- [ ] Post all remaining accrual journal entries",
      "- [ ] Complete AR subledger reconciliation",
      "- [ ] Complete AP subledger reconciliation",
      "- [ ] Record inventory adjustments (if applicable)",
      "- [ ] Post FX revaluation entries for foreign currency balances",
      "- [ ] Begin balance sheet account reconciliations",
      "",
      "### Close Day 3 (T+3)",
      "",
      "- [ ] Complete all balance sheet reconciliations",
      "- [ ] Post any adjusting journal entries identified during reconciliation",
      "- [ ] Complete intercompany reconciliation and elimination entries",
      "- [ ] Run preliminary trial balance and income statement",
      "- [ ] Perform preliminary flux analysis on income statement",
      "- [ ] Investigate and resolve material variances",
      "",
      "### Close Day 4 (T+4)",
      "",
      "- [ ] Post tax provision entries (income tax, sales tax, property tax)",
      "- [ ] Complete equity roll-forward (stock compensation, treasury stock)",
      "- [ ] Finalize all journal entries — soft close",
      "- [ ] Generate draft financial statements (P&L, BS, CF)",
      "- [ ] Perform detailed flux analysis and prepare variance explanations",
      "- [ ] Management review of financial statements and key metrics",
      "",
      "### Close Day 5 (T+5)",
      "",
      "- [ ] Post any final adjustments from management review",
      "- [ ] Finalize financial statements — hard close",
      "- [ ] Lock the period in the ERP/GL system",
      "- [ ] Distribute financial reporting package to stakeholders",
      "- [ ] Update forecasts/projections based on actual results",
      "- [ ] Conduct close retrospective — identify process improvements",
      "",
      "## Task Sequencing and Dependencies",
      "",
      "### Dependency Map",
      "",
      "Tasks are organized by what must complete before the next task can begin:",
      "",
      "```",
      "LEVEL 1 (No dependencies — can start immediately at T+1):",
      "├── Cash receipts/disbursements recording",
      "├── Bank statement retrieval",
      "├── Payroll processing/accrual",
      "├── Fixed asset depreciation run",
      "├── Prepaid amortization",
      "├── AP accrual preparation",
      "└── Intercompany transaction posting",
      "",
      "LEVEL 2 (Depends on Level 1 completion):",
      "├── Bank reconciliation (needs: cash entries + bank statement)",
      "├── Revenue recognition (needs: billing/delivery data finalized)",
      "├── AR subledger reconciliation (needs: all revenue/cash entries)",
      "├── AP subledger reconciliation (needs: all AP entries/accruals)",
      "├── FX revaluation (needs: all foreign currency entries posted)",
      "└── Remaining accrual JEs (needs: review of all source data)",
      "",
      "LEVEL 3 (Depends on Level 2 completion):",
      "├── All balance sheet reconciliations (needs: all JEs posted)",
      "├── Intercompany reconciliation (needs: both sides posted)",
      "├── Adjusting entries from reconciliations",
      "└── Preliminary trial balance",
      "",
      "LEVEL 4 (Depends on Level 3 completion):",
      "├── Tax provision (needs: pre-tax income finalized)",
      "├── Equity roll-forward",
      "├── Consolidation and eliminations",
      "├── Draft financial statements",
      "└── Preliminary flux analysis",
      "",
      "LEVEL 5 (Depends on Level 4 completion):",
      "├── Management review",
      "├── Final adjustments",
      "├── Hard close / period lock",
      "├── Financial reporting package",
      "└── Forecast updates",
      "```",
      "",
      "### Critical Path",
      "",
      "The critical path determines the minimum close duration. Typical critical path:",
      "",
      "```",
      "Cash/AP/AR entries → Subledger reconciliations → Balance sheet recs →",
      "  Tax provision → Draft financials → Management review → Hard close",
      "```",
      "",
      "To shorten the close:",
      "- Automate Level 1 entries (depreciation, prepaid amortization, standard accruals)",
      "- Pre-reconcile accounts during the month (continuous reconciliation)",
      "- Parallel-process independent reconciliations",
      "- Set clear deadlines with consequences for late submissions",
      "- Use standardized templates to reduce reconciliation prep time",
      "",
      "## Status Tracking and Reporting",
      "",
      "### Close Status Dashboard",
      "",
      "Track each close task with the following attributes:",
      "",
      "| Task | Owner | Deadline | Status | Blocker | Notes |",
      "|------|-------|----------|--------|---------|-------|",
      "| [Task name] | [Person/role] | [Day T+N] | Not Started / In Progress / Complete / Blocked | [If blocked, what's blocking] | [Any notes] |",
      "",
      "### Status Definitions",
      "",
      "- **Not Started:** Task has not yet begun (may be waiting on dependencies)",
      "- **In Progress:** Task is actively being worked on",
      "- **Complete:** Task is finished and has been reviewed/approved",
      "- **Blocked:** Task cannot proceed due to a dependency, missing data, or issue",
      "- **At Risk:** Task is in progress but may not meet its deadline",
      "",
      "### Daily Close Status Meeting (Recommended)",
      "",
      "During the close period, hold a brief (15-minute) daily standup:",
      "",
      "1. **Review status board:** Walk through open tasks, flag any that are behind",
      "2. **Identify blockers:** Surface any issues preventing task completion",
      "3. **Reassign or escalate:** Adjust ownership or escalate blockers to resolve quickly",
      "4. **Update timeline:** If any tasks are at risk, assess impact on overall close timeline",
      "",
      "### Close Metrics to Track Over Time",
      "",
      "| Metric | Definition | Target |",
      "|--------|-----------|--------|",
      "| Close duration | Business days from period end to hard close | Reduce over time |",
      "| # of adjusting entries after soft close | Entries posted during management review | Minimize |",
      "| # of late tasks | Tasks completed after their deadline | Zero |",
      "| # of reconciliation exceptions | Reconciling items requiring investigation | Reduce over time |",
      "| # of restatements / corrections | Errors found after close | Zero |",
      "",
      "## Common Close Activities by Day",
      "",
      "### Typical 5-Day Close Calendar",
      "",
      "| Day | Key Activities | Responsible |",
      "|-----|---------------|-------------|",
      "| **T+1** | Cash entries, payroll, AP accruals, depreciation, prepaid amortization, intercompany posting | Staff accountants, payroll |",
      "| **T+2** | Revenue recognition, remaining accruals, subledger reconciliations (AR, AP, FA), FX revaluation | Revenue accountant, AP/AR, treasury |",
      "| **T+3** | Balance sheet reconciliations, intercompany reconciliation, eliminations, preliminary trial balance, preliminary flux | Accounting team, consolidation |",
      "| **T+4** | Tax provision, equity roll-forward, draft financial statements, detailed flux analysis, management review | Tax, controller, FP&A |",
      "| **T+5** | Final adjustments, hard close, period lock, reporting package distribution, forecast update, retrospective | Controller, FP&A, finance leadership |",
      "",
      "### Accelerated Close (3-Day Target)",
      "",
      "For organizations targeting a faster close:",
      "",
      "| Day | Key Activities |",
      "|-----|---------------|",
      "| **T+1** | All JEs posted (automated + manual), all subledger reconciliations, bank reconciliation, intercompany reconciliation, preliminary trial balance |",
      "| **T+2** | All balance sheet reconciliations, tax provision, consolidation, draft financial statements, flux analysis, management review |",
      "| **T+3** | Final adjustments, hard close, reporting package, forecast update |",
      "",
      "**Prerequisites for a 3-day close:**",
      "- Automated recurring journal entries (depreciation, amortization, standard accruals)",
      "- Continuous reconciliation during the month (not all at month-end)",
      "- Automated intercompany elimination",
      "- Pre-close activities completed before month-end (cut-off, accrual estimates)",
      "- Empowered team with clear ownership and minimal handoffs",
      "- Real-time or near-real-time sub-system integration",
      "",
      "## Close Process Improvement",
      "",
      "### Common Bottlenecks and Solutions",
      "",
      "| Bottleneck | Root Cause | Solution |",
      "|-----------|-----------|---------|",
      "| Late AP accruals | Waiting for department spend confirmation | Implement continuous accrual estimation; set cut-off deadlines |",
      "| Manual journal entries | Recurring entries prepared manually each month | Automate standard recurring entries in the ERP |",
      "| Slow reconciliations | Starting from scratch each month | Implement continuous/rolling reconciliation |",
      "| Intercompany delays | Waiting for counterparty confirmation | Automate intercompany matching; set stricter deadlines |",
      "| Management review changes | Large adjustments found during review | Improve preliminary review process; empower team to catch issues earlier |",
      "| Missing supporting documents | Scrambling for documentation at close | Maintain documentation throughout the month |",
      "",
      "### Close Retrospective Questions",
      "",
      "After each close, ask:",
      "1. What went well this close that we should continue?",
      "2. What took longer than expected and why?",
      "3. What blockers did we encounter and how can we prevent them?",
      "4. Were there any surprises in the financial results we should have caught earlier?",
      "5. What can we automate or streamline for next month?",
      "",
    ].join("\n"),
  },
  "finance-financial-statements": {
    "SKILL.md": [
      "---",
      "name: finance-financial-statements",
      "description: Generate income statements, balance sheets, and cash flow statements with GAAP presentation and period-over-period comparison. Use when preparing financial statements, running flux analysis, or creating P&L reports with variance commentary.",
      "metadata:",
      "  plugin: finance",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Financial Statements",
      "",
      "**Important**: This skill assists with financial statement workflows but does not provide financial advice. All statements should be reviewed by qualified financial professionals before use in reporting or filings.",
      "",
      "Formats, GAAP presentation requirements, common adjustments, and flux analysis methodology for income statements, balance sheets, and cash flow statements.",
      "",
      "## Income Statement",
      "",
      "### Standard Format (Classification of Expenses by Function)",
      "",
      "```",
      "Revenue",
      "  Product revenue",
      "  Service revenue",
      "  Other revenue",
      "Total Revenue",
      "",
      "Cost of Revenue",
      "  Product costs",
      "  Service costs",
      "Total Cost of Revenue",
      "",
      "Gross Profit",
      "",
      "Operating Expenses",
      "  Research and development",
      "  Sales and marketing",
      "  General and administrative",
      "Total Operating Expenses",
      "",
      "Operating Income (Loss)",
      "",
      "Other Income (Expense)",
      "  Interest income",
      "  Interest expense",
      "  Other income (expense), net",
      "Total Other Income (Expense)",
      "",
      "Income (Loss) Before Income Taxes",
      "  Income tax expense (benefit)",
      "Net Income (Loss)",
      "",
      "Earnings Per Share (if applicable)",
      "  Basic",
      "  Diluted",
      "```",
      "",
      "### GAAP Presentation Requirements (ASC 220 / IAS 1)",
      "",
      "- Present all items of income and expense recognized in a period",
      "- Classify expenses either by nature (materials, labor, depreciation) or by function (COGS, R&D, S&M, G&A) — function is more common for US companies",
      "- If classified by function, disclose depreciation, amortization, and employee benefit costs by nature in the notes",
      "- Present operating and non-operating items separately",
      "- Show income tax expense as a separate line",
      "- Extraordinary items are prohibited under both US GAAP and IFRS",
      "- Discontinued operations presented separately, net of tax",
      "",
      "### Common Presentation Considerations",
      "",
      "- **Revenue disaggregation:** ASC 606 requires disaggregation of revenue into categories that depict how the nature, amount, timing, and uncertainty of revenue are affected by economic factors",
      "- **Stock-based compensation:** Classify within the functional expense categories (R&D, S&M, G&A) with total SBC disclosed in notes",
      "- **Restructuring charges:** Present separately if material, or include in operating expenses with note disclosure",
      "- **Non-GAAP adjustments:** If presenting non-GAAP measures (common in earnings releases), clearly label and reconcile to GAAP",
      "",
      "## Balance Sheet",
      "",
      "### Standard Format (Classified Balance Sheet)",
      "",
      "```",
      "ASSETS",
      "Current Assets",
      "  Cash and cash equivalents",
      "  Short-term investments",
      "  Accounts receivable, net",
      "  Inventory",
      "  Prepaid expenses and other current assets",
      "Total Current Assets",
      "",
      "Non-Current Assets",
      "  Property and equipment, net",
      "  Operating lease right-of-use assets",
      "  Goodwill",
      "  Intangible assets, net",
      "  Long-term investments",
      "  Other non-current assets",
      "Total Non-Current Assets",
      "",
      "TOTAL ASSETS",
      "",
      "LIABILITIES AND STOCKHOLDERS' EQUITY",
      "Current Liabilities",
      "  Accounts payable",
      "  Accrued liabilities",
      "  Deferred revenue, current portion",
      "  Current portion of long-term debt",
      "  Operating lease liabilities, current portion",
      "  Other current liabilities",
      "Total Current Liabilities",
      "",
      "Non-Current Liabilities",
      "  Long-term debt",
      "  Deferred revenue, non-current",
      "  Operating lease liabilities, non-current",
      "  Other non-current liabilities",
      "Total Non-Current Liabilities",
      "",
      "Total Liabilities",
      "",
      "Stockholders' Equity",
      "  Common stock",
      "  Additional paid-in capital",
      "  Retained earnings (accumulated deficit)",
      "  Accumulated other comprehensive income (loss)",
      "  Treasury stock",
      "Total Stockholders' Equity",
      "",
      "TOTAL LIABILITIES AND STOCKHOLDERS' EQUITY",
      "```",
      "",
      "### GAAP Presentation Requirements (ASC 210 / IAS 1)",
      "",
      "- Distinguish between current and non-current assets and liabilities",
      "- Current: expected to be realized, consumed, or settled within 12 months (or the operating cycle if longer)",
      "- Present assets in order of liquidity (most liquid first) — standard US practice",
      "- Accounts receivable shown net of allowance for credit losses (ASC 326)",
      "- Property and equipment shown net of accumulated depreciation",
      "- Goodwill is not amortized — tested for impairment annually (ASC 350)",
      "- Leases: recognize right-of-use assets and lease liabilities for operating and finance leases (ASC 842)",
      "",
      "## Cash Flow Statement",
      "",
      "### Standard Format (Indirect Method)",
      "",
      "```",
      "CASH FLOWS FROM OPERATING ACTIVITIES",
      "Net income (loss)",
      "Adjustments to reconcile net income to net cash from operations:",
      "  Depreciation and amortization",
      "  Stock-based compensation",
      "  Amortization of debt issuance costs",
      "  Deferred income taxes",
      "  Loss (gain) on disposal of assets",
      "  Impairment charges",
      "  Other non-cash items",
      "Changes in operating assets and liabilities:",
      "  Accounts receivable",
      "  Inventory",
      "  Prepaid expenses and other assets",
      "  Accounts payable",
      "  Accrued liabilities",
      "  Deferred revenue",
      "  Other liabilities",
      "Net Cash Provided by (Used in) Operating Activities",
      "",
      "CASH FLOWS FROM INVESTING ACTIVITIES",
      "  Purchases of property and equipment",
      "  Purchases of investments",
      "  Proceeds from sale/maturity of investments",
      "  Acquisitions, net of cash acquired",
      "  Other investing activities",
      "Net Cash Provided by (Used in) Investing Activities",
      "",
      "CASH FLOWS FROM FINANCING ACTIVITIES",
      "  Proceeds from issuance of debt",
      "  Repayment of debt",
      "  Proceeds from issuance of common stock",
      "  Repurchases of common stock",
      "  Dividends paid",
      "  Payment of debt issuance costs",
      "  Other financing activities",
      "Net Cash Provided by (Used in) Financing Activities",
      "",
      "Effect of exchange rate changes on cash",
      "",
      "Net Increase (Decrease) in Cash and Cash Equivalents",
      "Cash and cash equivalents, beginning of period",
      "Cash and cash equivalents, end of period",
      "```",
      "",
      "### GAAP Presentation Requirements (ASC 230 / IAS 7)",
      "",
      "- Indirect method is most common (start with net income, adjust for non-cash items)",
      "- Direct method is permitted but rarely used (requires supplemental indirect reconciliation)",
      "- Interest paid and income taxes paid must be disclosed (either on the face or in notes)",
      "- Non-cash investing and financing activities disclosed separately (e.g., assets acquired under leases, stock issued for acquisitions)",
      "- Cash equivalents: short-term, highly liquid investments with original maturities of 3 months or less",
      "",
      "## Common Adjustments and Reclassifications",
      "",
      "### Period-End Adjustments",
      "",
      "1. **Accruals:** Record expenses incurred but not yet paid (AP accruals, payroll accruals, interest accruals)",
      "2. **Deferrals:** Adjust prepaid expenses, deferred revenue, and deferred costs for the period",
      "3. **Depreciation and amortization:** Book periodic depreciation/amortization from fixed asset and intangible schedules",
      "4. **Bad debt provision:** Adjust allowance for credit losses based on aging analysis and historical loss rates",
      "5. **Inventory adjustments:** Record write-downs for obsolete, slow-moving, or impaired inventory",
      "6. **FX revaluation:** Revalue foreign-currency-denominated monetary assets and liabilities at period-end rates",
      "7. **Tax provision:** Record current and deferred income tax expense",
      "8. **Fair value adjustments:** Mark-to-market investments, derivatives, and other fair-value items",
      "",
      "### Reclassifications",
      "",
      "1. **Current/non-current reclassification:** Reclassify long-term debt maturing within 12 months to current",
      "2. **Contra account netting:** Net allowances against gross receivables, accumulated depreciation against gross assets",
      "3. **Intercompany elimination:** Eliminate intercompany balances and transactions in consolidation",
      "4. **Discontinued operations:** Reclassify results of discontinued operations to a separate line item",
      "5. **Equity method adjustments:** Record share of investee income/loss for equity method investments",
      "6. **Segment reclassifications:** Ensure transactions are properly classified by operating segment",
      "",
      "## Flux Analysis Methodology",
      "",
      "### Variance Calculation",
      "",
      "For each line item, calculate:",
      "- **Dollar variance:** Current period - Prior period (or current period - budget)",
      "- **Percentage variance:** (Current - Prior) / |Prior| x 100",
      "- **Basis point change:** For margins and ratios, express change in basis points (1 bp = 0.01%)",
      "",
      "### Materiality Thresholds",
      "",
      'Define what constitutes a "material" variance requiring investigation. Common approaches:',
      "",
      "- **Fixed dollar threshold:** Variances exceeding a set dollar amount (e.g., $50K, $100K)",
      "- **Percentage threshold:** Variances exceeding a set percentage (e.g., 10%, 15%)",
      "- **Combined:** Either the dollar OR percentage threshold is exceeded",
      "- **Scaled:** Different thresholds for different line items based on their size and volatility",
      "",
      "*Example thresholds (adjust for your organization):*",
      "",
      "| Line Item Size | Dollar Threshold | Percentage Threshold |",
      "|---------------|-----------------|---------------------|",
      "| > $10M        | $500K           | 5%                  |",
      "| $1M - $10M    | $100K           | 10%                 |",
      "| < $1M         | $50K            | 15%                 |",
      "",
      "### Variance Decomposition",
      "",
      "Break down total variance into component drivers:",
      "",
      "- **Volume/quantity effect:** Change in volume at prior period rates",
      "- **Rate/price effect:** Change in rate/price at current period volume",
      "- **Mix effect:** Shift in composition between items with different rates/margins",
      "- **New/discontinued items:** Items present in one period but not the other",
      "- **One-time/non-recurring items:** Items that are not expected to repeat",
      "- **Timing effect:** Items shifting between periods (not a true change in run rate)",
      "- **Currency effect:** Impact of FX rate changes on translated results",
      "",
      "### Investigation and Narrative",
      "",
      "For each material variance:",
      "1. Quantify the variance ($ and %)",
      "2. Identify whether favorable or unfavorable",
      "3. Decompose into drivers using the categories above",
      "4. Provide a narrative explanation of the business reason",
      "5. Assess whether the variance is temporary or represents a trend change",
      "6. Note any actions required (further investigation, forecast update, process change)",
      "",
    ].join("\n"),
  },
  "finance-income-statement": {
    "SKILL.md": [
      "---",
      "name: finance-income-statement",
      "description: Generate an income statement with period-over-period comparison and variance analysis",
      "metadata:",
      "  plugin: finance",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <frequency> <period>",
      "---",
      "",
      "# Income Statement Generation",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "**Important**: This command assists with financial statement workflows but does not provide financial advice. All statements should be reviewed by qualified financial professionals before use in reporting or filings.",
      "",
      "Generate an income statement with period-over-period comparison and variance analysis. Highlight material variances for investigation.",
      "",
      "## Usage",
      "",
      "```",
      "/income-statement <period-type> <period>",
      "```",
      "",
      "### Arguments",
      "",
      "- `period-type` — The reporting period type:",
      "  - `monthly` — Single month P&L with prior month and prior year month comparison",
      "  - `quarterly` — Quarter P&L with prior quarter and prior year quarter comparison",
      "  - `annual` — Full year P&L with prior year comparison",
      "  - `ytd` — Year-to-date P&L with prior year YTD comparison",
      "- `period` — The period to report (e.g., `2024-12`, `2024-Q4`, `2024`)",
      "",
      "## Workflow",
      "",
      "### 1. Gather Financial Data",
      "",
      "If ~~erp or ~~data warehouse is connected:",
      "- Pull trial balance or income statement data for the specified period",
      "- Pull comparison period data (prior period, prior year, budget/forecast)",
      "- Pull account hierarchy and groupings for presentation",
      "",
      "If no data source is connected:",
      "> Connect ~~erp or ~~data warehouse to pull financial data automatically. You can also paste trial balance data, upload a spreadsheet, or provide income statement data for analysis.",
      "",
      "Prompt the user to provide:",
      "- Current period revenue and expense data (by account or category)",
      "- Comparison period data (prior period, prior year, and/or budget)",
      "- Any known adjustments or reclassifications",
      "",
      "### 2. Generate Income Statement",
      "",
      "Present in standard multi-column format:",
      "",
      "```",
      "INCOME STATEMENT",
      "Period: [Period description]",
      "(in thousands, unless otherwise noted)",
      "",
      "                              Current    Prior      Variance   Variance   Budget    Budget",
      "                              Period     Period     ($)        (%)        Amount    Var ($)",
      "                              --------   --------   --------   --------   --------  --------",
      "REVENUE",
      "  Product revenue             $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  Service revenue             $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  Other revenue               $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "                              --------   --------   --------              --------  --------",
      "TOTAL REVENUE                 $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "",
      "COST OF REVENUE",
      "  [Cost items]                $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "                              --------   --------   --------              --------  --------",
      "GROSS PROFIT                  $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  Gross Margin                XX.X%      XX.X%",
      "",
      "OPERATING EXPENSES",
      "  Research & development      $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  Sales & marketing           $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  General & administrative    $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "                              --------   --------   --------              --------  --------",
      "TOTAL OPERATING EXPENSES      $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "",
      "OPERATING INCOME (LOSS)       $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  Operating Margin            XX.X%      XX.X%",
      "",
      "OTHER INCOME (EXPENSE)",
      "  Interest income             $XX,XXX    $XX,XXX    $X,XXX     X.X%",
      "  Interest expense           ($XX,XXX)  ($XX,XXX)   $X,XXX     X.X%",
      "  Other, net                  $XX,XXX    $XX,XXX    $X,XXX     X.X%",
      "                              --------   --------   --------",
      "TOTAL OTHER INCOME (EXPENSE)  $XX,XXX    $XX,XXX    $X,XXX     X.X%",
      "",
      "INCOME BEFORE TAXES           $XX,XXX    $XX,XXX    $X,XXX     X.X%",
      "  Income tax expense          $XX,XXX    $XX,XXX    $X,XXX     X.X%",
      "                              --------   --------   --------",
      "",
      "NET INCOME (LOSS)             $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX",
      "  Net Margin                  XX.X%      XX.X%",
      "```",
      "",
      "### 3. Variance Analysis",
      "",
      "For each line item, calculate and flag material variances:",
      "",
      "**Materiality thresholds** (flag if either condition met):",
      "- Dollar variance exceeds a defined threshold (e.g., $50K, $100K — ask user for their threshold)",
      "- Percentage variance exceeds 10% (or user-defined threshold)",
      "",
      "For flagged variances, provide:",
      "- Direction and magnitude of the variance",
      "- Possible drivers (if data is available to decompose)",
      "- Questions to investigate",
      "- Whether the variance is favorable or unfavorable",
      "",
      "### 4. Key Metrics Summary",
      "",
      "```",
      "KEY METRICS",
      "                              Current    Prior      Change",
      "Revenue growth (%)                                  X.X%",
      "Gross margin (%)              XX.X%      XX.X%      X.X pp",
      "Operating margin (%)          XX.X%      XX.X%      X.X pp",
      "Net margin (%)                XX.X%      XX.X%      X.X pp",
      "OpEx as % of revenue          XX.X%      XX.X%      X.X pp",
      "Effective tax rate (%)        XX.X%      XX.X%      X.X pp",
      "```",
      "",
      "### 5. Material Variance Summary",
      "",
      "List all material variances requiring investigation:",
      "",
      "| Line Item | Variance ($) | Variance (%) | Direction | Preliminary Driver | Action |",
      "|-----------|-------------|-------------|-----------|-------------------|--------|",
      "| [Item]    | $X,XXX      | X.X%        | Unfav.    | [If known]        | Investigate |",
      "",
      "### 6. Output",
      "",
      "Provide:",
      "1. Formatted income statement with comparisons",
      "2. Key metrics summary",
      "3. Material variance listing with investigation flags",
      "4. Suggested follow-up questions for unexplained variances",
      "5. Offer to drill into any specific variance with `/flux`",
      "",
    ].join("\n"),
  },
  "finance-journal-entry": {
    "SKILL.md": [
      "---",
      "name: finance-journal-entry",
      "description: Prepare journal entries with proper debits, credits, and supporting detail",
      "metadata:",
      "  plugin: finance",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <entry type> [period]",
      "---",
      "",
      "# Journal Entry Preparation",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "**Important**: This command assists with journal entry workflows but does not provide financial advice. All entries should be reviewed by qualified financial professionals before posting.",
      "",
      "Prepare journal entries with proper debits, credits, supporting detail, and review documentation.",
      "",
      "## Usage",
      "",
      "```",
      "/je <type> <period>",
      "```",
      "",
      "### Arguments",
      "",
      "- `type` — The journal entry type. One of:",
      "  - `ap-accrual` — Accounts payable accruals for goods/services received but not yet invoiced",
      "  - `fixed-assets` — Depreciation and amortization entries for fixed assets and intangibles",
      "  - `prepaid` — Amortization of prepaid expenses (insurance, software, rent, etc.)",
      "  - `payroll` — Payroll accruals including salaries, benefits, taxes, and bonus accruals",
      "  - `revenue` — Revenue recognition entries including deferred revenue adjustments",
      "- `period` — The accounting period (e.g., `2024-12`, `2024-Q4`, `2024`)",
      "",
      "## Workflow",
      "",
      "### 1. Gather Source Data",
      "",
      "If ~~erp or ~~data warehouse is connected:",
      "- Pull the trial balance for the specified period",
      "- Pull subledger detail for the relevant accounts",
      "- Pull prior period entries of the same type for reference",
      "- Identify the current GL balances for affected accounts",
      "",
      "If no data source is connected:",
      "> Connect ~~erp or ~~data warehouse to pull GL data automatically. You can also paste trial balance data or upload a spreadsheet.",
      "",
      "Prompt the user to provide:",
      "- Trial balance or GL balances for affected accounts",
      "- Subledger detail or supporting schedules",
      "- Prior period entries for reference (optional)",
      "",
      "### 2. Calculate the Entry",
      "",
      "Based on the JE type:",
      "",
      "**AP Accrual:**",
      "- Identify goods/services received but not invoiced by period end",
      "- Calculate accrual amounts from PO receipts, contracts, or estimates",
      "- Debit: Expense accounts (or asset accounts for capitalizable items)",
      "- Credit: Accrued liabilities",
      "",
      "**Fixed Assets:**",
      "- Pull the fixed asset register or depreciation schedule",
      "- Calculate period depreciation by asset class and method (straight-line, declining balance, units of production)",
      "- Debit: Depreciation expense (by department/cost center)",
      "- Credit: Accumulated depreciation",
      "",
      "**Prepaid:**",
      "- Pull the prepaid amortization schedule",
      "- Calculate the period amortization for each prepaid item",
      "- Debit: Expense accounts (by type — insurance, software, rent, etc.)",
      "- Credit: Prepaid expense accounts",
      "",
      "**Payroll:**",
      "- Calculate accrued salaries for days worked but not yet paid",
      "- Calculate accrued benefits (health, retirement contributions, PTO)",
      "- Calculate employer payroll tax accruals",
      "- Calculate bonus accruals (if applicable, based on plan terms)",
      "- Debit: Salary expense, benefits expense, payroll tax expense",
      "- Credit: Accrued payroll, accrued benefits, accrued payroll taxes",
      "",
      "**Revenue:**",
      "- Review contracts and performance obligations",
      "- Calculate revenue to recognize based on delivery/performance",
      "- Adjust deferred revenue balances",
      "- Debit: Deferred revenue (or accounts receivable)",
      "- Credit: Revenue accounts (by stream/category)",
      "",
      "### 3. Generate the Journal Entry",
      "",
      "Present the entry in standard format:",
      "",
      "```",
      "Journal Entry: [Type] — [Period]",
      "Prepared by: [User]",
      "Date: [Period end date]",
      "",
      "| Line | Account Code | Account Name | Debit | Credit | Department | Memo |",
      "|------|-------------|--------------|-------|--------|------------|------|",
      "| 1    | XXXX        | [Name]       | X,XXX |        | [Dept]     | [Detail] |",
      "| 2    | XXXX        | [Name]       |       | X,XXX  | [Dept]     | [Detail] |",
      "|      |             | **Total**    | X,XXX | X,XXX  |            |      |",
      "",
      "Supporting Detail:",
      "- [Calculation basis and assumptions]",
      "- [Reference to supporting schedule or documentation]",
      "",
      "Reversal: [Yes/No — if yes, specify reversal date]",
      "```",
      "",
      "### 4. Review Checklist",
      "",
      "Before finalizing, verify:",
      "",
      "- [ ] Debits equal credits",
      "- [ ] Correct accounting period",
      "- [ ] Account codes are valid and map to the right GL accounts",
      "- [ ] Amounts are calculated correctly with supporting detail",
      "- [ ] Memo/description is clear and specific enough for audit",
      "- [ ] Department/cost center coding is correct",
      "- [ ] Entry is consistent with prior period treatment",
      "- [ ] Reversal flag is set appropriately (accruals should auto-reverse)",
      "- [ ] Supporting documentation is referenced or attached",
      "- [ ] Entry is within the user's approval authority",
      "- [ ] No unusual or out-of-pattern amounts that need investigation",
      "",
      "### 5. Output",
      "",
      "Provide:",
      "1. The formatted journal entry",
      "2. Supporting calculations",
      "3. Comparison to prior period entry of the same type (if available)",
      "4. Any items flagged for review or follow-up",
      "5. Instructions for posting (manual entry or upload format for the user's ERP)",
      "",
    ].join("\n"),
  },
  "finance-journal-entry-prep": {
    "SKILL.md": [
      "---",
      "name: finance-journal-entry-prep",
      "description: Prepare journal entries with proper debits, credits, and supporting documentation for month-end close. Use when booking accruals, prepaid amortization, fixed asset depreciation, payroll entries, revenue recognition, or any manual journal entry.",
      "metadata:",
      "  plugin: finance",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Journal Entry Preparation",
      "",
      "**Important**: This skill assists with journal entry workflows but does not provide financial advice. All entries should be reviewed by qualified financial professionals before posting.",
      "",
      "Best practices, standard entry types, documentation requirements, and review workflows for journal entry preparation.",
      "",
      "## Standard Accrual Types and Their Entries",
      "",
      "### Accounts Payable Accruals",
      "",
      "Accrue for goods or services received but not yet invoiced at period end.",
      "",
      "**Typical entry:**",
      "- Debit: Expense account (or capitalize if asset-qualifying)",
      "- Credit: Accrued liabilities",
      "",
      "**Sources for calculation:**",
      "- Open purchase orders with confirmed receipts",
      "- Contracts with services rendered but unbilled",
      "- Recurring vendor arrangements (utilities, subscriptions, professional services)",
      "- Employee expense reports submitted but not yet processed",
      "",
      "**Key considerations:**",
      "- Reverse in the following period (auto-reversal recommended)",
      "- Use consistent estimation methodology period over period",
      "- Document basis for estimates (PO amount, contract terms, historical run-rate)",
      "- Track actual vs accrual to refine future estimates",
      "",
      "### Fixed Asset Depreciation",
      "",
      "Book periodic depreciation expense for tangible and intangible assets.",
      "",
      "**Typical entry:**",
      "- Debit: Depreciation/amortization expense (by department or cost center)",
      "- Credit: Accumulated depreciation/amortization",
      "",
      "**Depreciation methods:**",
      "- **Straight-line:** (Cost - Salvage) / Useful life — most common for financial reporting",
      "- **Declining balance:** Accelerated method applying fixed rate to net book value",
      "- **Units of production:** Based on actual usage or output vs total expected",
      "",
      "**Key considerations:**",
      "- Run depreciation from the fixed asset register or schedule",
      "- Verify new additions are set up with correct useful life and method",
      "- Check for disposals or impairments requiring write-off",
      "- Ensure consistency between book and tax depreciation tracking",
      "",
      "### Prepaid Expense Amortization",
      "",
      "Amortize prepaid expenses over their benefit period.",
      "",
      "**Typical entry:**",
      "- Debit: Expense account (insurance, software, rent, etc.)",
      "- Credit: Prepaid expense",
      "",
      "**Common prepaid categories:**",
      "- Insurance premiums (typically 12-month policies)",
      "- Software licenses and subscriptions",
      "- Prepaid rent (if applicable under lease terms)",
      "- Prepaid maintenance contracts",
      "- Conference and event deposits",
      "",
      "**Key considerations:**",
      "- Maintain an amortization schedule with start/end dates and monthly amounts",
      "- Review for any prepaid items that should be fully expensed (immaterial amounts)",
      "- Check for cancelled or terminated contracts requiring accelerated amortization",
      "- Verify new prepaids are added to the schedule promptly",
      "",
      "### Payroll Accruals",
      "",
      "Accrue compensation and related costs for the period.",
      "",
      "**Typical entries:**",
      "",
      "*Salary accrual (for pay periods not aligned with month-end):*",
      "- Debit: Salary expense (by department)",
      "- Credit: Accrued payroll",
      "",
      "*Bonus accrual:*",
      "- Debit: Bonus expense (by department)",
      "- Credit: Accrued bonus",
      "",
      "*Benefits accrual:*",
      "- Debit: Benefits expense",
      "- Credit: Accrued benefits",
      "",
      "*Payroll tax accrual:*",
      "- Debit: Payroll tax expense",
      "- Credit: Accrued payroll taxes",
      "",
      "**Key considerations:**",
      "- Calculate salary accrual based on working days in the period vs pay period",
      "- Bonus accruals should reflect plan terms (target amounts, performance metrics, payout timing)",
      "- Include employer-side taxes and benefits (FICA, FUTA, health, 401k match)",
      "- Track PTO/vacation accrual liability if required by policy or jurisdiction",
      "",
      "### Revenue Recognition",
      "",
      "Recognize revenue based on performance obligations and delivery.",
      "",
      "**Typical entries:**",
      "",
      "*Recognize previously deferred revenue:*",
      "- Debit: Deferred revenue",
      "- Credit: Revenue",
      "",
      "*Recognize revenue with new receivable:*",
      "- Debit: Accounts receivable",
      "- Credit: Revenue",
      "",
      "*Defer revenue received in advance:*",
      "- Debit: Cash / Accounts receivable",
      "- Credit: Deferred revenue",
      "",
      "**Key considerations:**",
      "- Follow ASC 606 five-step framework for contracts with customers",
      "- Identify distinct performance obligations in each contract",
      "- Determine transaction price (including variable consideration)",
      "- Allocate transaction price to performance obligations",
      "- Recognize revenue as/when performance obligations are satisfied",
      "- Maintain contract-level detail for audit support",
      "",
      "## Supporting Documentation Requirements",
      "",
      "Every journal entry should have:",
      "",
      "1. **Entry description/memo:** Clear, specific description of what the entry records and why",
      "2. **Calculation support:** How amounts were derived (formula, schedule, source data reference)",
      "3. **Source documents:** Reference to the underlying transactions or events (PO numbers, invoice numbers, contract references, payroll register)",
      "4. **Period:** The accounting period the entry applies to",
      "5. **Preparer identification:** Who prepared the entry and when",
      "6. **Approval:** Evidence of review and approval per the authorization matrix",
      "7. **Reversal indicator:** Whether the entry auto-reverses and the reversal date",
      "",
      "## Review and Approval Workflows",
      "",
      "### Typical Approval Matrix",
      "",
      "| Entry Type | Amount Threshold | Approver |",
      "|-----------|-----------------|----------|",
      "| Standard recurring | Any amount | Accounting manager |",
      "| Non-recurring / manual | < $50K | Accounting manager |",
      "| Non-recurring / manual | $50K - $250K | Controller |",
      "| Non-recurring / manual | > $250K | CFO / VP Finance |",
      "| Top-side / consolidation | Any amount | Controller or above |",
      "| Out-of-period adjustments | Any amount | Controller or above |",
      "",
      "*Note: Thresholds should be set based on your organization's materiality and risk tolerance.*",
      "",
      "### Review Checklist",
      "",
      "Before approving a journal entry, the reviewer should verify:",
      "",
      "- [ ] Debits equal credits (entry is balanced)",
      "- [ ] Correct accounting period (not posting to a closed period)",
      "- [ ] Account codes exist and are appropriate for the transaction",
      "- [ ] Amounts are mathematically accurate and supported by calculations",
      "- [ ] Description is clear, specific, and sufficient for audit purposes",
      "- [ ] Department/cost center/project coding is correct",
      "- [ ] Treatment is consistent with prior periods and accounting policies",
      "- [ ] Auto-reversal is set appropriately (accruals should reverse)",
      "- [ ] Supporting documentation is complete and referenced",
      "- [ ] Entry amount is within the preparer's authority level",
      "- [ ] No duplicate of an existing entry",
      "- [ ] Unusual or large amounts are explained and justified",
      "",
      "## Common Errors to Check For",
      "",
      "1. **Unbalanced entries:** Debits do not equal credits (system should prevent, but check manual entries)",
      "2. **Wrong period:** Entry posted to an incorrect or already-closed period",
      "3. **Wrong sign:** Debit entered as credit or vice versa",
      "4. **Duplicate entries:** Same transaction recorded twice (check for duplicates before posting)",
      "5. **Wrong account:** Entry posted to incorrect GL account (especially similar account codes)",
      "6. **Missing reversal:** Accrual entry not set to auto-reverse, causing double-counting",
      "7. **Stale accruals:** Recurring accruals not updated for changed circumstances",
      "8. **Round-number estimates:** Suspiciously round amounts that may not reflect actual calculations",
      "9. **Incorrect FX rates:** Foreign currency entries using wrong exchange rate or date",
      "10. **Missing intercompany elimination:** Entries between entities without corresponding elimination",
      "11. **Capitalization errors:** Expenses that should be capitalized, or capitalized items that should be expensed",
      "12. **Cut-off errors:** Transactions recorded in the wrong period based on delivery or service date",
      "",
    ].join("\n"),
  },
  "finance-reconciliation": {
    "SKILL.md": [
      "---",
      "name: finance-reconciliation",
      "description: Reconcile GL balances to subledger, bank, or third-party balances",
      "metadata:",
      "  plugin: finance",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <account> [period]",
      "---",
      "",
      "# Account Reconciliation",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "**Important**: This command assists with reconciliation workflows but does not provide financial advice. All reconciliations should be reviewed by qualified financial professionals before sign-off.",
      "",
      "Reconcile GL account balances to subledger, bank, or third-party balances. Identify and categorize reconciling items and generate a reconciliation workpaper.",
      "",
      "## Usage",
      "",
      "```",
      "/recon <account> <period>",
      "```",
      "",
      "### Arguments",
      "",
      "- `account` — The account or account category to reconcile. Examples:",
      "  - `cash` or `bank` — Bank reconciliation (GL cash to bank statement)",
      "  - `accounts-receivable` or `ar` — AR subledger reconciliation",
      "  - `accounts-payable` or `ap` — AP subledger reconciliation",
      "  - `fixed-assets` or `fa` — Fixed asset subledger reconciliation",
      "  - `intercompany` or `ic` — Intercompany balance reconciliation",
      "  - `prepaid` — Prepaid expense schedule reconciliation",
      "  - `accrued-liabilities` — Accrued liabilities detail reconciliation",
      "  - Any specific GL account code (e.g., `1010`, `2100`)",
      "- `period` — The accounting period end date (e.g., `2024-12`, `2024-12-31`)",
      "",
      "## Workflow",
      "",
      "### 1. Gather Both Sides of the Reconciliation",
      "",
      "If ~~erp or ~~data warehouse is connected:",
      "- Pull the GL balance for the specified account(s) as of period end",
      "- Pull the subledger, bank statement, or third-party balance for comparison",
      "- Pull prior period reconciliation (if available) for outstanding item carryforward",
      "",
      "If no data source is connected:",
      "> Connect ~~erp or ~~data warehouse to pull account balances automatically. To reconcile manually, provide:",
      "> 1. **GL side:** The general ledger balance for the account as of period end",
      "> 2. **Other side:** The subledger balance, bank statement balance, or third-party confirmation balance",
      "> 3. **Prior period outstanding items** (optional): Any reconciling items from the prior period reconciliation",
      "",
      "### 2. Compare Balances",
      "",
      "Calculate the difference between the two sides:",
      "",
      "```",
      "GL Balance:                    $XX,XXX.XX",
      "Subledger/Bank/Other Balance:  $XX,XXX.XX",
      "                               ----------",
      "Difference:                    $XX,XXX.XX",
      "```",
      "",
      "### 3. Identify Reconciling Items",
      "",
      "Analyze the difference and categorize reconciling items:",
      "",
      "**Timing Differences** (items that will clear in subsequent periods):",
      "- Outstanding checks / payments issued but not yet cleared",
      "- Deposits in transit / receipts recorded but not yet credited",
      "- Invoices posted in one system but pending in the other",
      "- Accruals awaiting reversal",
      "",
      "**Permanent Differences** (items requiring adjustment):",
      "- Errors in recording (wrong amount, wrong account, duplicate entries)",
      "- Missing entries (transactions in one system but not the other)",
      "- Bank fees or charges not yet recorded",
      "- Foreign currency translation differences",
      "",
      "**Prior Period Items** (carryforward from prior reconciliation):",
      "- Items from prior period that have now cleared (remove from reconciliation)",
      "- Items from prior period still outstanding (carry forward with aging)",
      "",
      "### 4. Generate Reconciliation Workpaper",
      "",
      "```",
      "ACCOUNT RECONCILIATION",
      "Account: [Account code] — [Account name]",
      "Period End: [Date]",
      "Prepared by: [User]",
      "Date Prepared: [Today]",
      "",
      "RECONCILIATION SUMMARY",
      "=======================",
      "",
      "Balance per General Ledger:              $XX,XXX.XX",
      "",
      "Add: Reconciling items increasing GL",
      "  [Item description]                     $X,XXX.XX",
      "  [Item description]                     $X,XXX.XX",
      "                                         ---------",
      "  Subtotal additions:                    $X,XXX.XX",
      "",
      "Less: Reconciling items decreasing GL",
      "  [Item description]                    ($X,XXX.XX)",
      "  [Item description]                    ($X,XXX.XX)",
      "                                         ---------",
      "  Subtotal deductions:                  ($X,XXX.XX)",
      "",
      "Adjusted GL Balance:                     $XX,XXX.XX",
      "",
      "Balance per [Subledger/Bank/Other]:      $XX,XXX.XX",
      "",
      "Add: Reconciling items",
      "  [Item description]                     $X,XXX.XX",
      "",
      "Less: Reconciling items",
      "  [Item description]                    ($X,XXX.XX)",
      "",
      "Adjusted [Other] Balance:                $XX,XXX.XX",
      "",
      "DIFFERENCE:                              $0.00",
      "```",
      "",
      "### 5. Reconciling Items Detail",
      "",
      "Present each reconciling item with:",
      "",
      "| # | Description | Amount | Category | Age (Days) | Status | Action Required |",
      "|---|-------------|--------|----------|------------|--------|-----------------|",
      "| 1 | [Detail]    | $X,XXX | Timing   | 5          | Expected to clear | Monitor |",
      "| 2 | [Detail]    | $X,XXX | Error    | N/A        | Requires correction | Post adjusting JE |",
      "",
      "### 6. Review and Escalation",
      "",
      "Flag items that require attention:",
      "",
      "- **Aged items:** Reconciling items outstanding more than 30/60/90 days",
      "- **Large items:** Individual items exceeding materiality thresholds",
      "- **Growing balances:** Reconciling item totals increasing period over period",
      "- **Unresolved prior period items:** Items carried forward without resolution",
      "- **Unexplained differences:** Amounts that cannot be tied to specific transactions",
      "",
      "### 7. Output",
      "",
      "Provide:",
      "1. The formatted reconciliation workpaper",
      "2. List of reconciling items with categorization and aging",
      "3. Required adjusting entries (if any permanent differences identified)",
      "4. Action items for items requiring follow-up",
      "5. Comparison to prior period reconciliation (if available)",
      "6. Sign-off section for preparer and reviewer",
      "",
    ].join("\n"),
  },
  "finance-reconciliation-guide": {
    "SKILL.md": [
      "---",
      "name: finance-reconciliation-guide",
      "description: Reconcile accounts by comparing GL balances to subledgers, bank statements, or third-party data. Use when performing bank reconciliations, GL-to-subledger recs, intercompany reconciliations, or identifying and categorizing reconciling items.",
      "metadata:",
      "  plugin: finance",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Reconciliation",
      "",
      "**Important**: This skill assists with reconciliation workflows but does not provide financial advice. All reconciliations should be reviewed by qualified financial professionals before sign-off.",
      "",
      "Methodology and best practices for account reconciliation, including GL-to-subledger, bank reconciliations, and intercompany. Covers reconciling item categorization, aging analysis, and escalation.",
      "",
      "## Reconciliation Types",
      "",
      "### GL to Subledger Reconciliation",
      "",
      "Compare the general ledger control account balance to the detailed subledger balance.",
      "",
      "**Common accounts:**",
      "- Accounts receivable (GL control vs AR subledger aging)",
      "- Accounts payable (GL control vs AP subledger aging)",
      "- Fixed assets (GL control vs fixed asset register)",
      "- Inventory (GL control vs inventory valuation report)",
      "- Prepaid expenses (GL control vs prepaid amortization schedule)",
      "- Accrued liabilities (GL control vs accrual detail schedules)",
      "",
      "**Process:**",
      "1. Pull GL balance for the control account as of period end",
      "2. Pull subledger trial balance or detail report as of the same date",
      "3. Compare totals — they should match if posting is real-time",
      "4. Investigate any differences (timing of posting, manual entries not reflected, interface errors)",
      "",
      "**Common causes of differences:**",
      "- Manual journal entries posted to the control account but not reflected in the subledger",
      "- Subledger transactions not yet interfaced to the GL",
      "- Timing differences in batch posting",
      "- Reclassification entries in the GL without subledger adjustment",
      "- System interface errors or failed postings",
      "",
      "### Bank Reconciliation",
      "",
      "Compare the GL cash balance to the bank statement balance.",
      "",
      "**Process:**",
      "1. Obtain the bank statement balance as of period end",
      "2. Pull the GL cash account balance as of the same date",
      "3. Identify outstanding checks (issued but not cleared at the bank)",
      "4. Identify deposits in transit (recorded in GL but not yet credited by bank)",
      "5. Identify bank charges, interest, or adjustments not yet recorded in GL",
      "6. Reconcile both sides to an adjusted balance",
      "",
      "**Standard format:**",
      "",
      "```",
      "Balance per bank statement:         $XX,XXX",
      "Add: Deposits in transit            $X,XXX",
      "Less: Outstanding checks           ($X,XXX)",
      "Add/Less: Bank errors               $X,XXX",
      "Adjusted bank balance:              $XX,XXX",
      "",
      "Balance per general ledger:         $XX,XXX",
      "Add: Interest/credits not recorded  $X,XXX",
      "Less: Bank fees not recorded       ($X,XXX)",
      "Add/Less: GL errors                 $X,XXX",
      "Adjusted GL balance:                $XX,XXX",
      "",
      "Difference:                         $0.00",
      "```",
      "",
      "### Intercompany Reconciliation",
      "",
      "Reconcile balances between related entities to ensure they net to zero on consolidation.",
      "",
      "**Process:**",
      "1. Pull intercompany receivable/payable balances for each entity pair",
      "2. Compare Entity A's receivable from Entity B to Entity B's payable to Entity A",
      "3. Identify and resolve differences",
      "4. Confirm all intercompany transactions have been recorded on both sides",
      "5. Verify elimination entries are correct for consolidation",
      "",
      "**Common causes of differences:**",
      "- Transactions recorded by one entity but not the other (timing)",
      "- Different FX rates used by each entity",
      "- Misclassification (intercompany vs third-party)",
      "- Disputed amounts or unapplied payments",
      "- Different period-end cut-off practices across entities",
      "",
      "## Reconciling Item Categorization",
      "",
      "### Category 1: Timing Differences",
      "",
      "Items that exist because of normal processing timing and will clear without action:",
      "",
      "- **Outstanding checks:** Checks issued and recorded in GL, pending bank clearance",
      "- **Deposits in transit:** Deposits made and recorded in GL, pending bank credit",
      "- **In-transit transactions:** Items posted in one system but pending interface to the other",
      "- **Pending approvals:** Transactions awaiting approval to post in one system",
      "",
      "**Expected resolution:** These items should clear within the normal processing cycle (typically 1-5 business days). No adjusting entry needed.",
      "",
      "### Category 2: Adjustments Required",
      "",
      "Items that require a journal entry to correct:",
      "",
      "- **Unrecorded bank charges:** Bank fees, wire charges, returned item fees",
      "- **Unrecorded interest:** Interest income or expense from bank/lender",
      "- **Recording errors:** Wrong amount, wrong account, duplicates",
      "- **Missing entries:** Transactions in one system with no corresponding entry in the other",
      "- **Classification errors:** Correctly recorded but in the wrong account",
      "",
      "**Action:** Prepare adjusting journal entry to correct the GL or subledger.",
      "",
      "### Category 3: Requires Investigation",
      "",
      "Items that cannot be immediately explained:",
      "",
      "- **Unidentified differences:** Variances with no obvious cause",
      "- **Disputed items:** Amounts contested between parties",
      "- **Aged outstanding items:** Items that have not cleared within expected timeframes",
      "- **Recurring unexplained differences:** Same type of difference appearing each period",
      "",
      "**Action:** Investigate root cause, document findings, escalate if unresolved.",
      "",
      "## Aging Analysis for Outstanding Items",
      "",
      "Track the age of reconciling items to identify stale items requiring escalation:",
      "",
      "| Age Bucket | Status | Action |",
      "|-----------|--------|--------|",
      "| 0-30 days | Current | Monitor — within normal processing cycle |",
      "| 31-60 days | Aging | Investigate — follow up on why item has not cleared |",
      "| 61-90 days | Overdue | Escalate — notify supervisor, document investigation |",
      "| 90+ days | Stale | Escalate to management — potential write-off or adjustment needed |",
      "",
      "### Aging Report Format",
      "",
      "| Item # | Description | Amount | Date Originated | Age (Days) | Category | Status | Owner |",
      "|--------|-------------|--------|-----------------|------------|----------|--------|-------|",
      "| 1      | [Detail]    | $X,XXX | [Date]          | XX         | [Type]   | [Status] | [Name] |",
      "",
      "### Trending",
      "",
      "Track reconciling item totals over time to identify growing balances:",
      "",
      "- Compare total outstanding items to prior period",
      "- Flag if total reconciling items exceed materiality threshold",
      "- Flag if number of items is growing period over period",
      "- Identify recurring items that appear every period (may indicate process issue)",
      "",
      "## Escalation Thresholds",
      "",
      "Define escalation triggers based on your organization's risk tolerance:",
      "",
      "| Trigger | Threshold (Example) | Escalation |",
      "|---------|---------------------|------------|",
      "| Individual item amount | > $10,000 | Supervisor review |",
      "| Individual item amount | > $50,000 | Controller review |",
      "| Total reconciling items | > $100,000 | Controller review |",
      "| Item age | > 60 days | Supervisor follow-up |",
      "| Item age | > 90 days | Controller / management review |",
      "| Unreconciled difference | Any amount | Cannot close — must resolve or document |",
      "| Growing trend | 3+ consecutive periods | Process improvement investigation |",
      "",
      "*Note: Set thresholds based on your organization's materiality level and risk appetite. The examples above are illustrative.*",
      "",
      "## Reconciliation Best Practices",
      "",
      "1. **Timeliness:** Complete reconciliations within the close calendar deadline (typically T+3 to T+5 business days after period end)",
      "2. **Completeness:** Reconcile all balance sheet accounts on a defined frequency (monthly for material accounts, quarterly for immaterial)",
      "3. **Documentation:** Every reconciliation should include preparer, reviewer, date, and clear explanation of all reconciling items",
      "4. **Segregation:** The person who reconciles should not be the same person who processes transactions in that account",
      "5. **Follow-through:** Track open items to resolution — do not just carry items forward indefinitely",
      "6. **Root cause analysis:** For recurring reconciling items, investigate and fix the underlying process issue",
      "7. **Standardization:** Use consistent templates and procedures across all accounts",
      "8. **Retention:** Maintain reconciliations and supporting detail per your organization's document retention policy",
      "",
    ].join("\n"),
  },
  "finance-sox-testing": {
    "SKILL.md": [
      "---",
      "name: finance-sox-testing",
      "description: Generate SOX sample selections, testing workpapers, and control assessments",
      "metadata:",
      "  plugin: finance",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <control area> [period]",
      "---",
      "",
      "# SOX Compliance Testing",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "**Important**: This command assists with SOX compliance workflows but does not provide audit or legal advice. All testing workpapers and assessments should be reviewed by qualified financial professionals before use in audit documentation.",
      "",
      "Generate sample selections, create testing workpapers, document control assessments, and provide testing templates for SOX 404 internal controls over financial reporting.",
      "",
      "## Usage",
      "",
      "```",
      "/sox <control-area> <period>",
      "```",
      "",
      "### Arguments",
      "",
      "- `control-area` — The control area to test:",
      "  - `revenue-recognition` — Revenue cycle controls (order-to-cash)",
      "  - `procure-to-pay` or `p2p` — Procurement and AP controls (purchase-to-pay)",
      "  - `payroll` — Payroll processing and compensation controls",
      "  - `financial-close` — Period-end close and reporting controls",
      "  - `treasury` — Cash management and treasury controls",
      "  - `fixed-assets` — Capital asset lifecycle controls",
      "  - `inventory` — Inventory valuation and management controls",
      "  - `itgc` — IT general controls (access, change management, operations)",
      "  - `entity-level` — Entity-level and monitoring controls",
      "  - `journal-entries` — Journal entry processing controls",
      "  - Any specific control ID or name",
      "- `period` — The testing period (e.g., `2024-Q4`, `2024`, `2024-H2`)",
      "",
      "## Workflow",
      "",
      "### 1. Identify Controls to Test",
      "",
      "Based on the control area, identify the key controls. Present the control matrix:",
      "",
      "| Control # | Control Description | Type | Frequency | Key/Non-Key | Risk | Assertion |",
      "|-----------|-------------------|------|-----------|-------------|------|-----------|",
      "| [ID]      | [Description]     | Manual/Automated/IT-Dependent | Daily/Weekly/Monthly/Quarterly/Annual | Key | High/Medium/Low | [CEAVOP] |",
      "",
      "**Control types:**",
      "- **Automated:** System-enforced controls with no manual intervention",
      "- **Manual:** Controls performed by personnel with judgment",
      "- **IT-dependent manual:** Manual controls that rely on system-generated data",
      "",
      "**Assertions (CEAVOP):**",
      "- **C**ompleteness — All transactions are recorded",
      "- **E**xistence/Occurrence — Transactions actually occurred",
      "- **A**ccuracy — Amounts are correctly recorded",
      "- **V**aluation — Assets/liabilities are properly valued",
      "- **O**bligations/Rights — Entity has rights to assets, obligations for liabilities",
      "- **P**resentation/Disclosure — Properly classified and disclosed",
      "",
      "### 2. Determine Sample Size",
      "",
      "Calculate sample sizes based on control frequency and risk:",
      "",
      "| Control Frequency | Population Size (approx.) | Recommended Sample |",
      "|------------------|--------------------------|-------------------|",
      "| Annual           | 1                        | 1 (test the instance) |",
      "| Quarterly        | 4                        | 2 |",
      "| Monthly          | 12                       | 2-4 (based on risk) |",
      "| Weekly           | 52                       | 5-15 (based on risk) |",
      "| Daily            | ~250                     | 20-40 (based on risk) |",
      "| Per-transaction  | Varies                   | 25-60 (based on risk and volume) |",
      "",
      "Adjust for:",
      "- **Risk level:** Higher risk controls require larger samples",
      "- **Prior year results:** Controls with prior deficiencies need larger samples",
      "- **Reliance:** Controls relied upon by external auditors may need larger samples",
      "",
      "### 3. Generate Sample Selection",
      "",
      "Select samples from the population using the appropriate method:",
      "",
      "**Random selection** (default for transaction-level controls):",
      "- Generate random numbers to select specific items from the population",
      "- Ensure coverage across the full period",
      "",
      "**Systematic selection** (for periodic controls):",
      "- Select items at fixed intervals with a random start point",
      "- Ensure representation across all sub-periods",
      "",
      "**Targeted selection** (supplement to random, for risk-based testing):",
      "- Select items with specific risk characteristics (high dollar, unusual, period-end)",
      "- Document rationale for targeted selections",
      "",
      "Present the sample:",
      "",
      "```",
      "SAMPLE SELECTION",
      "Control: [Control ID] — [Description]",
      "Period: [Testing period]",
      "Population: [Count] items, $[Total value]",
      "Sample size: [N] items",
      "Selection method: [Random/Systematic/Targeted]",
      "",
      "| Sample # | Transaction Date | Reference/ID | Amount | Selection Basis |",
      "|----------|-----------------|--------------|--------|-----------------|",
      "| 1        | [Date]          | [Ref]        | $X,XXX | Random          |",
      "| 2        | [Date]          | [Ref]        | $X,XXX | Random          |",
      "| ...      | ...             | ...          | ...    | ...             |",
      "```",
      "",
      "### 4. Create Testing Workpaper",
      "",
      "Generate a testing template for each control:",
      "",
      "```",
      "SOX CONTROL TESTING WORKPAPER",
      "==============================",
      "Control #: [ID]",
      "Control Description: [Full description of the control activity]",
      "Control Owner: [Role/title — to be filled by tester]",
      "Control Type: [Manual/Automated/IT-Dependent Manual]",
      "Frequency: [How often the control operates]",
      "Key Control: [Yes/No]",
      "Relevant Assertion(s): [CEAVOP]",
      "Testing Period: [Period]",
      "",
      "TEST OBJECTIVE:",
      "To determine whether [control description] operated effectively throughout the testing period.",
      "",
      "TEST PROCEDURES:",
      "1. [Step 1 — What to inspect, examine, or re-perform]",
      "2. [Step 2 — What evidence to obtain]",
      "3. [Step 3 — What to compare or verify]",
      "4. [Step 4 — How to evaluate completeness of performance]",
      "5. [Step 5 — How to assess timeliness of performance]",
      "",
      "EXPECTED EVIDENCE:",
      "- [Document type 1 — e.g., signed approval form]",
      "- [Document type 2 — e.g., system screenshot showing review]",
      "- [Document type 3 — e.g., reconciliation with preparer sign-off]",
      "",
      "TEST RESULTS:",
      "",
      "| Sample # | Ref | Procedure 1 | Procedure 2 | Procedure 3 | Result | Exception? | Notes |",
      "|----------|-----|-------------|-------------|-------------|--------|------------|-------|",
      "| 1        |     | Pass/Fail   | Pass/Fail   | Pass/Fail   | Pass/Fail | Y/N    |       |",
      "| 2        |     | Pass/Fail   | Pass/Fail   | Pass/Fail   | Pass/Fail | Y/N    |       |",
      "",
      "EXCEPTIONS NOTED:",
      "| Sample # | Exception Description | Root Cause | Compensating Control | Impact |",
      "|----------|----------------------|------------|---------------------|--------|",
      "|          |                      |            |                     |        |",
      "",
      "CONCLUSION:",
      "[ ] Effective — Control operated effectively with no exceptions",
      "[ ] Effective with exceptions — Control operated effectively; exceptions are isolated",
      "[ ] Deficiency — Control did not operate effectively",
      "[ ] Significant Deficiency — Deficiency is more than inconsequential",
      "[ ] Material Weakness — Reasonable possibility of material misstatement not prevented/detected",
      "",
      "Tested by: ________________  Date: ________",
      "Reviewed by: _______________  Date: ________",
      "```",
      "",
      "### 5. Provide Common Control Templates",
      "",
      "Based on the control area, provide pre-built test step templates:",
      "",
      "**Revenue Recognition:**",
      "- Verify sales order approval and authorization",
      "- Confirm delivery/performance evidence",
      "- Test revenue recognition timing against contract terms",
      "- Verify pricing accuracy to contract/price list",
      "- Test credit memo approval and validity",
      "",
      "**Procure to Pay:**",
      "- Verify purchase order approval and authorization limits",
      "- Confirm three-way match (PO, receipt, invoice)",
      "- Test vendor master data change controls",
      "- Verify payment approval and segregation of duties",
      "- Test duplicate payment prevention controls",
      "",
      "**Financial Close:**",
      "- Verify account reconciliation completeness and timeliness",
      "- Test journal entry approval and segregation of duties",
      "- Verify management review of financial statements",
      "- Test consolidation and elimination entries",
      "- Verify disclosure checklist completion",
      "",
      "**ITGC:**",
      "- Test user access provisioning and de-provisioning",
      "- Verify privileged access reviews",
      "- Test change management approval and testing",
      "- Verify batch job monitoring and exception handling",
      "- Test backup and recovery procedures",
      "",
      "### 6. Document Control Assessment",
      "",
      "Classify any identified deficiencies:",
      "",
      "**Deficiency:** A control does not allow management or employees to prevent or detect misstatements on a timely basis. Consider:",
      "- Likelihood of misstatement",
      "- Magnitude of potential misstatement",
      "- Whether compensating controls exist",
      "",
      "**Significant Deficiency:** A deficiency (or combination) that is less severe than a material weakness but important enough to merit attention by those responsible for oversight.",
      "",
      "**Material Weakness:** A deficiency (or combination) such that there is a reasonable possibility that a material misstatement will not be prevented or detected on a timely basis.",
      "",
      "### 7. Output",
      "",
      "Provide:",
      "1. Control matrix for the selected area",
      "2. Sample selections with methodology documentation",
      "3. Testing workpaper templates with pre-populated test steps",
      "4. Results documentation template",
      "5. Deficiency evaluation framework (if exceptions are identified)",
      "6. Suggested remediation actions for any noted deficiencies",
      "",
    ].join("\n"),
  },
  "finance-variance-analysis": {
    "SKILL.md": [
      "---",
      "name: finance-variance-analysis",
      "description: Decompose variances into drivers with narrative explanations and waterfall analysis",
      "metadata:",
      "  plugin: finance",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <line item> <period> vs <comparison>",
      "---",
      "",
      "# Variance / Flux Analysis",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "**Important**: This command assists with variance analysis workflows but does not provide financial advice. All analyses should be reviewed by qualified financial professionals before use in reporting.",
      "",
      "Decompose variances into underlying drivers, provide narrative explanations for significant variances, and generate waterfall analysis.",
      "",
      "## Usage",
      "",
      "```",
      "/flux <area> <period-comparison>",
      "```",
      "",
      "### Arguments",
      "",
      "- `area` — The area to analyze:",
      "  - `revenue` — Revenue variance by stream, product, geography, customer segment",
      "  - `opex` — Operating expense variance by category, department, cost center",
      "  - `capex` — Capital expenditure variance vs budget by project and asset class",
      "  - `headcount` — Headcount and compensation variance by department and role level",
      "  - `cogs` or `cost-of-revenue` — Cost of revenue variance by component",
      "  - `gross-margin` — Gross margin analysis with mix and rate effects",
      "  - Any specific GL account or account group",
      "- `period-comparison` — The periods to compare. Formats:",
      "  - `2024-12 vs 2024-11` — Month over month",
      "  - `2024-12 vs 2023-12` — Year over year",
      "  - `2024-Q4 vs 2024-Q3` — Quarter over quarter",
      "  - `2024-12 vs budget` — Actual vs budget",
      "  - `2024-12 vs forecast` — Actual vs forecast",
      "  - `2024-Q4 vs 2024-Q3 vs 2023-Q4` — Three-way comparison",
      "",
      "## Workflow",
      "",
      "### 1. Gather Data",
      "",
      "If ~~erp or ~~data warehouse is connected:",
      "- Pull actuals for both comparison periods at the detail level",
      "- Pull budget/forecast data if comparing to plan",
      "- Pull supporting operational metrics (headcount, volumes, rates)",
      "- Pull prior variance analyses for context",
      "",
      "If no data source is connected:",
      "> Connect ~~erp or ~~data warehouse to pull financial data automatically. To analyze manually, provide:",
      "> 1. Actual data for both comparison periods (at account or line-item detail)",
      "> 2. Budget/forecast data (if comparing to plan)",
      "> 3. Any operational metrics that drive the financial results (headcount, volumes, pricing, etc.)",
      "",
      "### 2. Calculate Top-Level Variance",
      "",
      "```",
      "VARIANCE SUMMARY: [Area] — [Period 1] vs [Period 2]",
      "",
      "                              Period 1   Period 2   Variance ($)   Variance (%)",
      "                              --------   --------   ------------   ------------",
      "Total [Area]                  $XX,XXX    $XX,XXX    $X,XXX         X.X%",
      "```",
      "",
      "### 3. Decompose Variance by Driver",
      "",
      "Break down the total variance into constituent drivers. Use the appropriate decomposition method for the area:",
      "",
      "**Revenue Decomposition:**",
      "- **Volume effect:** Change in units/customers/transactions at prior period pricing",
      "- **Price/rate effect:** Change in pricing/ASP applied to current period volume",
      "- **Mix effect:** Shift between products/segments at different margin levels",
      "- **New vs existing:** Revenue from new customers/products vs base business",
      "- **Currency effect:** FX impact on international revenue (if applicable)",
      "",
      "**Operating Expense Decomposition:**",
      "- **Headcount-driven:** Salary and benefits changes from headcount additions/reductions",
      "- **Compensation changes:** Merit increases, promotions, bonus accruals",
      "- **Volume-driven:** Expenses that scale with business activity (hosting, commissions, travel)",
      "- **New programs/investments:** Incremental spend on new initiatives",
      "- **One-time items:** Non-recurring expenses (severance, legal settlements, write-offs)",
      "- **Timing:** Expenses shifted between periods (prepaid amortization changes, contract timing)",
      "",
      "**CapEx Decomposition:**",
      "- **Headquarter-level:** Variance by capital project vs approved budget",
      "- **Timing:** Headquarters ahead of or behind schedule",
      "- **Scope changes:** Approved scope expansions or reductions",
      "- **Cost overruns:** Unit cost increases vs plan",
      "",
      "**Headcount Decomposition:**",
      "- **Hiring pace:** Actual hires vs plan by department and level",
      "- **Attrition:** Unplanned departures and backfill timing",
      "- **Compensation mix:** Salary, bonus, equity, benefits variance",
      "- **Contractor/temp:** Supplemental workforce changes",
      "",
      "### 4. Waterfall Analysis",
      "",
      "Generate a text-based waterfall showing how each driver contributes to the total variance:",
      "",
      "```",
      "WATERFALL: [Area] — [Period 1] vs [Period 2]",
      "",
      "[Period 2 Base]                           $XX,XXX",
      "  |",
      "  |--[+] [Driver 1 description]          +$X,XXX",
      "  |--[+] [Driver 2 description]          +$X,XXX",
      "  |--[-] [Driver 3 description]          -$X,XXX",
      "  |--[+] [Driver 4 description]          +$X,XXX",
      "  |--[-] [Driver 5 description]          -$X,XXX",
      "  |",
      "[Period 1 Actual]                         $XX,XXX",
      "",
      "Variance Reconciliation:",
      "  Driver 1:    +$X,XXX  (XX% of total variance)",
      "  Driver 2:    +$X,XXX  (XX% of total variance)",
      "  Driver 3:    -$X,XXX  (XX% of total variance)",
      "  Driver 4:    +$X,XXX  (XX% of total variance)",
      "  Driver 5:    -$X,XXX  (XX% of total variance)",
      "  Unexplained: $X,XXX   (XX% of total variance)",
      "               --------",
      "  Total:       $X,XXX   (100%)",
      "```",
      "",
      "### 5. Narrative Explanations",
      "",
      "For each significant driver, generate a narrative explanation:",
      "",
      "> **[Driver name]** — [Favorable/Unfavorable] variance of $X,XXX (X.X%)",
      ">",
      "> [2-3 sentence explanation of what caused this variance, referencing specific operational factors, business events, or decisions. Include quantification where possible.]",
      ">",
      "> *Outlook:* [Whether this is expected to continue, reverse, or change in future periods]",
      "",
      "### 6. Identify Unexplained Variances",
      "",
      "If the decomposition does not fully explain the total variance, flag the residual:",
      "",
      "> **Unexplained variance:** $X,XXX (X.X% of total)",
      ">",
      "> Possible causes to investigate:",
      "> - [Suggested area 1]",
      "> - [Suggested area 2]",
      "> - [Suggested area 3]",
      "",
      "Ask the user for additional context on unexplained variances:",
      '- "Can you provide context on [specific unexplained item]?"',
      '- "Were there any business events in [period] that would explain [variance area]?"',
      '- "Is the [specific driver] variance expected or a surprise?"',
      "",
      "### 7. Output",
      "",
      "Provide:",
      "1. Top-level variance summary",
      "2. Detailed variance decomposition by driver",
      "3. Waterfall analysis (text format, or suggest chart if spreadsheet tool is connected)",
      "4. Narrative explanations for each significant driver",
      "5. Unexplained variance flag with investigation suggestions",
      "6. Trend context (is this variance new, growing, or consistent with recent periods?)",
      "7. Suggested actions or follow-ups",
      "",
    ].join("\n"),
  },
  "finance-variance-analysis-guide": {
    "SKILL.md": [
      "---",
      "name: finance-variance-analysis-guide",
      "description: Decompose financial variances into drivers with narrative explanations and waterfall analysis. Use when analyzing budget vs. actual, period-over-period changes, revenue or expense variances, or preparing variance commentary for leadership.",
      "metadata:",
      "  plugin: finance",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Variance Analysis",
      "",
      "**Important**: This skill assists with variance analysis workflows but does not provide financial advice. All analyses should be reviewed by qualified financial professionals before use in reporting.",
      "",
      "Techniques for decomposing variances, materiality thresholds, narrative generation, waterfall chart methodology, and budget vs actual vs forecast comparisons.",
      "",
      "## Variance Decomposition Techniques",
      "",
      "### Price / Volume Decomposition",
      "",
      "The most fundamental variance decomposition. Used for revenue, cost of goods, and any metric that can be expressed as Price x Volume.",
      "",
      "**Formula:**",
      "```",
      "Total Variance = Actual - Budget (or Prior)",
      "",
      "Volume Effect  = (Actual Volume - Budget Volume) x Budget Price",
      "Price Effect   = (Actual Price - Budget Price) x Actual Volume",
      "Mix Effect     = Residual (interaction term), or allocated proportionally",
      "",
      "Verification:  Volume Effect + Price Effect = Total Variance",
      "               (when mix is embedded in the price/volume terms)",
      "```",
      "",
      "**Three-way decomposition (separating mix):**",
      "```",
      "Volume Effect = (Actual Volume - Budget Volume) x Budget Price x Budget Mix",
      "Price Effect  = (Actual Price - Budget Price) x Budget Volume x Actual Mix",
      "Mix Effect    = Budget Price x Budget Volume x (Actual Mix - Budget Mix)",
      "```",
      "",
      "**Example — Revenue variance:**",
      "- Budget: 10,000 units at $50 = $500,000",
      "- Actual: 11,000 units at $48 = $528,000",
      "- Total variance: +$28,000 favorable",
      "  - Volume effect: +1,000 units x $50 = +$50,000 (favorable — sold more units)",
      "  - Price effect: -$2 x 11,000 units = -$22,000 (unfavorable — lower ASP)",
      "  - Net: +$28,000",
      "",
      "### Rate / Mix Decomposition",
      "",
      "Used when analyzing blended rates across segments with different unit economics.",
      "",
      "**Formula:**",
      "```",
      "Rate Effect = Sum of (Actual Volume_i x (Actual Rate_i - Budget Rate_i))",
      "Mix Effect  = Sum of (Budget Rate_i x (Actual Volume_i - Expected Volume_i at Budget Mix))",
      "```",
      "",
      "**Example — Gross margin variance:**",
      "- Product A: 60% margin, Product B: 40% margin",
      "- Budget mix: 50% A, 50% B → Blended margin 50%",
      "- Actual mix: 40% A, 60% B → Blended margin 48%",
      "- Mix effect explains 2pp of margin compression",
      "",
      "### Headcount / Compensation Decomposition",
      "",
      "Used for analyzing payroll and people-cost variances.",
      "",
      "```",
      "Total Comp Variance = Actual Compensation - Budget Compensation",
      "",
      "Decompose into:",
      "1. Headcount variance    = (Actual HC - Budget HC) x Budget Avg Comp",
      "2. Rate variance         = (Actual Avg Comp - Budget Avg Comp) x Budget HC",
      "3. Mix variance          = Difference due to level/department mix shift",
      "4. Timing variance       = Hiring earlier/later than planned (partial-period effect)",
      "5. Attrition impact      = Savings from unplanned departures (partially offset by backfill costs)",
      "```",
      "",
      "### Spend Category Decomposition",
      "",
      "Used for operating expense analysis when price/volume is not applicable.",
      "",
      "```",
      "Total OpEx Variance = Actual OpEx - Budget OpEx",
      "",
      "Decompose by:",
      "1. Headcount-driven costs    (salaries, benefits, payroll taxes, recruiting)",
      "2. Volume-driven costs       (hosting, transaction fees, commissions, shipping)",
      "3. Discretionary spend       (travel, events, professional services, marketing programs)",
      "4. Contractual/fixed costs   (rent, insurance, software licenses, subscriptions)",
      "5. One-time / non-recurring  (severance, legal settlements, write-offs, project costs)",
      "6. Timing / phasing          (spend shifted between periods vs plan)",
      "```",
      "",
      "## Materiality Thresholds and Investigation Triggers",
      "",
      "### Setting Thresholds",
      "",
      "Materiality thresholds determine which variances require investigation and narrative explanation. Set thresholds based on:",
      "",
      "1. **Financial statement materiality:** Typically 1-5% of a key benchmark (revenue, total assets, net income)",
      "2. **Line item size:** Larger line items warrant lower percentage thresholds",
      "3. **Volatility:** More volatile line items may need higher thresholds to avoid noise",
      "4. **Management attention:** What level of variance would change a decision?",
      "",
      "### Recommended Threshold Framework",
      "",
      "| Comparison Type | Dollar Threshold | Percentage Threshold | Trigger |",
      "|----------------|-----------------|---------------------|---------|",
      "| Actual vs Budget | Organization-specific | 10% | Either exceeded |",
      "| Actual vs Prior Period | Organization-specific | 15% | Either exceeded |",
      "| Actual vs Forecast | Organization-specific | 5% | Either exceeded |",
      "| Sequential (MoM) | Organization-specific | 20% | Either exceeded |",
      "",
      "*Set dollar thresholds based on your organization's size. Common practice: 0.5%-1% of revenue for income statement items.*",
      "",
      "### Investigation Priority",
      "",
      "When multiple variances exceed thresholds, prioritize investigation by:",
      "",
      "1. **Largest absolute dollar variance** — biggest P&L impact",
      "2. **Largest percentage variance** — may indicate process issue or error",
      "3. **Unexpected direction** — variance opposite to trend or expectation",
      "4. **New variance** — item that was on track and is now off",
      "5. **Cumulative/trending variance** — growing each period",
      "",
      "## Narrative Generation for Variance Explanations",
      "",
      "### Structure for Each Variance Narrative",
      "",
      "```",
      "[Line Item]: [Favorable/Unfavorable] variance of $[amount] ([percentage]%)",
      "vs [comparison basis] for [period]",
      "",
      "Driver: [Primary driver description]",
      "[2-3 sentences explaining the business reason for the variance, with specific",
      "quantification of contributing factors]",
      "",
      "Outlook: [One-time / Expected to continue / Improving / Deteriorating]",
      "Action: [None required / Monitor / Investigate further / Update forecast]",
      "```",
      "",
      "### Narrative Quality Checklist",
      "",
      "Good variance narratives should be:",
      "",
      '- [ ] **Specific:** Names the actual driver, not just "higher than expected"',
      "- [ ] **Quantified:** Includes dollar and percentage impact of each driver",
      "- [ ] **Causal:** Explains WHY it happened, not just WHAT happened",
      "- [ ] **Forward-looking:** States whether the variance is expected to continue",
      "- [ ] **Actionable:** Identifies any required follow-up or decision",
      "- [ ] **Concise:** 2-4 sentences, not a paragraph of filler",
      "",
      "### Common Narrative Anti-Patterns to Avoid",
      "",
      '- "Revenue was higher than budget due to higher revenue" (circular — no actual explanation)',
      '- "Expenses were elevated this period" (vague — which expenses? why?)',
      '- "Timing" without specifying what was early/late and when it will normalize',
      '- "One-time" without explaining what the item was',
      '- "Various small items" for a material variance (must decompose further)',
      "- Focusing only on the largest driver and ignoring offsetting items",
      "",
      "## Waterfall Chart Methodology",
      "",
      "### Concept",
      "",
      "A waterfall (or bridge) chart shows how you get from one value to another through a series of positive and negative contributors. Used to visualize variance decomposition.",
      "",
      "### Data Structure",
      "",
      "```",
      "Starting value:  [Base/Budget/Prior period amount]",
      "Drivers:         [List of contributing factors with signed amounts]",
      "Ending value:    [Actual/Current period amount]",
      "",
      "Verification:    Starting value + Sum of all drivers = Ending value",
      "```",
      "",
      "### Text-Based Waterfall Format",
      "",
      "When a charting tool is not available, present as a text waterfall:",
      "",
      "```",
      "WATERFALL: Revenue — Q4 Actual vs Q4 Budget",
      "",
      "Q4 Budget Revenue                                    $10,000K",
      "  |",
      "  |--[+] Volume growth (new customers)               +$800K",
      "  |--[+] Expansion revenue (existing customers)      +$400K",
      "  |--[-] Price reductions / discounting               -$200K",
      "  |--[-] Churn / contraction                          -$350K",
      "  |--[+] FX tailwind                                  +$50K",
      "  |--[-] Timing (deals slipped to Q1)                 -$150K",
      "  |",
      "Q4 Actual Revenue                                    $10,550K",
      "",
      "Net Variance: +$550K (+5.5% favorable)",
      "```",
      "",
      "### Bridge Reconciliation Table",
      "",
      "Complement the waterfall with a reconciliation table:",
      "",
      "| Driver | Amount | % of Variance | Cumulative |",
      "|--------|--------|---------------|------------|",
      "| Volume growth | +$800K | 145% | +$800K |",
      "| Expansion revenue | +$400K | 73% | +$1,200K |",
      "| Price reductions | -$200K | -36% | +$1,000K |",
      "| Churn / contraction | -$350K | -64% | +$650K |",
      "| FX tailwind | +$50K | 9% | +$700K |",
      "| Timing (deal slippage) | -$150K | -27% | +$550K |",
      "| **Total variance** | **+$550K** | **100%** | |",
      "",
      "*Note: Percentages can exceed 100% for individual drivers when there are offsetting items.*",
      "",
      "### Waterfall Best Practices",
      "",
      "1. Order drivers from largest positive to largest negative (or in logical business sequence)",
      '2. Keep to 5-8 drivers maximum — aggregate smaller items into "Other"',
      "3. Verify the waterfall reconciles (start + drivers = end)",
      "4. Color-code: green for favorable, red for unfavorable (in visual charts)",
      "5. Label each bar with both the amount and a brief description",
      '6. Include a "Total Variance" summary bar',
      "",
      "## Budget vs Actual vs Forecast Comparisons",
      "",
      "### Three-Way Comparison Framework",
      "",
      "| Metric | Budget | Forecast | Actual | Bud Var ($) | Bud Var (%) | Fcast Var ($) | Fcast Var (%) |",
      "|--------|--------|----------|--------|-------------|-------------|---------------|---------------|",
      "| Revenue | $X | $X | $X | $X | X% | $X | X% |",
      "| COGS | $X | $X | $X | $X | X% | $X | X% |",
      "| Gross Profit | $X | $X | $X | $X | X% | $X | X% |",
      "",
      "### When to Use Each Comparison",
      "",
      "- **Actual vs Budget:** Annual performance measurement, compensation decisions, board reporting. Budget is set at the beginning of the year and typically not changed.",
      "- **Actual vs Forecast:** Operational management, identifying emerging issues. Forecast is updated periodically (monthly or quarterly) to reflect current expectations.",
      "- **Forecast vs Budget:** Understanding how expectations have changed since planning. Useful for identifying planning accuracy issues.",
      "- **Actual vs Prior Period:** Trend analysis, sequential performance. Useful when budget is not meaningful (new business lines, post-acquisition).",
      "- **Actual vs Prior Year:** Year-over-year growth analysis, seasonality-adjusted comparison.",
      "",
      "### Forecast Accuracy Analysis",
      "",
      "Track how accurate forecasts are over time to improve planning:",
      "",
      "```",
      "Forecast Accuracy = 1 - |Actual - Forecast| / |Actual|",
      "",
      "MAPE (Mean Absolute Percentage Error) = Average of |Actual - Forecast| / |Actual| across periods",
      "```",
      "",
      "| Period | Forecast | Actual | Variance | Accuracy |",
      "|--------|----------|--------|----------|----------|",
      "| Jan    | $X       | $X     | $X (X%)  | XX%      |",
      "| Feb    | $X       | $X     | $X (X%)  | XX%      |",
      "| ...    | ...      | ...    | ...      | ...      |",
      "| **Avg**|          |        | **MAPE** | **XX%**  |",
      "",
      "### Variance Trending",
      "",
      "Track how variances evolve over the year to identify systematic bias:",
      "",
      "- **Consistently favorable:** Budget may be too conservative (sandbagging)",
      "- **Consistently unfavorable:** Budget may be too aggressive or execution issues",
      "- **Growing unfavorable:** Deteriorating performance or unrealistic targets",
      "- **Shrinking variance:** Forecast accuracy improving through the year (normal pattern)",
      "- **Volatile:** Unpredictable business or poor forecasting methodology",
      "",
    ].join("\n"),
  },
  init: {
    "SKILL.md": [
      "---",
      "name: init",
      "description: Bootstrap an AGENTS.md file in a new or existing project",
      "---",
      "",
      "<system>",
      "Use your tools to create or improve an AGENTS.md file in the root of the workspace which will serve as a contribution guide for AI agents.",
      "If an AGENTS.md file already exists, focus on additive improvement (preserve intent and useful information; refine, extend, and reorganize as needed) rather than replacing it wholesale.",
      "Inspect the workspace layout, code, documentation and git history to ensure correctness and accuracy.",
      "",
      "Ensure the following preamble exists at the top of the file before any other sections. Do not include the surrounding code fence backticks; only include the text.",
      "",
      "```md",
      "You are an experienced, pragmatic software engineering AI agent. Do not over-engineer a solution when a simple one is possible. Keep edits minimal. If you want an exception to ANY rule, you MUST stop and get permission first.",
      "```",
      "",
      "Recommended sections:",
      "",
      "- Headquarter Overview (mandatory)",
      "  - Basic details about the project (e.g., high-level overview and goals).",
      "  - Technology choices (e.g., languages, databases, frameworks, libraries, build tools).",
      "- Reference (mandatory)",
      "  - List important code files.",
      "  - List important directories and basic code structure tips.",
      "  - Headquarter architecture.",
      "- Essential commands (mandatory)",
      "  - build",
      "  - format",
      "  - lint",
      "  - test",
      "  - clean",
      "  - development server",
      "  - other _important_ scripts (use `find -type f -name '*.sh'` or similar)",
      "- Patterns (optional)",
      "  - List any important or uncommon patterns (compared to other similar codebases), with examples (e.g., how to authorize an HTTP request).",
      "  - List any important workflows and their steps (e.g., how to make a database migration).",
      "  - Testing patterns.",
      "- Anti-patterns (optional)",
      "  - Search git history and comments to find recurring mistakes or forbidden patterns.",
      "  - List each pattern and its reason.",
      "- Code style (optional)",
      "  - Style guide to follow (with link).",
      "- Commit and Pull Request Guidelines (mandatory)",
      "  - Required steps for validating changes before committing.",
      "  - Commit message conventions (read `git log`, or use `type: message` by default).",
      "  - Pull request description requirements.",
      "",
      "You can add other sections if they are necessary.",
      "If the information required for mandatory sections isn't available due to the workspace being empty or sparse, add TODO text in its place.",
      "Optional sections should be scrapped if the information is too thin.",
      "",
      "Some investigation tips:",
      "",
      "- Read existing lint configs, tsconfig, and CI workflows to find any style or layout rules.",
      '- Search for "TODO", "HACK", "FIXME", "don\'t", "never", "always" in comments.',
      "- Examine test files for patterns.",
      "- Read PR templates and issue templates if they exist.",
      "- Check for existing CONTRIBUTING.md, CODE_OF_CONDUCT.md, or similar documentation files.",
      "",
      "Some writing tips:",
      "",
      '- Each "do X" should have a corresponding "don\'t Y" where applicable.',
      "- Commands should be easily copy-pastable and tested.",
      "- Terms or phrases specific to this project should be explained on first use.",
      "- Anything that is against the norm should be explicitly highlighted and called out.",
      "",
      "Above all things:",
      "",
      "- The document must be clear and concise. Simple projects should need less than 400 words, but larger and more mature codebases will likely need 700+. Prioritize completeness over brevity.",
      "- Don't include useless fluff.",
      "- The document must be in Markdown format and use headings for structure.",
      "- Give examples where necessary or helpful (commands, directory paths, naming patterns).",
      "- Explanations and examples must be correct and specific to this codebase.",
      "- Maintain a professional, instructional tone.",
      "",
      "If the workspace is empty or sparse, ask the user for more information. Avoid hallucinating important decisions. You can provide suggestions to the user for language/technology/tool choices, but always respect the user's decision.",
      "",
      "- Headquarter description and goals.",
      "- Language(s).",
      "- Technologies (database?), frameworks, libraries.",
      "- Tools.",
      "- Any other questions as you deem necessary.",
      "",
      "For empty or sparse workspaces ONLY, when finished writing/updating AGENTS.md, ask the user if they would like you to do the following:",
      "",
      "- initialize git IF it's not already set up (e.g., `git init`, `git remote add`, etc.)",
      "- write a concise README.md file",
      "- generate the bare minimum project scaffolding (e.g., initializing the package manager, writing a minimal build tool config)",
      "  </system>",
      "",
    ].join("\n"),
  },
  "lattice-docs": {
    "SKILL.md": [
      "---",
      "name: lattice-docs",
      "description: Index + offline snapshot of Lattice documentation (progressive disclosure).",
      "---",
      "",
      "# lattice docs",
      "",
      "This built-in skill helps the agent answer questions about **Lattice** (Runtime Enforcement and Identity Infrastructure for Autonomous AI Agents) without dumping the entire docs into context.",
      "",
      "## How to use",
      "",
      "### Prefer: read the bundled docs snapshot (recommended)",
      "",
      "This skill bundles an **offline snapshot of the Lattice docs** under `references/docs/`.",
      "",
      "Why prefer the bundled snapshot?",
      "",
      "1. The docs tree below is guaranteed to match what’s embedded.",
      "2. It’s more likely to match *your installed Lattice version* (the live site may be ahead).",
      "",
      "To read a specific page:",
      "",
      "```ts",
      "agent_skill_read_file({",
      '  name: "lattice-docs",',
      '  filePath: "references/docs/config/models.mdx",',
      "});",
      "```",
      "",
      "### Fallback: bundled docs",
      "",
      "If the bundled docs don't mention something, refer to the local documentation in the `docs/` directory.",
      "",
      "#### Docs tree (auto-generated)",
      "",
      "Use this index to find a page's:",
      "",
      "- **Docs route** (for `web_fetch`)",
      "- **Embedded file path** (for `agent_skill_read_file`)",
      "",
      "<!-- BEGIN DOCS_TREE -->",
      "- **Documentation**",
      "  - **Getting Started**",
      "    - Introduction (`/`) → `references/docs/index.mdx`",
      "    - Install (`/install`) → `references/docs/install.mdx` — Download and install Lattice for macOS, Linux, and Windows",
      "    - **Models**",
      "      - Models (`/config/models`) → `references/docs/config/models.mdx` — Select and configure AI models in Lattice",
      "      - Providers (`/config/providers`) → `references/docs/config/providers.mdx` — Configure API keys and settings for AI providers",
      "    - Why Parallelize? (`/getting-started/why-parallelize`) → `references/docs/getting-started/why-parallelize.mdx` — Use cases for running multiple AI agents in parallel",
      "    - Lattice Codes (`/getting-started/lattice-codes`) → `references/docs/getting-started/lattice-codes.mdx` — Redeem free LLM token credits for evaluating Lattice",
      "    - CLI (`/reference/cli`) → `references/docs/reference/cli.mdx` — Run one-off agent tasks from the command line with `lattice run`",
      "  - **Workspaces**",
      "    - Workspaces (`/workspaces`) → `references/docs/workspaces/index.mdx` — Isolated development environments for parallel agent work",
      "    - Forking Workspaces (`/workspaces/fork`) → `references/docs/workspaces/fork.mdx` — Clone workspaces with conversation history to explore alternatives",
      "    - Message Sharing (`/workspaces/sharing`) → `references/docs/workspaces/sharing.mdx` — Share encrypted messages with cryptographic signatures via Lattice",
      "    - **Compaction**",
      "      - Compaction (`/workspaces/compaction`) → `references/docs/workspaces/compaction/index.mdx` — Managing conversation context size with compaction",
      "      - Manual Compaction (`/workspaces/compaction/manual`) → `references/docs/workspaces/compaction/manual.mdx` — Commands for manually managing conversation context",
      "      - Automatic Compaction (`/workspaces/compaction/automatic`) → `references/docs/workspaces/compaction/automatic.mdx` — Let Lattice automatically compact your conversations based on usage or idle time",
      "      - Customization (`/workspaces/compaction/customization`) → `references/docs/workspaces/compaction/customization.mdx` — Customize the compaction system prompt",
      "    - **Runtimes**",
      "      - Runtimes (`/runtime`) → `references/docs/runtime/index.mdx` — Configure where and how Lattice executes agent workspaces",
      "      - Local Runtime (`/runtime/local`) → `references/docs/runtime/local.mdx` — Run agents directly in your project directory",
      "      - Worktree Runtime (`/runtime/worktree`) → `references/docs/runtime/worktree.mdx` — Isolated git worktree environments for parallel agent work",
      "      - SSH Runtime (`/runtime/ssh`) → `references/docs/runtime/ssh.mdx` — Run agents on remote hosts over SSH for security and performance",
      "      - Lattice Runtime (`/runtime/lattice`) → `references/docs/runtime/lattice.mdx` — Run agents on Lattice workspaces",
      "      - Docker Runtime (`/runtime/docker`) → `references/docs/runtime/docker.mdx` — Run agents in isolated Docker containers",
      "      - Dev Container Runtime (`/runtime/devcontainer`) → `references/docs/runtime/devcontainer.mdx` — Run agents in containers defined by devcontainer.json",
      "    - **Hooks**",
      "      - Init Hooks (`/hooks/init`) → `references/docs/hooks/init.mdx` — Run setup commands automatically when creating new workspaces",
      "      - Tool Hooks (`/hooks/tools`) → `references/docs/hooks/tools.mdx` — Block dangerous commands, lint after edits, and set up your environment",
      "      - Environment Variables (`/hooks/environment-variables`) → `references/docs/hooks/environment-variables.mdx` — Environment variables available in agent bash commands and hooks",
      "  - **Agents**",
      "    - Agents (`/agents`) → `references/docs/agents/index.mdx` — Define custom agents (modes + subagents) with Markdown files",
      "    - Instruction Files (`/agents/instruction-files`) → `references/docs/agents/instruction-files.mdx` — Configure agent behavior with AGENTS.md files",
      "    - Agent Skills (`/agents/agent-skills`) → `references/docs/agents/agent-skills.mdx` — Share reusable workflows and references with skills",
      "    - Plan Mode (`/agents/plan-mode`) → `references/docs/agents/plan-mode.mdx` — Review and collaborate on plans before execution",
      "    - System Prompt (`/agents/system-prompt`) → `references/docs/agents/system-prompt.mdx` — How Lattice constructs the system prompt for AI models",
      "    - Prompting Tips (`/agents/prompting-tips`) → `references/docs/agents/prompting-tips.mdx` — Tips and tricks for getting the most out of your AI agents",
      "  - **Configuration**",
      "    - MCP Servers (`/config/mcp-servers`) → `references/docs/config/mcp-servers.mdx` — Extend agent capabilities with Model Context Protocol servers",
      "    - Headquarter Secrets (`/config/project-secrets`) → `references/docs/config/project-secrets.mdx` — Manage environment variables and API keys for your projects",
      "    - Agentic Git Identity (`/config/agentic-git-identity`) → `references/docs/config/agentic-git-identity.mdx` — Configure a separate Git identity for AI-generated commits",
      "    - Keyboard Shortcuts (`/config/keybinds`) → `references/docs/config/keybinds.mdx` — Complete keyboard shortcut reference for Lattice",
      "    - Notifications (`/config/notifications`) → `references/docs/config/notifications.mdx` — Configure how agents notify you about important events",
      "    - Vim Mode (`/config/vim-mode`) → `references/docs/config/vim-mode.mdx` — Vim-style editing in the Lattice chat input",
      "  - **Guides**",
      "    - GitHub Actions (`/guides/github-actions`) → `references/docs/guides/github-actions.mdx` — Automate your workflows with lattice run in GitHub Actions",
      "    - Agentic Git Identity (`/config/agentic-git-identity`) → `references/docs/config/agentic-git-identity.mdx` — Configure a separate Git identity for AI-generated commits",
      "    - Prompting Tips (`/agents/prompting-tips`) → `references/docs/agents/prompting-tips.mdx` — Tips and tricks for getting the most out of your AI agents",
      "  - **Integrations**",
      "    - VS Code Extension (`/integrations/vscode-extension`) → `references/docs/integrations/vscode-extension.mdx` — Pair Lattice workspaces with VS Code and Cursor editors",
      "  - **Reference**",
      "    - Telemetry (`/reference/telemetry`) → `references/docs/reference/telemetry.mdx` — What Lattice collects, what it doesn’t, and how to disable it",
      "    - Storybook (`/reference/storybook`) → `references/docs/reference/storybook.mdx` — Develop and test Lattice UI states in isolation",
      "    - Terminal Benchmarking (`/reference/benchmarking`) → `references/docs/reference/benchmarking.mdx` — Run Terminal-Bench benchmarks with the Lattice adapter",
      "    - AGENTS.md (`/AGENTS`) → `references/docs/AGENTS.md` — Agent instructions for AI assistants working on the Lattice codebase",
      "<!-- END DOCS_TREE -->",
      "",
      "1. Read the docs navigation (source of truth for which pages exist):",
      "",
      "```ts",
      'agent_skill_read_file({ name: "lattice-docs", filePath: "references/docs/docs.json" });',
      "```",
      "",
      "2. Read a specific page by path (mirrors `docs/` in the lattice repo):",
      "",
      "- `/agents` → `references/docs/agents/index.mdx`",
      "- `/config/models` → `references/docs/config/models.mdx`",
      "- `/runtime` → `references/docs/runtime/index.mdx`",
      "",
      "```ts",
      "agent_skill_read_file({",
      '  name: "lattice-docs",',
      '  filePath: "references/docs/config/models.mdx",',
      "});",
      "```",
      "",
      "Notes:",
      "",
      "- Many pages are `.mdx`; some are `.../index.mdx`.",
      "- Images are not embedded; you may see `/img/...` references.",
      "",
      "## When to use",
      "",
      "Use this skill when the user asks how lattice works (workspaces, runtimes, agents, models, hooks, keybinds, etc.).",
      "",
      "## About",
      "",
      "LATTICE RUNTIME- runtime enforcement and identity infrastructure for autonomous ai agents for internal use.",
      "",
    ].join("\n"),
  },
  "legal-brief": {
    "SKILL.md": [
      "---",
      "name: legal-brief",
      "description: Generate contextual briefings for legal work — daily summary, topic research, or incident response",
      "metadata:",
      "  plugin: legal",
      "  type: command",
      '  plugin-version: "1.0.0"',
      '  argument-hint: "[daily | topic <query> | incident]"',
      "---",
      "",
      "# /brief -- Legal Team Briefing",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate contextual briefings for legal work. Supports three modes: daily brief, topic brief, and incident brief.",
      "",
      "**Important**: This command assists with legal workflows but does not provide legal advice. Briefings should be reviewed by qualified legal professionals before being relied upon.",
      "",
      "## Invocation",
      "",
      "```",
      "/brief daily              # Morning brief of legal-relevant items",
      "/brief topic [query]      # Research brief on a specific legal question",
      "/brief incident [topic]   # Rapid brief on a developing situation",
      "```",
      "",
      "If no mode is specified, ask the user which type of brief they need.",
      "",
      "## Modes",
      "",
      "---",
      "",
      "### Daily Brief",
      "",
      "A morning summary of everything a legal team member needs to know to start their day.",
      "",
      "#### Sources to Scan",
      "",
      "Check each connected source for legal-relevant items:",
      "",
      "**Email (if connected):**",
      "- New contract requests or review requests",
      "- Compliance questions or reports",
      "- Responses from counterparties on active negotiations",
      "- Flagged or urgent items from the legal team inbox",
      "- External counsel communications",
      "- Regulatory or legal update newsletters",
      "",
      "**Calendar (if connected):**",
      "- Today's meetings that need legal prep (board meetings, deal reviews, vendor calls)",
      "- Upcoming deadlines this week (contract expirations, filing deadlines, response deadlines)",
      "- Recurring legal team syncs",
      "",
      "**Chat (if connected):**",
      "- Overnight messages in legal team channels",
      "- Direct messages requesting legal input",
      "- Mentions of legal-relevant topics (contract, compliance, privacy, NDA, terms)",
      "- Escalations or urgent requests",
      "",
      "**CLM (if connected):**",
      "- Contracts awaiting review or signature",
      "- Approaching expiration dates (next 30 days)",
      "- Newly executed agreements",
      "",
      "**CRM (if connected):**",
      "- Deals moving to stages that require legal involvement",
      "- New opportunities flagged for legal review",
      "",
      "#### Output Format",
      "",
      "```",
      "## Daily Legal Brief -- [Date]",
      "",
      "### Urgent / Action Required",
      "[Items needing immediate attention, sorted by urgency]",
      "",
      "### Contract Pipeline",
      "- **Awaiting Your Review**: [count and list]",
      "- **Pending Counterparty Response**: [count and list]",
      "- **Approaching Deadlines**: [items due this week]",
      "",
      "### New Requests",
      "[Contract review requests, NDA requests, compliance questions received since last brief]",
      "",
      "### Calendar Today",
      "[Meetings with legal relevance and what prep is needed]",
      "",
      "### Team Activity",
      "[Key messages or updates from legal team channels]",
      "",
      "### This Week's Deadlines",
      "[Upcoming deadlines and filing dates]",
      "",
      "### Sources Not Available",
      "[Any sources that were not connected or returned errors]",
      "```",
      "",
      "---",
      "",
      "### Topic Brief",
      "",
      "Research and brief on a specific legal question or topic across available sources.",
      "",
      "#### Workflow",
      "",
      "1. Accept the topic query from the user",
      "2. Search across connected sources:",
      "   - **Documents**: Internal memos, prior analyses, playbooks, precedent",
      "   - **Email**: Prior communications on the topic",
      "   - **Chat**: Team discussions about the topic",
      "   - **CLM**: Related contracts or clauses",
      "3. Synthesize findings into a structured brief",
      "",
      "#### Output Format",
      "",
      "```",
      "## Topic Brief: [Topic]",
      "",
      "### Summary",
      "[2-3 sentence executive summary of findings]",
      "",
      "### Background",
      "[Context and history from internal sources]",
      "",
      "### Current State",
      "[What the organization's current position or approach is, based on available documents]",
      "",
      "### Key Considerations",
      "[Important factors, risks, or open questions]",
      "",
      "### Internal Precedent",
      "[Prior decisions, memos, or positions found in internal sources]",
      "",
      "### Gaps",
      "[What information is missing or what sources were not available]",
      "",
      "### Recommended Next Steps",
      "[What the user should do with this information]",
      "```",
      "",
      "#### Important Notes",
      "- Topic briefs synthesize what is available in connected sources; they do not substitute for formal legal research",
      "- If the topic requires current legal authority or case law, recommend the user consult a legal research platform (Westlaw, Lexis, etc.) or outside counsel",
      "- Always note the limitations of the sources searched",
      "",
      "---",
      "",
      "### Incident Brief",
      "",
      "Rapid briefing for developing situations that require immediate legal attention (data breaches, litigation threats, regulatory inquiries, IP disputes, etc.).",
      "",
      "#### Workflow",
      "",
      "1. Accept the incident topic or description",
      "2. Rapidly scan all connected sources for relevant context:",
      "   - **Email**: Communications about the incident",
      "   - **Chat**: Real-time discussions and escalations",
      "   - **Documents**: Relevant policies, response plans, insurance coverage",
      "   - **Calendar**: Scheduled response meetings",
      "   - **CLM**: Affected contracts, indemnification provisions, insurance requirements",
      "3. Compile into an actionable incident brief",
      "",
      "#### Output Format",
      "",
      "```",
      "## Incident Brief: [Topic]",
      "**Prepared**: [timestamp]",
      "**Classification**: [severity assessment if determinable]",
      "",
      "### Situation Summary",
      "[What is known about the incident]",
      "",
      "### Timeline",
      "[Chronological summary of events based on available sources]",
      "",
      "### Immediate Legal Considerations",
      "[Regulatory notification requirements, preservation obligations, privilege concerns]",
      "",
      "### Relevant Agreements",
      "[Contracts, insurance policies, or other agreements that may be implicated]",
      "",
      "### Internal Response",
      "[What response activity has already occurred based on email/chat]",
      "",
      "### Key Contacts",
      "[Relevant internal and external contacts identified from sources]",
      "",
      "### Recommended Immediate Actions",
      "1. [Most urgent action]",
      "2. [Second priority]",
      "3. [etc.]",
      "",
      "### Information Gaps",
      "[What is not yet known and needs to be determined]",
      "",
      "### Sources Checked",
      "[What was searched and what was not available]",
      "```",
      "",
      "#### Important Notes for Incident Briefs",
      "- Speed matters. Produce the brief quickly with available information rather than waiting for complete information",
      "- Flag any litigation hold or preservation obligations immediately",
      "- Note privilege considerations (mark the brief as attorney-client privileged / work product if appropriate)",
      "- If the incident may involve a data breach, flag applicable notification deadlines (e.g., 72 hours for GDPR)",
      "- Recommend outside counsel engagement if the matter is significant",
      "",
      "## General Notes",
      "",
      "- If sources are unavailable, note the gaps prominently so the user knows what was not checked",
      "- For daily briefs, learn the user's preferences over time (what they find useful, what they want filtered out)",
      "- Briefs should be actionable: every item should have a clear next step or reason for inclusion",
      "- Keep briefs concise. Link to source materials rather than reproducing them in full",
      "",
    ].join("\n"),
  },
  "legal-canned-responses": {
    "SKILL.md": [
      "---",
      "name: legal-canned-responses",
      "description: Generate templated responses for common legal inquiries and identify when situations require individualized attention. Use when responding to routine legal questions — data subject requests, vendor inquiries, NDA requests, discovery holds — or when managing response templates.",
      "metadata:",
      "  plugin: legal",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Canned Responses Skill",
      "",
      "You are a response template assistant for an in-house legal team. You help manage, customize, and generate templated responses for common legal inquiries, and you identify when a situation should NOT use a templated response and instead requires individualized attention.",
      "",
      "**Important**: You assist with legal workflows but do not provide legal advice. Templated responses should be reviewed before sending, especially for regulated communications.",
      "",
      "## Template Management Methodology",
      "",
      "### Template Organization",
      "",
      "Templates should be organized by category and maintained in the team's local settings. Each template should include:",
      "",
      "1. **Category**: The type of inquiry the template addresses",
      "2. **Template name**: A descriptive identifier",
      "3. **Use case**: When this template is appropriate",
      "4. **Escalation triggers**: When this template should NOT be used",
      "5. **Required variables**: Information that must be customized for each use",
      "6. **Template body**: The response text with variable placeholders",
      "7. **Follow-up actions**: Standard steps after sending the response",
      "8. **Last reviewed date**: When the template was last verified for accuracy",
      "",
      "### Template Lifecycle",
      "",
      "1. **Creation**: Draft template based on best practices and team input",
      "2. **Review**: Legal team review and approval of template content",
      "3. **Publication**: Add to template library with metadata",
      "4. **Use**: Generate responses using the template",
      "5. **Feedback**: Track when templates are modified during use to identify improvement opportunities",
      "6. **Update**: Revise templates when laws, policies, or best practices change",
      "7. **Retirement**: Archive templates that are no longer applicable",
      "",
      "## Response Categories",
      "",
      "### 1. Data Subject Requests (DSRs)",
      "",
      "**Sub-categories**:",
      "- Acknowledgment of receipt",
      "- Identity verification request",
      "- Fulfillment response (access, deletion, correction)",
      "- Partial denial with explanation",
      "- Full denial with explanation",
      "- Extension notification",
      "",
      "**Key template elements**:",
      "- Reference to applicable regulation (GDPR, CCPA, etc.)",
      "- Specific timeline for response",
      "- Identity verification requirements",
      "- Rights of the data subject (including right to complain to supervisory authority)",
      "- Contact information for follow-up",
      "",
      "**Example template structure**:",
      "```",
      "Subject: Your Data [Access/Deletion/Correction] Request - Reference {{request_id}}",
      "",
      "Dear {{requester_name}},",
      "",
      "We have received your request dated {{request_date}} to [access/delete/correct] your personal data under [applicable regulation].",
      "",
      "[Acknowledgment / verification request / fulfillment details / denial basis]",
      "",
      "We will respond substantively by {{response_deadline}}.",
      "",
      "[Contact information]",
      "[Rights information]",
      "```",
      "",
      "### 2. Discovery Holds (Litigation Holds)",
      "",
      "**Sub-categories**:",
      "- Initial hold notice to custodians",
      "- Hold reminder / periodic reaffirmation",
      "- Hold modification (scope change)",
      "- Hold release",
      "",
      "**Key template elements**:",
      "- Matter name and reference number",
      "- Clear preservation obligations",
      "- Scope of preservation (date range, data types, systems, communication types)",
      "- Prohibition on spoliation",
      "- Contact for questions",
      "- Acknowledgment requirement",
      "",
      "**Example template structure**:",
      "```",
      "Subject: LEGAL HOLD NOTICE - {{matter_name}} - Action Required",
      "",
      "PRIVILEGED AND CONFIDENTIAL",
      "ATTORNEY-CLIENT COMMUNICATION",
      "",
      "Dear {{custodian_name}},",
      "",
      "You are receiving this notice because you may possess documents, communications, or data relevant to the matter referenced above.",
      "",
      "PRESERVATION OBLIGATION:",
      "Effective immediately, you must preserve all documents and electronically stored information (ESI) related to:",
      "- Subject matter: {{hold_scope}}",
      "- Date range: {{start_date}} to present",
      "- Document types: {{document_types}}",
      "",
      "DO NOT delete, destroy, modify, or discard any potentially relevant materials.",
      "",
      "[Specific instructions for systems, email, chat, local files]",
      "",
      "Please acknowledge receipt of this notice by {{acknowledgment_deadline}}.",
      "",
      "Contact {{legal_contact}} with any questions.",
      "```",
      "",
      "### 3. Privacy Inquiries",
      "",
      "**Sub-categories**:",
      "- Cookie/tracking inquiry responses",
      "- Privacy policy questions",
      "- Data sharing practice inquiries",
      "- Children's data inquiries",
      "- Cross-border transfer questions",
      "",
      "**Key template elements**:",
      "- Reference to the organization's privacy notice",
      "- Specific answers based on current practices",
      "- Links to relevant privacy documentation",
      "- Contact information for the privacy team",
      "",
      "### 4. Vendor Legal Questions",
      "",
      "**Sub-categories**:",
      "- Contract status inquiry response",
      "- Amendment request response",
      "- Compliance certification requests",
      "- Audit request responses",
      "- Insurance certificate requests",
      "",
      "**Key template elements**:",
      "- Reference to the applicable agreement",
      "- Specific response to the vendor's question",
      "- Any required caveats or limitations",
      "- Next steps and timeline",
      "",
      "### 5. NDA Requests",
      "",
      "**Sub-categories**:",
      "- Sending the organization's standard form NDA",
      "- Accepting a counterparty's NDA (with markup)",
      "- Declining an NDA request with explanation",
      "- NDA renewal or extension",
      "",
      "**Key template elements**:",
      "- Purpose of the NDA",
      "- Standard terms summary",
      "- Execution instructions",
      "- Timeline expectations",
      "",
      "### 6. Subpoena / Legal Process",
      "",
      "**Sub-categories**:",
      "- Acknowledgment of receipt",
      "- Objection letter",
      "- Request for extension",
      "- Compliance cover letter",
      "",
      "**Key template elements**:",
      "- Case reference and jurisdiction",
      "- Specific objections (if any)",
      "- Preservation confirmation",
      "- Timeline for compliance",
      "- Privilege log reference (if applicable)",
      "",
      "**Critical note**: Subpoena responses almost always require individualized counsel review. Templates serve as starting frameworks, not final responses.",
      "",
      "### 7. Insurance Notifications",
      "",
      "**Sub-categories**:",
      "- Initial claim notification",
      "- Supplemental information",
      "- Reservation of rights response",
      "",
      "**Key template elements**:",
      "- Policy number and coverage period",
      "- Description of the matter or incident",
      "- Timeline of events",
      "- Requested coverage confirmation",
      "",
      "## Customization Guidelines",
      "",
      "When generating a response from a template:",
      "",
      "### Required Customization",
      "Every templated response MUST be customized with:",
      "- Correct names, dates, and reference numbers",
      "- Specific facts of the situation",
      "- Applicable jurisdiction and regulation",
      "- Correct response deadlines based on when the inquiry was received",
      "- Appropriate signature block and contact information",
      "",
      "### Tone Adjustment",
      "Adjust tone based on:",
      "- **Audience**: Internal vs. external, business vs. legal, individual vs. regulatory authority",
      "- **Relationship**: New counterparty vs. existing partner vs. adversarial party",
      "- **Sensitivity**: Routine inquiry vs. contentious matter vs. regulatory investigation",
      "- **Urgency**: Standard timeline vs. expedited response needed",
      "",
      "### Jurisdiction-Specific Adjustments",
      "- Verify that cited regulations are correct for the requester's jurisdiction",
      "- Adjust timelines to match applicable law",
      "- Include jurisdiction-specific rights information",
      "- Use jurisdiction-appropriate legal terminology",
      "",
      "## Escalation Trigger Identification",
      "",
      "Every template category has situations where a templated response is inappropriate. Before generating any response, check for these escalation triggers:",
      "",
      "### Universal Escalation Triggers (Apply to All Categories)",
      "- The matter involves potential litigation or regulatory investigation",
      "- The inquiry is from a regulator, government agency, or law enforcement",
      "- The response could create a binding legal commitment or waiver",
      "- The matter involves potential criminal liability",
      "- Media attention is involved or likely",
      "- The situation is unprecedented (no prior handling by the team)",
      "- Multiple jurisdictions are involved with conflicting requirements",
      "- The matter involves executive leadership or board members",
      "",
      "### Category-Specific Escalation Triggers",
      "",
      "**Data Subject Requests**:",
      "- Request from a minor or on behalf of a minor",
      "- Request involves data subject to litigation hold",
      "- Requester is in active litigation or dispute with the organization",
      "- Request is from an employee with an active HR matter",
      "- Request scope is so broad it appears to be a fishing expedition",
      "- Request involves special category data (health, biometric, genetic)",
      "",
      "**Discovery Holds**:",
      "- Potential criminal liability",
      "- Unclear or disputed preservation scope",
      "- Hold conflicts with regulatory deletion requirements",
      "- Prior holds exist for related matters",
      "- Custodian objects to the hold scope",
      "",
      "**Vendor Questions**:",
      "- Vendor is disputing contract terms",
      "- Vendor is threatening litigation or termination",
      "- Response could affect ongoing negotiation",
      "- Question involves regulatory compliance (not just contract interpretation)",
      "",
      "**Subpoena / Legal Process**:",
      "- ALWAYS requires counsel review (templates are starting points only)",
      "- Privilege issues identified",
      "- Third-party data involved",
      "- Cross-border production issues",
      "- Unreasonable timeline",
      "",
      "### When an Escalation Trigger is Detected",
      "",
      "1. **Stop**: Do not generate a templated response",
      "2. **Alert**: Inform the user that an escalation trigger has been detected",
      "3. **Explain**: Describe which trigger was detected and why it matters",
      "4. **Recommend**: Suggest the appropriate escalation path (senior counsel, outside counsel, specific team member)",
      '5. **Offer**: Provide a draft for counsel review (clearly marked as "DRAFT - FOR COUNSEL REVIEW ONLY") rather than a final response',
      "",
      "## Template Creation Guide",
      "",
      "When helping users create new templates:",
      "",
      "### Step 1: Define the Use Case",
      "- What type of inquiry does this address?",
      "- How frequently does this come up?",
      "- Who is the typical audience?",
      "- What is the typical urgency level?",
      "",
      "### Step 2: Identify Required Elements",
      "- What information must be included in every response?",
      "- What regulatory requirements apply?",
      "- What organizational policies govern this type of response?",
      "",
      "### Step 3: Define Variables",
      "- What changes with each use? (names, dates, specifics)",
      "- What stays the same? (legal requirements, standard language)",
      "- Use clear variable names: `{{requester_name}}`, `{{response_deadline}}`, `{{matter_reference}}`",
      "",
      "### Step 4: Draft the Template",
      "- Write in clear, professional language",
      "- Avoid unnecessary legal jargon for business audiences",
      "- Include all legally required elements",
      "- Add placeholders for all variable content",
      "- Include a subject line template if for email use",
      "",
      "### Step 5: Define Escalation Triggers",
      "- What situations should NOT use this template?",
      "- What characteristics indicate the matter needs individualized attention?",
      "- Be specific: vague triggers are not useful",
      "",
      "### Step 6: Add Metadata",
      "- Template name and category",
      "- Version number and last reviewed date",
      "- Author and approver",
      "- Follow-up actions checklist",
      "",
      "### Template Format",
      "",
      "```markdown",
      "## Template: {{template_name}}",
      "**Category**: {{category}}",
      "**Version**: {{version}} | **Last Reviewed**: {{date}}",
      "**Approved By**: {{approver}}",
      "",
      "### Use When",
      "- [Condition 1]",
      "- [Condition 2]",
      "",
      "### Do NOT Use When (Escalation Triggers)",
      "- [Trigger 1]",
      "- [Trigger 2]",
      "",
      "### Variables",
      "| Variable | Description | Example |",
      "|---|---|---|",
      "| {{var1}} | [what it is] | [example value] |",
      "| {{var2}} | [what it is] | [example value] |",
      "",
      "### Subject Line",
      "[Subject template with {{variables}}]",
      "",
      "### Body",
      "[Response body with {{variables}}]",
      "",
      "### Follow-Up Actions",
      "1. [Action 1]",
      "2. [Action 2]",
      "",
      "### Notes",
      "[Any special instructions for users of this template]",
      "```",
      "",
    ].join("\n"),
  },
  "legal-compliance": {
    "SKILL.md": [
      "---",
      "name: legal-compliance",
      "description: Navigate privacy regulations (GDPR, CCPA), review DPAs, and handle data subject requests. Use when reviewing data processing agreements, responding to data subject access or deletion requests, assessing cross-border data transfer requirements, or evaluating privacy compliance.",
      "metadata:",
      "  plugin: legal",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Compliance Skill",
      "",
      "You are a compliance assistant for an in-house legal team. You help with privacy regulation compliance, DPA reviews, data subject request handling, and regulatory monitoring.",
      "",
      "**Important**: You assist with legal workflows but do not provide legal advice. Compliance determinations should be reviewed by qualified legal professionals. Regulatory requirements change frequently; always verify current requirements with authoritative sources.",
      "",
      "## Privacy Regulation Overview",
      "",
      "### GDPR (General Data Protection Regulation)",
      "",
      "**Scope**: Applies to processing of personal data of individuals in the EU/EEA, regardless of where the processing organization is located.",
      "",
      "**Key Obligations for In-House Legal Teams**:",
      "- **Lawful basis**: Identify and document lawful basis for each processing activity (consent, contract, legitimate interest, legal obligation, vital interest, public task)",
      "- **Data subject rights**: Respond to access, rectification, erasure, portability, restriction, and objection requests within 30 days (extendable by 60 days for complex requests)",
      "- **Data protection impact assessments (DPIAs)**: Required for processing likely to result in high risk to individuals",
      "- **Breach notification**: Notify supervisory authority within 72 hours of becoming aware of a personal data breach; notify affected individuals without undue delay if high risk",
      "- **Records of processing**: Maintain Article 30 records of processing activities",
      "- **International transfers**: Ensure appropriate safeguards for transfers outside EEA (SCCs, adequacy decisions, BCRs)",
      "- **DPO requirement**: Appoint a Data Protection Officer if required (public authority, large-scale processing of special categories, large-scale systematic monitoring)",
      "",
      "**Common In-House Legal Touchpoints**:",
      "- Reviewing vendor DPAs for GDPR compliance",
      "- Advising product teams on privacy by design requirements",
      "- Responding to supervisory authority inquiries",
      "- Managing cross-border data transfer mechanisms",
      "- Reviewing consent mechanisms and privacy notices",
      "",
      "### CCPA / CPRA (California Consumer Privacy Act / California Privacy Rights Act)",
      "",
      "**Scope**: Applies to businesses that collect personal information of California residents and meet revenue, data volume, or data sale thresholds.",
      "",
      "**Key Obligations**:",
      "- **Right to know**: Consumers can request disclosure of personal information collected, used, and shared",
      "- **Right to delete**: Consumers can request deletion of their personal information",
      "- **Right to opt-out**: Consumers can opt out of the sale or sharing of personal information",
      "- **Right to correct**: Consumers can request correction of inaccurate personal information (CPRA addition)",
      "- **Right to limit use of sensitive personal information**: Consumers can limit use of sensitive PI to specific purposes (CPRA addition)",
      "- **Non-discrimination**: Cannot discriminate against consumers who exercise their rights",
      "- **Privacy notice**: Must provide a privacy notice at or before collection describing categories of PI collected and purposes",
      "- **Service provider agreements**: Contracts with service providers must restrict use of PI to the specified business purpose",
      "",
      "**Response Timelines**:",
      "- Acknowledge receipt within 10 business days",
      "- Respond substantively within 45 calendar days (extendable by 45 days with notice)",
      "",
      "### Other Key Regulations to Monitor",
      "",
      "| Regulation | Jurisdiction | Key Differentiators |",
      "|---|---|---|",
      "| **LGPD** (Brazil) | Brazil | Similar to GDPR; requires DPO appointment; National Data Protection Authority (ANPD) enforcement |",
      "| **POPIA** (South Africa) | South Africa | Information Regulator oversight; required registration of processing |",
      "| **PIPEDA** (Canada) | Canada (federal) | Consent-based framework; OPC oversight; being modernized |",
      "| **PDPA** (Singapore) | Singapore | Do Not Call registry; mandatory breach notification; PDPC enforcement |",
      "| **Privacy Act** (Australia) | Australia | Australian Privacy Principles (APPs); notifiable data breaches scheme |",
      "| **PIPL** (China) | China | Strict cross-border transfer rules; data localization requirements; CAC oversight |",
      "| **UK GDPR** | United Kingdom | Post-Brexit UK version; ICO oversight; similar to EU GDPR with UK-specific adequacy |",
      "",
      "## DPA Review Checklist",
      "",
      "When reviewing a Data Processing Agreement or Data Processing Addendum, verify the following:",
      "",
      "### Required Elements (GDPR Article 28)",
      "",
      "- [ ] **Subject matter and duration**: Clearly defined scope and term of processing",
      "- [ ] **Nature and purpose**: Specific description of what processing will occur and why",
      "- [ ] **Type of personal data**: Categories of personal data being processed",
      "- [ ] **Categories of data subjects**: Whose personal data is being processed",
      "- [ ] **Controller obligations and rights**: Controller's instructions and oversight rights",
      "",
      "### Processor Obligations",
      "",
      "- [ ] **Process only on documented instructions**: Processor commits to process only per controller's instructions (with exception for legal requirements)",
      "- [ ] **Confidentiality**: Personnel authorized to process have committed to confidentiality",
      "- [ ] **Security measures**: Appropriate technical and organizational measures described (Article 32 reference)",
      "- [ ] **Sub-processor requirements**:",
      "  - [ ] Written authorization requirement (general or specific)",
      "  - [ ] If general authorization: notification of changes with opportunity to object",
      "  - [ ] Sub-processors bound by same obligations via written agreement",
      "  - [ ] Processor remains liable for sub-processor performance",
      "- [ ] **Data subject rights assistance**: Processor will assist controller in responding to data subject requests",
      "- [ ] **Security and breach assistance**: Processor will assist with security obligations, breach notification, DPIAs, and prior consultation",
      "- [ ] **Deletion or return**: On termination, delete or return all personal data (at controller's choice) and delete existing copies unless legal retention required",
      "- [ ] **Audit rights**: Controller has right to conduct audits and inspections (or accept third-party audit reports)",
      "- [ ] **Breach notification**: Processor will notify controller of personal data breaches without undue delay (ideally within 24-48 hours; must enable controller to meet 72-hour regulatory deadline)",
      "",
      "### International Transfers",
      "",
      "- [ ] **Transfer mechanism identified**: SCCs, adequacy decision, BCRs, or other valid mechanism",
      "- [ ] **SCCs version**: Using current EU SCCs (June 2021 version) if applicable",
      "- [ ] **Correct module**: Appropriate SCC module selected (C2P, C2C, P2P, P2C)",
      "- [ ] **Transfer impact assessment**: Completed if transferring to countries without adequacy decisions",
      "- [ ] **Supplementary measures**: Technical, organizational, or contractual measures to address gaps identified in transfer impact assessment",
      "- [ ] **UK addendum**: If UK personal data is in scope, UK International Data Transfer Addendum included",
      "",
      "### Practical Considerations",
      "",
      "- [ ] **Liability**: DPA liability provisions align with (or don't conflict with) the main services agreement",
      "- [ ] **Termination alignment**: DPA term aligns with the services agreement",
      "- [ ] **Data locations**: Processing locations specified and acceptable",
      "- [ ] **Security standards**: Specific security standards or certifications required (SOC 2, ISO 27001, etc.)",
      "- [ ] **Insurance**: Adequate insurance coverage for data processing activities",
      "",
      "### Common DPA Issues",
      "",
      "| Issue | Risk | Standard Position |",
      "|---|---|---|",
      "| Blanket sub-processor authorization without notification | Loss of control over processing chain | Require notification with right to object |",
      "| Breach notification timeline > 72 hours | May prevent timely regulatory notification | Require notification within 24-48 hours |",
      "| No audit rights (or audit rights only via third-party reports) | Cannot verify compliance | Accept SOC 2 Type II + right to audit upon cause |",
      "| Data deletion timeline not specified | Data retained indefinitely | Require deletion within 30-90 days of termination |",
      "| No data processing locations specified | Data could be processed anywhere | Require disclosure of processing locations |",
      "| Outdated SCCs | Invalid transfer mechanism | Require current EU SCCs (2021 version) |",
      "",
      "## Data Subject Request Handling",
      "",
      "### Request Intake",
      "",
      "When a data subject request is received:",
      "",
      "1. **Identify the request type**:",
      "   - Access (copy of personal data)",
      "   - Rectification (correction of inaccurate data)",
      '   - Erasure / deletion ("right to be forgotten")',
      "   - Restriction of processing",
      "   - Data portability (structured, machine-readable format)",
      "   - Objection to processing",
      "   - Opt-out of sale/sharing (CCPA/CPRA)",
      "   - Limit use of sensitive personal information (CPRA)",
      "",
      "2. **Identify applicable regulation(s)**:",
      "   - Where is the data subject located?",
      "   - Which laws apply based on your organization's presence and activities?",
      "   - What are the specific requirements and timelines?",
      "",
      "3. **Verify identity**:",
      "   - Confirm the requester is who they claim to be",
      "   - Use reasonable verification measures proportionate to the sensitivity of the data",
      "   - Do not require excessive documentation",
      "",
      "4. **Log the request**:",
      "   - Date received",
      "   - Request type",
      "   - Requester identity",
      "   - Applicable regulation",
      "   - Response deadline",
      "   - Assigned handler",
      "",
      "### Response Timelines",
      "",
      "| Regulation | Initial Acknowledgment | Substantive Response | Extension |",
      "|---|---|---|---|",
      "| GDPR | Not specified (best practice: promptly) | 30 days | +60 days (with notice) |",
      "| CCPA/CPRA | 10 business days | 45 calendar days | +45 days (with notice) |",
      "| UK GDPR | Not specified (best practice: promptly) | 30 days | +60 days (with notice) |",
      "| LGPD | Not specified | 15 days | Limited extensions |",
      "",
      "### Exemptions and Exceptions",
      "",
      "Before fulfilling a request, check whether any exemptions apply:",
      "",
      "**Common exemptions across regulations**:",
      "- Legal claims defense or establishment",
      "- Legal obligations requiring retention",
      "- Public interest or official authority",
      "- Freedom of expression and information (for erasure requests)",
      "- Archiving in the public interest or scientific/historical research",
      "",
      "**Organization-specific considerations**:",
      "- Litigation hold: Data subject to a legal hold cannot be deleted",
      "- Regulatory retention: Financial records, employment records, and other categories may have mandatory retention periods",
      "- Third-party rights: Fulfilling the request might adversely affect the rights of others",
      "",
      "### Response Process",
      "",
      "1. Gather all personal data of the requester across systems",
      "2. Apply any exemptions and document the basis",
      "3. Prepare response: fulfill the request or explain why (in whole or part) it cannot be fulfilled",
      "4. If denying (in whole or part): cite the specific legal basis for denial",
      "5. Inform the requester of their right to lodge a complaint with the supervisory authority",
      "6. Document the response and retain records of the request and response",
      "",
      "## Regulatory Monitoring Basics",
      "",
      "### What to Monitor",
      "",
      "Maintain awareness of developments in:",
      "- **Regulatory guidance**: New or updated guidance from supervisory authorities (ICO, CNIL, FTC, state AGs, etc.)",
      "- **Enforcement actions**: Fines, orders, and settlements that signal regulatory priorities",
      "- **Legislative changes**: New privacy laws, amendments to existing laws, implementing regulations",
      "- **Industry standards**: Updates to ISO 27001, SOC 2, NIST frameworks, and sector-specific requirements",
      "- **Cross-border transfer developments**: Adequacy decisions, SCC updates, data localization requirements",
      "",
      "### Monitoring Approach",
      "",
      "1. **Subscribe to regulatory authority communications** (newsletters, RSS feeds, official announcements)",
      "2. **Track relevant legal publications** for analysis of new developments",
      "3. **Review industry association updates** for sector-specific guidance",
      "4. **Maintain a regulatory calendar** of known upcoming deadlines, effective dates, and compliance milestones",
      "5. **Brief the legal team** on material developments that affect the organization's processing activities",
      "",
      "### Escalation Criteria",
      "",
      "Escalate regulatory developments to senior counsel or leadership when:",
      "- A new regulation or guidance directly affects the organization's core business activities",
      "- An enforcement action in the organization's sector signals heightened regulatory scrutiny",
      "- A compliance deadline is approaching that requires organizational changes",
      "- A data transfer mechanism the organization relies on is challenged or invalidated",
      "- A regulatory authority initiates an inquiry or investigation involving the organization",
      "",
    ].join("\n"),
  },
  "legal-contract-review": {
    "SKILL.md": [
      "---",
      "name: legal-contract-review",
      'description: "Review contracts against your organization\'s negotiation playbook, flagging deviations and generating redline suggestions. Use when reviewing vendor contracts, customer agreements, or any commercial agreement where you need clause-by-clause analysis against standard positions."',
      "metadata:",
      "  plugin: legal",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Contract Review Skill",
      "",
      "You are a contract review assistant for an in-house legal team. You analyze contracts against the organization's negotiation playbook, identify deviations, classify their severity, and generate actionable redline suggestions.",
      "",
      "**Important**: You assist with legal workflows but do not provide legal advice. All analysis should be reviewed by qualified legal professionals before being relied upon.",
      "",
      "## Playbook-Based Review Methodology",
      "",
      "### Loading the Playbook",
      "",
      "Before reviewing any contract, check for a configured playbook in the user's local settings. The playbook defines the organization's standard positions, acceptable ranges, and escalation triggers for each major clause type.",
      "",
      "If no playbook is available:",
      "- Inform the user and offer to help create one",
      "- If proceeding without a playbook, use widely-accepted commercial standards as a baseline",
      '- Clearly label the review as "based on general commercial standards" rather than organizational positions',
      "",
      "### Review Process",
      "",
      "1. **Identify the contract type**: SaaS agreement, professional services, license, partnership, procurement, etc. The contract type affects which clauses are most material.",
      "2. **Determine the user's side**: Vendor, customer, licensor, licensee, partner. This fundamentally changes the analysis (e.g., limitation of liability protections favor different parties).",
      "3. **Read the entire contract** before flagging issues. Clauses interact with each other (e.g., an uncapped indemnity may be partially mitigated by a broad limitation of liability).",
      "4. **Analyze each material clause** against the playbook position.",
      "5. **Consider the contract holistically**: Are the overall risk allocation and commercial terms balanced?",
      "",
      "## Common Clause Analysis",
      "",
      "### Limitation of Liability",
      "",
      "**Key elements to review:**",
      "- Cap amount (fixed dollar amount, multiple of fees, or uncapped)",
      "- Whether the cap is mutual or applies differently to each party",
      "- Carveouts from the cap (what liabilities are uncapped)",
      "- Whether consequential, indirect, special, or punitive damages are excluded",
      "- Whether the exclusion is mutual",
      "- Carveouts from the consequential damages exclusion",
      "- Whether the cap applies per-claim, per-year, or aggregate",
      "",
      "**Common issues:**",
      '- Cap set at a fraction of fees paid (e.g., "fees paid in the prior 3 months" on a low-value contract)',
      "- Asymmetric carveouts favoring the drafter",
      '- Broad carveouts that effectively eliminate the cap (e.g., "any breach of Section X" where Section X covers most obligations)',
      "- No consequential damages exclusion for one party's breaches",
      "",
      "### Indemnification",
      "",
      "**Key elements to review:**",
      "- Whether indemnification is mutual or unilateral",
      "- Scope: what triggers the indemnification obligation (IP infringement, data breach, bodily injury, breach of reps and warranties)",
      "- Whether indemnification is capped (often subject to the overall liability cap, or sometimes uncapped)",
      "- Procedure: notice requirements, right to control defense, right to settle",
      "- Whether the indemnitee must mitigate",
      "- Relationship between indemnification and the limitation of liability clause",
      "",
      "**Common issues:**",
      "- Unilateral indemnification for IP infringement when both parties contribute IP",
      '- Indemnification for "any breach" (too broad; essentially converts the liability cap to uncapped liability)',
      "- No right to control defense of claims",
      "- Indemnification obligations that survive termination indefinitely",
      "",
      "### Intellectual Property",
      "",
      "**Key elements to review:**",
      "- Ownership of pre-existing IP (each party should retain their own)",
      "- Ownership of IP developed during the engagement",
      "- Work-for-hire provisions and their scope",
      "- License grants: scope, exclusivity, territory, sublicensing rights",
      "- Open source considerations",
      "- Feedback clauses (grants on suggestions or improvements)",
      "",
      "**Common issues:**",
      "- Broad IP assignment that could capture the customer's pre-existing IP",
      "- Work-for-hire provisions extending beyond the deliverables",
      "- Unrestricted feedback clauses granting perpetual, irrevocable licenses",
      "- License scope broader than needed for the business relationship",
      "",
      "### Data Protection",
      "",
      "**Key elements to review:**",
      "- Whether a Data Processing Agreement/Addendum (DPA) is required",
      "- Data controller vs. data processor classification",
      "- Sub-processor rights and notification obligations",
      "- Data breach notification timeline (72 hours for GDPR)",
      "- Cross-border data transfer mechanisms (SCCs, adequacy decisions, binding corporate rules)",
      "- Data deletion or return obligations on termination",
      "- Data security requirements and audit rights",
      "- Purpose limitation for data processing",
      "",
      "**Common issues:**",
      "- No DPA when personal data is being processed",
      "- Blanket authorization for sub-processors without notification",
      "- Breach notification timeline longer than regulatory requirements",
      "- No cross-border transfer protections when data moves internationally",
      "- Inadequate data deletion provisions",
      "",
      "### Term and Termination",
      "",
      "**Key elements to review:**",
      "- Initial term and renewal terms",
      "- Auto-renewal provisions and notice periods",
      "- Termination for convenience: available? notice period? early termination fees?",
      "- Termination for cause: cure period? what constitutes cause?",
      "- Effects of termination: data return, transition assistance, survival clauses",
      "- Wind-down period and obligations",
      "",
      "**Common issues:**",
      "- Long initial terms with no termination for convenience",
      "- Auto-renewal with short notice windows (e.g., 30-day notice for annual renewal)",
      "- No cure period for termination for cause",
      "- Inadequate transition assistance provisions",
      "- Survival clauses that effectively extend the agreement indefinitely",
      "",
      "### Governing Law and Dispute Resolution",
      "",
      "**Key elements to review:**",
      "- Choice of law (governing jurisdiction)",
      "- Dispute resolution mechanism (litigation, arbitration, mediation first)",
      "- Venue and jurisdiction for litigation",
      "- Arbitration rules and seat (if arbitration)",
      "- Jury waiver",
      "- Class action waiver",
      "- Prevailing party attorney's fees",
      "",
      "**Common issues:**",
      "- Unfavorable jurisdiction (unusual or remote venue)",
      "- Mandatory arbitration with rules favorable to the drafter",
      "- Waiver of jury trial without corresponding protections",
      "- No escalation process before formal dispute resolution",
      "",
      "## Deviation Severity Classification",
      "",
      "### GREEN -- Acceptable",
      "",
      "The clause aligns with or is better than the organization's standard position. Minor variations that are commercially reasonable and do not increase risk materially.",
      "",
      "**Examples:**",
      "- Liability cap at 18 months of fees when standard is 12 months (better for the customer)",
      "- Mutual NDA term of 2 years when standard is 3 years (shorter but reasonable)",
      "- Governing law in a well-established commercial jurisdiction close to the preferred one",
      "",
      "**Action**: Note for awareness. No negotiation needed.",
      "",
      "### YELLOW -- Negotiate",
      "",
      "The clause falls outside the standard position but within a negotiable range. The term is common in the market but not the organization's preference. Requires attention and likely negotiation, but not escalation.",
      "",
      "**Examples:**",
      "- Liability cap at 6 months of fees when standard is 12 months (below standard but negotiable)",
      "- Unilateral indemnification for IP infringement when standard is mutual (common market position but not preferred)",
      "- Auto-renewal with 60-day notice when standard is 90 days",
      "- Governing law in an acceptable but not preferred jurisdiction",
      "",
      "**Action**: Generate specific redline language. Provide fallback position. Estimate business impact of accepting vs. negotiating.",
      "",
      "### RED -- Escalate",
      "",
      "The clause falls outside acceptable range, triggers a defined escalation criterion, or poses material risk. Requires senior counsel review, outside counsel involvement, or business decision-maker sign-off.",
      "",
      "**Examples:**",
      "- Uncapped liability or no limitation of liability clause",
      "- Unilateral broad indemnification with no cap",
      "- IP assignment of pre-existing IP",
      "- No DPA offered when personal data is processed",
      "- Unreasonable non-compete or exclusivity provisions",
      "- Governing law in a problematic jurisdiction with mandatory arbitration",
      "",
      "**Action**: Explain the specific risk. Provide market-standard alternative language. Estimate exposure. Recommend escalation path.",
      "",
      "## Redline Generation Best Practices",
      "",
      "When generating redline suggestions:",
      "",
      "1. **Be specific**: Provide exact language, not vague guidance. The redline should be ready to insert.",
      "2. **Be balanced**: Propose language that is firm on critical points but commercially reasonable. Overly aggressive redlines slow negotiations.",
      "3. **Explain the rationale**: Include a brief, professional rationale suitable for sharing with the counterparty's counsel.",
      "4. **Provide fallback positions**: For YELLOW items, include a fallback position if the primary ask is rejected.",
      "5. **Prioritize**: Not all redlines are equal. Indicate which are must-haves and which are nice-to-haves.",
      "6. **Consider the relationship**: Adjust tone and approach based on whether this is a new vendor, strategic partner, or commodity supplier.",
      "",
      "### Redline Format",
      "",
      "For each redline:",
      "```",
      "**Clause**: [Section reference and clause name]",
      '**Current language**: "[exact quote from the contract]"',
      '**Proposed redline**: "[specific alternative language with additions in bold and deletions struck through conceptually]"',
      "**Rationale**: [1-2 sentences explaining why, suitable for external sharing]",
      "**Priority**: [Must-have / Should-have / Nice-to-have]",
      "**Fallback**: [Alternative position if primary redline is rejected]",
      "```",
      "",
      "## Negotiation Priority Framework",
      "",
      "When presenting redlines, organize by negotiation priority:",
      "",
      "### Tier 1 -- Must-Haves (Deal Breakers)",
      "Issues where the organization cannot proceed without resolution:",
      "- Uncapped or materially insufficient liability protections",
      "- Missing data protection requirements for regulated data",
      "- IP provisions that could jeopardize core assets",
      "- Terms that conflict with regulatory obligations",
      "",
      "### Tier 2 -- Should-Haves (Strong Preferences)",
      "Issues that materially affect risk but have negotiation room:",
      "- Liability cap adjustments within range",
      "- Indemnification scope and mutuality",
      "- Termination flexibility",
      "- Audit and compliance rights",
      "",
      "### Tier 3 -- Nice-to-Haves (Concession Candidates)",
      "Issues that improve the position but can be conceded strategically:",
      "- Preferred governing law (if alternative is acceptable)",
      "- Notice period preferences",
      "- Minor definitional improvements",
      "- Insurance certificate requirements",
      "",
      "**Negotiation strategy**: Lead with Tier 1 items. Trade Tier 3 concessions to secure Tier 2 wins. Never concede on Tier 1 without escalation.",
      "",
    ].join("\n"),
  },
  "legal-legal-risk-assessment": {
    "SKILL.md": [
      "---",
      "name: legal-legal-risk-assessment",
      "description: Assess and classify legal risks using a severity-by-likelihood framework with escalation criteria. Use when evaluating contract risk, assessing deal exposure, classifying issues by severity, or determining whether a matter needs senior counsel or outside legal review.",
      "metadata:",
      "  plugin: legal",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Legal Risk Assessment Skill",
      "",
      "You are a legal risk assessment assistant for an in-house legal team. You help evaluate, classify, and document legal risks using a structured framework based on severity and likelihood.",
      "",
      "**Important**: You assist with legal workflows but do not provide legal advice. Risk assessments should be reviewed by qualified legal professionals. The framework provided is a starting point that organizations should customize to their specific risk appetite and industry context.",
      "",
      "## Risk Assessment Framework",
      "",
      "### Severity x Likelihood Matrix",
      "",
      "Legal risks are assessed on two dimensions:",
      "",
      "**Severity** (impact if the risk materializes):",
      "",
      "| Level | Label | Description |",
      "|---|---|---|",
      "| 1 | **Negligible** | Minor inconvenience; no material financial, operational, or reputational impact. Can be handled within normal operations. |",
      "| 2 | **Low** | Limited impact; minor financial exposure (< 1% of relevant contract/deal value); minor operational disruption; no public attention. |",
      "| 3 | **Moderate** | Meaningful impact; material financial exposure (1-5% of relevant value); noticeable operational disruption; potential for limited public attention. |",
      "| 4 | **High** | Significant impact; substantial financial exposure (5-25% of relevant value); significant operational disruption; likely public attention; potential regulatory scrutiny. |",
      "| 5 | **Critical** | Severe impact; major financial exposure (> 25% of relevant value); fundamental business disruption; significant reputational damage; regulatory action likely; potential personal liability for officers/directors. |",
      "",
      "**Likelihood** (probability the risk materializes):",
      "",
      "| Level | Label | Description |",
      "|---|---|---|",
      "| 1 | **Remote** | Highly unlikely to occur; no known precedent in similar situations; would require exceptional circumstances. |",
      "| 2 | **Unlikely** | Could occur but not expected; limited precedent; would require specific triggering events. |",
      "| 3 | **Possible** | May occur; some precedent exists; triggering events are foreseeable. |",
      "| 4 | **Likely** | Probably will occur; clear precedent; triggering events are common in similar situations. |",
      "| 5 | **Almost Certain** | Expected to occur; strong precedent or pattern; triggering events are present or imminent. |",
      "",
      "### Risk Score Calculation",
      "",
      "**Risk Score = Severity x Likelihood**",
      "",
      "| Score Range | Risk Level | Color |",
      "|---|---|---|",
      "| 1-4 | **Low Risk** | GREEN |",
      "| 5-9 | **Medium Risk** | YELLOW |",
      "| 10-15 | **High Risk** | ORANGE |",
      "| 16-25 | **Critical Risk** | RED |",
      "",
      "### Risk Matrix Visualization",
      "",
      "```",
      "                    LIKELIHOOD",
      "                Remote  Unlikely  Possible  Likely  Almost Certain",
      "                  (1)     (2)       (3)      (4)        (5)",
      "SEVERITY",
      "Critical (5)  |   5    |   10   |   15   |   20   |     25     |",
      "High     (4)  |   4    |    8   |   12   |   16   |     20     |",
      "Moderate (3)  |   3    |    6   |    9   |   12   |     15     |",
      "Low      (2)  |   2    |    4   |    6   |    8   |     10     |",
      "Negligible(1) |   1    |    2   |    3   |    4   |      5     |",
      "```",
      "",
      "## Risk Classification Levels with Recommended Actions",
      "",
      "### GREEN -- Low Risk (Score 1-4)",
      "",
      "**Characteristics**:",
      "- Minor issues that are unlikely to materialize",
      "- Standard business risks within normal operating parameters",
      "- Well-understood risks with established mitigations in place",
      "",
      "**Recommended Actions**:",
      "- **Accept**: Acknowledge the risk and proceed with standard controls",
      "- **Document**: Record in the risk register for tracking",
      "- **Monitor**: Include in periodic reviews (quarterly or annually)",
      "- **No escalation required**: Can be managed by the responsible team member",
      "",
      "**Examples**:",
      "- Vendor contract with minor deviation from standard terms in a non-critical area",
      "- Routine NDA with a well-known counterparty in a standard jurisdiction",
      "- Minor administrative compliance task with clear deadline and owner",
      "",
      "### YELLOW -- Medium Risk (Score 5-9)",
      "",
      "**Characteristics**:",
      "- Moderate issues that could materialize under foreseeable circumstances",
      "- Risks that warrant attention but do not require immediate action",
      "- Issues with established precedent for management",
      "",
      "**Recommended Actions**:",
      "- **Mitigate**: Implement specific controls or negotiate to reduce exposure",
      "- **Monitor actively**: Review at regular intervals (monthly or as triggers occur)",
      "- **Document thoroughly**: Record risk, mitigations, and rationale in risk register",
      "- **Assign owner**: Ensure a specific person is responsible for monitoring and mitigation",
      "- **Brief stakeholders**: Inform relevant business stakeholders of the risk and mitigation plan",
      "- **Escalate if conditions change**: Define trigger events that would elevate the risk level",
      "",
      "**Examples**:",
      "- Contract with liability cap below standard but within negotiable range",
      "- Vendor processing personal data in a jurisdiction without clear adequacy determination",
      "- Regulatory development that may affect a business activity in the medium term",
      "- IP provision that is broader than preferred but common in the market",
      "",
      "### ORANGE -- High Risk (Score 10-15)",
      "",
      "**Characteristics**:",
      "- Significant issues with meaningful probability of materializing",
      "- Risks that could result in substantial financial, operational, or reputational impact",
      "- Issues that require senior attention and dedicated mitigation efforts",
      "",
      "**Recommended Actions**:",
      "- **Escalate to senior counsel**: Brief the head of legal or designated senior counsel",
      "- **Develop mitigation plan**: Create a specific, actionable plan to reduce the risk",
      "- **Brief leadership**: Inform relevant business leaders of the risk and recommended approach",
      "- **Set review cadence**: Review weekly or at defined milestones",
      "- **Consider outside counsel**: Engage outside counsel for specialized advice if needed",
      "- **Document in detail**: Full risk memo with analysis, options, and recommendations",
      "- **Define contingency plan**: What will the organization do if the risk materializes?",
      "",
      "**Examples**:",
      "- Contract with uncapped indemnification in a material area",
      "- Data processing activity that may violate a regulatory requirement if not restructured",
      "- Threatened litigation from a significant counterparty",
      "- IP infringement allegation with colorable basis",
      "- Regulatory inquiry or audit request",
      "",
      "### RED -- Critical Risk (Score 16-25)",
      "",
      "**Characteristics**:",
      "- Severe issues that are likely or certain to materialize",
      "- Risks that could fundamentally impact the business, its officers, or its stakeholders",
      "- Issues requiring immediate executive attention and rapid response",
      "",
      "**Recommended Actions**:",
      "- **Immediate escalation**: Brief General Counsel, C-suite, and/or Board as appropriate",
      "- **Engage outside counsel**: Retain specialized outside counsel immediately",
      "- **Establish response team**: Dedicated team to manage the risk with clear roles",
      "- **Consider insurance notification**: Notify insurers if applicable",
      "- **Crisis management**: Activate crisis management protocols if reputational risk is involved",
      "- **Preserve evidence**: Implement litigation hold if legal proceedings are possible",
      "- **Daily or more frequent review**: Active management until the risk is resolved or reduced",
      "- **Board reporting**: Include in board risk reporting as appropriate",
      "- **Regulatory notifications**: Make any required regulatory notifications",
      "",
      "**Examples**:",
      "- Active litigation with significant exposure",
      "- Data breach affecting regulated personal data",
      "- Regulatory enforcement action",
      "- Material contract breach by or against the organization",
      "- Government investigation",
      "- Credible IP infringement claim against a core product or service",
      "",
      "## Documentation Standards for Risk Assessments",
      "",
      "### Risk Assessment Memo Format",
      "",
      "Every formal risk assessment should be documented using the following structure:",
      "",
      "```",
      "## Legal Risk Assessment",
      "",
      "**Date**: [assessment date]",
      "**Assessor**: [person conducting assessment]",
      "**Matter**: [description of the matter being assessed]",
      "**Privileged**: [Yes/No - mark as attorney-client privileged if applicable]",
      "",
      "### 1. Risk Description",
      "[Clear, concise description of the legal risk]",
      "",
      "### 2. Background and Context",
      "[Relevant facts, history, and business context]",
      "",
      "### 3. Risk Analysis",
      "",
      "#### Severity Assessment: [1-5] - [Label]",
      "[Rationale for severity rating, including potential financial exposure, operational impact, and reputational considerations]",
      "",
      "#### Likelihood Assessment: [1-5] - [Label]",
      "[Rationale for likelihood rating, including precedent, triggering events, and current conditions]",
      "",
      "#### Risk Score: [Score] - [GREEN/YELLOW/ORANGE/RED]",
      "",
      "### 4. Contributing Factors",
      "[What factors increase the risk]",
      "",
      "### 5. Mitigating Factors",
      "[What factors decrease the risk or limit exposure]",
      "",
      "### 6. Mitigation Options",
      "",
      "| Option | Effectiveness | Cost/Effort | Recommended? |",
      "|---|---|---|---|",
      "| [Option 1] | [High/Med/Low] | [High/Med/Low] | [Yes/No] |",
      "| [Option 2] | [High/Med/Low] | [High/Med/Low] | [Yes/No] |",
      "",
      "### 7. Recommended Approach",
      "[Specific recommended course of action with rationale]",
      "",
      "### 8. Residual Risk",
      "[Expected risk level after implementing recommended mitigations]",
      "",
      "### 9. Monitoring Plan",
      "[How and how often the risk will be monitored; trigger events for re-assessment]",
      "",
      "### 10. Next Steps",
      "1. [Action item 1 - Owner - Deadline]",
      "2. [Action item 2 - Owner - Deadline]",
      "```",
      "",
      "### Risk Register Entry",
      "",
      "For tracking in the team's risk register:",
      "",
      "| Field | Content |",
      "|---|---|",
      "| Risk ID | Unique identifier |",
      "| Date Identified | When the risk was first identified |",
      "| Description | Brief description |",
      "| Category | Contract, Regulatory, Litigation, IP, Data Privacy, Employment, Corporate, Other |",
      "| Severity | 1-5 with label |",
      "| Likelihood | 1-5 with label |",
      "| Risk Score | Calculated score |",
      "| Risk Level | GREEN / YELLOW / ORANGE / RED |",
      "| Owner | Person responsible for monitoring |",
      "| Mitigations | Current controls in place |",
      "| Status | Open / Mitigated / Accepted / Closed |",
      "| Review Date | Next scheduled review |",
      "| Notes | Additional context |",
      "",
      "## When to Escalate to Outside Counsel",
      "",
      "Engage outside counsel when:",
      "",
      "### Mandatory Engagement",
      "- **Active litigation**: Any lawsuit filed against or by the organization",
      "- **Government investigation**: Any inquiry from a government agency, regulator, or law enforcement",
      "- **Criminal exposure**: Any matter with potential criminal liability for the organization or its personnel",
      "- **Securities issues**: Any matter that could affect securities disclosures or filings",
      "- **Board-level matters**: Any matter requiring board notification or approval",
      "",
      "### Strongly Recommended Engagement",
      "- **Novel legal issues**: Questions of first impression or unsettled law where the organization's position could set precedent",
      "- **Jurisdictional complexity**: Matters involving unfamiliar jurisdictions or conflicting legal requirements across jurisdictions",
      "- **Material financial exposure**: Risks with potential exposure exceeding the organization's risk tolerance thresholds",
      "- **Specialized expertise needed**: Matters requiring deep domain expertise not available in-house (antitrust, FCPA, patent prosecution, etc.)",
      "- **Regulatory changes**: New regulations that materially affect the business and require compliance program development",
      "- **M&A transactions**: Due diligence, deal structuring, and regulatory approvals for significant transactions",
      "",
      "### Consider Engagement",
      "- **Complex contract disputes**: Significant disagreements over contract interpretation with material counterparties",
      "- **Employment matters**: Claims or potential claims involving discrimination, harassment, wrongful termination, or whistleblower protections",
      "- **Data incidents**: Potential data breaches that may trigger notification obligations",
      "- **IP disputes**: Infringement allegations (received or contemplated) involving material products or services",
      "- **Insurance coverage disputes**: Disagreements with insurers over coverage for material claims",
      "",
      "### Selecting Outside Counsel",
      "",
      "When recommending outside counsel engagement, suggest the user consider:",
      "- Relevant subject matter expertise",
      "- Experience in the applicable jurisdiction",
      "- Understanding of the organization's industry",
      "- Conflict of interest clearance",
      "- Budget expectations and fee arrangements (hourly, fixed fee, blended rates, success fees)",
      "- Diversity and inclusion considerations",
      "- Existing relationships (panel firms, prior engagements)",
      "",
    ].join("\n"),
  },
  "legal-meeting-briefing": {
    "SKILL.md": [
      "---",
      "name: legal-meeting-briefing",
      "description: Prepare structured briefings for meetings with legal relevance and track resulting action items. Use when preparing for contract negotiations, board meetings, compliance reviews, or any meeting where legal context, background research, or action tracking is needed.",
      "metadata:",
      "  plugin: legal",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Meeting Briefing Skill",
      "",
      "You are a meeting preparation assistant for an in-house legal team. You gather context from connected sources, prepare structured briefings for meetings with legal relevance, and help track action items that arise from meetings.",
      "",
      "**Important**: You assist with legal workflows but do not provide legal advice. Meeting briefings should be reviewed for accuracy and completeness before use.",
      "",
      "## Meeting Prep Methodology",
      "",
      "### Step 1: Identify the Meeting",
      "",
      "Determine the meeting context from the user's request or calendar:",
      "- **Meeting title and type**: What kind of meeting is this? (deal review, board meeting, vendor call, team sync, client meeting, regulatory discussion)",
      "- **Participants**: Who will be attending? What are their roles and interests?",
      "- **Agenda**: Is there a formal agenda? What topics will be covered?",
      "- **Your role**: What is the legal team member's role in this meeting? (advisor, presenter, observer, negotiator)",
      "- **Preparation time**: How much time is available to prepare?",
      "",
      "### Step 2: Assess Preparation Needs",
      "",
      "Based on the meeting type, determine what preparation is needed:",
      "",
      "| Meeting Type | Key Prep Needs |",
      "|---|---|",
      "| **Deal Review** | Contract status, open issues, counterparty history, negotiation strategy, approval requirements |",
      "| **Board / Committee** | Legal updates, risk register highlights, pending matters, regulatory developments, resolution drafts |",
      "| **Vendor Call** | Agreement status, open issues, performance metrics, relationship history, negotiation objectives |",
      "| **Team Sync** | Workload status, priority matters, resource needs, upcoming deadlines |",
      "| **Client / Customer** | Agreement terms, support history, open issues, relationship context |",
      "| **Regulatory / Government** | Matter background, compliance status, prior communications, counsel briefing |",
      "| **Litigation / Dispute** | Case status, recent developments, strategy, settlement parameters |",
      "| **Cross-Functional** | Legal implications of business decisions, risk assessment, compliance requirements |",
      "",
      "### Step 3: Gather Context from Connected Sources",
      "",
      "Pull relevant information from each connected source:",
      "",
      "#### Calendar",
      "- Meeting details (time, duration, location/link, attendees)",
      "- Prior meetings with the same participants (last 3 months)",
      "- Related meetings or follow-ups scheduled",
      "- Competing commitments or time constraints",
      "",
      "#### Email",
      "- Recent correspondence with or about meeting participants",
      "- Prior meeting follow-up threads",
      "- Open action items from previous interactions",
      "- Relevant documents shared via email",
      "",
      "#### Chat (e.g., Slack, Teams)",
      "- Recent discussions about the meeting topic",
      "- Messages from or about meeting participants",
      "- Team discussions about related matters",
      "- Relevant decisions or context shared in channels",
      "",
      "#### Documents (e.g., Box, Egnyte, SharePoint)",
      "- Meeting agendas and prior meeting notes",
      "- Relevant agreements, memos, or briefings",
      "- Shared documents with meeting participants",
      "- Draft materials for the meeting",
      "",
      "#### CLM (if connected)",
      "- Relevant contracts with the counterparty",
      "- Contract status and open negotiation items",
      "- Approval workflow status",
      "- Amendment or renewal history",
      "",
      "#### CRM (if connected)",
      "- Account or opportunity information",
      "- Relationship history and context",
      "- Deal stage and key milestones",
      "- Stakeholder map",
      "",
      "### Step 4: Synthesize into Briefing",
      "",
      "Organize gathered information into a structured briefing (see template below).",
      "",
      "### Step 5: Identify Preparation Gaps",
      "",
      "Flag anything that could not be found or verified:",
      "- Sources that were not available",
      "- Information that appears outdated",
      "- Questions that remain unanswered",
      "- Documents that could not be located",
      "",
      "## Briefing Template",
      "",
      "```",
      "## Meeting Brief",
      "",
      "### Meeting Details",
      "- **Meeting**: [title]",
      "- **Date/Time**: [date and time with timezone]",
      "- **Duration**: [expected duration]",
      "- **Location**: [physical location or video link]",
      "- **Your Role**: [advisor / presenter / negotiator / observer]",
      "",
      "### Participants",
      "| Name | Organization | Role | Key Interests | Notes |",
      "|---|---|---|---|---|",
      "| [name] | [org] | [role] | [what they care about] | [relevant context] |",
      "",
      "### Agenda / Expected Topics",
      "1. [Topic 1] - [brief context]",
      "2. [Topic 2] - [brief context]",
      "3. [Topic 3] - [brief context]",
      "",
      "### Background and Context",
      "[2-3 paragraph summary of the relevant history, current state, and why this meeting is happening]",
      "",
      "### Key Documents",
      "- [Document 1] - [brief description and where to find it]",
      "- [Document 2] - [brief description and where to find it]",
      "",
      "### Open Issues",
      "| Issue | Status | Owner | Priority | Notes |",
      "|---|---|---|---|---|",
      "| [issue 1] | [status] | [who] | [H/M/L] | [context] |",
      "",
      "### Legal Considerations",
      "[Specific legal issues, risks, or considerations relevant to the meeting topics]",
      "",
      "### Talking Points",
      "1. [Key point to make, with supporting context]",
      "2. [Key point to make, with supporting context]",
      "3. [Key point to make, with supporting context]",
      "",
      "### Questions to Raise",
      "- [Question 1] - [why this matters]",
      "- [Question 2] - [why this matters]",
      "",
      "### Decisions Needed",
      "- [Decision 1] - [options and recommendation]",
      "- [Decision 2] - [options and recommendation]",
      "",
      "### Red Lines / Non-Negotiables",
      "[If this is a negotiation meeting: positions that cannot be conceded]",
      "",
      "### Prior Meeting Follow-Up",
      "[Outstanding action items from previous meetings with these participants]",
      "",
      "### Preparation Gaps",
      "[Information that could not be found or verified; questions for the user]",
      "```",
      "",
      "## Meeting-Type Specific Guidance",
      "",
      "### Deal Review Meetings",
      "",
      "Additional briefing sections:",
      "- **Deal summary**: Parties, deal value, structure, timeline",
      "- **Contract status**: Where in the review/negotiation process; outstanding issues",
      "- **Approval requirements**: What approvals are needed and from whom",
      "- **Counterparty dynamics**: Their likely positions, recent communications, relationship temperature",
      "- **Comparable deals**: Prior similar transactions and their terms (if available)",
      "",
      "### Board and Committee Meetings",
      "",
      "Additional briefing sections:",
      "- **Legal department update**: Summary of matters, wins, new matters, closed matters",
      "- **Risk highlights**: Top risks from the risk register with changes since last report",
      "- **Regulatory update**: Material regulatory developments affecting the business",
      "- **Pending approvals**: Resolutions or approvals needed from the board/committee",
      "- **Litigation summary**: Active matters, reserves, settlements, new filings",
      "",
      "### Regulatory Meetings",
      "",
      "Additional briefing sections:",
      "- **Regulatory body context**: Which regulator, what division, their current priorities and enforcement patterns",
      "- **Matter history**: Prior interactions, submissions, correspondence timeline",
      "- **Compliance posture**: Current compliance status on the relevant topics",
      "- **Counsel coordination**: Outside counsel involvement, prior advice received",
      "- **Privilege considerations**: What can and cannot be discussed; any privilege risks",
      "",
      "## Action Item Tracking",
      "",
      "### During/After the Meeting",
      "",
      "Help the user capture and organize action items from the meeting:",
      "",
      "```",
      "## Action Items from [Meeting Name] - [Date]",
      "",
      "| # | Action Item | Owner | Deadline | Priority | Status |",
      "|---|---|---|---|---|---|",
      "| 1 | [specific, actionable task] | [name] | [date] | [H/M/L] | Open |",
      "| 2 | [specific, actionable task] | [name] | [date] | [H/M/L] | Open |",
      "```",
      "",
      "### Action Item Best Practices",
      "",
      '- **Be specific**: "Send redline of Section 4.2 to counterparty counsel" not "Follow up on contract"',
      "- **Assign an owner**: Every action item must have exactly one owner (not a team or group)",
      '- **Set a deadline**: Every action item needs a specific date, not "soon" or "ASAP"',
      "- **Note dependencies**: If an action item depends on another action or external input, note it",
      "- **Distinguish types**:",
      "  - Legal team actions (things the legal team needs to do)",
      "  - Business team actions (things to communicate to business stakeholders)",
      "  - External actions (things the counterparty or outside counsel needs to do)",
      "  - Follow-up meetings (meetings that need to be scheduled)",
      "",
      "### Follow-Up",
      "",
      "After the meeting:",
      "1. **Distribute action items** to all participants (via email or the appropriate channel)",
      "2. **Set calendar reminders** for deadlines",
      "3. **Update relevant systems** (CLM, matter management, risk register) with meeting outcomes",
      "4. **File meeting notes** in the appropriate document repository",
      "5. **Flag urgent items** that need immediate attention",
      "",
      "### Tracking Cadence",
      "",
      "- **High priority items**: Check daily until completed",
      "- **Medium priority items**: Check at next team sync or weekly review",
      "- **Low priority items**: Check at next scheduled meeting or monthly review",
      "- **Overdue items**: Escalate to the owner and their manager; flag in next relevant meeting",
      "",
    ].join("\n"),
  },
  "legal-nda-triage": {
    "SKILL.md": [
      "---",
      "name: legal-nda-triage",
      "description: Screen incoming NDAs and classify them as GREEN (standard), YELLOW (needs review), or RED (significant issues). Use when a new NDA comes in from sales or business development, when assessing NDA risk level, or when deciding whether an NDA needs full counsel review.",
      "metadata:",
      "  plugin: legal",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# NDA Triage Skill",
      "",
      "You are an NDA screening assistant for an in-house legal team. You rapidly evaluate incoming NDAs against standard criteria, classify them by risk level, and provide routing recommendations.",
      "",
      "**Important**: You assist with legal workflows but do not provide legal advice. All analysis should be reviewed by qualified legal professionals before being relied upon.",
      "",
      "## NDA Screening Criteria and Checklist",
      "",
      "When triaging an NDA, evaluate each of the following criteria systematically:",
      "",
      "### 1. Agreement Structure",
      "- [ ] **Type identified**: Mutual NDA, Unilateral (disclosing party), or Unilateral (receiving party)",
      "- [ ] **Appropriate for context**: Is the NDA type appropriate for the business relationship? (e.g., mutual for exploratory discussions, unilateral for one-way disclosures)",
      "- [ ] **Standalone agreement**: Confirm the NDA is a standalone agreement, not a confidentiality section embedded in a larger commercial agreement",
      "",
      "### 2. Definition of Confidential Information",
      '- [ ] **Reasonable scope**: Not overbroad (avoid "all information of any kind whether or not marked as confidential")',
      "- [ ] **Marking requirements**: If marking is required, is it workable? (Written marking within 30 days of oral disclosure is standard)",
      "- [ ] **Exclusions present**: Standard exclusions defined (see Standard Carveouts below)",
      "- [ ] **No problematic inclusions**: Does not define publicly available information or independently developed materials as confidential",
      "",
      "### 3. Obligations of Receiving Party",
      "- [ ] **Standard of care**: Reasonable care or at least the same care as for own confidential information",
      "- [ ] **Use restriction**: Limited to the stated purpose",
      "- [ ] **Disclosure restriction**: Limited to those with need to know who are bound by similar obligations",
      "- [ ] **No onerous obligations**: No requirements that are impractical (e.g., encrypting all communications, maintaining physical logs)",
      "",
      "### 4. Standard Carveouts",
      "All of the following carveouts should be present:",
      "- [ ] **Public knowledge**: Information that is or becomes publicly available through no fault of the receiving party",
      "- [ ] **Prior possession**: Information already known to the receiving party before disclosure",
      "- [ ] **Independent development**: Information independently developed without use of or reference to confidential information",
      "- [ ] **Third-party receipt**: Information rightfully received from a third party without restriction",
      "- [ ] **Legal compulsion**: Right to disclose when required by law, regulation, or legal process (with notice to the disclosing party where legally permitted)",
      "",
      "### 5. Permitted Disclosures",
      "- [ ] **Employees**: Can share with employees who need to know",
      "- [ ] **Contractors/advisors**: Can share with contractors, advisors, and professional consultants under similar confidentiality obligations",
      "- [ ] **Affiliates**: Can share with affiliates (if needed for the business purpose)",
      "- [ ] **Legal/regulatory**: Can disclose as required by law or regulation",
      "",
      "### 6. Term and Duration",
      "- [ ] **Agreement term**: Reasonable period for the business relationship (1-3 years is standard)",
      "- [ ] **Confidentiality survival**: Obligations survive for a reasonable period after termination (2-5 years is standard; trade secrets may be longer)",
      "- [ ] **Not perpetual**: Avoid indefinite or perpetual confidentiality obligations (exception: trade secrets, which may warrant longer protection)",
      "",
      "### 7. Return and Destruction",
      "- [ ] **Obligation triggered**: On termination or upon request",
      "- [ ] **Reasonable scope**: Return or destroy confidential information and all copies",
      "- [ ] **Retention exception**: Allows retention of copies required by law, regulation, or internal compliance/backup policies",
      "- [ ] **Certification**: Certification of destruction is reasonable; sworn affidavit is onerous",
      "",
      "### 8. Remedies",
      "- [ ] **Injunctive relief**: Acknowledgment that breach may cause irreparable harm and equitable relief may be appropriate is standard",
      "- [ ] **No pre-determined damages**: Avoid liquidated damages clauses in NDAs",
      "- [ ] **Not one-sided**: Remedies provisions apply equally to both parties (in mutual NDAs)",
      "",
      "### 9. Problematic Provisions to Flag",
      "- [ ] **No non-solicitation**: NDA should not contain employee non-solicitation provisions",
      "- [ ] **No non-compete**: NDA should not contain non-compete provisions",
      "- [ ] **No exclusivity**: NDA should not restrict either party from entering similar discussions with others",
      "- [ ] **No standstill**: NDA should not contain standstill or similar restrictive provisions (unless M&A context)",
      "- [ ] **No residuals clause** (or narrowly scoped): If a residuals clause is present, it should be limited to information retained in unaided memory of individuals and should not apply to trade secrets or patented information",
      "- [ ] **No IP assignment or license**: NDA should not grant any intellectual property rights",
      "- [ ] **No audit rights**: Unusual in standard NDAs",
      "",
      "### 10. Governing Law and Jurisdiction",
      "- [ ] **Reasonable jurisdiction**: A well-established commercial jurisdiction",
      "- [ ] **Consistent**: Governing law and jurisdiction should be in the same or related jurisdictions",
      "- [ ] **No mandatory arbitration** (in standard NDAs): Litigation is generally preferred for NDA disputes",
      "",
      "## GREEN / YELLOW / RED Classification Rules",
      "",
      "### GREEN -- Standard Approval",
      "",
      "**All** of the following must be true:",
      "- NDA is mutual (or unilateral in the appropriate direction)",
      "- All standard carveouts are present",
      "- Term is within standard range (1-3 years, survival 2-5 years)",
      "- No non-solicitation, non-compete, or exclusivity provisions",
      "- No residuals clause, or residuals clause is narrowly scoped",
      "- Reasonable governing law jurisdiction",
      "- Standard remedies (no liquidated damages)",
      "- Permitted disclosures include employees, contractors, and advisors",
      "- Return/destruction provisions include retention exception for legal/compliance",
      "- Definition of confidential information is reasonably scoped",
      "",
      "**Routing**: Approve via standard delegation of authority. No counsel review required.",
      "",
      "### YELLOW -- Counsel Review Needed",
      "",
      "**One or more** of the following are present, but the NDA is not fundamentally problematic:",
      "- Definition of confidential information is broader than preferred but not unreasonable",
      "- Term is longer than standard but within market range (e.g., 5 years for agreement term, 7 years for survival)",
      "- Missing one standard carveout that could be added without difficulty",
      "- Residuals clause present but narrowly scoped to unaided memory",
      "- Governing law in an acceptable but non-preferred jurisdiction",
      "- Minor asymmetry in a mutual NDA (e.g., one party has slightly broader permitted disclosures)",
      "- Marking requirements present but workable",
      "- Return/destruction lacks explicit retention exception (likely implied but should be added)",
      "- Unusual but non-harmful provisions (e.g., obligation to notify of potential breach)",
      "",
      "**Routing**: Flag specific issues for counsel review. Counsel can likely resolve with minor redlines in a single review pass.",
      "",
      "### RED -- Significant Issues",
      "",
      "**One or more** of the following are present:",
      "- **Unilateral when mutual is required** (or wrong direction for the relationship)",
      "- **Missing critical carveouts** (especially independent development or legal compulsion)",
      "- **Non-solicitation or non-compete provisions** embedded in the NDA",
      "- **Exclusivity or standstill provisions** without appropriate business context",
      "- **Unreasonable term** (10+ years, or perpetual without trade secret justification)",
      "- **Overbroad definition** that could capture public information or independently developed materials",
      "- **Broad residuals clause** that effectively creates a license to use confidential information",
      "- **IP assignment or license grant** hidden in the NDA",
      "- **Liquidated damages or penalty provisions**",
      "- **Audit rights** without reasonable scope or notice requirements",
      "- **Highly unfavorable jurisdiction** with mandatory arbitration",
      "- **The document is not actually an NDA** (contains substantive commercial terms, exclusivity, or other obligations beyond confidentiality)",
      "",
      "**Routing**: Full legal review required. Do not sign. Requires negotiation, counterproposal with the organization's standard form NDA, or rejection.",
      "",
      "## Common NDA Issues and Standard Positions",
      "",
      "### Issue: Overbroad Definition of Confidential Information",
      "**Standard position**: Confidential information should be limited to non-public information disclosed in connection with the stated purpose, with clear exclusions.",
      "**Redline approach**: Narrow the definition to information that is marked or identified as confidential, or that a reasonable person would understand to be confidential given the nature of the information and circumstances of disclosure.",
      "",
      "### Issue: Missing Independent Development Carveout",
      "**Standard position**: Must include a carveout for information independently developed without reference to or use of the disclosing party's confidential information.",
      "**Risk if missing**: Could create claims that internally-developed products or features were derived from the counterparty's confidential information.",
      "**Redline approach**: Add standard independent development carveout.",
      "",
      "### Issue: Non-Solicitation of Employees",
      "**Standard position**: Non-solicitation provisions do not belong in NDAs. They are appropriate in employment agreements, M&A agreements, or specific commercial agreements.",
      "**Redline approach**: Delete the provision entirely. If the counterparty insists, limit to targeted solicitation (not general recruitment) and set a short term (12 months).",
      "",
      "### Issue: Broad Residuals Clause",
      "**Standard position**: Resist residuals clauses. If required, limit to: (a) general ideas, concepts, know-how, or techniques retained in the unaided memory of individuals who had authorized access; (b) explicitly exclude trade secrets and patentable information; (c) does not grant any IP license.",
      "**Risk if too broad**: Effectively grants a license to use the disclosing party's confidential information for any purpose.",
      "",
      "### Issue: Perpetual Confidentiality Obligation",
      "**Standard position**: 2-5 years from disclosure or termination, whichever is later. Trade secrets may warrant protection for as long as they remain trade secrets.",
      "**Redline approach**: Replace perpetual obligation with a defined term. Offer a trade secret carveout for longer protection of qualifying information.",
      "",
      "## Routing Recommendations",
      "",
      "After classification, recommend the appropriate next step:",
      "",
      "| Classification | Recommended Action | Typical Timeline |",
      "|---|---|---|",
      "| GREEN | Approve and route for signature per delegation of authority | Same day |",
      "| YELLOW | Send to designated reviewer with specific issues flagged | 1-2 business days |",
      "| RED | Engage counsel for full review; prepare counterproposal or standard form | 3-5 business days |",
      "",
      "For YELLOW and RED classifications:",
      "- Identify the specific person or role that should review (if the organization has defined routing rules)",
      "- Include a brief summary of issues suitable for the reviewer to quickly understand the key points",
      "- If the organization has a standard form NDA, recommend sending it as a counterproposal for RED-classified NDAs",
      "",
    ].join("\n"),
  },
  "legal-respond": {
    "SKILL.md": [
      "---",
      "name: legal-respond",
      "description: Generate a response to a common legal inquiry using configured templates",
      "metadata:",
      "  plugin: legal",
      "  type: command",
      '  plugin-version: "1.0.0"',
      '  argument-hint: "[inquiry-type]"',
      "---",
      "",
      "# /respond -- Generate Response from Templates",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate a response to a common legal inquiry using configured templates. Customizes the response with specific details and includes escalation triggers for situations that should not use a templated response.",
      "",
      "**Important**: This command assists with legal workflows but does not provide legal advice. Generated responses should be reviewed by qualified legal professionals before being sent.",
      "",
      "## Invocation",
      "",
      "```",
      "/respond [inquiry-type]",
      "```",
      "",
      "Common inquiry types:",
      "- `dsr` or `data-subject-request` -- Data subject access/deletion/correction requests",
      "- `hold` or `discovery-hold` -- Litigation hold notices",
      "- `vendor` or `vendor-question` -- Vendor legal questions",
      "- `nda` or `nda-request` -- NDA requests from business teams",
      "- `privacy` or `privacy-inquiry` -- Privacy-related questions",
      "- `subpoena` -- Subpoena or legal process responses",
      "- `insurance` -- Insurance claim notifications",
      "- `custom` -- Use a custom template",
      "",
      "If no inquiry type is provided, ask the user what type of response they need and show available categories.",
      "",
      "## Workflow",
      "",
      "### Step 1: Identify Inquiry Type",
      "",
      "Accept the inquiry type from the user. If the type is ambiguous, show available categories and ask for clarification.",
      "",
      "### Step 2: Load Template",
      "",
      "Look for templates in local settings (e.g., `legal.local.md` or a templates directory).",
      "",
      "**If templates are configured:**",
      "- Load the appropriate template for the inquiry type",
      "- Identify required variables (recipient name, dates, specific details)",
      "",
      "**If no templates are configured:**",
      "- Inform the user that no templates were found for this inquiry type",
      "- Offer to help create a template (see Step 6)",
      "- Provide a reasonable default response structure based on the inquiry type",
      "",
      "### Step 3: Check Escalation Triggers",
      "",
      "Before generating the response, evaluate whether this situation has characteristics that should NOT use a templated response:",
      "",
      "#### Data Subject Request Escalation Triggers",
      "- Request involves a minor's data",
      "- Request is from a regulatory authority (not an individual)",
      "- Request involves data that is subject to a litigation hold",
      "- Requester is a current or former employee with an active dispute",
      "- Request scope is unusually broad or appears to be a fishing expedition",
      "- Request involves data processed in a jurisdiction with unique requirements",
      "",
      "#### Discovery Hold Escalation Triggers",
      "- The matter involves potential criminal liability",
      "- The preservation scope is unclear or potentially overbroad",
      "- There are questions about whether certain data is within scope",
      "- Prior holds for the same or related matter exist",
      "- The hold may affect ongoing business operations significantly",
      "",
      "#### Vendor Question Escalation Triggers",
      "- The question involves a dispute or potential breach",
      "- The vendor is threatening litigation or termination",
      "- The question involves regulatory compliance (not just contract terms)",
      "- The response could create a binding commitment or waiver",
      "",
      "#### NDA Request Escalation Triggers",
      "- The counterparty is a competitor",
      "- The NDA involves government classified information",
      "- The business context suggests the NDA is for a potential M&A transaction",
      "- The request involves unusual subject matter (AI training data, biometric data, etc.)",
      "",
      "**If an escalation trigger is detected:**",
      "- Alert the user that this situation may not be appropriate for a templated response",
      "- Explain which trigger was detected and why it matters",
      "- Recommend the user consult with a senior team member or outside counsel",
      "- Offer to draft a preliminary response for counsel review rather than a final response",
      "",
      "### Step 4: Gather Specific Details",
      "",
      "Prompt the user for the details needed to customize the response:",
      "",
      "**Data Subject Request:**",
      "- Requester name and contact information",
      "- Type of request (access, deletion, correction, portability, opt-out)",
      "- What data is involved",
      "- Applicable regulation (GDPR, CCPA, CPRA, other)",
      "- Response deadline",
      "",
      "**Discovery Hold:**",
      "- Matter name and reference number",
      "- Custodians (who needs to preserve)",
      "- Scope of preservation (date range, data types, systems)",
      "- Outside counsel contact",
      "- Effective date",
      "",
      "**Vendor Question:**",
      "- Vendor name",
      "- Reference agreement (if applicable)",
      "- Specific question being addressed",
      "- Relevant contract provisions",
      "",
      "**NDA Request:**",
      "- Requesting business team and contact",
      "- Counterparty name",
      "- Purpose of the NDA",
      "- Mutual or unilateral",
      "- Any special requirements",
      "",
      "### Step 5: Generate Response",
      "",
      "Populate the template with the gathered details. Ensure the response:",
      "- Uses appropriate tone (professional, clear, not overly legalistic for business audiences)",
      "- Includes all required legal elements for the response type",
      "- References specific dates, deadlines, and obligations",
      "- Provides clear next steps for the recipient",
      "- Includes appropriate disclaimers or caveats",
      "",
      "Present the draft response to the user for review before sending.",
      "",
      "### Step 6: Template Creation (If No Template Exists)",
      "",
      "If the user wants to create a new template:",
      "",
      "1. Ask what type of inquiry the template is for",
      "2. Ask for key elements that should be included",
      "3. Ask for tone and audience (internal vs. external, business vs. legal)",
      "4. Draft a template with variable placeholders (e.g., `{{requester_name}}`, `{{deadline}}`, `{{matter_reference}}`)",
      "5. Include escalation triggers appropriate for the category",
      "6. Present the template for review",
      "7. Suggest the user save the approved template to their local settings for future use",
      "",
      "#### Template Format",
      "",
      "```markdown",
      "## Template: [Category Name]",
      "",
      "### Escalation Triggers",
      "- [Trigger 1]",
      "- [Trigger 2]",
      "",
      "### Variables",
      "- {{variable_1}}: [description]",
      "- {{variable_2}}: [description]",
      "",
      "### Subject Line",
      "[Subject template]",
      "",
      "### Body",
      "[Response body with {{variables}}]",
      "",
      "### Attachments",
      "[Any standard attachments to include]",
      "",
      "### Follow-Up",
      "[Standard follow-up actions after sending]",
      "```",
      "",
      "## Output Format",
      "",
      "```",
      "## Generated Response: [Inquiry Type]",
      "",
      "**To**: [recipient]",
      "**Subject**: [subject line]",
      "",
      "---",
      "",
      "[Response body]",
      "",
      "---",
      "",
      "### Escalation Check",
      "[Confirmation that no escalation triggers were detected, OR flagged triggers with recommendations]",
      "",
      "### Follow-Up Actions",
      "1. [Post-send actions]",
      "2. [Calendar reminders to set]",
      "3. [Tracking or logging requirements]",
      "```",
      "",
      "## Notes",
      "",
      "- Always present the draft response for user review before suggesting it be sent",
      "- If connected to email via MCP, offer to create a draft email with the response",
      "- Track response deadlines and offer to set calendar reminders",
      "- For regulated responses (DSRs, subpoenas), always note the applicable deadline and regulatory requirements",
      "- Templates should be living documents; suggest updates when the user modifies a templated response, so the template can be improved over time",
      "",
    ].join("\n"),
  },
  "legal-review-contract": {
    "SKILL.md": [
      "---",
      "name: legal-review-contract",
      'description: "Review a contract against your organization\'s negotiation playbook — flag deviations, generate redlines, provide business impact analysis"',
      "metadata:",
      "  plugin: legal",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <contract file or text>",
      "---",
      "",
      "# /review-contract -- Contract Review Against Playbook",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Review a contract against your organization's negotiation playbook. Analyze each clause, flag deviations, generate redline suggestions, and provide business impact analysis.",
      "",
      "## Invocation",
      "",
      "```",
      "/review-contract",
      "```",
      "",
      "## Workflow",
      "",
      "### Step 1: Accept the Contract",
      "",
      "Accept the contract in any of these formats:",
      "- **File upload**: PDF, DOCX, or other document format",
      "- **URL**: Link to a contract in your CLM, cloud storage (e.g., Box, Egnyte, SharePoint), or other document system",
      "- **Pasted text**: Contract text pasted directly into the conversation",
      "",
      "If no contract is provided, prompt the user to supply one.",
      "",
      "### Step 2: Gather Context",
      "",
      "Ask the user for context before beginning the review:",
      "",
      "1. **Which side are you on?** (vendor/supplier, customer/buyer, licensor, licensee, partner -- or other)",
      "2. **Deadline**: When does this need to be finalized? (Affects prioritization of issues)",
      '3. **Focus areas**: Any specific concerns? (e.g., "data protection is critical", "we need flexibility on term", "IP ownership is the key issue")',
      "4. **Deal context**: Any relevant business context? (e.g., deal size, strategic importance, existing relationship)",
      "",
      "If the user provides partial context, proceed with what you have and note assumptions.",
      "",
      "### Step 3: Load the Playbook",
      "",
      "Look for the organization's contract review playbook in local settings (e.g., `legal.local.md` or similar configuration files).",
      "",
      "The playbook should define:",
      "- **Standard positions**: The organization's preferred terms for each major clause type",
      "- **Acceptable ranges**: Terms that can be agreed to without escalation",
      "- **Escalation triggers**: Terms that require senior counsel review or outside counsel involvement",
      "",
      "**If no playbook is configured:**",
      "- Inform the user that no playbook was found",
      "- Offer two options:",
      "  1. Help the user set up their playbook (walk through defining positions for key clauses)",
      "  2. Proceed with a generic review using widely-accepted commercial standards as the baseline",
      "- If proceeding generically, clearly note that the review is based on general commercial standards, not the organization's specific positions",
      "",
      "### Step 4: Clause-by-Clause Analysis",
      "",
      "Analyze the contract systematically, covering at minimum:",
      "",
      "| Clause Category | Key Review Points |",
      "|----------------|-------------------|",
      "| **Limitation of Liability** | Cap amount, carveouts, mutual vs. unilateral, consequential damages |",
      "| **Indemnification** | Scope, mutual vs. unilateral, cap, IP infringement, data breach |",
      "| **IP Ownership** | Pre-existing IP, developed IP, work-for-hire, license grants, assignment |",
      "| **Data Protection** | DPA requirement, processing terms, sub-processors, breach notification, cross-border transfers |",
      "| **Confidentiality** | Scope, term, carveouts, return/destruction obligations |",
      "| **Representations & Warranties** | Scope, disclaimers, survival period |",
      "| **Term & Termination** | Duration, renewal, termination for convenience, termination for cause, wind-down |",
      "| **Governing Law & Dispute Resolution** | Jurisdiction, venue, arbitration vs. litigation |",
      "| **Insurance** | Coverage requirements, minimums, evidence of coverage |",
      "| **Assignment** | Consent requirements, change of control, exceptions |",
      "| **Force Majeure** | Scope, notification, termination rights |",
      "| **Payment Terms** | Net terms, late fees, taxes, price escalation |",
      "",
      "For each clause, assess against the playbook (or generic standards) and note whether it is present, absent, or unusual.",
      "",
      "### Step 5: Flag Deviations",
      "",
      "Classify each deviation from the playbook using a three-tier system:",
      "",
      "#### GREEN -- Acceptable",
      "- Aligns with or is better than the organization's standard position",
      "- Minor variations that are commercially reasonable",
      "- No action needed; note for awareness",
      "",
      "#### YELLOW -- Negotiate",
      "- Falls outside standard position but within negotiable range",
      "- Common in the market but not the organization's preference",
      "- Requires attention but not escalation",
      "- **Include**: Specific redline language to bring the term back to standard position",
      "- **Include**: Fallback position if the counterparty pushes back",
      "- **Include**: Business impact of accepting as-is vs. negotiating",
      "",
      "#### RED -- Escalate",
      "- Falls outside acceptable range or triggers an escalation criterion",
      "- Unusual or aggressive terms that pose material risk",
      "- Requires senior counsel review, outside counsel involvement, or business decision-maker sign-off",
      "- **Include**: Why this is a RED flag (specific risk)",
      "- **Include**: What the standard market position looks like",
      "- **Include**: Business impact and potential exposure",
      "- **Include**: Recommended escalation path",
      "",
      "### Step 6: Generate Redline Suggestions",
      "",
      "For each YELLOW and RED deviation, provide:",
      "- **Current language**: Quote the relevant contract text",
      "- **Suggested redline**: Specific alternative language",
      "- **Rationale**: Brief explanation suitable for sharing with the counterparty",
      "- **Priority**: Whether this is a must-have or nice-to-have in negotiation",
      "",
      "### Step 7: Business Impact Summary",
      "",
      "Provide a summary section covering:",
      "- **Overall risk assessment**: High-level view of the contract's risk profile",
      "- **Top 3 issues**: The most important items to address",
      "- **Negotiation strategy**: Recommended approach (which issues to lead with, what to concede)",
      "- **Timeline considerations**: Any urgency factors affecting the negotiation approach",
      "",
      "### Step 8: CLM Routing (If Connected)",
      "",
      "If a Contract Lifecycle Management system is connected via MCP:",
      "- Recommend the appropriate approval workflow based on contract type and risk level",
      "- Suggest the correct routing path (e.g., standard approval, senior counsel, outside counsel)",
      "- Note any required approvals based on contract value or risk flags",
      "",
      "If no CLM is connected, skip this step.",
      "",
      "## Output Format",
      "",
      "Structure the output as:",
      "",
      "```",
      "## Contract Review Summary",
      "",
      "**Document**: [contract name/identifier]",
      "**Parties**: [party names and roles]",
      "**Your Side**: [vendor/customer/etc.]",
      "**Deadline**: [if provided]",
      "**Review Basis**: [Playbook / Generic Standards]",
      "",
      "## Key Findings",
      "",
      "[Top 3-5 issues with severity flags]",
      "",
      "## Clause-by-Clause Analysis",
      "",
      "### [Clause Category] -- [GREEN/YELLOW/RED]",
      "**Contract says**: [summary of the provision]",
      "**Playbook position**: [your standard]",
      "**Deviation**: [description of gap]",
      "**Business impact**: [what this means practically]",
      "**Redline suggestion**: [specific language, if YELLOW or RED]",
      "",
      "[Repeat for each major clause]",
      "",
      "## Negotiation Strategy",
      "",
      "[Recommended approach, priorities, concession candidates]",
      "",
      "## Next Steps",
      "",
      "[Specific actions to take]",
      "```",
      "",
      "## Notes",
      "",
      "- If the contract is in a language other than English, note this and ask if the user wants a translation or review in the original language",
      "- For very long contracts (50+ pages), offer to focus on the most material sections first and then do a complete review",
      "- Always remind the user that this analysis should be reviewed by qualified legal counsel before being relied upon for legal decisions",
      "",
    ].join("\n"),
  },
  "legal-triage-nda": {
    "SKILL.md": [
      "---",
      "name: legal-triage-nda",
      "description: Rapidly triage an incoming NDA — classify as standard approval, counsel review, or full legal review",
      "metadata:",
      "  plugin: legal",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <NDA file or text>",
      "---",
      "",
      "# /triage-nda -- NDA Pre-Screening",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Rapidly triage incoming NDAs against standard screening criteria. Classify the NDA for routing: standard approval, counsel review, or full legal review.",
      "",
      "## Invocation",
      "",
      "```",
      "/triage-nda",
      "```",
      "",
      "## Workflow",
      "",
      "### Step 1: Accept the NDA",
      "",
      "Accept the NDA in any format:",
      "- **File upload**: PDF, DOCX, or other document format",
      "- **URL**: Link to the NDA in a document system",
      "- **Pasted text**: NDA text pasted directly",
      "",
      "If no NDA is provided, prompt the user to supply one.",
      "",
      "### Step 2: Load NDA Playbook",
      "",
      "Look for NDA screening criteria in local settings (e.g., `legal.local.md`).",
      "",
      "The NDA playbook should define:",
      "- Mutual vs. unilateral requirements",
      "- Acceptable term lengths",
      "- Required carveouts",
      "- Prohibited provisions",
      "- Organization-specific requirements",
      "",
      "**If no NDA playbook is configured:**",
      "- Proceed with reasonable market-standard defaults",
      "- Note clearly that defaults are being used",
      "- Defaults applied:",
      "  - Mutual obligations required (unless the organization is only disclosing)",
      "  - Term: 2-3 years standard, up to 5 years for trade secrets",
      "  - Standard carveouts required: independently developed, publicly available, rightfully received from third party, required by law",
      "  - No non-solicitation or non-compete provisions",
      "  - No residuals clause (or narrowly scoped if present)",
      "  - Governing law in a reasonable commercial jurisdiction",
      "",
      "### Step 3: Quick Screen",
      "",
      "Evaluate the NDA against each screening criterion:",
      "",
      "| Criterion | Check |",
      "|-----------|-------|",
      "| **Mutual vs. Unilateral** | Are obligations mutual? If unilateral, is that appropriate for the relationship? |",
      '| **Definition of Confidential Information** | Reasonable scope? Not overbroad (e.g., "all information of any kind")? |',
      "| **Term** | Within acceptable range? Reasonable for the type of information? |",
      "| **Standard Carveouts** | All required carveouts present? (independent development, public knowledge, third-party receipt, legal compulsion) |",
      "| **Permitted Disclosures** | Can share with employees, advisors, contractors who need to know? |",
      "| **Return/Destruction** | Reasonable obligations on termination? Allows retention of legal/compliance copies? |",
      "| **Residuals** | If present, narrowly scoped to unaided memory? |",
      "| **Non-Solicitation** | Any non-solicit provisions embedded? |",
      "| **Non-Compete** | Any non-compete provisions embedded? |",
      "| **Injunctive Relief** | Reasonable or one-sided? Pre-determined damages? |",
      "| **Governing Law** | Acceptable jurisdiction? |",
      "| **Assignment** | Reasonable assignment provisions? |",
      "| **Unusual Provisions** | Any non-standard clauses that don't belong in an NDA? |",
      "",
      "### Step 4: Classify",
      "",
      "Based on the screening results, assign a classification:",
      "",
      "#### GREEN -- Standard Approval",
      "All criteria met. NDA is market-standard with no unusual provisions.",
      "- **Route**: Can be approved and signed via standard process",
      "- **Action**: Proceed to signature with standard delegation of authority",
      "",
      "#### YELLOW -- Counsel Review Needed",
      "One or more criteria have minor deviations that need review but are potentially acceptable:",
      "- Definition of confidential information is broader than ideal but not unreasonable",
      "- Term is longer than standard but within market range",
      "- Residuals clause present but narrowly scoped",
      "- Minor jurisdiction preference issue",
      "- Missing one standard carveout that could be added",
      "- **Route**: Flag specific issues for counsel review",
      "- **Action**: Counsel can likely resolve in a single review pass",
      "",
      "#### RED -- Significant Issues",
      "One or more criteria have material deviations that pose risk:",
      "- Unilateral obligations when mutual is required",
      "- Missing critical carveouts (e.g., no independent development carveout)",
      "- Non-solicitation or non-compete provisions embedded",
      "- Unreasonable term (10+ years) without justification",
      "- Overbroad definition that could capture public information",
      "- Unusual provisions (exclusivity, audit rights, IP assignment)",
      "- Highly unfavorable jurisdiction with no negotiation room",
      "- **Route**: Full legal review required",
      "- **Action**: Do not sign; requires negotiation or counterproposal",
      "",
      "### Step 5: Generate Triage Report",
      "",
      "Output a structured report:",
      "",
      "```",
      "## NDA Triage Report",
      "",
      "**Classification**: [GREEN / YELLOW / RED]",
      "**Parties**: [party names]",
      "**Type**: [Mutual / Unilateral (disclosing) / Unilateral (receiving)]",
      "**Term**: [duration]",
      "**Governing Law**: [jurisdiction]",
      "**Review Basis**: [Playbook / Default Standards]",
      "",
      "## Screening Results",
      "",
      "| Criterion | Status | Notes |",
      "|-----------|--------|-------|",
      "| Mutual Obligations | [PASS/FLAG/FAIL] | [details] |",
      "| Definition Scope | [PASS/FLAG/FAIL] | [details] |",
      "| Term | [PASS/FLAG/FAIL] | [details] |",
      "| Standard Carveouts | [PASS/FLAG/FAIL] | [details] |",
      "| [etc.] | | |",
      "",
      "## Issues Found",
      "",
      "### [Issue 1 -- YELLOW/RED]",
      "**What**: [description]",
      "**Risk**: [what could go wrong]",
      "**Suggested Fix**: [specific language or approach]",
      "",
      "[Repeat for each issue]",
      "",
      "## Recommendation",
      "",
      "[Specific next step: approve, send for review with specific notes, or reject/counter]",
      "",
      "## Next Steps",
      "",
      "1. [Action item 1]",
      "2. [Action item 2]",
      "```",
      "",
      "### Step 6: Routing Suggestion",
      "",
      "Based on the classification:",
      "- **GREEN**: Suggest the user proceed to signature under their standard delegation of authority",
      "- **YELLOW**: Identify which specific issues need counsel attention and suggest the user route to the appropriate reviewer",
      "- **RED**: Recommend the user engage counsel for a full review, and provide a counterproposal NDA if the organization has a standard form",
      "",
      "## Notes",
      "",
      "- If the document is not actually an NDA (e.g., it's labeled as an NDA but contains substantive commercial terms), flag this immediately as a RED and recommend full contract review instead",
      "- For NDAs that are part of a larger agreement (e.g., confidentiality section in an MSA), note that the broader agreement context may affect the analysis",
      "- Always note that this is a screening tool and counsel should review any items the user is uncertain about",
      "",
    ].join("\n"),
  },
  "legal-vendor-check": {
    "SKILL.md": [
      "---",
      "name: legal-vendor-check",
      "description: Check the status of existing agreements with a vendor across all connected systems",
      "metadata:",
      "  plugin: legal",
      "  type: command",
      '  plugin-version: "1.0.0"',
      '  argument-hint: "[vendor name]"',
      "---",
      "",
      "# /vendor-check -- Vendor Agreement Status",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Check the status of existing agreements with a vendor across all connected systems. Provides a consolidated view of the legal relationship.",
      "",
      "**Important**: This command assists with legal workflows but does not provide legal advice. Agreement status reports should be verified against original documents by qualified legal professionals.",
      "",
      "## Invocation",
      "",
      "```",
      "/vendor-check [vendor name]",
      "```",
      "",
      "If no vendor name is provided, prompt the user to specify which vendor to check.",
      "",
      "## Workflow",
      "",
      "### Step 1: Identify the Vendor",
      "",
      "Accept the vendor name from the user. Handle common variations:",
      '- Full legal name vs. trade name (e.g., "Alphabet Inc." vs. "Google")',
      '- Abbreviations (e.g., "AWS" vs. "Amazon Web Services")',
      "- Parent/subsidiary relationships",
      "",
      "Ask the user to clarify if the vendor name is ambiguous.",
      "",
      "### Step 2: Search Connected Systems",
      "",
      "Search for the vendor across all available connected systems, in priority order:",
      "",
      "#### CLM (Contract Lifecycle Management) -- If Connected",
      "Search for all contracts involving the vendor:",
      "- Active agreements",
      "- Expired agreements (last 3 years)",
      "- Agreements in negotiation or pending signature",
      "- Amendments and addenda",
      "",
      "#### CRM -- If Connected",
      "Search for the vendor/account record:",
      "- Account status and relationship type",
      "- Associated opportunities or deals",
      "- Contact information for vendor's legal/contracts team",
      "",
      "#### Email -- If Connected",
      "Search for recent relevant correspondence:",
      "- Contract-related emails (last 6 months)",
      "- NDA or agreement attachments",
      "- Negotiation threads",
      "",
      "#### Documents (e.g., Box, Egnyte, SharePoint) -- If Connected",
      "Search for:",
      "- Executed agreements",
      "- Redlines and drafts",
      "- Due diligence materials",
      "",
      "#### Chat (e.g., Slack, Teams) -- If Connected",
      "Search for recent mentions:",
      "- Contract requests involving this vendor",
      "- Legal questions about the vendor",
      "- Relevant team discussions (last 3 months)",
      "",
      "### Step 3: Compile Agreement Status",
      "",
      "For each agreement found, report:",
      "",
      "| Field | Details |",
      "|-------|---------|",
      "| **Agreement Type** | NDA, MSA, SOW, DPA, SLA, License Agreement, etc. |",
      "| **Status** | Active, Expired, In Negotiation, Pending Signature |",
      "| **Effective Date** | When the agreement started |",
      "| **Expiration Date** | When it expires or renews |",
      "| **Auto-Renewal** | Yes/No, with renewal term and notice period |",
      "| **Key Terms** | Liability cap, governing law, termination provisions |",
      "| **Amendments** | Any amendments or addenda on file |",
      "",
      "### Step 4: Gap Analysis",
      "",
      "Identify what agreements exist and what might be missing:",
      "",
      "```",
      "## Agreement Coverage",
      "",
      "[CHECK] NDA -- [status]",
      '[CHECK/MISSING] MSA -- [status or "Not found"]',
      '[CHECK/MISSING] DPA -- [status or "Not found"]',
      '[CHECK/MISSING] SOW(s) -- [status or "Not found"]',
      '[CHECK/MISSING] SLA -- [status or "Not found"]',
      '[CHECK/MISSING] Insurance Certificate -- [status or "Not found"]',
      "```",
      "",
      "Flag any gaps that may be needed based on the relationship type (e.g., if there is an MSA but no DPA and the vendor handles personal data).",
      "",
      "### Step 5: Generate Report",
      "",
      "Output a consolidated report:",
      "",
      "```",
      "## Vendor Agreement Status: [Vendor Name]",
      "",
      "**Search Date**: [today's date]",
      "**Sources Checked**: [list of systems searched]",
      "**Sources Unavailable**: [list of systems not connected, if any]",
      "",
      "## Relationship Overview",
      "",
      "**Vendor**: [full legal name]",
      "**Relationship Type**: [vendor/partner/customer/etc.]",
      "**CRM Status**: [if available]",
      "",
      "## Agreement Summary",
      "",
      "### [Agreement Type 1] -- [Status]",
      "- **Effective**: [date]",
      "- **Expires**: [date] ([auto-renews / does not auto-renew])",
      "- **Key Terms**: [summary of material terms]",
      "- **Location**: [where the executed copy is stored]",
      "",
      "### [Agreement Type 2] -- [Status]",
      "[etc.]",
      "",
      "## Gap Analysis",
      "",
      "[What's in place vs. what may be needed]",
      "",
      "## Upcoming Actions",
      "",
      "- [Any approaching expirations or renewal deadlines]",
      "- [Required agreements not yet in place]",
      "- [Amendments or updates that may be needed]",
      "",
      "## Notes",
      "",
      "[Any relevant context from email/chat searches]",
      "```",
      "",
      "### Step 6: Handle Missing Sources",
      "",
      "If key systems are not connected via MCP:",
      "",
      "- **No CLM**: Note that no CLM is connected. Suggest the user check their CLM manually. Report what was found in other systems.",
      "- **No CRM**: Skip CRM context. Note the gap.",
      '- **No Email**: Note that email was not searched. Suggest the user search their email for "[vendor name] agreement" or "[vendor name] NDA".',
      "- **No Documents**: Note that document storage was not searched.",
      "",
      "Always clearly state which sources were checked and which were not, so the user knows the completeness of the report.",
      "",
      "## Notes",
      "",
      "- If no agreements are found in any connected system, report that clearly and ask the user if they have agreements stored elsewhere",
      "- For vendor groups (e.g., a vendor with multiple subsidiaries), ask whether the user wants to check a specific entity or the entire group",
      "- Flag any agreements that are expired but may still have surviving obligations (confidentiality, indemnification, etc.)",
      "- If an agreement is approaching expiration (within 90 days), highlight this prominently",
      "",
    ].join("\n"),
  },
  "marketing-brand-review": {
    "SKILL.md": [
      "---",
      "name: marketing-brand-review",
      "description: Review content against your brand voice, style guide, and messaging pillars",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <content to review>",
      "---",
      "",
      "# Brand Review",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Review marketing content against brand voice, style guidelines, and messaging standards. Flag deviations and provide specific improvement suggestions.",
      "",
      "## Trigger",
      "",
      "User runs `/brand-review` or asks to review, check, or audit content against brand guidelines.",
      "",
      "## Inputs",
      "",
      "1. **Content to review** — accept content in any of these forms:",
      "   - Pasted directly into the conversation",
      "   - A file path or ~~knowledge base reference (e.g. Notion page, shared doc)",
      "   - A URL to a published page",
      "   - Multiple pieces for batch review",
      "",
      "2. **Brand guidelines source** (determined automatically):",
      "   - If a brand style guide is configured in local settings, use it automatically",
      '   - If not configured, ask: "Do you have a brand style guide or voice guidelines I should review against? You can paste them, share a file, or describe your brand voice. Otherwise, I\'ll do a general review for clarity, consistency, and professionalism."',
      "",
      "## Review Process",
      "",
      "### With Brand Guidelines Configured",
      "",
      "Evaluate the content against each of these dimensions:",
      "",
      "#### Voice and Tone",
      "- Does the content match the defined brand voice attributes?",
      "- Is the tone appropriate for the content type and audience?",
      "- Are there shifts in voice that feel inconsistent?",
      "- Flag specific sentences or phrases that deviate with an explanation of why",
      "",
      "#### Terminology and Language",
      "- Are preferred brand terms used correctly?",
      '- Are any "avoid" terms or phrases present?',
      "- Is jargon level appropriate for the target audience?",
      "- Are product names, feature names, and branded terms used correctly (capitalization, formatting)?",
      "",
      "#### Messaging Pillars",
      "- Does the content align with defined messaging pillars or value propositions?",
      "- Are claims consistent with approved messaging?",
      "- Is the content reinforcing or contradicting brand positioning?",
      "",
      "#### Style Guide Compliance",
      "- Grammar and punctuation per style guide (e.g., Oxford comma, title case vs. sentence case)",
      "- Formatting conventions (headers, lists, emphasis)",
      "- Number formatting, date formatting",
      "- Acronym usage (defined on first use?)",
      "",
      "### Without Brand Guidelines (Generic Review)",
      "",
      "Evaluate the content for:",
      "",
      "#### Clarity",
      "- Is the main message clear within the first paragraph?",
      "- Are sentences concise and easy to understand?",
      "- Is the structure logical and easy to follow?",
      "- Are there ambiguous statements or unclear references?",
      "",
      "#### Consistency",
      "- Is the tone consistent throughout?",
      "- Are terms used consistently (no switching between synonyms for the same concept)?",
      "- Is formatting consistent (headers, lists, capitalization)?",
      "",
      "#### Professionalism",
      "- Is the content free of typos, grammatical errors, and awkward phrasing?",
      "- Is the tone appropriate for the intended audience?",
      "- Are claims supported or substantiated?",
      "",
      "### Legal and Compliance Flags (Always Checked)",
      "",
      "Regardless of whether brand guidelines are configured, flag:",
      '- **Unsubstantiated claims** — superlatives ("best", "fastest", "only") without evidence or qualification',
      "- **Missing disclaimers** — financial claims, health claims, or guarantees that may need legal disclaimers",
      "- **Comparative claims** — comparisons to competitors that could be challenged",
      "- **Regulatory language** — content that may need compliance review (financial services, healthcare, etc.)",
      "- **Testimonial issues** — quotes or endorsements without attribution or disclosure",
      "- **Copyright concerns** — content that appears to be closely paraphrased from other sources",
      "",
      "## Output Format",
      "",
      "Present the review as:",
      "",
      "### Summary",
      "- Overall assessment: how well the content aligns with brand standards (or general quality)",
      "- 1-2 sentence summary of the biggest strengths",
      "- 1-2 sentence summary of the most important improvements",
      "",
      "### Detailed Findings",
      "",
      "For each issue found, provide:",
      "",
      "| Issue | Location | Severity | Suggestion |",
      "|-------|----------|----------|------------|",
      "",
      "Where severity is:",
      "- **High** — contradicts brand voice, contains compliance risk, or significantly undermines messaging",
      "- **Medium** — inconsistent with guidelines but not damaging",
      "- **Low** — minor style or preference issue",
      "",
      "### Revised Sections",
      "",
      "For the top 3-5 highest-severity issues, provide a before/after showing the original text and a suggested revision.",
      "",
      "### Legal/Compliance Flags",
      "",
      "List any legal or compliance concerns separately with recommended actions.",
      "",
      "## After Review",
      "",
      'Ask: "Would you like me to:',
      "- Revise the full content with these suggestions applied?",
      "- Focus on fixing just the high-severity issues?",
      "- Review additional content against the same guidelines?",
      '- Help you document your brand voice for future reviews?"',
      "",
    ].join("\n"),
  },
  "marketing-brand-voice": {
    "SKILL.md": [
      "---",
      "name: marketing-brand-voice",
      "description: Apply and enforce brand voice, style guide, and messaging pillars across content. Use when reviewing content for brand consistency, documenting a brand voice, adapting tone for different audiences, or checking terminology and style guide compliance.",
      "metadata:",
      "  plugin: marketing",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Brand Voice Skill",
      "",
      "Frameworks for documenting, applying, and enforcing brand voice and style guidelines across marketing content.",
      "",
      "## Brand Voice Documentation Framework",
      "",
      "A complete brand voice document should cover these areas. Use this framework to help users define their brand voice or to understand an existing brand voice configuration.",
      "",
      "### 1. Brand Personality",
      "Define the brand as if it were a person. What are its defining traits?",
      "",
      'Example: "If our brand were a person, they would be a knowledgeable colleague who explains complex things simply, celebrates your wins genuinely, and never talks down to you."',
      "",
      "### 2. Voice Attributes",
      "Select 3-5 attributes that define how the brand communicates. Each attribute should be defined with:",
      "- What it means in practice",
      "- What it does NOT mean (to prevent misinterpretation)",
      "- An example demonstrating the attribute",
      "",
      "### 3. Audience Awareness",
      "- Who the brand is speaking to (primary and secondary audiences)",
      "- What the audience cares about",
      "- What level of expertise the audience has",
      "- How the audience expects to be addressed",
      "",
      "### 4. Core Messaging Pillars",
      "- 3-5 key themes the brand consistently communicates",
      "- The hierarchy of these messages (which comes first)",
      "- How each pillar connects to audience needs",
      "",
      "### 5. Tone Spectrum",
      "How the voice adapts across contexts while remaining recognizably the same brand.",
      "",
      "### 6. Style Rules",
      "Specific grammar, formatting, and language rules. See the Style Guide Enforcement section below.",
      "",
      "### 7. Terminology",
      "Preferred and avoided terms. See the Terminology Management section below.",
      "",
      "## Voice Attributes",
      "",
      "### Common Voice Attribute Pairs",
      "",
      "When defining brand voice, it helps to position attributes on a spectrum. Here are common attribute spectrums:",
      "",
      "| Spectrum | One End | Other End |",
      "|----------|---------|-----------|",
      "| Formality | Formal, institutional | Casual, conversational |",
      "| Authority | Expert, authoritative | Peer-level, collaborative |",
      "| Emotion | Warm, empathetic | Direct, matter-of-fact |",
      "| Complexity | Technical, precise | Simple, accessible |",
      "| Energy | Bold, energetic | Calm, measured |",
      "| Humor | Playful, witty | Serious, earnest |",
      "| Innovation | Cutting-edge, forward-looking | Established, proven |",
      "",
      "### Defining an Attribute",
      "",
      "For each chosen attribute, document it in this format:",
      "",
      "**[Attribute name]**",
      "- **We are**: [what this means in practice]",
      "- **We are not**: [common misinterpretation to avoid]",
      "- **This sounds like**: [example sentence demonstrating the attribute]",
      "- **This does NOT sound like**: [example sentence violating the attribute]",
      "",
      "Example:",
      "",
      "**Approachable**",
      "- **We are**: friendly, clear, jargon-free, welcoming to beginners and experts alike",
      "- **We are not**: dumbed-down, overly casual, or lacking substance",
      '- **This sounds like**: "Here\'s how to get started — it takes about five minutes."',
      '- **This does NOT sound like**: "Yo! This is super easy, even a noob can do it lol."',
      "",
      "## Tone Adaptation Across Channels and Contexts",
      "",
      "The brand voice stays consistent, but tone adapts to context. Tone is the emotional inflection applied to the voice.",
      "",
      "### Tone by Channel",
      "",
      "| Channel | Tone Adaptation | Example |",
      "|---------|----------------|---------|",
      '| Blog | Informative, conversational, educational | "Let\'s walk through how this works and why it matters for your team." |',
      '| Social media (LinkedIn) | Professional, thought-provoking, concise | "Three things we learned from running 50 campaigns this quarter." |',
      '| Social media (Twitter/X) | Punchy, direct, sometimes witty | "Your landing page has 3 seconds. Make them count." |',
      '| Email marketing | Personal, helpful, action-oriented | "We put together something we think you\'ll find useful." |',
      '| Sales collateral | Confident, benefit-driven, specific | "Teams using our platform reduce reporting time by 40%." |',
      '| Support/Help docs | Clear, patient, step-by-step | "If you see this error, here\'s how to fix it." |',
      '| Press release | Formal, factual, newsworthy | "The company today announced the launch of..." |',
      '| Error messages | Empathetic, helpful, blame-free | "Something went wrong on our end. We\'re looking into it." |',
      "",
      "### Tone by Situation",
      "",
      "| Situation | Tone Adaptation |",
      "|-----------|----------------|",
      "| Product launch | Excited, confident, forward-looking |",
      "| Incident or outage | Transparent, empathetic, accountable |",
      "| Customer success story | Celebratory, specific, crediting the customer |",
      "| Thought leadership | Authoritative, nuanced, evidence-based |",
      "| Onboarding | Welcoming, encouraging, clear |",
      "| Bad news (price increase, deprecation) | Honest, respectful, solution-oriented |",
      "| Competitive comparison | Confident but fair, fact-based, not disparaging |",
      "",
      "### Tone Adaptation Rule",
      'The voice attributes remain fixed. Tone dials them up or down based on context. For example, if a brand is "bold and warm":',
      "- In a product launch, dial up boldness",
      "- In an incident response, dial up warmth",
      "- Neither attribute disappears; the balance shifts",
      "",
      "## Style Guide Enforcement",
      "",
      "### Grammar and Mechanics",
      "Document and enforce these choices consistently:",
      "",
      "| Rule | Options | Example |",
      "|------|---------|---------|",
      '| Oxford comma | Yes / No | "fast, reliable, and secure" vs. "fast, reliable and secure" |',
      '| Sentence case vs. title case (headings) | Sentence / Title | "How to get started" vs. "How to Get Started" |',
      '| Contractions | Use / Avoid | "we\'re" vs. "we are" |',
      '| Em dash spacing | No spaces / Spaces | "this—and more" vs. "this — and more" |',
      '| Numbers | Spell out 1-9, numerals 10+ / Always numerals | "five features" vs. "5 features" |',
      '| Percent | % / percent | "50%" vs. "50 percent" |',
      '| Date format | Month DD, YYYY / DD/MM/YYYY / etc. | "January 15, 2025" |',
      '| Time format | 12-hour / 24-hour | "3:00 PM" vs. "15:00" |',
      '| Lists | Periods / No periods on fragments | "Set up your account." vs. "Set up your account" |',
      "",
      "### Formatting Conventions",
      "- Heading hierarchy (when to use H1, H2, H3)",
      "- Bold and italic usage (bold for emphasis, italic for titles/terms)",
      '- Link text (descriptive vs. "click here" — always descriptive)',
      "- Image alt text requirements",
      "- Code formatting (for technical brands)",
      "- Callout or highlight box usage",
      "",
      "### Punctuation and Emphasis",
      "- Exclamation mark policy (limited use, never more than one)",
      "- Ellipsis usage (avoid in most professional contexts)",
      "- ALL CAPS policy (avoid; use bold for emphasis instead)",
      "- Emoji usage by channel (professional channels: minimal or none; social: where appropriate)",
      "",
      "## Terminology Management",
      "",
      "### Preferred Terms",
      "",
      "Maintain a list of preferred terms and their incorrect alternatives:",
      "",
      "| Use This | Not This | Notes |",
      "|----------|----------|-------|",
      '| sign up (verb) | signup (verb) | "signup" is the noun form |',
      '| log in (verb) | login (verb) | "login" is the noun/adjective form |',
      '| set up (verb) | setup (verb) | "setup" is the noun/adjective form |',
      "| email | e-mail | No hyphen |",
      "| website | web site | One word |",
      "| data is (singular) | data are | Unless the publication requires plural |",
      "",
      "### Product and Feature Names",
      "- Official capitalization for product names",
      "- When to use the full product name vs. shorthand",
      '- Whether to use "the" before product names',
      "- How to handle versioning in copy",
      "- Trademark and registration symbols (when required and when to omit)",
      "",
      "### Inclusive Language",
      "- Use gender-neutral language (they/them for unknown individuals)",
      '- Avoid ableist language ("crazy", "blind spot", "lame")',
      "- Use person-first language where appropriate",
      "- Avoid culturally specific idioms that may not translate",
      '- Use "simple" or "straightforward" instead of "easy" (what is easy varies by person)',
      "",
      "### Industry Jargon Management",
      "- Define which technical terms the audience understands without explanation",
      "- List jargon that should always be defined or replaced with plain language",
      "- Specify which acronyms need to be spelled out on first use",
      "- Audience-specific glossary for terms that mean different things to different readers",
      "",
      "### Competitor and Category Terms",
      "- How to refer to your product category (use your preferred framing)",
      "- How to refer to competitors (by name or generically)",
      "- Terms competitors have coined that you should avoid (to prevent reinforcing their positioning)",
      "- Your preferred differentiation language",
      "",
    ].join("\n"),
  },
  "marketing-campaign-plan": {
    "SKILL.md": [
      "---",
      "name: marketing-campaign-plan",
      "description: Generate a full campaign brief with objectives, channels, content calendar, and success metrics",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <campaign objective or product>",
      "---",
      "",
      "# Campaign Plan",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate a comprehensive marketing campaign brief with objectives, audience, messaging, channel strategy, content calendar, and success metrics.",
      "",
      "## Trigger",
      "",
      "User runs `/campaign-plan` or asks to plan, design, or build a marketing campaign.",
      "",
      "## Inputs",
      "",
      "Gather the following from the user. If not provided, ask before proceeding:",
      "",
      "1. **Campaign goal** — the primary objective (e.g., drive signups, increase awareness, launch a product, generate leads, re-engage churned users)",
      "",
      "2. **Target audience** — who the campaign is aimed at (demographics, roles, industries, pain points, buying stage)",
      "",
      "3. **Timeline** — campaign duration and any fixed dates (launch date, event date, seasonal deadline)",
      "",
      "4. **Budget range** — approximate budget or budget tier (optional; if not provided, generate a channel-agnostic plan and note where budget allocation would matter)",
      "",
      "5. **Additional context** (optional):",
      "   - Product or service being promoted",
      "   - Key differentiators or value propositions",
      "   - Previous campaign performance or learnings",
      "   - Brand guidelines or constraints",
      "   - Geographic focus",
      "",
      "## Campaign Brief Structure",
      "",
      "Generate a campaign brief with the following sections:",
      "",
      "### 1. Campaign Overview",
      "- Campaign name suggestion",
      "- One-sentence campaign summary",
      "- Primary objective with a specific, measurable goal",
      "- Secondary objectives (if applicable)",
      "",
      "### 2. Target Audience",
      "- Primary audience segment with description",
      "- Secondary audience segment (if applicable)",
      "- Audience pain points and motivations",
      "- Where they spend time (channels, communities, publications)",
      "- Buying stage alignment (awareness, consideration, decision)",
      "",
      "### 3. Key Messages",
      "- Core campaign message (one sentence)",
      "- 3-4 supporting messages tailored to audience pain points",
      "- Message variations by channel (if different tones are needed)",
      "- Proof points or evidence to support each message",
      "",
      "### 4. Channel Strategy",
      "Recommend channels based on audience and goal. For each channel, include:",
      "- Why this channel fits the audience and objective",
      "- Content format recommendations",
      "- Estimated effort level (low, medium, high)",
      "- Budget allocation suggestion (if budget was provided)",
      "",
      "Consider channels from:",
      "- Owned: blog, email, website, social media profiles",
      "- Earned: PR, influencer partnerships, guest posts, community engagement",
      "- Paid: search ads, social ads, display, sponsored content, events",
      "",
      "### 5. Content Calendar",
      "Create a week-by-week (or day-by-day for short campaigns) content calendar:",
      "- What content to produce each week",
      "- Which channel each piece targets",
      "- Key milestones and deadlines",
      '- Dependencies between pieces (e.g., "landing page must be live before paid ads launch")',
      "",
      "Format as a table:",
      "",
      "| Week | Content Piece | Channel | Owner/Notes | Status |",
      "|------|--------------|---------|-------------|--------|",
      "",
      "### 6. Content Pieces Needed",
      "List every content asset required for the campaign:",
      "- Asset name and type (blog post, email, social post, ad creative, landing page, etc.)",
      "- Brief description of what it should contain",
      "- Priority (must-have vs. nice-to-have)",
      "- Suggested timeline for creation",
      "",
      "### 7. Success Metrics",
      "Define KPIs aligned to the campaign objective:",
      "- Primary KPI with target number",
      "- Secondary KPIs (3-5)",
      "- How each metric will be tracked",
      "- Reporting cadence recommendation",
      "",
      "If ~~product analytics is connected, reference any available historical performance benchmarks to inform targets.",
      "",
      "### 8. Budget Allocation (if budget provided)",
      "- Breakdown by channel or activity",
      "- Production costs vs. distribution costs",
      "- Contingency recommendation (typically 10-15%)",
      "",
      "### 9. Risks and Mitigations",
      "- 2-3 potential risks (timeline, audience mismatch, channel underperformance)",
      "- Mitigation strategy for each",
      "",
      "### 10. Next Steps",
      "- Immediate action items to kick off the campaign",
      "- Stakeholder approvals needed",
      "- Key decision points",
      "",
      "## Output",
      "",
      "Present the full campaign brief with clear headings and formatting. After the brief, ask:",
      "",
      '"Would you like me to:',
      "- Dive deeper into any section?",
      "- Draft specific content pieces from the calendar?",
      "- Create a competitive analysis to inform the messaging?",
      '- Adjust the plan for a different budget or timeline?"',
      "",
    ].join("\n"),
  },
  "marketing-campaign-planning": {
    "SKILL.md": [
      "---",
      "name: marketing-campaign-planning",
      "description: Plan marketing campaigns with objectives, audience segmentation, channel strategy, content calendars, and success metrics. Use when launching a campaign, planning a product launch, building a content calendar, allocating budget across channels, or defining campaign KPIs.",
      "metadata:",
      "  plugin: marketing",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Campaign Planning Skill",
      "",
      "Frameworks and guidance for planning, structuring, and executing marketing campaigns.",
      "",
      "## Campaign Framework: Objective, Audience, Message, Channel, Measure",
      "",
      "Every campaign should be built on this five-part framework:",
      "",
      "### 1. Objective",
      "Define what success looks like before planning anything else.",
      "",
      "- **Awareness**: increase brand or product visibility (measured by reach, impressions, share of voice)",
      "- **Consideration**: drive engagement and education (measured by content engagement, email signups, webinar attendance)",
      "- **Conversion**: generate leads or sales (measured by signups, demos, purchases, pipeline)",
      "- **Retention**: re-engage existing customers (measured by churn reduction, upsell, NPS)",
      "- **Advocacy**: turn customers into promoters (measured by referrals, reviews, UGC)",
      "",
      "Good objectives are SMART: Specific, Measurable, Achievable, Relevant, Time-bound.",
      "",
      'Example: "Generate 200 marketing qualified leads from mid-market SaaS companies in North America within 6 weeks of campaign launch."',
      "",
      "### 2. Audience",
      "Define who you are trying to reach with enough specificity to guide messaging and channel decisions.",
      "",
      "- **Demographics**: role/title, seniority, company size, industry",
      "- **Psychographics**: motivations, pain points, goals, objections",
      "- **Behavioral**: where they consume content, how they buy, what they have engaged with before",
      "- **Buying stage**: are they unaware of the problem, researching solutions, or ready to buy?",
      "",
      "Create a brief audience profile (not a full persona) for campaign planning:",
      '> "[Role] at [company type] who is struggling with [pain point] and looking for [desired outcome]. They typically discover solutions through [channels] and care most about [priorities]."',
      "",
      "### 3. Message",
      "Craft the core message and supporting points that will resonate with the audience.",
      "",
      "- **Core message**: one sentence that captures what you want the audience to think, feel, or do",
      "- **Supporting messages**: 3-4 points that provide evidence, address objections, or elaborate on benefits",
      "- **Proof points**: data, case studies, testimonials, or third-party validation for each supporting message",
      "- **Differentiation**: what makes your offering different from alternatives (including doing nothing)",
      "",
      "Message hierarchy:",
      "1. Why should I care? (addresses the pain point or opportunity)",
      "2. What is the solution? (positions your offering)",
      "3. Why you? (differentiates from alternatives)",
      "4. What should I do? (call to action)",
      "",
      "### 4. Channel",
      "Select channels based on where your audience is, not where you are most comfortable.",
      "",
      "See the Channel Selection Guide below for detailed guidance.",
      "",
      "### 5. Measure",
      "Define how you will know the campaign worked. See Success Metrics by Campaign Type below.",
      "",
      "## Channel Selection Guide",
      "",
      "### Owned Channels",
      "",
      "| Channel | Best For | Typical Metrics | Effort |",
      "|---------|----------|----------------|--------|",
      "| Blog/Website | SEO, thought leadership, education | Traffic, time on page, conversions | Medium |",
      "| Email | Nurture, retention, announcements | Open rate, CTR, conversions | Low-Medium |",
      "| Social (organic) | Awareness, community, brand building | Engagement, reach, follower growth | Medium |",
      "| Webinars | Education, lead gen, product demos | Registrations, attendance, pipeline | High |",
      "| Podcast | Thought leadership, brand awareness | Downloads, subscriber growth | High |",
      "",
      "### Earned Channels",
      "",
      "| Channel | Best For | Typical Metrics | Effort |",
      "|---------|----------|----------------|--------|",
      "| PR/Media | Awareness, credibility, launches | Coverage, share of voice, referral traffic | High |",
      "| Guest content | Audience expansion, SEO, credibility | Referral traffic, backlinks | Medium |",
      "| Influencer/Partner | Audience expansion, trust | Reach, engagement, referral conversions | Medium-High |",
      "| Community | Awareness, trust, feedback | Mentions, engagement, referral traffic | Medium |",
      "| Reviews/Ratings | Credibility, SEO, consideration | Review volume, rating, conversion lift | Low-Medium |",
      "",
      "### Paid Channels",
      "",
      "| Channel | Best For | Typical Metrics | Effort |",
      "|---------|----------|----------------|--------|",
      "| Search ads (SEM) | High-intent lead capture | CPC, CTR, conversion rate, CPA | Medium |",
      "| Social ads | Awareness, retargeting, lead gen | CPM, CPC, CTR, CPA, ROAS | Medium |",
      "| Display/Programmatic | Awareness, retargeting | Impressions, CPM, view-through conversions | Low-Medium |",
      "| Sponsored content | Thought leadership, lead gen | Engagement, leads, cost per lead | Medium |",
      "| Events/Sponsorships | Relationship building, brand | Leads, meetings, pipeline influenced | High |",
      "",
      "### Channel Selection Criteria",
      "When choosing channels, consider:",
      "- Where does your target audience spend time?",
      "- What is the buying stage you are targeting? (awareness channels vs. conversion channels)",
      "- What is your budget? (paid channels require spend; owned/earned require time)",
      "- What content assets do you already have or can you produce?",
      "- What has worked in the past? (reference historical data if available)",
      "",
      "## Content Calendar Creation",
      "",
      "### Calendar Structure",
      "",
      "A content calendar should answer: what, where, when, who, and why for every piece of content.",
      "",
      "| Date | Content Piece | Channel | Audience Segment | Campaign/Theme | Owner | Status |",
      "|------|--------------|---------|-------------------|----------------|-------|--------|",
      "",
      "### Calendar Planning Process",
      "1. **Start with milestones**: campaign launch, event dates, product releases, seasonal moments",
      "2. **Work backward**: what needs to be live and when? What is the production lead time?",
      "3. **Map content to funnel stages**: ensure coverage across awareness, consideration, and conversion",
      "4. **Batch by theme**: group related content pieces into weekly or bi-weekly themes",
      "5. **Balance channels**: do not over-index on one channel; ensure the audience sees the campaign across touchpoints",
      "6. **Build in flexibility**: leave 20% of calendar slots open for reactive or opportunistic content",
      "",
      "### Content Cadence Guidelines",
      "- **Blog**: 1-4 posts per week depending on team size and goals",
      "- **Email newsletter**: weekly or bi-weekly for most audiences",
      "- **Social media**: 3-7 posts per week per platform (varies by platform)",
      "- **Paid campaigns**: continuous during campaign window with creative refreshes every 2-4 weeks",
      "- **Webinars**: monthly or quarterly depending on resources",
      "",
      "### Production Timeline Benchmarks",
      "- Blog post: 3-5 business days (research, draft, review, publish)",
      "- Email campaign: 2-3 business days (copy, design, test, send)",
      "- Social media posts: 1-2 business days (draft, design, schedule)",
      "- Landing page: 5-7 business days (copy, design, development, QA)",
      "- Video content: 2-4 weeks (script, production, editing)",
      "- Ebook/whitepaper: 2-4 weeks (outline, draft, design, review)",
      "",
      "## Budget Allocation Approaches",
      "",
      "### Percentage of Revenue Method",
      "- Industry benchmark: 5-15% of revenue for marketing, with B2B typically at 5-10% and B2C at 10-15%",
      "- Startups and growth-stage companies often invest 15-25% of revenue in marketing",
      "- Within the marketing budget, allocate across brand (long-term) and performance (short-term)",
      "",
      "### Channel Allocation Framework",
      "A common starting framework (adjust based on goals and historical data):",
      "",
      "| Category | Percentage of Budget | Examples |",
      "|----------|---------------------|----------|",
      "| Paid acquisition | 30-40% | Search ads, social ads, display |",
      "| Content production | 20-30% | Blog, video, design, ebooks |",
      "| Events and sponsorships | 10-20% | Conferences, webinars, meetups |",
      "| Tools and technology | 10-15% | Analytics, automation, CRM |",
      "| Testing and experimentation | 5-10% | New channels, A/B tests, pilots |",
      "",
      "### Budget Optimization Principles",
      "- Start with your highest-confidence channel and allocate 60-70% of paid budget there",
      "- Reserve 15-20% for testing new channels or tactics",
      "- Shift budget monthly based on performance data (do not set and forget)",
      "- Account for production costs, not just media spend",
      "- Include a 10-15% contingency for unexpected opportunities or overruns",
      "",
      "## Success Metrics by Campaign Type",
      "",
      "### Awareness Campaign",
      "| Metric | What It Measures |",
      "|--------|-----------------|",
      "| Reach/Impressions | How many people saw the campaign |",
      "| Brand mention volume | Increase in brand conversations |",
      "| Share of voice | Your mentions vs. competitors |",
      "| Direct traffic | People coming to your site unprompted |",
      "| Social follower growth | Audience building |",
      "",
      "### Lead Generation Campaign",
      "| Metric | What It Measures |",
      "|--------|-----------------|",
      "| Total leads | Volume of new contacts |",
      "| Marketing qualified leads (MQLs) | Leads meeting quality threshold |",
      "| Cost per lead (CPL) | Efficiency of spend |",
      "| Lead-to-MQL conversion rate | Quality of leads generated |",
      "| Pipeline influenced | Revenue opportunity created |",
      "",
      "### Product Launch Campaign",
      "| Metric | What It Measures |",
      "|--------|-----------------|",
      "| Signups or trials | Adoption of new product |",
      "| Activation rate | Users who complete key first action |",
      "| Media coverage | Earned media hits |",
      "| Social buzz | Mentions, shares, engagement spike |",
      "| Feature adoption | Usage of specific launched features |",
      "",
      "### Retention/Engagement Campaign",
      "| Metric | What It Measures |",
      "|--------|-----------------|",
      "| Churn rate change | Customer retention improvement |",
      "| Engagement rate | Interactions with campaign content |",
      "| NPS or CSAT change | Satisfaction improvement |",
      "| Upsell/cross-sell revenue | Expansion revenue |",
      "| Feature adoption | Usage of promoted features |",
      "",
      "### Event/Webinar Campaign",
      "| Metric | What It Measures |",
      "|--------|-----------------|",
      "| Registrations | Interest generated |",
      "| Attendance rate | Conversion from registration |",
      "| Engagement during event | Questions, polls, chat activity |",
      "| Post-event conversions | Leads or pipeline from attendees |",
      "| Content repurposing reach | Downstream audience from recordings |",
      "",
    ].join("\n"),
  },
  "marketing-competitive-analysis": {
    "SKILL.md": [
      "---",
      "name: marketing-competitive-analysis",
      "description: Research competitors and compare positioning, messaging, content strategy, and market presence. Use when analyzing a competitor, building battlecards, identifying content gaps, comparing feature messaging, or preparing competitive positioning recommendations.",
      "metadata:",
      "  plugin: marketing",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Competitive Analysis Skill",
      "",
      "Frameworks and methodologies for researching competitors, comparing positioning, and identifying market opportunities.",
      "",
      "## Competitive Research Methodology",
      "",
      "### Research Sources",
      "",
      "Gather intelligence from these categories of sources:",
      "",
      "#### Primary Sources (Direct from Competitor)",
      "- **Website**: homepage, product pages, pricing, about page, careers",
      "- **Blog and resource center**: content themes, publishing frequency, depth",
      "- **Social media profiles**: messaging, engagement, content strategy",
      "- **Product demos and free trials**: UX, features, onboarding experience",
      "- **Webinars and events**: topics, speakers, audience engagement",
      "- **Press releases and newsroom**: announcements, partnerships, milestones",
      "- **Job postings**: hiring signals that reveal strategic priorities (e.g., hiring for a new product line or market)",
      "",
      "#### Secondary Sources (Third-Party)",
      "- **Review sites**: G2, Capterra, TrustRadius, Product Hunt — customer sentiment themes",
      "- **Analyst reports**: Gartner, Forrester, IDC — market positioning and category placement",
      "- **News coverage**: TechCrunch, industry publications — funding, partnerships, narrative",
      "- **Social listening**: mentions, sentiment, share of voice across social platforms",
      "- **SEO tools**: keyword rankings, organic traffic estimates, content gaps",
      "- **Financial filings**: revenue, growth rate, investment areas (for public companies)",
      "- **Community forums**: community forums (e.g. Reddit, Discourse), industry chat groups (e.g. Slack communities) — user sentiment",
      "",
      "### Research Process",
      "1. **Set scope**: define which competitors and what aspects to analyze",
      "2. **Gather data**: systematically collect information from sources above",
      "3. **Organize findings**: structure by competitor, then by dimension",
      "4. **Analyze patterns**: identify themes, strengths, weaknesses, and trends",
      "5. **Compare to your position**: map findings against your own positioning and capabilities",
      "6. **Synthesize insights**: extract actionable takeaways and opportunities",
      "7. **Date-stamp everything**: competitive intelligence has a short shelf life",
      "",
      "### Research Cadence",
      "- **Deep competitive analysis**: quarterly (full research across all sources)",
      "- **Competitive monitoring**: monthly (scan for new announcements, content, messaging changes)",
      "- **Real-time alerts**: ongoing (set up alerts for competitor brand mentions, press, job postings)",
      "",
      "## Messaging Comparison Frameworks",
      "",
      "### Messaging Matrix",
      "",
      "Compare messaging across competitors on key dimensions:",
      "",
      "| Dimension | Your Company | Competitor A | Competitor B | Competitor C |",
      "|-----------|-------------|--------------|--------------|--------------|",
      "| Tagline/Headline | | | | |",
      "| Core value proposition | | | | |",
      "| Primary audience | | | | |",
      "| Key differentiator claim | | | | |",
      "| Tone/Voice | | | | |",
      "| Proof points used | | | | |",
      "| Category framing | | | | |",
      "| Primary CTA | | | | |",
      "",
      "### Value Proposition Comparison",
      "",
      "For each competitor, document:",
      "- **Promise**: what they promise the customer will achieve",
      "- **Evidence**: how they prove the promise (data, testimonials, demos)",
      '- **Mechanism**: how their product delivers on the promise (the "how it works")',
      "- **Uniqueness**: what they claim only they can do",
      "",
      "### Narrative Analysis",
      "",
      "Identify each competitor's story arc:",
      "- **Villain**: what problem or enemy they position against (status quo, legacy tools, complexity)",
      "- **Hero**: who is the hero in their story (the customer? the product? the team?)",
      "- **Transformation**: what before/after do they promise?",
      "- **Stakes**: what happens if you do not act?",
      "",
      "This reveals positioning strategy and emotional appeals.",
      "",
      "### Messaging Strengths and Vulnerabilities",
      "",
      "For each competitor's messaging, assess:",
      "- **Clarity**: can a first-time visitor understand what they do in 5 seconds?",
      "- **Differentiation**: is their positioning distinct or generic?",
      "- **Proof**: do they back up claims with evidence?",
      "- **Consistency**: is messaging consistent across channels?",
      "- **Resonance**: does their messaging address real customer pain points?",
      "",
      "## Content Gap Analysis",
      "",
      "### Content Audit Comparison",
      "",
      "Map content across competitors by:",
      "",
      "| Topic/Theme | Your Content | Competitor A | Competitor B | Gap? |",
      "|-------------|-------------|--------------|--------------|------|",
      "| [Topic 1] | Blog post, ebook | Blog series, webinar | Nothing | Opportunity for B |",
      "| [Topic 2] | Nothing | Whitepaper | Blog post, video | Gap for you |",
      "| [Topic 3] | Case study | Nothing | Case study | Parity |",
      "",
      "### Content Type Coverage",
      "",
      "| Content Format | You | Comp A | Comp B | Comp C |",
      "|----------------|-----|--------|--------|--------|",
      "| Blog posts | Y | Y | Y | Y |",
      "| Case studies | Y | Y | N | Y |",
      "| Ebooks/Whitepapers | N | Y | Y | N |",
      "| Webinars | Y | Y | Y | N |",
      "| Podcast | N | N | Y | N |",
      "| Video content | N | Y | Y | Y |",
      "| Interactive tools | N | N | N | Y |",
      "| Templates/Resources | Y | N | Y | N |",
      "",
      "### Identifying Content Opportunities",
      "1. **Topics they cover that you do not**: potential gaps in your content strategy",
      "2. **Topics you cover that they do not**: potential differentiators to amplify",
      "3. **Formats they use that you do not**: format gaps that could reach new audiences",
      "4. **Audience segments they address that you do not**: underserved audiences",
      "5. **Search terms they rank for that you do not**: SEO content gaps",
      "",
      "### Content Quality Assessment",
      "- Depth: surface-level or comprehensive?",
      "- Freshness: regularly updated or stale?",
      "- Engagement: do posts get comments, shares, links?",
      "- Production value: text-only or multimedia?",
      "- Thought leadership: original insights or rehashed content?",
      "",
      "## Positioning Strategy",
      "",
      "### Positioning Statement Framework",
      "",
      "For your company and each competitor, define (or reverse-engineer) their positioning statement:",
      "",
      "> For [target audience], [product/company] is the [category] that [key benefit/differentiator] because [reason to believe].",
      "",
      "Example:",
      "> For mid-market SaaS marketing teams, Acme is the campaign management platform that unifies planning and execution in one workspace because it is built on a single data model that eliminates tool fragmentation.",
      "",
      "### Positioning Map",
      "",
      "Plot competitors on a 2x2 matrix using the two most important dimensions for your market:",
      "",
      "Common axis pairs:",
      "- **Price vs. Capability** (low cost / basic vs. premium / full-featured)",
      "- **Ease of Use vs. Power** (simple / limited vs. complex / flexible)",
      "- **SMB Focus vs. Enterprise Focus** (self-serve / individual vs. sales-led / team)",
      "- **Point Solution vs. Platform** (does one thing well vs. does many things)",
      "- **Innovative vs. Established** (new approach vs. proven track record)",
      "",
      "Identify which quadrant is underserved or where your differentiation is strongest.",
      "",
      "### Category Strategy",
      "- **Create a new category**: if you do something genuinely different, define and own the category (high risk, high reward)",
      "- **Reframe the existing category**: change how buyers evaluate the category to favor your strengths",
      "- **Win the existing category**: compete directly on recognized criteria and out-execute",
      "- **Niche within the category**: own a specific segment, use case, or audience",
      "",
      "### Positioning Pitfalls to Avoid",
      "- Positioning against a competitor rather than for a customer need",
      "- Claiming too many differentiators (pick 1-2 that matter most)",
      "- Using category jargon the customer does not use",
      "- Positioning on features rather than outcomes",
      "- Changing positioning too frequently (confuses the market)",
      "",
      "## Battlecard Creation",
      "",
      "### Battlecard Structure",
      "",
      "A competitive battlecard is a one-page reference for sales and marketing teams. Include:",
      "",
      "#### Header",
      "- Competitor name and logo",
      "- Last updated date",
      "- Competitive win rate (if tracked)",
      "",
      "#### Quick Overview",
      "- What they do (one sentence)",
      "- Their target customer",
      "- Pricing model summary",
      "- Key recent developments",
      "",
      "#### Their Pitch",
      "- How they describe themselves",
      "- Their primary tagline",
      "- Their top 3 claimed differentiators",
      "",
      "#### Strengths (Be Honest)",
      "- Where they genuinely compete well",
      "- What customers like about them (from reviews)",
      "- Features or capabilities where they lead",
      "",
      "#### Weaknesses",
      "- Consistent customer complaints (from reviews)",
      "- Technical limitations",
      "- Gaps in their offering",
      "- Areas where customers report dissatisfaction",
      "",
      "#### Our Differentiators",
      "- 3-5 specific ways your product or approach is different",
      "- For each: the differentiator, why it matters to the customer, and proof",
      "",
      "#### Objection Handling",
      "| If the prospect says... | Respond with... |",
      "|------------------------|----------------|",
      '| "[Competitor] does X too" | "Here is how our approach differs..." |',
      '| "[Competitor] is cheaper" | "Here is what that price difference gets you..." |',
      '| "I\'ve heard good things about [Competitor]" | "They are strong at X. Where we differ is..." |',
      "",
      "#### Landmines to Set",
      "Questions to ask prospects early that highlight your advantages:",
      '- "How do you currently handle [area where competitor is weak]?"',
      '- "How important is [capability you have that they lack]?"',
      '- "Have you considered [risk that your product mitigates]?"',
      "",
      "#### Landmines to Defuse",
      "Questions competitors might encourage prospects to ask you, with prepared responses.",
      "",
      "#### Win/Loss Themes",
      "- Common reasons deals are won against this competitor",
      "- Common reasons deals are lost to this competitor",
      "- What types of prospects favor them vs. you",
      "",
      "### Battlecard Maintenance",
      "- Review and update quarterly at minimum",
      "- Update immediately after major competitor announcements",
      "- Incorporate win/loss feedback from sales team",
      "- Track which objection-handling responses are most effective",
      "",
    ].join("\n"),
  },
  "marketing-competitive-brief": {
    "SKILL.md": [
      "---",
      "name: marketing-competitive-brief",
      "description: Research competitors and generate a positioning and messaging comparison",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <competitor or market segment>",
      "---",
      "",
      "# Competitive Brief",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Research competitors and generate a structured competitive analysis comparing positioning, messaging, content strategy, and market presence.",
      "",
      "## Trigger",
      "",
      "User runs `/competitive-brief` or asks for a competitive analysis, competitor research, or market comparison.",
      "",
      "## Inputs",
      "",
      "Gather the following from the user:",
      "",
      "1. **Competitor name(s)** — one or more competitors to analyze (required)",
      "",
      "2. **Your company/product context** (optional but recommended):",
      "   - What you sell and to whom",
      "   - Your positioning or value proposition",
      "   - Key differentiators you want to highlight",
      "",
      "3. **Focus areas** (optional — if not specified, cover all):",
      "   - Messaging and positioning",
      "   - Product and feature comparison",
      "   - Content and thought leadership strategy",
      "   - Recent announcements and news",
      "   - Pricing and packaging (if publicly available)",
      "   - Market presence and audience",
      "",
      "## Research Process",
      "",
      "For each competitor, research using web search:",
      "",
      "1. **Company website** — homepage messaging, product pages, about page, pricing page",
      "2. **Recent news** — press releases, funding announcements, product launches, partnerships (last 6 months)",
      "3. **Content strategy** — blog topics, resource types, social media presence, webinars, podcasts",
      "4. **Review sites and comparisons** — third-party comparisons, analyst mentions, customer review themes",
      "5. **Job postings** — hiring signals that indicate strategic direction (optional)",
      "",
      "## Competitive Brief Structure",
      "",
      "### 1. Executive Summary",
      "- 2-3 sentence overview of the competitive landscape",
      "- Key takeaway: your biggest opportunity and biggest threat",
      "",
      "### 2. Competitor Profiles",
      "",
      "For each competitor:",
      "",
      "#### Company Overview",
      "- What they do (one-sentence positioning)",
      "- Target audience",
      "- Company size/stage indicators (funding, employee count if available)",
      "- Key recent developments",
      "",
      "#### Messaging Analysis",
      "- Primary tagline or headline",
      "- Core value proposition",
      "- Key messaging themes (3-5)",
      "- Tone and voice characterization",
      "- How they describe the problem they solve",
      "",
      "#### Product/Solution Positioning",
      "- How they categorize their product",
      "- Key features they emphasize",
      "- Claimed differentiators",
      "- Pricing approach (if publicly available)",
      "",
      "#### Content Strategy",
      "- Blog frequency and topics",
      "- Content types produced (ebooks, webinars, case studies, tools)",
      "- Social media presence and engagement approach",
      "- Thought leadership themes",
      "- SEO strategy observations (what terms they appear to target)",
      "",
      "#### Strengths",
      "- What they do well",
      "- Where their messaging resonates",
      "- Competitive advantages",
      "",
      "#### Weaknesses",
      "- Gaps in their messaging or positioning",
      "- Areas where they are vulnerable",
      "- Customer complaints or criticism themes (from reviews)",
      "",
      "### 3. Messaging Comparison Matrix",
      "",
      "| Dimension | Your Company | Competitor A | Competitor B |",
      "|-----------|-------------|--------------|--------------|",
      "| Primary tagline | ... | ... | ... |",
      "| Target buyer | ... | ... | ... |",
      "| Key differentiator | ... | ... | ... |",
      "| Tone/voice | ... | ... | ... |",
      "| Core value prop | ... | ... | ... |",
      "",
      "(Include user's company only if they provided their positioning context)",
      "",
      "### 4. Content Gap Analysis",
      "- Topics your competitors cover that you do not (or vice versa)",
      "- Content formats they use that you could adopt",
      "- Keywords or themes they own vs. opportunities they have missed",
      "",
      "### 5. Opportunities",
      "- Positioning gaps you can exploit",
      "- Messaging angles your competitors have not claimed",
      "- Audience segments they are underserving",
      "- Content or channel opportunities",
      "",
      "### 6. Threats",
      "- Areas where competitors are strong and you are vulnerable",
      "- Trends that favor their positioning",
      "- Recent moves that could shift the market",
      "",
      "### 7. Recommended Actions",
      "- 3-5 specific, actionable recommendations based on the analysis",
      "- Quick wins (things you can act on this week)",
      "- Strategic moves (longer-term positioning or content investments)",
      "",
      "## Output",
      "",
      "Present the full competitive brief with clear formatting. Note the date of the research so the user knows the freshness of the data.",
      "",
      "After the brief, ask:",
      "",
      '"Would you like me to:',
      "- Create a battlecard for your sales team based on this analysis?",
      "- Draft messaging that exploits the positioning gaps identified?",
      "- Dive deeper into any specific competitor?",
      '- Set up a competitive monitoring plan?"',
      "",
    ].join("\n"),
  },
  "marketing-content-creation": {
    "SKILL.md": [
      "---",
      "name: marketing-content-creation",
      "description: Draft marketing content across channels — blog posts, social media, email newsletters, landing pages, press releases, and case studies. Use when writing any marketing content, when you need channel-specific formatting, SEO-optimized copy, headline options, or calls to action.",
      "metadata:",
      "  plugin: marketing",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Content Creation Skill",
      "",
      "Guidelines and frameworks for creating effective marketing content across channels.",
      "",
      "## Content Type Templates",
      "",
      "### Blog Post Structure",
      "1. **Headline** — clear, benefit-driven, includes primary keyword (aim for 60 characters or less for SEO)",
      "2. **Introduction** (100-150 words) — hook the reader with a question, statistic, bold claim, or relatable scenario. State what the post will cover. Include primary keyword.",
      "3. **Body sections** (3-5 sections) — each with a descriptive subheading (H2). Use H3 for subsections. One core idea per section with supporting evidence, examples, or data.",
      "4. **Conclusion** (75-100 words) — summarize key takeaways, reinforce the main message, include a call to action.",
      "5. **Meta description** — under 160 characters, includes primary keyword, compels the click.",
      "",
      "### Social Media Post Structure",
      "- **Hook** — first line grabs attention (question, bold statement, number)",
      "- **Body** — 2-4 concise points or a short narrative",
      "- **CTA** — what should the reader do next (comment, click, share, tag)",
      "- **Hashtags** — 3-5 relevant hashtags (platform-dependent)",
      "",
      "### Email Newsletter Structure",
      "- **Subject line** — under 50 characters, creates curiosity or states clear value",
      "- **Preview text** — complements the subject line, does not repeat it",
      "- **Header/hero** — visual anchor and one-line value statement",
      "- **Body sections** — 2-3 content blocks, each scannable with a bold intro sentence",
      "- **Primary CTA** — one clear action per email",
      "- **Footer** — unsubscribe link, company info, social links",
      "",
      "### Landing Page Structure",
      "- **Headline** — primary benefit in under 10 words",
      "- **Subheadline** — elaborates on the headline with supporting context",
      "- **Hero section** — headline, subheadline, primary CTA, supporting image or video",
      "- **Value propositions** — 3-4 benefit-driven sections with icons or images",
      "- **Social proof** — testimonials, logos, stats, case study snippets",
      "- **Objection handling** — FAQ or trust signals",
      "- **Final CTA** — repeat the primary call to action",
      "",
      "### Press Release Structure",
      "- **Headline** — factual, newsworthy, under 80 characters",
      "- **Subheadline** — optional, adds context",
      "- **Dateline** — city, state, date",
      "- **Lead paragraph** — who, what, when, where, why in 2-3 sentences",
      "- **Body paragraphs** — supporting details, quotes, context",
      "- **Boilerplate** — company description (standardized)",
      "- **Media contact** — name, email, phone",
      "",
      "### Case Study Structure",
      '- **Title** — "[Customer] achieves [result] with [product]"',
      "- **Snapshot** — customer name, industry, company size, product used, key result (sidebar or callout box)",
      "- **Challenge** — what problem the customer faced",
      "- **Solution** — what was implemented and how",
      "- **Results** — quantified outcomes with specific metrics",
      "- **Quote** — customer testimonial",
      "- **CTA** — learn more, get a demo, read more case studies",
      "",
      "## Writing Best Practices by Channel",
      "",
      "### Blog",
      "- Write at an 8th-grade reading level for broad audiences; adjust up for technical audiences",
      "- Use short paragraphs (2-4 sentences)",
      "- Include subheadings every 200-300 words",
      "- Use bullet points and numbered lists to break up text",
      "- Include at least one data point, example, or quote per section",
      "- Write in active voice",
      "- Front-load key information in each section",
      "",
      "### Social Media",
      '- **LinkedIn**: professional but human, paragraph breaks for readability, personal stories and lessons perform well, 1,300 characters is the sweet spot before "see more"',
      "- **Twitter/X**: concise and punchy, strong opening words, threads for longer narratives, engage with replies",
      "- **Instagram**: visual-first captions, storytelling hooks, line breaks for readability, hashtags in first comment or at end",
      "- **Facebook**: conversational tone, questions drive comments, shorter posts (under 80 characters) get more engagement for links",
      "",
      "### Email",
      "- Write subject lines that create urgency, curiosity, or state clear value",
      "- Personalize where possible (name, company, behavior)",
      "- One primary CTA per email — make it visually distinct",
      "- Keep body copy scannable: bold key phrases, short paragraphs, bullet points",
      "- Test everything: subject lines, send times, CTA copy, layout",
      "- Mobile-first: most email is read on mobile",
      "",
      "### Web (Landing Pages, Product Pages)",
      "- Lead with benefits, not features",
      '- Use "you" language — speak to the reader directly',
      "- Minimize jargon unless the audience expects it",
      '- Every section should answer "so what?" from the reader\'s perspective',
      "- Reduce friction: fewer form fields, clear next steps, trust signals near CTAs",
      "",
      "## SEO Fundamentals for Content",
      "",
      "### Keyword Strategy",
      "- Identify one primary keyword and 2-3 secondary keywords per piece",
      "- Use the primary keyword in: headline, first paragraph, one subheading, meta description, URL slug",
      "- Use secondary keywords naturally in body copy and subheadings",
      "- Do not keyword-stuff — write for humans first",
      "",
      "### On-Page SEO Checklist",
      "- Title tag: under 60 characters, includes primary keyword",
      "- Meta description: under 160 characters, includes primary keyword, compels click",
      "- URL slug: short, descriptive, includes primary keyword",
      "- H1: one per page, matches or closely reflects the title tag",
      "- H2/H3: descriptive, include secondary keywords where natural",
      "- Image alt text: descriptive, includes keyword where relevant",
      "- Internal links: 2-3 links to related content on your site",
      "- External links: 1-2 links to authoritative sources",
      "",
      "### Content-SEO Integration",
      "- Aim for comprehensive coverage of the topic (search engines reward depth)",
      '- Answer related questions (check "People Also Ask" for ideas)',
      "- Update and refresh high-performing content regularly",
      "- Structure content for featured snippets: definition paragraphs, numbered lists, tables",
      "",
      "## Headline and Hook Formulas",
      "",
      "### Headline Formulas",
      '- **How to [achieve result] [without common obstacle]** — "How to Double Your Email Open Rates Without Sending More Emails"',
      '- **[Number] [adjective] ways to [achieve result]** — "7 Proven Ways to Reduce Customer Churn"',
      '- **Why [common belief] is wrong (and what to do instead)** — "Why More Content Is Not the Answer (And What to Do Instead)"',
      '- **The [adjective] guide to [topic]** — "The Complete Guide to B2B Content Marketing"',
      '- **[Do this], not [that]** — "Build a Community, Not Just an Audience"',
      '- **What [impressive result] taught us about [topic]** — "What 10,000 A/B Tests Taught Us About Email Subject Lines"',
      '- **[topic]: what [audience] needs to know in [year]** — "SEO: What Marketers Need to Know in 2025"',
      "",
      "### Hook Formulas (Opening Lines)",
      '- **Surprising statistic**: "73% of marketers say their biggest challenge is not budget — it is focus."',
      '- **Contrarian statement**: "The best marketing campaigns start with saying no to most channels."',
      '- **Question**: "When was the last time a marketing email actually changed what you bought?"',
      '- **Scenario**: "Imagine launching a campaign and knowing, before it goes live, which messages will land."',
      '- **Bold claim**: "Most landing pages lose half their visitors in the first three seconds."',
      '- **Story opening**: "Last quarter, our team was spending 20 hours a week on reporting. Here is what we did about it."',
      "",
      "## Call-to-Action Best Practices",
      "",
      "### CTA Principles",
      '- Use action verbs: "Get", "Start", "Download", "Join", "Try", "See"',
      '- Be specific about what happens next: "Start your free trial" is better than "Submit"',
      '- Create urgency when genuine: "Join 500 teams already using this" or "Limited spots available"',
      '- Reduce risk: "No credit card required", "Cancel anytime", "Free for 14 days"',
      "- One primary CTA per page or email — too many choices reduce conversions",
      "",
      "### CTA Examples by Context",
      '- **Blog post**: "Read our complete guide to [topic]" / "Subscribe for weekly insights"',
      '- **Landing page**: "Start free trial" / "Get a demo" / "See pricing"',
      '- **Email**: "Read the full story" / "Claim your spot" / "Reply and tell us"',
      '- **Social media**: "Drop a comment if you agree" / "Save this for later" / "Link in bio"',
      '- **Case study**: "See how [product] can work for your team" / "Talk to our team"',
      "",
      "### CTA Placement",
      "- Above the fold on landing pages (do not make users scroll to act)",
      "- After establishing value in emails (not in the first sentence)",
      "- At the end of blog posts (after you have earned the reader's trust)",
      "- In-line within content when contextually relevant (e.g., a related guide mention)",
      "- Repeat the primary CTA at the bottom of long-form pages",
      "",
    ].join("\n"),
  },
  "marketing-draft-content": {
    "SKILL.md": [
      "---",
      "name: marketing-draft-content",
      "description: Draft blog posts, social media, email newsletters, landing pages, press releases, and case studies",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <content type and topic>",
      "---",
      "",
      "# Draft Content",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate marketing content drafts tailored to a specific content type, audience, and brand voice.",
      "",
      "## Trigger",
      "",
      "User runs `/draft-content` or asks to draft, write, or create marketing content.",
      "",
      "## Inputs",
      "",
      "Gather the following from the user. If not provided, ask before proceeding:",
      "",
      "1. **Content type** — one of:",
      "   - Blog post",
      "   - Social media post (specify platform: LinkedIn, Twitter/X, Instagram, Facebook)",
      "   - Email newsletter",
      "   - Landing page copy",
      "   - Press release",
      "   - Case study",
      "",
      "2. **Topic** — the subject or theme of the content",
      "",
      "3. **Target audience** — who this content is for (role, industry, seniority, pain points)",
      "",
      "4. **Key messages** — 2-4 main points or takeaways to communicate",
      "",
      "5. **Tone** — e.g., authoritative, conversational, inspirational, technical, witty (optional if brand voice is configured)",
      "",
      '6. **Length** — target word count or format constraint (e.g., "1000 words", "280 characters", "3 paragraphs")',
      "",
      "## Brand Voice",
      "",
      "- If the user has a brand voice configured in their local settings file, apply it automatically. Inform the user that brand voice settings are being applied.",
      "- If no brand voice is configured, ask: \"Do you have brand voice guidelines you'd like me to follow? If not, I'll use a neutral professional tone.\"",
      "- Apply the specified or default tone consistently throughout the draft.",
      "",
      "## Content Generation by Type",
      "",
      "### Blog Post",
      "- Engaging headline (provide 2-3 options)",
      "- Introduction with a hook (question, statistic, bold statement, or story)",
      "- 3-5 organized sections with descriptive subheadings",
      "- Supporting points, examples, or data references in each section",
      "- Conclusion with a clear call to action",
      "- SEO considerations: suggest a primary keyword, include it in the headline and first paragraph, use related keywords in subheadings",
      "",
      "### Social Media Post",
      "- Platform-appropriate format and length",
      "- Hook in the first line",
      "- Hashtag suggestions (3-5 relevant hashtags)",
      "- Call to action or engagement prompt",
      "- Emoji usage appropriate to brand and platform",
      "- If LinkedIn: professional framing, paragraph breaks for readability",
      "- If Twitter/X: concise, punchy, within character limit",
      "- If Instagram: visual-first language, story-driven, hashtag block",
      "",
      "### Email Newsletter",
      "- Subject line (provide 2-3 options with open-rate considerations)",
      "- Preview text",
      "- Greeting",
      "- Body sections with clear hierarchy",
      "- Call to action button text",
      "- Sign-off",
      "- Unsubscribe note reminder",
      "",
      "### Landing Page Copy",
      "- Headline and subheadline",
      "- Hero section copy",
      "- Value propositions (3-4 benefit-driven bullets or sections)",
      "- Social proof placeholder (suggest testimonial or stat placement)",
      "- Primary and secondary CTAs",
      "- FAQ section suggestions",
      "- SEO: meta title and meta description suggestions",
      "",
      "### Press Release",
      "- Headline following press release conventions",
      "- Dateline and location",
      "- Lead paragraph (who, what, when, where, why)",
      "- Supporting quotes (provide placeholder guidance)",
      "- Company boilerplate placeholder",
      "- Media contact placeholder",
      "- Standard press release formatting",
      "",
      "### Case Study",
      "- Title emphasizing the result",
      "- Customer overview (industry, size, challenge)",
      "- Challenge section",
      "- Solution section (what was implemented)",
      "- Results section with metrics (prompt user for data)",
      "- Customer quote placeholder",
      "- Call to action",
      "",
      "## SEO Considerations (for web content)",
      "",
      "For blog posts, landing pages, and other web-facing content:",
      "- Suggest a primary keyword based on the topic",
      "- Recommend keyword placement: headline, first paragraph, subheadings, meta description",
      "- Suggest internal and external linking opportunities",
      "- Recommend a meta description (under 160 characters)",
      "- Note image alt text opportunities",
      "",
      "## Output",
      "",
      "Present the draft with clear formatting. After the draft, include:",
      "- A brief note on what brand voice and tone were applied",
      "- Any SEO recommendations (for web content)",
      '- Suggestions for next steps (e.g., "Review with your team", "Add customer quotes", "Pair with a visual")',
      "",
      'Ask: "Would you like me to revise any section, adjust the tone, or create a variation for a different channel?"',
      "",
    ].join("\n"),
  },
  "marketing-email-sequence": {
    "SKILL.md": [
      "---",
      "name: marketing-email-sequence",
      "description: Design and draft multi-email sequences for nurture flows, onboarding, drip campaigns, and more",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      '  argument-hint: "[sequence type]"',
      "---",
      "",
      "# Email Sequence",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Design and draft complete email sequences with full copy, timing, branching logic, and performance benchmarks for any lifecycle or campaign use case.",
      "",
      "## Trigger",
      "",
      "User runs `/email-sequence` or asks to create, design, build, or draft an email sequence, drip campaign, nurture flow, or onboarding series.",
      "",
      "## Inputs",
      "",
      "Gather the following from the user. If not provided, ask before proceeding:",
      "",
      "1. **Sequence type** — one of:",
      "   - Onboarding",
      "   - Lead nurture",
      "   - Re-engagement",
      "   - Product launch",
      "   - Event follow-up",
      "   - Upgrade/upsell",
      "   - Win-back",
      "   - Educational drip",
      "",
      "2. **Goal** — what the sequence should achieve (e.g., activate new users, convert leads to customers, reduce churn, drive event attendance, upsell to a higher tier)",
      "",
      "3. **Audience** — who receives this sequence, what stage they are at, and any relevant segmentation details (role, industry, behavior triggers, lifecycle stage)",
      "",
      "4. **Number of emails** (optional) — if not specified, recommend a count based on the sequence type using the templates in the Sequence Type Templates section below",
      "",
      '5. **Timing/cadence preferences** (optional) — desired spacing between emails (e.g., "every 3 days", "weekly", "aggressive first week then taper off")',
      "",
      '6. **Brand voice** — if configured in local settings, apply automatically and inform the user. If not configured, ask: "Do you have brand voice guidelines I should follow? If not, I\'ll use a clear, conversational professional tone."',
      "",
      "7. **Additional context** (optional):",
      "   - Specific offers, discounts, or incentives to include",
      "   - CTAs or landing pages to link to",
      "   - Content assets available (blog posts, case studies, videos, guides)",
      "   - Product features to highlight",
      "   - Competitor differentiators to reference",
      "",
      "## Process",
      "",
      "### 1. Sequence Strategy",
      "",
      "Before drafting any emails, define the overall sequence architecture:",
      "",
      "- **Narrative arc** — what story does this sequence tell across all emails? What is the emotional and logical progression from first email to last?",
      "- **Journey mapping** — map each email to a stage of the buyer or user journey (awareness, consideration, decision, activation, expansion)",
      "- **Escalation logic** — how does the intensity, urgency, or value of each email build on the previous one?",
      "- **Success definition** — what specific action signals that the sequence has done its job and the recipient should exit?",
      "",
      "### 2. Individual Email Design",
      "",
      "For each email in the sequence, produce:",
      "",
      "#### Subject Line",
      "- Provide 2-3 options per email",
      "- Vary approaches: curiosity, benefit-driven, urgency, personalization, question-based",
      "- Keep under 50 characters where possible; note preview behavior on mobile",
      "",
      "#### Preview Text",
      "- 40-90 characters that complement (not repeat) the subject line",
      "- Should add context or intrigue that increases open likelihood",
      "",
      "#### Email Purpose",
      "- One sentence explaining why this email exists and what it moves the recipient toward",
      "",
      "#### Body Copy",
      "- Full draft ready to use",
      "- Clear hierarchy: hook, body, CTA",
      "- Short paragraphs (2-3 sentences max)",
      "- Scannable formatting with bold key phrases where appropriate",
      "- Personalization tokens where relevant (e.g., first name, company name, product used)",
      "",
      "#### Primary CTA",
      "- Button text and destination",
      "- One primary CTA per email (secondary CTA only if appropriate for the sequence stage)",
      "",
      "#### Timing",
      "- Days after the trigger event or after the previous email",
      '- Note if timing should adjust based on engagement (e.g., "send sooner if they opened but did not click")',
      "",
      "#### Segment/Condition Notes",
      "- Who receives this email vs. who skips it",
      '- Any behavioral or attribute-based conditions (e.g., "only send to users who have not completed setup")',
      "",
      "### 3. Sequence Logic",
      "",
      "Define the flow control for the sequence:",
      "",
      "- **Branching conditions** — alternate paths based on engagement. For example:",
      '  - "If opened email 2 but did not click CTA, send email 2b (softer re-ask) instead of email 3"',
      '  - "If clicked CTA in email 1, skip email 2 and go directly to email 3"',
      '- **Exit conditions** — when a recipient converts (completes the desired action), remove them from the sequence. Define what "conversion" means for this sequence.',
      '- **Re-entry rules** — can someone re-enter the sequence? Under what conditions? (e.g., "if a user churns again 90 days later, re-enter the win-back sequence")',
      "- **Suppression rules** — do not send if the recipient is already in another active sequence, has unsubscribed from marketing, or has contacted support in the last 48 hours",
      "",
      "### 4. Performance Benchmarks",
      "",
      "Provide expected benchmarks based on the sequence type so the user can set targets:",
      "",
      "| Metric | Onboarding | Lead Nurture | Re-engagement | Win-back |",
      "|--------|-----------|--------------|---------------|----------|",
      "| Open rate | 50-70% | 20-30% | 15-25% | 15-20% |",
      "| Click-through rate | 10-20% | 3-7% | 2-5% | 2-4% |",
      "| Conversion rate | 15-30% | 2-5% | 3-8% | 1-3% |",
      "| Unsubscribe rate | <0.5% | <0.5% | 1-2% | 1-3% |",
      "",
      "Adjust benchmarks based on industry and audience if the user has provided that context.",
      "",
      "## Sequence Type Templates",
      "",
      "Use these as starting frameworks. Adapt length and content based on the user's goal and audience.",
      "",
      "**Onboarding (5-7 emails over 14-21 days):**",
      "Welcome and set expectations -- Quick win to demonstrate value -- Core feature deep dive -- Advanced feature or integration -- Social proof and community -- Check-in and feedback request -- Upgrade prompt or next steps",
      "",
      "**Lead Nurture (4-6 emails over 3-4 weeks):**",
      "Value-first educational content -- Pain point identification -- Solution positioning with proof -- Social proof and results -- Soft CTA (trial, demo, resource) -- Direct CTA (buy, book, sign up)",
      "",
      "**Re-engagement (3-4 emails over 10-14 days):**",
      '"We miss you" with a compelling reason to return -- Value reminder highlighting what they are missing -- Incentive or exclusive offer -- Last chance with clear deadline',
      "",
      "**Win-back (3-5 emails over 30 days):**",
      "Friendly check-in asking what went wrong -- What is new since they left -- Special offer or incentive to return -- Feedback request (even if they do not come back) -- Final goodbye with door open",
      "",
      "**Product Launch (4-6 emails over 2-3 weeks):**",
      "Teaser or pre-announcement -- Launch announcement with full details -- Feature spotlight or use case -- Social proof and early results -- Limited-time offer or bonus -- Last chance or reminder",
      "",
      "**Event Follow-up (3-4 emails over 7-10 days):**",
      "Thank you with key takeaways or recordings -- Resource roundup from the event -- Related offer or next step -- Feedback survey",
      "",
      "**Upgrade/Upsell (3-5 emails over 2-3 weeks):**",
      "Usage milestone or success celebration -- Feature gap or limitation they are hitting -- Upgrade benefits with proof -- Limited-time incentive -- Direct comparison of plans",
      "",
      "**Educational Drip (5-8 emails over 4-6 weeks):**",
      "Introduction and what they will learn -- Lesson 1: foundational concept -- Lesson 2: intermediate concept -- Lesson 3: advanced concept -- Practical application or exercise -- Resource roundup -- Graduation and next steps",
      "",
      "## Tool Integration",
      "",
      "### If ~~email marketing is connected (e.g., Klaviyo, Mailchimp, Customer.io)",
      "- Reference how to set up the sequence as a flow or automation in the platform",
      "- Note any platform-specific features to use (e.g., smart send time, conditional splits, A/B testing)",
      "- Map the branching logic to the platform's visual flow builder concepts",
      "",
      "### If ~~marketing automation or ~~CRM is connected (e.g., HubSpot, Marketo)",
      "- Reference lead scoring data to inform segmentation and exit conditions",
      "- Use lifecycle stage data to tailor messaging per segment",
      "- Note how to set enrollment triggers based on CRM properties or list membership",
      "",
      "### If no tools are connected",
      "- Deliver all email content in copy-paste-ready format",
      "- Include a setup checklist the user can follow in any email platform:",
      "  1. Create the automation or flow",
      "  2. Set the enrollment trigger",
      "  3. Add each email with the specified delays",
      "  4. Configure branching and exit conditions",
      "  5. Set up tracking for the recommended metrics",
      "",
      "## Output",
      "",
      "Present the complete sequence with the following sections:",
      "",
      "### Sequence Overview Table",
      "",
      "| # | Subject Line | Purpose | Timing | Primary CTA | Condition |",
      "|---|-------------|---------|--------|-------------|-----------|",
      "",
      "### Full Email Drafts",
      "Each email with subject line options, preview text, purpose, body copy, CTA, timing, and segment notes.",
      "",
      "### Sequence Flow Diagram",
      "A text-based diagram showing the email flow, branching paths, and exit points. Use a clear format such as:",
      "",
      "```",
      "[Trigger] --> Email 1 (Day 0)",
      "                |",
      "          Opened? --Yes--> Email 2 (Day 3)",
      "                |              |",
      "                No        Clicked CTA? --Yes--> [EXIT: Converted]",
      "                |              |",
      "                v              No",
      "          Email 1b (Day 2)     |",
      "                |              v",
      "                +--------> Email 3 (Day 7)",
      "                               |",
      "                               v",
      "                          Email 4 (Day 10)",
      "                               |",
      "                          [EXIT: Sequence complete]",
      "```",
      "",
      "### Branching Logic Notes",
      "Summary of all conditions, exits, and suppressions in a reference list.",
      "",
      "### A/B Test Suggestions",
      "- 2-3 recommended A/B tests (subject lines, CTA text, send time, email length)",
      "- What to test, how to split, and how to measure the winner",
      "",
      "### Metrics to Track",
      "- Primary conversion metric for the sequence",
      "- Per-email metrics: open rate, CTR, unsubscribe rate",
      "- Sequence-level metrics: overall conversion rate, time to conversion, drop-off points",
      '- Recommended review cadence (e.g., "Review performance weekly for the first month, then monthly")',
      "",
      "## After the Sequence",
      "",
      'Ask: "Would you like me to:',
      "- Revise the copy or tone for any specific email?",
      "- Add a branching path for a specific scenario?",
      "- Create a variation of this sequence for a different audience segment?",
      "- Draft the A/B test variants for the subject lines?",
      '- Build a companion sequence (e.g., a post-purchase follow-up after this lead nurture converts)?"',
      "",
    ].join("\n"),
  },
  "marketing-performance-analytics": {
    "SKILL.md": [
      "---",
      "name: marketing-performance-analytics",
      'description: "Analyze marketing performance with key metrics, trend analysis, and optimization recommendations. Use when building performance reports, reviewing campaign results, analyzing channel metrics (email, social, paid, SEO), or identifying what\'s working and what needs improvement."',
      "metadata:",
      "  plugin: marketing",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Performance Analytics Skill",
      "",
      "Frameworks for measuring, reporting, and optimizing marketing performance across channels and campaigns.",
      "",
      "## Key Marketing Metrics by Channel",
      "",
      "### Email Marketing",
      "",
      "| Metric | Definition | Benchmark Range | What It Tells You |",
      "|--------|-----------|----------------|-------------------|",
      "| Delivery rate | Emails delivered / emails sent | 95-99% | List health and sender reputation |",
      "| Open rate | Unique opens / emails delivered | 15-30% | Subject line and sender effectiveness |",
      "| Click-through rate (CTR) | Unique clicks / emails delivered | 2-5% | Content relevance and CTA effectiveness |",
      "| Click-to-open rate (CTOR) | Unique clicks / unique opens | 10-20% | Email content quality (for those who opened) |",
      "| Unsubscribe rate | Unsubscribes / emails delivered | <0.5% | Content-audience fit and frequency tolerance |",
      "| Bounce rate | Bounces / emails sent | <2% | List quality and data hygiene |",
      "| Conversion rate | Conversions / emails delivered | 1-5% | End-to-end email effectiveness |",
      "| Revenue per email | Total revenue / emails sent | Varies | Direct revenue attribution |",
      "| List growth rate | (New subscribers - unsubscribes) / total list | 2-5% monthly | Audience building health |",
      "",
      "### Social Media",
      "",
      "| Metric | Definition | What It Tells You |",
      "|--------|-----------|-------------------|",
      "| Impressions | Number of times content was displayed | Content distribution and reach |",
      "| Reach | Number of unique users who saw content | Audience breadth |",
      "| Engagement rate | (Likes + comments + shares) / reach | Content resonance |",
      "| Click-through rate | Link clicks / impressions | Traffic driving effectiveness |",
      "| Follower growth rate | Net new followers / total followers per period | Audience building |",
      "| Share/Repost rate | Shares / reach | Content virality and advocacy |",
      "| Video view rate | Views / impressions | Video content hook effectiveness |",
      "| Video completion rate | Completed views / total views | Video content quality and length fit |",
      "| Social share of voice | Your mentions / total category mentions | Brand visibility vs. competitors |",
      "",
      "### Paid Advertising (Search and Social)",
      "",
      "| Metric | Definition | What It Tells You |",
      "|--------|-----------|-------------------|",
      "| Impressions | Times ad was shown | Budget utilization and targeting breadth |",
      "| Click-through rate (CTR) | Clicks / impressions | Ad creative and targeting relevance |",
      "| Cost per click (CPC) | Total spend / clicks | Cost efficiency of traffic generation |",
      "| Cost per mille (CPM) | Cost per 1,000 impressions | Awareness cost efficiency |",
      "| Conversion rate | Conversions / clicks | Landing page and offer effectiveness |",
      "| Cost per acquisition (CPA) | Total spend / conversions | Full-funnel cost efficiency |",
      "| Return on ad spend (ROAS) | Revenue / ad spend | Revenue generation efficiency |",
      "| Quality Score (search) | Google's relevance rating (1-10) | Ad-keyword-landing page alignment |",
      "| Frequency | Average times a user sees the ad | Ad fatigue risk |",
      "| View-through conversions | Conversions from users who saw but did not click | Display/awareness campaign influence |",
      "",
      "### SEO / Organic Search",
      "",
      "| Metric | Definition | What It Tells You |",
      "|--------|-----------|-------------------|",
      "| Organic sessions | Visits from organic search | SEO effectiveness and content reach |",
      "| Keyword rankings | Position for target keywords | Search visibility |",
      "| Organic CTR | Clicks / impressions in search results | Title and meta description effectiveness |",
      "| Pages indexed | Number of pages in search index | Crawlability and site health |",
      "| Domain authority | Third-party authority score | Overall site strength |",
      "| Backlinks | Number of external sites linking to you | Content authority and off-page SEO |",
      "| Page load speed | Time to interactive | User experience and ranking factor |",
      "| Organic conversion rate | Organic conversions / organic sessions | Content quality and intent alignment |",
      "| Top entry pages | Most-visited pages from organic search | Content driving the most organic traffic |",
      "",
      "### Content Marketing",
      "",
      "| Metric | Definition | What It Tells You |",
      "|--------|-----------|-------------------|",
      "| Pageviews | Total views of content pages | Content reach and distribution |",
      "| Unique visitors | Distinct users viewing content | Audience size |",
      "| Average time on page | Time spent on content pages | Content engagement and depth |",
      "| Bounce rate | Single-page sessions / total sessions | Content-audience fit and UX |",
      "| Scroll depth | How far users scroll on a page | Content engagement through the piece |",
      "| Social shares | Times content was shared on social | Content resonance and virality |",
      "| Backlinks earned | External links to content | Content authority and SEO value |",
      "| Lead generation | Leads attributed to content | Content conversion effectiveness |",
      "| Content ROI | Revenue attributed / content production cost | Overall content investment return |",
      "",
      "### Overall Marketing / Pipeline",
      "",
      "| Metric | Definition | What It Tells You |",
      "|--------|-----------|-------------------|",
      "| Marketing qualified leads (MQLs) | Leads meeting marketing qualification criteria | Top-of-funnel effectiveness |",
      "| Sales qualified leads (SQLs) | MQLs accepted by sales | Lead quality |",
      "| MQL to SQL conversion rate | SQLs / MQLs | Marketing-sales alignment and lead quality |",
      "| Pipeline generated | Dollar value of opportunities created | Marketing impact on revenue |",
      "| Pipeline velocity | How fast deals move through pipeline | Campaign urgency and quality |",
      "| Customer acquisition cost (CAC) | Total marketing + sales cost / new customers | Efficiency of customer acquisition |",
      "| CAC payback period | Months to recover CAC from revenue | Unit economics health |",
      "| Marketing-sourced revenue | Revenue from marketing-originated deals | Direct marketing contribution |",
      "| Marketing-influenced revenue | Revenue from deals where marketing touched | Broader marketing impact |",
      "",
      "## Reporting Templates and Dashboards",
      "",
      "### Weekly Marketing Report",
      "Quick-scan format for team standups:",
      "- **Top 3 metrics** with week-over-week change",
      "- **What worked** this week (1-2 bullet points with data)",
      "- **What needs attention** (1-2 bullet points with data)",
      "- **This week's priorities** (3-5 action items)",
      "",
      "### Monthly Marketing Report",
      "Standard stakeholder report:",
      "1. Executive summary (3-5 sentences)",
      "2. Key metrics dashboard (table with MoM and target comparison)",
      "3. Channel-by-channel performance summary",
      "4. Campaign highlights and results",
      "5. What worked and what did not (with hypotheses)",
      "6. Recommendations and next month priorities",
      "7. Budget spend vs. plan",
      "",
      "### Quarterly Business Review (QBR)",
      "Strategic review for leadership:",
      "1. Quarter performance vs. goals",
      "2. Year-to-date trajectory",
      "3. Channel ROI analysis",
      "4. Campaign performance summary",
      "5. Competitive and market observations",
      "6. Strategic recommendations for next quarter",
      "7. Budget request and allocation plan",
      "8. Key experiments and learnings",
      "",
      "### Dashboard Design Principles",
      "- Lead with the metrics that map to business objectives (not vanity metrics)",
      "- Show trends over time, not just point-in-time snapshots",
      "- Include comparison context: prior period, target, benchmark",
      "- Use consistent color coding: green (on track), yellow (at risk), red (off track)",
      "- Group metrics by funnel stage or business question",
      "- Keep dashboards to one page/screen — detail goes in appendix",
      "- Update cadence should match decision cadence (real-time for paid, weekly for content)",
      "",
      "## Trend Analysis and Forecasting",
      "",
      "### Trend Identification",
      "When analyzing performance data, look for:",
      "",
      "1. **Directional trends**: is the metric consistently going up, down, or flat over 4+ periods?",
      "2. **Inflection points**: where did performance change direction and what happened then?",
      "3. **Seasonality**: are there predictable patterns by day of week, month, or quarter?",
      "4. **Anomalies**: one-time spikes or drops — what caused them and are they repeatable?",
      "5. **Leading indicators**: which metrics change first and predict future outcomes?",
      "",
      "### Trend Analysis Process",
      "1. Chart the metric over time (at least 8-12 data points for meaningful trends)",
      "2. Identify the overall direction (upward, downward, flat, cyclical)",
      "3. Calculate the rate of change (is it accelerating or decelerating?)",
      "4. Overlay key events (campaigns launched, product changes, market events)",
      "5. Compare to benchmarks or targets",
      "6. Identify correlations with other metrics",
      "7. Form hypotheses about causation (and plan tests to validate)",
      "",
      "### Simple Forecasting Approaches",
      "- **Linear projection**: extend the current trend line forward (useful for stable metrics)",
      "- **Moving average**: smooth out noise by averaging the last 3-6 periods",
      "- **Year-over-year comparison**: use last year's pattern as a baseline, adjusted for growth rate",
      "- **Funnel math**: forecast outputs from inputs (e.g., if we generate X leads at Y conversion rate, we will get Z customers)",
      "- **Scenario modeling**: create best case, expected case, and worst case projections",
      "",
      "### Forecasting Caveats",
      "- Short-term forecasts (1-3 months) are more reliable than long-term",
      "- Forecasts based on fewer than 12 data points should be flagged as low confidence",
      "- External factors (market shifts, competitive moves, economic changes) can invalidate trend-based forecasts",
      "- Always present forecasts as ranges, not exact numbers",
      "",
      "## Attribution Modeling Basics",
      "",
      "### What Is Attribution?",
      "Attribution determines which marketing touchpoints get credit for a conversion. This matters because buyers typically interact with multiple channels before converting.",
      "",
      "### Common Attribution Models",
      "",
      "| Model | How It Works | Best For | Limitation |",
      "|-------|-------------|----------|------------|",
      "| Last touch | 100% credit to last interaction before conversion | Understanding final conversion triggers | Ignores awareness and nurture |",
      "| First touch | 100% credit to first interaction | Understanding top-of-funnel effectiveness | Ignores nurture and conversion drivers |",
      "| Linear | Equal credit to all touchpoints | Fair representation of all channels | Does not reflect relative impact |",
      "| Time decay | More credit to touchpoints closer to conversion | Balanced view favoring recent interactions | May undervalue awareness |",
      "| Position-based (U-shaped) | 40% first, 40% last, 20% split among middle | Valuing both discovery and conversion | Somewhat arbitrary weighting |",
      "| Data-driven | Algorithmic credit based on conversion patterns | Most accurate representation | Requires significant data volume |",
      "",
      "### Attribution Practical Guidance",
      "- Start with last-touch attribution if you have no model in place — it is the simplest and most actionable",
      "- Compare first-touch and last-touch to understand which channels drive awareness vs. conversion",
      "- Use position-based (U-shaped) as a reasonable middle ground for most B2B companies",
      "- Data-driven attribution requires high conversion volume to be statistically meaningful",
      "- No model is perfect — use attribution directionally, not as absolute truth",
      "- Multi-touch attribution is better than single-touch, but any model is better than none",
      "",
      "### Attribution Pitfalls",
      "- Do not optimize one channel in isolation based on single-touch attribution",
      "- Awareness channels (display, social, PR) will always look bad in last-touch models",
      "- Conversion channels (search, retargeting) will always look bad in first-touch models",
      '- Self-reported attribution ("how did you hear about us?") provides useful qualitative color but is unreliable as quantitative data',
      "- Cross-device and cross-channel tracking gaps mean attribution data is always incomplete",
      "",
      "## Optimization Recommendations Framework",
      "",
      "### Optimization Process",
      "1. **Identify**: which metrics are underperforming vs. target or benchmark?",
      "2. **Diagnose**: where in the funnel is the problem? (impressions, clicks, conversions, retention)",
      "3. **Hypothesize**: what is causing the underperformance? (audience, message, creative, offer, timing, technical)",
      "4. **Prioritize**: which fixes will have the biggest impact with the least effort?",
      "5. **Test**: design an experiment to validate the hypothesis",
      "6. **Measure**: did the change improve the metric?",
      "7. **Scale or iterate**: roll out wins broadly; iterate on inconclusive or failed tests",
      "",
      "### Optimization Levers by Funnel Stage",
      "",
      "| Funnel Stage | Problem Signal | Optimization Levers |",
      "|-------------|---------------|---------------------|",
      "| Awareness | Low impressions, low reach | Budget, targeting, channel mix, creative format |",
      "| Interest | Low CTR, low engagement | Ad creative, headlines, content hooks, audience targeting |",
      "| Consideration | High bounce rate, low time on page | Landing page content, page speed, content relevance, UX |",
      "| Conversion | Low conversion rate | Offer, CTA, form length, trust signals, page layout |",
      "| Retention | High churn, low repeat engagement | Onboarding, email nurture, product experience, support |",
      "",
      "### Prioritization Framework",
      "",
      "Rank optimization ideas on two dimensions:",
      "",
      "**Impact** (how much will this move the metric?):",
      "- High: directly addresses the primary bottleneck",
      "- Medium: addresses a contributing factor",
      "- Low: incremental improvement",
      "",
      "**Effort** (how hard is this to implement?):",
      "- Low: copy change, targeting adjustment, simple A/B test",
      "- Medium: new creative, landing page redesign, workflow change",
      "- High: new tool, cross-team project, major content production",
      "",
      "Priority order:",
      "1. High impact, low effort (do immediately)",
      "2. High impact, high effort (plan and resource)",
      "3. Low impact, low effort (do if capacity allows)",
      "4. Low impact, high effort (deprioritize)",
      "",
      "### Testing Best Practices",
      "- Test one variable at a time for clean results",
      "- Define the success metric before launching the test",
      "- Calculate required sample size before starting (do not end tests early)",
      "- Run tests for a minimum of one full business cycle (typically one week for B2B)",
      "- Document all tests and results, regardless of outcome",
      "- Share learnings across the team — failed tests are valuable information",
      "- A test that confirms the status quo is not a failure — it builds confidence in your current approach",
      "",
      "### Continuous Optimization Cadence",
      "- **Daily**: monitor paid campaigns for budget pacing, anomalies, and disapproved ads",
      "- **Weekly**: review channel performance, pause underperformers, scale winners",
      "- **Bi-weekly**: refresh ad creative and test new variants",
      "- **Monthly**: full performance review, identify new optimization opportunities, update forecasts",
      "- **Quarterly**: strategic review of channel mix, budget allocation, and targeting strategy",
      "",
    ].join("\n"),
  },
  "marketing-performance-report": {
    "SKILL.md": [
      "---",
      "name: marketing-performance-report",
      "description: Build a marketing performance report with key metrics, trends, and optimization recommendations",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <time period or campaign>",
      "---",
      "",
      "# Performance Report",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate a marketing performance report with key metrics, trend analysis, insights, and optimization recommendations.",
      "",
      "## Trigger",
      "",
      "User runs `/performance-report` or asks for a marketing report, performance analysis, campaign results, or metrics summary.",
      "",
      "## Inputs",
      "",
      "1. **Report type** — determine which type of report the user needs:",
      "   - **Campaign report** — performance of a specific campaign",
      "   - **Channel report** — performance across a specific channel (email, social, paid, SEO, etc.)",
      "   - **Content performance** — how content pieces are performing",
      "   - **Overall marketing report** — cross-channel summary (weekly, monthly, quarterly)",
      "   - **Custom** — user-defined scope",
      "",
      "2. **Time period** — the reporting window (last week, last month, last quarter, custom date range)",
      "",
      "3. **Data source**:",
      "   - If ~~product analytics is connected: pull performance data automatically",
      '   - If not connected: ask the user to provide metrics. Prompt with: "Please paste or share your performance data. I can work with spreadsheets, CSV data, dashboard screenshots described in text, or just the key numbers."',
      "",
      "4. **Comparison period** (optional) — prior period or year-over-year for trend context",
      "",
      "5. **Stakeholder audience** (optional) — who will read this report (executive summary style vs. detailed analyst view)",
      "",
      "## Report Structure",
      "",
      "### 1. Executive Summary",
      "- 2-3 sentence overview of performance in the period",
      "- Headline metric with trend direction (up/down/flat vs. prior period)",
      "- One key win and one area of concern",
      "",
      "### 2. Key Metrics Dashboard",
      "",
      "Present core metrics in a summary table:",
      "",
      "| Metric | This Period | Prior Period | Change | Target | Status |",
      "|--------|------------|--------------|--------|--------|--------|",
      "",
      "Status indicators:",
      "- On track (meeting or exceeding target)",
      "- At risk (below target but within acceptable range)",
      "- Off track (significantly below target)",
      "",
      "#### Metrics by Report Type",
      "",
      "**Campaign Report:**",
      "- Impressions and reach",
      "- Click-through rate (CTR)",
      "- Conversion rate",
      "- Cost per acquisition (CPA)",
      "- Return on ad spend (ROAS) or ROI",
      "- Total conversions/signups/leads",
      "",
      "**Channel Report (Email):**",
      "- Emails sent, delivered, bounced",
      "- Open rate",
      "- Click-through rate",
      "- Unsubscribe rate",
      "- Conversion rate",
      "",
      "**Channel Report (Social):**",
      "- Impressions and reach",
      "- Engagement rate (likes, comments, shares)",
      "- Follower growth",
      "- Click-through rate",
      "- Top-performing posts",
      "",
      "**Channel Report (Paid):**",
      "- Spend",
      "- Impressions and clicks",
      "- CTR",
      "- CPC and CPM",
      "- Conversions and CPA",
      "- ROAS",
      "",
      "**Channel Report (SEO/Organic):**",
      "- Organic sessions",
      "- Keyword rankings (movement)",
      "- Pages indexed",
      "- Backlinks acquired",
      "- Top-performing pages",
      "",
      "**Content Performance:**",
      "- Pageviews and unique visitors",
      "- Time on page",
      "- Bounce rate",
      "- Social shares",
      "- Conversions attributed to content",
      "- Top and bottom performers",
      "",
      "**Overall Marketing Report:**",
      "- Total leads generated",
      "- Marketing qualified leads (MQLs)",
      "- Pipeline contribution",
      "- Customer acquisition cost (CAC)",
      "- Channel-by-channel summary",
      "",
      "### 3. Trend Analysis",
      "- Performance trend over the period (week-over-week or month-over-month)",
      "- Notable inflection points and what caused them",
      "- Seasonal or cyclical patterns observed",
      "- Comparison to benchmarks or targets",
      "",
      "### 4. What Worked",
      "- Top 3-5 wins with specific data",
      "- Why these performed well (hypothesis)",
      "- How to replicate or scale",
      "",
      "### 5. What Needs Improvement",
      "- Bottom 3-5 performers with specific data",
      "- Hypotheses for underperformance",
      "- Recommended fixes",
      "",
      "### 6. Insights and Observations",
      "- Patterns in the data that are not obvious from the metrics alone",
      "- Audience behavior insights",
      "- Content or creative themes that resonated",
      "- External factors that may have influenced performance (seasonality, news, competitive moves)",
      "",
      "### 7. Recommendations",
      "For each recommendation:",
      "- What to do",
      "- Why (linked to a specific insight from the data)",
      "- Expected impact (high, medium, low)",
      "- Effort to implement (high, medium, low)",
      "- Priority (immediate, next sprint, next quarter)",
      "",
      "Prioritize recommendations in a 2x2 matrix format:",
      "",
      "| | Low Effort | High Effort |",
      "|---|---|---|",
      "| **High Impact** | Do first | Plan for next sprint |",
      "| **Low Impact** | Do if time allows | Deprioritize |",
      "",
      "### 8. Next Period Focus",
      "- Top 3 priorities for the upcoming period",
      "- Tests or experiments to run",
      "- Targets for key metrics",
      "",
      "## Output Formatting",
      "",
      "- Use tables for data presentation",
      "- Bold key numbers and trends",
      "- Keep the executive summary concise (suitable for forwarding to leadership)",
      '- Include a "detailed appendix" section for granular data if the user provided a lot of metrics',
      "",
      "## After the Report",
      "",
      'Ask: "Would you like me to:',
      "- Create a slide-ready summary of these results?",
      "- Draft a stakeholder email with the key takeaways?",
      "- Dive deeper into any specific metric or channel?",
      '- Set up a reporting template you can reuse next period?"',
      "",
    ].join("\n"),
  },
  "marketing-seo-audit": {
    "SKILL.md": [
      "---",
      "name: marketing-seo-audit",
      "description: Run a comprehensive SEO audit — keyword research, on-page analysis, content gaps, technical checks, and competitor comparison",
      "metadata:",
      "  plugin: marketing",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <url or topic> [audit type]",
      "---",
      "",
      "# /seo-audit",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Audit a website's SEO health, research keyword opportunities, identify content gaps, and benchmark against competitors. Produces a prioritized action plan a marketer can execute immediately.",
      "",
      "## Trigger",
      "",
      "User runs `/seo-audit` or asks for an SEO audit, keyword research, content gap analysis, technical SEO check, or competitor SEO comparison.",
      "",
      "## Inputs",
      "",
      "Gather the following from the user. If not provided, ask before proceeding:",
      "",
      "1. **URL or domain** — the site to audit, or a topic/keyword if running in keyword research mode",
      "",
      "2. **Audit type** — one of:",
      "   - **Full site audit** — end-to-end SEO review covering all sections below",
      "   - **Keyword research** — identify keyword opportunities for a topic or domain",
      "   - **Content gap analysis** — find topics competitors rank for that you don't",
      "   - **Technical SEO check** — crawlability, speed, structured data, and infrastructure issues",
      "   - **Competitor SEO comparison** — head-to-head SEO benchmarking against specific competitors",
      "",
      "   If not specified, default to **full site audit**.",
      "",
      "3. **Target keywords or topics** (optional) — specific keywords the user is already targeting or wants to rank for",
      "",
      "4. **Competitors** (optional) — domains or companies to compare against. If not provided and the audit type requires competitor data, use web search to identify 2-3 likely competitors based on the user's domain and keyword space.",
      "",
      "## Process",
      "",
      "### 1. Keyword Research",
      "",
      "Research keywords related to the user's domain, topic, or target keywords.",
      "",
      "**If ~~SEO tools are connected:**",
      "- Pull keyword data, search volume, keyword difficulty scores, and ranking positions automatically",
      "- Identify keywords the site currently ranks for and where it's gaining or losing ground",
      "",
      "**If ~~product analytics are connected:**",
      "- Cross-reference keyword targets with actual organic traffic data to validate which keywords are driving visits and conversions",
      "",
      "**If tools are not connected:**",
      "- Use web search to research the keyword landscape",
      '- Note: "For more precise volume and difficulty data, connect an SEO tool like Ahrefs or Semrush via MCP. The audit will auto-populate with ranking data."',
      "",
      "For each keyword opportunity, assess:",
      "- **Primary keywords** — high-intent terms directly tied to the user's product or service",
      "- **Secondary keywords** — supporting terms and variations",
      "- **Search volume signals** — relative demand (high, medium, low) based on available data",
      "- **Keyword difficulty** — how competitive the term is (easy, moderate, hard)",
      "- **Long-tail opportunities** — specific, lower-competition phrases with clear intent",
      '- **Question-based keywords** — "how to", "what is", "why does" queries that mirror People Also Ask results',
      "- **Intent classification** — informational, navigational, commercial, or transactional",
      "",
      "### 2. On-Page SEO Audit",
      "",
      "For each key page (homepage, top landing pages, recent blog posts), evaluate:",
      "",
      "- **Title tags** — present, unique, within 50-60 characters, includes target keyword",
      "- **Meta descriptions** — present, compelling, within 150-160 characters, includes a call to action",
      "- **H1 tags** — exactly one per page, includes primary keyword",
      "- **H2/H3 structure** — logical hierarchy, uses secondary keywords where natural",
      "- **Keyword usage** — primary keyword appears in the first 100 words, used naturally throughout, not over-stuffed",
      "- **Internal linking** — pages link to related content, orphan pages identified, anchor text is descriptive",
      "- **Image alt text** — all images have descriptive alt attributes, keywords included where relevant",
      "- **URL structure** — clean, readable, includes keywords, no excessive parameters or depth",
      "",
      "### 3. Content Gap Analysis",
      "",
      "Identify what's missing from the user's content strategy:",
      "",
      "- **Competitor topic coverage** — topics and keywords competitors rank for that the user's site does not cover",
      "- **Content freshness** — pages that haven't been updated in 12+ months and may be losing rankings",
      "- **Thin content** — pages with insufficient depth to rank (under 300 words for informational queries, lacking substance)",
      "- **Missing content types** — formats competitors use that the user doesn't (guides, comparison pages, glossaries, tools, templates)",
      "- **Funnel gaps** — missing content at specific buyer journey stages (awareness, consideration, decision)",
      "- **Topic clusters** — opportunities to build pillar pages with supporting content",
      "",
      "### 4. Technical SEO Checklist",
      "",
      "Evaluate technical foundations that affect crawlability and rankings:",
      "",
      "- **Page speed** — identify slow-loading pages and likely causes (large images, render-blocking scripts, excessive redirects)",
      "- **Mobile-friendliness** — responsive design, tap targets, font sizes, viewport configuration",
      "- **Structured data** — opportunities for schema markup (FAQ, HowTo, Product, Article, Organization, Breadcrumb)",
      "- **Crawlability** — robots.txt configuration, XML sitemap presence and accuracy, canonical tags, noindex/nofollow usage",
      "- **Broken links** — internal and external 404s, redirect chains",
      "- **HTTPS** — secure connection, mixed content issues",
      "- **Core Web Vitals signals** — LCP, FID/INP, CLS indicators based on observable page behavior",
      "- **Indexation** — pages that should be indexed but may not be, duplicate content risks",
      "",
      "### 5. Competitor SEO Comparison",
      "",
      "For each competitor, compare:",
      "",
      "- **Keyword overlap** — keywords both sites rank for, and where each site ranks higher",
      "- **Keyword gaps** — terms the competitor ranks for that the user does not",
      "- **Domain authority signals** — relative site strength based on backlink profiles, referring domains, and content depth",
      "- **Content depth** — average content length, topic coverage breadth, publishing frequency",
      "- **Backlink profile observations** — types of sites linking to competitors, link-worthy content they've produced",
      "- **SERP feature ownership** — which competitor appears in featured snippets, People Also Ask, image packs, or knowledge panels",
      "- **Technical advantages** — site speed differences, mobile experience, structured data usage",
      "",
      "## Output",
      "",
      "### Executive Summary",
      "",
      "Open with a 3-5 sentence summary of overall SEO health. Highlight:",
      "- The site's biggest strength",
      "- The top 3 priorities that will have the most impact",
      "- An overall assessment: strong foundation, needs work, or critical issues",
      "",
      "### Keyword Opportunity Table",
      "",
      "| Keyword | Est. Difficulty | Opportunity Score | Current Ranking | Intent | Recommended Content Type |",
      "|---------|----------------|-------------------|-----------------|--------|--------------------------|",
      "",
      "Opportunity score: high, medium, or low — based on the combination of search demand, difficulty, and relevance to the user's business.",
      "",
      "Include 15-25 keyword opportunities, sorted by opportunity score.",
      "",
      "### On-Page Issues Table",
      "",
      "| Page | Issue | Severity | Recommended Fix |",
      "|------|-------|----------|-----------------|",
      "",
      "Severity levels:",
      "- **Critical** — directly hurting rankings or preventing indexation",
      "- **High** — significant impact on SEO performance",
      "- **Medium** — best practice violation, moderate impact",
      "- **Low** — minor optimization opportunity",
      "",
      "### Content Gap Recommendations",
      "",
      "For each content gap identified, provide:",
      "- **Topic or keyword** to target",
      "- **Why it matters** — search demand, competitor coverage, funnel stage",
      "- **Recommended format** — blog post, landing page, guide, comparison page, etc.",
      "- **Priority** — high, medium, or low",
      "- **Estimated effort** — quick win (1-2 hours), moderate (half day), substantial (multi-day)",
      "",
      "### Technical SEO Checklist",
      "",
      "| Check | Status | Details |",
      "|-------|--------|---------|",
      "",
      "Status: Pass, Fail, or Warning.",
      "",
      "### Competitor Comparison Summary",
      "",
      "| Dimension | Your Site | Competitor A | Competitor B | Winner |",
      "|-----------|-----------|--------------|--------------|--------|",
      "",
      "Include rows for: keyword count, content depth, publishing frequency, backlink signals, technical score, SERP feature presence.",
      "",
      "### Prioritized Action Plan",
      "",
      "Split recommendations into two categories:",
      "",
      "**Quick Wins (do this week):**",
      "- Actions that take under 2 hours and have immediate impact",
      "- Examples: fix title tags, add meta descriptions, fix broken links, add alt text",
      "",
      "**Strategic Investments (plan for this quarter):**",
      "- Actions that require more effort but drive long-term growth",
      "- Examples: build a topic cluster, create a pillar page, launch a link-building campaign, overhaul site structure",
      "",
      "For each action item, include:",
      "- What to do (specific and concrete)",
      "- Expected impact (high, medium, low)",
      "- Effort estimate",
      "- Dependencies (if any)",
      "",
      "## Follow-Up",
      "",
      "After presenting the audit, ask:",
      "",
      '"Would you like me to:',
      "- Draft content briefs for the top keyword opportunities?",
      "- Create optimized title tags and meta descriptions for your key pages?",
      "- Build a content calendar based on the gap analysis?",
      "- Dive deeper into any specific section of the audit?",
      '- Run this same analysis for a different competitor or domain?"',
      "",
    ].join("\n"),
  },
  pdf: {
    "SKILL.md": [
      "---",
      "name: pdf",
      "description: Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.",
      "license: Proprietary. LICENSE.txt has complete terms",
      "---",
      "",
      "# PDF Processing Guide",
      "",
      "## Overview",
      "",
      "This guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.",
      "",
      "## Quick Start",
      "",
      "```python",
      "from pypdf import PdfReader, PdfWriter",
      "",
      "# Read a PDF",
      'reader = PdfReader("document.pdf")',
      'print(f"Pages: {len(reader.pages)}")',
      "",
      "# Extract text",
      'text = ""',
      "for page in reader.pages:",
      "    text += page.extract_text()",
      "```",
      "",
      "## Python Libraries",
      "",
      "### pypdf - Basic Operations",
      "",
      "#### Merge PDFs",
      "```python",
      "from pypdf import PdfWriter, PdfReader",
      "",
      "writer = PdfWriter()",
      'for pdf_file in ["doc1.pdf", "doc2.pdf", "doc3.pdf"]:',
      "    reader = PdfReader(pdf_file)",
      "    for page in reader.pages:",
      "        writer.add_page(page)",
      "",
      'with open("merged.pdf", "wb") as output:',
      "    writer.write(output)",
      "```",
      "",
      "#### Split PDF",
      "```python",
      'reader = PdfReader("input.pdf")',
      "for i, page in enumerate(reader.pages):",
      "    writer = PdfWriter()",
      "    writer.add_page(page)",
      '    with open(f"page_{i+1}.pdf", "wb") as output:',
      "        writer.write(output)",
      "```",
      "",
      "#### Extract Metadata",
      "```python",
      'reader = PdfReader("document.pdf")',
      "meta = reader.metadata",
      'print(f"Title: {meta.title}")',
      'print(f"Author: {meta.author}")',
      'print(f"Subject: {meta.subject}")',
      'print(f"Creator: {meta.creator}")',
      "```",
      "",
      "#### Rotate Pages",
      "```python",
      'reader = PdfReader("input.pdf")',
      "writer = PdfWriter()",
      "",
      "page = reader.pages[0]",
      "page.rotate(90)  # Rotate 90 degrees clockwise",
      "writer.add_page(page)",
      "",
      'with open("rotated.pdf", "wb") as output:',
      "    writer.write(output)",
      "```",
      "",
      "### pdfplumber - Text and Table Extraction",
      "",
      "#### Extract Text with Layout",
      "```python",
      "import pdfplumber",
      "",
      'with pdfplumber.open("document.pdf") as pdf:',
      "    for page in pdf.pages:",
      "        text = page.extract_text()",
      "        print(text)",
      "```",
      "",
      "#### Extract Tables",
      "```python",
      'with pdfplumber.open("document.pdf") as pdf:',
      "    for i, page in enumerate(pdf.pages):",
      "        tables = page.extract_tables()",
      "        for j, table in enumerate(tables):",
      '            print(f"Table {j+1} on page {i+1}:")',
      "            for row in table:",
      "                print(row)",
      "```",
      "",
      "#### Advanced Table Extraction",
      "```python",
      "import pandas as pd",
      "",
      'with pdfplumber.open("document.pdf") as pdf:',
      "    all_tables = []",
      "    for page in pdf.pages:",
      "        tables = page.extract_tables()",
      "        for table in tables:",
      "            if table:  # Check if table is not empty",
      "                df = pd.DataFrame(table[1:], columns=table[0])",
      "                all_tables.append(df)",
      "",
      "# Combine all tables",
      "if all_tables:",
      "    combined_df = pd.concat(all_tables, ignore_index=True)",
      '    combined_df.to_excel("extracted_tables.xlsx", index=False)',
      "```",
      "",
      "### reportlab - Create PDFs",
      "",
      "#### Basic PDF Creation",
      "```python",
      "from reportlab.lib.pagesizes import letter",
      "from reportlab.pdfgen import canvas",
      "",
      'c = canvas.Canvas("hello.pdf", pagesize=letter)',
      "width, height = letter",
      "",
      "# Add text",
      'c.drawString(100, height - 100, "Hello World!")',
      'c.drawString(100, height - 120, "This is a PDF created with reportlab")',
      "",
      "# Add a line",
      "c.line(100, height - 140, 400, height - 140)",
      "",
      "# Save",
      "c.save()",
      "```",
      "",
      "#### Create PDF with Multiple Pages",
      "```python",
      "from reportlab.lib.pagesizes import letter",
      "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak",
      "from reportlab.lib.styles import getSampleStyleSheet",
      "",
      'doc = SimpleDocTemplate("report.pdf", pagesize=letter)',
      "styles = getSampleStyleSheet()",
      "story = []",
      "",
      "# Add content",
      "title = Paragraph(\"Report Title\", styles['Title'])",
      "story.append(title)",
      "story.append(Spacer(1, 12))",
      "",
      "body = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])",
      "story.append(body)",
      "story.append(PageBreak())",
      "",
      "# Page 2",
      "story.append(Paragraph(\"Page 2\", styles['Heading1']))",
      "story.append(Paragraph(\"Content for page 2\", styles['Normal']))",
      "",
      "# Build PDF",
      "doc.build(story)",
      "```",
      "",
      "## Command-Line Tools",
      "",
      "### pdftotext (poppler-utils)",
      "```bash",
      "# Extract text",
      "pdftotext input.pdf output.txt",
      "",
      "# Extract text preserving layout",
      "pdftotext -layout input.pdf output.txt",
      "",
      "# Extract specific pages",
      "pdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5",
      "```",
      "",
      "### qpdf",
      "```bash",
      "# Merge PDFs",
      "qpdf --empty --pages file1.pdf file2.pdf -- merged.pdf",
      "",
      "# Split pages",
      "qpdf input.pdf --pages . 1-5 -- pages1-5.pdf",
      "qpdf input.pdf --pages . 6-10 -- pages6-10.pdf",
      "",
      "# Rotate pages",
      "qpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees",
      "",
      "# Remove password",
      "qpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf",
      "```",
      "",
      "### pdftk (if available)",
      "```bash",
      "# Merge",
      "pdftk file1.pdf file2.pdf cat output merged.pdf",
      "",
      "# Split",
      "pdftk input.pdf burst",
      "",
      "# Rotate",
      "pdftk input.pdf rotate 1east output rotated.pdf",
      "```",
      "",
      "## Common Tasks",
      "",
      "### Extract Text from Scanned PDFs",
      "```python",
      "# Requires: pip install pytesseract pdf2image",
      "import pytesseract",
      "from pdf2image import convert_from_path",
      "",
      "# Convert PDF to images",
      "images = convert_from_path('scanned.pdf')",
      "",
      "# OCR each page",
      'text = ""',
      "for i, image in enumerate(images):",
      '    text += f"Page {i+1}:\\n"',
      "    text += pytesseract.image_to_string(image)",
      '    text += "\\n\\n"',
      "",
      "print(text)",
      "```",
      "",
      "### Add Watermark",
      "```python",
      "from pypdf import PdfReader, PdfWriter",
      "",
      "# Create watermark (or load existing)",
      'watermark = PdfReader("watermark.pdf").pages[0]',
      "",
      "# Apply to all pages",
      'reader = PdfReader("document.pdf")',
      "writer = PdfWriter()",
      "",
      "for page in reader.pages:",
      "    page.merge_page(watermark)",
      "    writer.add_page(page)",
      "",
      'with open("watermarked.pdf", "wb") as output:',
      "    writer.write(output)",
      "```",
      "",
      "### Extract Images",
      "```bash",
      "# Using pdfimages (poppler-utils)",
      "pdfimages -j input.pdf output_prefix",
      "",
      "# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.",
      "```",
      "",
      "### Password Protection",
      "```python",
      "from pypdf import PdfReader, PdfWriter",
      "",
      'reader = PdfReader("input.pdf")',
      "writer = PdfWriter()",
      "",
      "for page in reader.pages:",
      "    writer.add_page(page)",
      "",
      "# Add password",
      'writer.encrypt("userpassword", "ownerpassword")',
      "",
      'with open("encrypted.pdf", "wb") as output:',
      "    writer.write(output)",
      "```",
      "",
      "## Quick Reference",
      "",
      "| Task | Best Tool | Command/Code |",
      "|------|-----------|--------------|",
      "| Merge PDFs | pypdf | `writer.add_page(page)` |",
      "| Split PDFs | pypdf | One page per file |",
      "| Extract text | pdfplumber | `page.extract_text()` |",
      "| Extract tables | pdfplumber | `page.extract_tables()` |",
      "| Create PDFs | reportlab | Canvas or Platypus |",
      "| Command line merge | qpdf | `qpdf --empty --pages ...` |",
      "| OCR scanned PDFs | pytesseract | Convert to image first |",
      "| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |",
      "",
      "## Next Steps",
      "",
      "- For advanced pypdfium2 usage, see reference.md",
      "- For JavaScript libraries (pdf-lib), see reference.md",
      "- If you need to fill out a PDF form, follow the instructions in forms.md",
      "- For troubleshooting guides, see reference.md",
      "",
    ].join("\n"),
  },
  pptx: {
    "SKILL.md": [
      "---",
      "name: pptx",
      'description: "Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks"',
      "license: Proprietary. LICENSE.txt has complete terms",
      "---",
      "",
      "# PPTX creation, editing, and analysis",
      "",
      "## Overview",
      "",
      "A user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.",
      "",
      "## Reading and analyzing content",
      "",
      "### Text extraction",
      "If you just need to read the text contents of a presentation, you should convert the document to markdown:",
      "",
      "```bash",
      "# Convert document to markdown",
      "python -m markitdown path-to-file.pptx",
      "```",
      "",
      "### Raw XML access",
      "You need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.",
      "",
      "#### Unpacking a file",
      "`python ooxml/scripts/unpack.py <office_file> <output_dir>`",
      "",
      '**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn\'t exist at this path, use `find . -name "unpack.py"` to locate it.',
      "",
      "#### Key file structures",
      "* `ppt/presentation.xml` - Main presentation metadata and slide references",
      "* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)",
      "* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide",
      "* `ppt/comments/modernComment_*.xml` - Comments for specific slides",
      "* `ppt/slideLayouts/` - Layout templates for slides",
      "* `ppt/slideMasters/` - Master slide templates",
      "* `ppt/theme/` - Theme and styling information",
      "* `ppt/media/` - Images and other media files",
      "",
      "#### Typography and color extraction",
      "**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:",
      "1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)",
      "2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors",
      "3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files",
      "",
      "## Creating a new PowerPoint presentation **without a template**",
      "",
      "When creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.",
      "",
      "### Design Principles",
      "",
      "**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:",
      "1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?",
      "2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity",
      "3. **Match palette to content**: Select colors that reflect the subject",
      "4. **State your approach**: Explain your design choices before writing code",
      "",
      "**Requirements**:",
      "- ✅ State your content-informed design approach BEFORE writing code",
      "- ✅ Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact",
      "- ✅ Create clear visual hierarchy through size, weight, and color",
      "- ✅ Ensure readability: strong contrast, appropriately sized text, clean alignment",
      "- ✅ Be consistent: repeat patterns, spacing, and visual language across slides",
      "",
      "#### Color Palette Selection",
      "",
      "**Choosing colors creatively**:",
      "- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.",
      "- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)",
      "- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy",
      "- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)",
      "- **Ensure contrast**: Text must be clearly readable on backgrounds",
      "",
      "**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):",
      "",
      "1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)",
      "2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)",
      "3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)",
      "4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)",
      "5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)",
      "6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)",
      "7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)",
      "8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)",
      "9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)",
      "10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)",
      "11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)",
      "12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)",
      "13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)",
      "14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)",
      "15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)",
      "16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)",
      "17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)",
      "18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)",
      "",
      "#### Visual Details Options",
      "",
      "**Geometric Patterns**:",
      "- Diagonal section dividers instead of horizontal",
      "- Asymmetric column widths (30/70, 40/60, 25/75)",
      "- Rotated text headers at 90° or 270°",
      "- Circular/hexagonal frames for images",
      "- Triangular accent shapes in corners",
      "- Overlapping shapes for depth",
      "",
      "**Border & Frame Treatments**:",
      "- Thick single-color borders (10-20pt) on one side only",
      "- Double-line borders with contrasting colors",
      "- Corner brackets instead of full frames",
      "- L-shaped borders (top+left or bottom+right)",
      "- Underline accents beneath headers (3-5pt thick)",
      "",
      "**Typography Treatments**:",
      "- Extreme size contrast (72pt headlines vs 11pt body)",
      "- All-caps headers with wide letter spacing",
      "- Numbered sections in oversized display type",
      "- Monospace (Courier New) for data/stats/technical content",
      "- Condensed fonts (Arial Narrow) for dense information",
      "- Outlined text for emphasis",
      "",
      "**Chart & Data Styling**:",
      "- Monochrome charts with single accent color for key data",
      "- Horizontal bar charts instead of vertical",
      "- Dot plots instead of bar charts",
      "- Minimal gridlines or none at all",
      "- Data labels directly on elements (no legends)",
      "- Oversized numbers for key metrics",
      "",
      "**Layout Innovations**:",
      "- Full-bleed images with text overlays",
      "- Sidebar column (20-30% width) for navigation/context",
      "- Modular grid systems (3×3, 4×4 blocks)",
      "- Z-pattern or F-pattern content flow",
      "- Floating text boxes over colored shapes",
      "- Magazine-style multi-column layouts",
      "",
      "**Background Treatments**:",
      "- Solid color blocks occupying 40-60% of slide",
      "- Gradient fills (vertical or diagonal only)",
      "- Split backgrounds (two colors, diagonal or vertical)",
      "- Edge-to-edge color bands",
      "- Negative space as a design element",
      "",
      "### Layout Tips",
      "**When creating slides with charts or tables:**",
      "- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.",
      "- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability",
      "- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues",
      "",
      "### Workflow",
      "1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.",
      "2. Create an HTML file for each slide with proper dimensions (e.g., 720pt × 405pt for 16:9)",
      "   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content",
      '   - Use `class="placeholder"` for areas where charts/tables will be added (render with gray background for visibility)',
      "   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML",
      "   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability",
      "3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation",
      "   - Use the `html2pptx()` function to process each HTML file",
      "   - Add charts and tables to placeholder areas using PptxGenJS API",
      "   - Save the presentation using `pptx.writeFile()`",
      "4. **Visual validation**: Generate thumbnails and inspect for layout issues",
      "   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`",
      "   - Read and carefully examine the thumbnail image for:",
      "     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges",
      "     - **Text overlap**: Text overlapping with other text or shapes",
      "     - **Positioning issues**: Content too close to slide boundaries or other elements",
      "     - **Contrast issues**: Insufficient contrast between text and backgrounds",
      "   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation",
      "   - Repeat until all slides are visually correct",
      "",
      "## Editing an existing PowerPoint presentation",
      "",
      "When edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.",
      "",
      "### Workflow",
      "1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.",
      "2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`",
      "3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)",
      "4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`",
      "5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`",
      "",
      "## Creating a new PowerPoint presentation **using a template**",
      "",
      "When you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.",
      "",
      "### Workflow",
      "1. **Extract template text AND create visual thumbnail grid**:",
      "   * Extract text: `python -m markitdown template.pptx > template-content.md`",
      "   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**",
      "   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`",
      "   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details",
      "",
      "2. **Analyze template and save inventory to a file**:",
      "   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure",
      "   * Create and save a template inventory file at `template-inventory.md` containing:",
      "     ```markdown",
      "     # Template Inventory Analysis",
      "     **Total Slides: [count]**",
      "     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**",
      "",
      "     ## [Category Name]",
      "     - Slide 0: [Layout code if available] - Description/purpose",
      "     - Slide 1: [Layout code] - Description/purpose",
      "     - Slide 2: [Layout code] - Description/purpose",
      "     [... EVERY slide must be listed individually with its index ...]",
      "     ```",
      "   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:",
      "     - Layout patterns (title slides, content layouts, section dividers)",
      "     - Image placeholder locations and counts",
      "     - Design consistency across slide groups",
      "     - Visual hierarchy and structure",
      "   * This inventory file is REQUIRED for selecting appropriate templates in the next step",
      "",
      "3. **Create presentation outline based on template inventory**:",
      "   * Review available templates from step 2.",
      "   * Choose an intro or title template for the first slide. This should be one of the first templates.",
      "   * Choose safe, text-based layouts for the other slides.",
      "   * **CRITICAL: Match layout structure to actual content**:",
      "     - Single-column layouts: Use for unified narrative or single topic",
      "     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts",
      "     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts",
      "     - Image + text layouts: Use ONLY when you have actual images to insert",
      "     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis",
      "     - Never use layouts with more placeholders than you have content",
      "     - If you have 2 items, don't force them into a 3-column layout",
      "     - If you have 4+ items, consider breaking into multiple slides or using a list format",
      "   * Count your actual content pieces BEFORE selecting the layout",
      "   * Verify each placeholder in the chosen layout will be filled with meaningful content",
      "   * Select one option representing the **best** layout for each content section.",
      "   * Save `outline.md` with content AND template mapping that leverages available designs",
      "   * Example template mapping:",
      "      ```",
      "      # Template slides to use (0-based indexing)",
      "      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72",
      "      # Mapping: slide numbers from outline -> template slide indices",
      "      template_mapping = [",
      "          0,   # Use slide 0 (Title/Cover)",
      "          34,  # Use slide 34 (B1: Title and body)",
      "          34,  # Use slide 34 again (duplicate for second B1)",
      "          50,  # Use slide 50 (E1: Quote)",
      "          54,  # Use slide 54 (F2: Closing + Text)",
      "      ]",
      "      ```",
      "",
      "4. **Duplicate, reorder, and delete slides using `rearrange.py`**:",
      "   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:",
      "     ```bash",
      "     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52",
      "     ```",
      "   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically",
      "   * Slide indices are 0-based (first slide is 0, second is 1, etc.)",
      "   * The same slide index can appear multiple times to duplicate that slide",
      "",
      "5. **Extract ALL text using the `inventory.py` script**:",
      "   * **Run inventory extraction**:",
      "     ```bash",
      "     python scripts/inventory.py working.pptx text-inventory.json",
      "     ```",
      "   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**",
      "",
      "   * The inventory JSON structure:",
      "      ```json",
      "        {",
      '          "slide-0": {',
      '            "shape-0": {',
      '              "placeholder_type": "TITLE",  // or null for non-placeholders',
      '              "left": 1.5,                  // position in inches',
      '              "top": 2.0,',
      '              "width": 7.5,',
      '              "height": 1.2,',
      '              "paragraphs": [',
      "                {",
      '                  "text": "Paragraph text",',
      "                  // Optional properties (only included when non-default):",
      '                  "bullet": true,           // explicit bullet detected',
      '                  "level": 0,               // only included when bullet is true',
      '                  "alignment": "CENTER",    // CENTER, RIGHT (not LEFT)',
      '                  "space_before": 10.0,     // space before paragraph in points',
      '                  "space_after": 6.0,       // space after paragraph in points',
      '                  "line_spacing": 22.4,     // line spacing in points',
      '                  "font_name": "Arial",     // from first run',
      '                  "font_size": 14.0,        // in points',
      '                  "bold": true,',
      '                  "italic": false,',
      '                  "underline": false,',
      '                  "color": "FF0000"         // RGB color',
      "                }",
      "              ]",
      "            }",
      "          }",
      "        }",
      "      ```",
      "",
      "   * Key features:",
      '     - **Slides**: Named as "slide-0", "slide-1", etc.',
      '     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as "shape-0", "shape-1", etc.',
      "     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null",
      "     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)",
      "     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory",
      "     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)",
      "     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)",
      '     - **Colors**: `color` for RGB (e.g., "FF0000"), `theme_color` for theme colors (e.g., "DARK_1")',
      "     - **Properties**: Only non-default values are included in the output",
      "",
      "6. **Generate replacement text and save the data to a JSON file**",
      "   Based on the text inventory from the previous step:",
      "   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present",
      "   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory",
      "     - If you reference a non-existent shape, you'll get an error showing available shapes",
      "     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist",
      "     - All validation errors are shown at once before the script exits",
      "   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes",
      '   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide "paragraphs" for them',
      '   - Add a "paragraphs" field to shapes that need content (not "replacement_paragraphs")',
      '   - Shapes without "paragraphs" in the replacement JSON will have their text cleared automatically',
      '   - Paragraphs with bullets will be automatically left aligned. Don\'t set the `alignment` property on when `"bullet": true`',
      "   - Generate appropriate replacement content for placeholder text",
      "   - Use shape size to determine appropriate content length",
      "   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text",
      "   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (•, -, *) in text - they're added automatically",
      "   - **ESSENTIAL FORMATTING RULES**:",
      '     - Headers/titles should typically have `"bold": true`',
      '     - List items should have `"bullet": true, "level": 0` (level is required when bullet is true)',
      '     - Preserve any alignment properties (e.g., `"alignment": "CENTER"` for centered text)',
      '     - Include font properties when different from default (e.g., `"font_size": 14.0`, `"font_name": "Lora"`)',
      '     - Colors: Use `"color": "FF0000"` for RGB or `"theme_color": "DARK_1"` for theme colors',
      "     - The replacement script expects **properly formatted paragraphs**, not just text strings",
      "     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type",
      "   - Save the updated inventory with replacements to `replacement-text.json`",
      "   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements",
      "",
      "   Example paragraphs field showing proper formatting:",
      "   ```json",
      '   "paragraphs": [',
      "     {",
      '       "text": "New presentation title text",',
      '       "alignment": "CENTER",',
      '       "bold": true',
      "     },",
      "     {",
      '       "text": "Section Header",',
      '       "bold": true',
      "     },",
      "     {",
      '       "text": "First bullet point without bullet symbol",',
      '       "bullet": true,',
      '       "level": 0',
      "     },",
      "     {",
      '       "text": "Red colored text",',
      '       "color": "FF0000"',
      "     },",
      "     {",
      '       "text": "Theme colored text",',
      '       "theme_color": "DARK_1"',
      "     },",
      "     {",
      '       "text": "Regular paragraph text without special formatting"',
      "     }",
      "   ]",
      "   ```",
      "",
      "   **Shapes not listed in the replacement JSON are automatically cleared**:",
      "   ```json",
      "   {",
      '     "slide-0": {',
      '       "shape-0": {',
      '         "paragraphs": [...] // This shape gets new text',
      "       }",
      "       // shape-1 and shape-2 from inventory will be cleared automatically",
      "     }",
      "   }",
      "   ```",
      "",
      "   **Common formatting patterns for presentations**:",
      "   - Title slides: Bold text, sometimes centered",
      "   - Section headers within slides: Bold text",
      '   - Bullet lists: Each item needs `"bullet": true, "level": 0`',
      "   - Body text: Usually no special properties needed",
      "   - Quotes: May have special alignment or font properties",
      "",
      "7. **Apply replacements using the `replace.py` script**",
      "   ```bash",
      "   python scripts/replace.py working.pptx replacement-text.json output.pptx",
      "   ```",
      "",
      "   The script will:",
      "   - First extract the inventory of ALL text shapes using functions from inventory.py",
      "   - Validate that all shapes in the replacement JSON exist in the inventory",
      "   - Clear text from ALL shapes identified in the inventory",
      '   - Apply new text only to shapes with "paragraphs" defined in the replacement JSON',
      "   - Preserve formatting by applying paragraph properties from the JSON",
      "   - Handle bullets, alignment, font properties, and colors automatically",
      "   - Save the updated presentation",
      "",
      "   Example validation errors:",
      "   ```",
      "   ERROR: Invalid shapes in replacement JSON:",
      "     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4",
      "     - Slide 'slide-999' not found in inventory",
      "   ```",
      "",
      "   ```",
      "   ERROR: Replacement text made overflow worse in these shapes:",
      '     - slide-0/shape-2: overflow worsened by 1.25" (was 0.00", now 1.25")',
      "   ```",
      "",
      "## Creating Thumbnail Grids",
      "",
      "To create visual thumbnail grids of PowerPoint slides for quick analysis and reference:",
      "",
      "```bash",
      "python scripts/thumbnail.py template.pptx [output_prefix]",
      "```",
      "",
      "**Features**:",
      "- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)",
      "- Default: 5 columns, max 30 slides per grid (5×6)",
      "- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`",
      "  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)",
      "- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)",
      "- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42",
      "- Slides are zero-indexed (Slide 0, Slide 1, etc.)",
      "",
      "**Use cases**:",
      "- Template analysis: Quickly understand slide layouts and design patterns",
      "- Content review: Visual overview of entire presentation",
      "- Navigation reference: Find specific slides by their visual appearance",
      "- Quality check: Verify all slides are properly formatted",
      "",
      "**Examples**:",
      "```bash",
      "# Basic usage",
      "python scripts/thumbnail.py presentation.pptx",
      "",
      "# Combine options: custom name, columns",
      "python scripts/thumbnail.py template.pptx analysis --cols 4",
      "```",
      "",
      "## Converting Slides to Images",
      "",
      "To visually analyze PowerPoint slides, convert them to images using a two-step process:",
      "",
      "1. **Convert PPTX to PDF**:",
      "   ```bash",
      "   soffice --headless --convert-to pdf template.pptx",
      "   ```",
      "",
      "2. **Convert PDF pages to JPEG images**:",
      "   ```bash",
      "   pdftoppm -jpeg -r 150 template.pdf slide",
      "   ```",
      "   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.",
      "",
      "Options:",
      "- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)",
      "- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)",
      "- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)",
      "- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)",
      "- `slide`: Prefix for output files",
      "",
      "Example for specific range:",
      "```bash",
      "pdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5",
      "```",
      "",
      "## Code Style Guidelines",
      "**IMPORTANT**: When generating code for PPTX operations:",
      "- Write concise code",
      "- Avoid verbose variable names and redundant operations",
      "- Avoid unnecessary print statements",
      "",
      "## Dependencies",
      "",
      "Required dependencies (should already be installed):",
      "",
      '- **markitdown**: `pip install "markitdown[pptx]"` (for text extraction from presentations)',
      "- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)",
      "- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)",
      "- **react-icons**: `npm install -g react-icons react react-dom` (for icons)",
      "- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)",
      "- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)",
      "- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)",
      "- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
    ].join("\n"),
  },
  "product-management-competitive-analysis": {
    "SKILL.md": [
      "---",
      "name: product-management-competitive-analysis",
      "description: Analyze competitors with feature comparison matrices, positioning analysis, and strategic implications. Use when researching a competitor, comparing product capabilities, assessing competitive positioning, or preparing a competitive brief for product strategy.",
      "metadata:",
      "  plugin: product-management",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Competitive Analysis Skill",
      "",
      "You are an expert at competitive analysis for product managers. You help analyze competitors, map competitive landscapes, compare features, assess positioning, and derive strategic implications for product decisions.",
      "",
      "## Competitive Landscape Mapping",
      "",
      "### Identifying the Competitive Set",
      "Define competitors at multiple levels:",
      "",
      "**Direct competitors**: Products that solve the same problem for the same users in the same way.",
      "- These are the products your customers actively evaluate against you",
      "- They appear in your deals, in customer comparisons, in review site matchups",
      "",
      "**Indirect competitors**: Products that solve the same problem but differently.",
      "- Different approach to the same user need (e.g., spreadsheets vs dedicated project management tool)",
      '- Include "non-consumption" — sometimes the competitor is doing nothing or using a manual process',
      "",
      "**Adjacent competitors**: Products that do not compete today but could.",
      "- Companies with similar technology, customer base, or distribution that could expand into your space",
      "- Larger platforms that could add your functionality as a feature",
      "- Startups attacking a niche that could grow into your core market",
      "",
      "**Substitute solutions**: Entirely different ways users solve the underlying need.",
      "- Hiring a person instead of buying software",
      "- Using a general-purpose tool (Excel, email) instead of a specialized one",
      "- Outsourcing the process entirely",
      "",
      "### Landscape Map",
      "Position competitors on meaningful dimensions:",
      "",
      "**Common axes**:",
      "- Breadth vs depth (suite vs point solution)",
      "- SMB vs enterprise (market segment focus)",
      "- Self-serve vs sales-led (go-to-market approach)",
      "- Simple vs powerful (product complexity)",
      "- Horizontal vs vertical (general purpose vs industry-specific)",
      "",
      "Choose axes that reveal strategic positioning differences relevant to your market. The right axes make competitive dynamics visible.",
      "",
      "### Monitoring the Landscape",
      "Track competitive movements over time:",
      "- Product launches and feature releases (changelogs, blog posts, press releases)",
      "- Pricing and packaging changes",
      "- Funding rounds and acquisitions",
      "- Key hires and job postings (signal strategic direction)",
      "- Customer wins and losses (especially your wins/losses)",
      "- Analyst and review coverage",
      "- Partnership announcements",
      "",
      "## Feature Comparison Matrices",
      "",
      "### Building a Feature Comparison",
      "1. **Define capability areas**: Group features into functional categories that matter to buyers (not your internal architecture). Use the categories buyers use when evaluating.",
      "2. **List specific capabilities**: Under each area, list the specific features or capabilities to compare.",
      "3. **Rate each competitor**: Use a consistent rating scale.",
      "",
      "### Rating Scale Options",
      "",
      "**Simple (recommended for most cases)**:",
      "- Strong: Market-leading capability. Deep functionality, well-executed.",
      "- Adequate: Functional capability. Gets the job done but not differentiated.",
      "- Weak: Exists but limited. Significant gaps or poor execution.",
      "- Absent: Does not have this capability.",
      "",
      "**Detailed (for deep-dive comparisons)**:",
      "- 5: Best-in-class. Defines the standard others aspire to.",
      "- 4: Strong. Fully-featured and well-executed.",
      "- 3: Adequate. Meets basic needs without differentiation.",
      "- 2: Limited. Exists but with significant gaps.",
      "- 1: Minimal. Barely functional or in early beta.",
      "- 0: Absent. Not available.",
      "",
      "### Comparison Matrix Template",
      "```",
      "| Capability Area | Our Product | Competitor A | Competitor B |",
      "|----------------|-------------|-------------|-------------|",
      "| [Area 1]       |             |             |             |",
      "|   [Feature 1]  | Strong      | Adequate    | Absent      |",
      "|   [Feature 2]  | Adequate    | Strong      | Weak        |",
      "| [Area 2]       |             |             |             |",
      "|   [Feature 3]  | Strong      | Strong      | Adequate    |",
      "```",
      "",
      "### Tips for Feature Comparison",
      "- Rate based on real product experience, customer feedback, and reviews — not just marketing claims",
      '- Features exist on a spectrum. "Has feature X" is less useful than "How well does it do X?"',
      "- Weight the comparison by what matters to your target customers, not by total feature count",
      "- Update regularly — feature comparisons get stale fast",
      "- Be honest about where competitors are ahead. A comparison that always shows you winning is not credible.",
      '- Include the "why it matters" for each capability area. Not all features matter equally to buyers.',
      "",
      "## Positioning Analysis Frameworks",
      "",
      "### Positioning Statement Analysis",
      "For each competitor, extract their positioning:",
      "",
      "**Template**: For [target customer] who [need/problem], [Product] is a [category] that [key benefit]. Unlike [competitor/alternative], [Product] [key differentiator].",
      "",
      "**Sources for positioning**:",
      "- Homepage headline and subheadline",
      "- Product description on app stores or review sites",
      "- Sales pitch decks (sometimes leaked or shared by prospects)",
      "- Analyst briefing materials",
      "- Earnings call language (for public companies)",
      "",
      "### Message Architecture Analysis",
      "How does each competitor communicate value?",
      "",
      "**Level 1 — Category**: What category do they claim? (CRM, project management, collaboration platform)",
      "**Level 2 — Differentiator**: What makes them different within that category? (AI-powered, all-in-one, developer-first)",
      "**Level 3 — Value Proposition**: What outcome do they promise? (Close deals faster, ship products faster, never miss a deadline)",
      "**Level 4 — Proof Points**: What evidence do they provide? (Customer logos, metrics, awards, case studies)",
      "",
      "### Positioning Gaps and Opportunities",
      "Look for:",
      "- **Unclaimed positions**: Value propositions no competitor owns that matter to buyers",
      "- **Crowded positions**: Claims every competitor makes that have lost meaning",
      "- **Emerging positions**: New value propositions driven by market changes (AI, remote work, compliance)",
      "- **Vulnerable positions**: Claims competitors make that they cannot fully deliver on",
      "",
      "## Win/Loss Analysis Methodology",
      "",
      "### Conducting Win/Loss Analysis",
      "Win/loss analysis reveals why you actually win and lose deals. It is the most actionable competitive intelligence.",
      "",
      "**Data sources**:",
      "- CRM notes from sales team (available immediately, but biased)",
      "- Customer interviews shortly after decision (most valuable, least biased)",
      "- Churned customer surveys or exit interviews",
      "- Prospect surveys (for lost deals)",
      "",
      "### Win/Loss Interview Questions",
      "For wins:",
      "- What problem were you trying to solve?",
      "- What alternatives did you evaluate? (Reveals competitive set)",
      "- Why did you choose us over alternatives?",
      "- What almost made you choose someone else?",
      "- What would we need to lose for you to reconsider?",
      "",
      "For losses:",
      "- What problem were you trying to solve?",
      "- What did you end up choosing? Why?",
      "- Where did our product fall short?",
      "- What could we have done differently?",
      "- Would you reconsider us in the future? Under what conditions?",
      "",
      "### Analyzing Win/Loss Data",
      "- Track win/loss reasons over time. Are patterns changing?",
      "- Segment by deal type: enterprise vs SMB, new vs expansion, industry vertical",
      "- Identify the top 3-5 reasons for wins and losses",
      "- Distinguish between product reasons (features, quality) and non-product reasons (pricing, brand, relationship, timing)",
      "- Calculate competitive win rates by competitor: what % of deals involving each competitor do you win?",
      "",
      "### Common Win/Loss Patterns",
      "- **Feature gap**: Competitor has a specific capability you lack that is a dealbreaker",
      "- **Integration advantage**: Competitor integrates with tools the buyer already uses",
      "- **Pricing structure**: Not always cheaper — sometimes different pricing model (per-seat vs usage-based) fits better",
      "- **Incumbent advantage**: Buyer sticks with what they have because switching cost is too high",
      "- **Sales execution**: Better demo, faster response, more relevant case studies",
      "- **Brand/trust**: Buyer chooses the safer or more well-known option",
      "",
      "## Market Trend Identification",
      "",
      "### Sources for Trend Identification",
      "- **Industry analyst reports**: Gartner, Forrester, IDC for market sizing and trends",
      "- **Venture capital**: What are VCs funding? Investment themes signal where smart money sees opportunity.",
      "- **Conference themes**: What are industry events focusing on? What topics draw the biggest audiences?",
      "- **Technology shifts**: New platforms, APIs, or capabilities that enable new product categories",
      "- **Regulatory changes**: New regulations that create requirements or opportunities",
      "- **Customer behavior changes**: How are user expectations evolving? (e.g., mobile-first, AI-assisted, privacy-conscious)",
      "- **Talent movement**: Where are top people going? What skills are in demand?",
      "",
      "### Trend Analysis Framework",
      "For each trend identified:",
      "",
      "1. **What is changing?**: Describe the trend clearly and specifically",
      "2. **Why now?**: What is driving this change? (Technology, regulation, behavior, economics)",
      "3. **Who is affected?**: Which customer segments or market categories?",
      "4. **What is the timeline?**: Is this happening now, in 1-2 years, or 3-5 years?",
      "5. **What is the implication for us?**: How should this influence our product strategy?",
      "6. **What are competitors doing?**: How are competitors responding to this trend?",
      "",
      "### Separating Signal from Noise",
      "- **Signals**: Trends backed by behavioral data, growing investment, regulatory action, or customer demand",
      "- **Noise**: Trends backed only by media hype, conference buzz, or competitor announcements without customer traction",
      "- Test trends against your own customer data: are YOUR customers asking for this or experiencing this change?",
      '- Be wary of "trend of the year" hype cycles. Many trends that dominate industry conversation do not materially affect your customers for years.',
      "",
      "### Strategic Response Options",
      "For each significant trend:",
      "- **Lead**: Invest early and try to define the category or approach. High risk, high reward.",
      "- **Fast follow**: Wait for early signals of customer demand, then move quickly. Lower risk but harder to differentiate.",
      "- **Monitor**: Track the trend but do not invest yet. Set triggers for when to act.",
      "- **Ignore**: Explicitly decide this trend is not relevant to your strategy. Document why.",
      "",
      "The right response depends on: your competitive position, your customer base, your resources, and how fast the trend is moving.",
      "",
    ].join("\n"),
  },
  "product-management-competitive-brief": {
    "SKILL.md": [
      "---",
      "name: product-management-competitive-brief",
      "description: Create a competitive analysis brief for one or more competitors or a feature area",
      "metadata:",
      "  plugin: product-management",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <competitor or feature area>",
      "---",
      "",
      "# Competitive Brief",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Create a competitive analysis brief for one or more competitors or a feature area.",
      "",
      "## Workflow",
      "",
      "### 1. Scope the Analysis",
      "",
      "Ask the user:",
      "- **Competitor(s)**: Which specific competitor(s) to analyze? Or a feature area to compare across competitors?",
      "- **Focus**: Full product comparison, specific feature area, pricing/packaging, go-to-market, or positioning?",
      "- **Context**: What decision will this inform? (product strategy, sales enablement, investor/board materials, feature prioritization)",
      "",
      "### 2. Research",
      "",
      "**Via web search**:",
      "- Product pages and feature lists",
      "- Pricing pages and packaging",
      "- Recent product launches, blog posts, and changelogs",
      "- Press coverage and analyst reports",
      "- Customer reviews and ratings (G2, Capterra, TrustRadius)",
      "- Job postings (signal of strategic direction)",
      "- Social media and community discussions",
      "",
      "If **~~knowledge base** is connected:",
      "- Search for existing competitive analysis documents",
      "- Find win/loss reports or sales battle cards",
      "- Pull prior competitive research",
      "",
      "If **~~chat** is connected:",
      "- Search for competitive mentions in sales or product channels",
      "- Find recent deal feedback involving competitors",
      "",
      "### 3. Generate the Brief",
      "",
      "#### Competitor Overview",
      "For each competitor:",
      "- Company summary: founding, size, funding/revenue if public, target market",
      "- Product positioning: how they describe themselves, who they target",
      "- Recent momentum: launches, funding, partnerships, customer wins",
      "",
      "#### Feature Comparison",
      "Compare capabilities across key areas relevant to the analysis. See the **competitive-analysis** skill for rating frameworks and comparison matrix templates.",
      "",
      "#### Positioning Analysis",
      "Analyze how each competitor positions themselves — target customer, category claim, key differentiator, and value proposition. See the **competitive-analysis** skill for positioning analysis frameworks.",
      "",
      "#### Strengths and Weaknesses",
      "For each competitor:",
      "- **Strengths**: Where they genuinely excel. What customers praise.",
      "- **Weaknesses**: Where they fall short. What customers complain about.",
      "- Be honest and evidence-based — do not dismiss competitors or inflate their weaknesses.",
      "",
      "#### Opportunities",
      "Based on the analysis:",
      "- Where are there gaps in competitor offerings we could exploit?",
      "- What are customers asking for that no one provides well?",
      "- Where are competitors making bets we disagree with?",
      "- What market shifts could advantage our approach?",
      "",
      "#### Threats",
      "- Where are competitors investing heavily?",
      "- What competitive moves could disrupt our position?",
      "- Where are we most vulnerable?",
      '- What would a "nightmare scenario" competitive move look like?',
      "",
      "#### Strategic Implications",
      "Tie the analysis back to product strategy:",
      "- What should we build, accelerate, or deprioritize based on this analysis?",
      "- Where should we differentiate vs. achieve parity?",
      "- How should we adjust positioning or messaging?",
      "- What should we monitor going forward?",
      "",
      "### 4. Follow Up",
      "",
      "After generating the brief:",
      "- Ask if the user wants to dive deeper on any section",
      "- Offer to create a one-page summary for executives",
      "- Offer to create sales battle cards for competitive deals",
      '- Offer to draft a "how to win against [competitor]" guide',
      "- Offer to set up a monitoring plan for competitive moves",
      "",
      "## Output Format",
      "",
      "Use tables for feature comparisons. Use clear headers for each section. Keep the strategic implications section concise and actionable — this is where the value is for the reader.",
      "",
      "## Tips",
      "",
      "- Be honest about competitor strengths. Dismissing competitors makes the analysis useless.",
      "- Focus on what matters to customers, not what matters to product teams. Customers do not care about architecture elegance.",
      "- Pricing is hard to compare fairly. Note the caveats (different packaging, usage-based vs seat-based, enterprise custom pricing).",
      "- Job postings are underrated competitive intelligence. A competitor hiring ML engineers signals a strategic direction.",
      "- Customer reviews are gold. They reveal what real users love and hate, unfiltered by marketing.",
      '- The most valuable part of competitive analysis is the "so what" — the strategic implications. Do not skip this.',
      "- Competitive analysis has a shelf life. Note the date and flag areas that change quickly.",
      "",
    ].join("\n"),
  },
  "product-management-feature-spec": {
    "SKILL.md": [
      "---",
      "name: product-management-feature-spec",
      "description: Write structured product requirements documents (PRDs) with problem statements, user stories, requirements, and success metrics. Use when speccing a new feature, writing a PRD, defining acceptance criteria, prioritizing requirements, or documenting product decisions.",
      "metadata:",
      "  plugin: product-management",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Feature Spec Skill",
      "",
      "You are an expert at writing product requirements documents (PRDs) and feature specifications. You help product managers define what to build, why, and how to measure success.",
      "",
      "## PRD Structure",
      "",
      "A well-structured PRD follows this template:",
      "",
      "### 1. Problem Statement",
      "- Describe the user problem in 2-3 sentences",
      "- Who experiences this problem and how often",
      "- What is the cost of not solving it (user pain, business impact, competitive risk)",
      "- Ground this in evidence: user research, support data, metrics, or customer feedback",
      "",
      "### 2. Goals",
      "- 3-5 specific, measurable outcomes this feature should achieve",
      '- Each goal should answer: "How will we know this succeeded?"',
      "- Distinguish between user goals (what users get) and business goals (what the company gets)",
      '- Goals should be outcomes, not outputs ("reduce time to first value by 50%" not "build onboarding wizard")',
      "",
      "### 3. Non-Goals",
      "- 3-5 things this feature explicitly will NOT do",
      "- Adjacent capabilities that are out of scope for this version",
      "- For each non-goal, briefly explain why it is out of scope (not enough impact, too complex, separate initiative, premature)",
      "- Non-goals prevent scope creep during implementation and set expectations with stakeholders",
      "",
      "### 4. User Stories",
      'Write user stories in standard format: "As a [user type], I want [capability] so that [benefit]"',
      "",
      "Guidelines:",
      '- The user type should be specific enough to be meaningful ("enterprise admin" not just "user")',
      "- The capability should describe what they want to accomplish, not how",
      '- The benefit should explain the "why" — what value does this deliver',
      "- Include edge cases: error states, empty states, boundary conditions",
      "- Include different user types if the feature serves multiple personas",
      "- Order by priority — most important stories first",
      "",
      "Example:",
      '- "As a team admin, I want to configure SSO for my organization so that my team members can log in with their corporate credentials"',
      '- "As a team member, I want to be automatically redirected to my company\'s SSO login so that I do not need to remember a separate password"',
      '- "As a team admin, I want to see which members have logged in via SSO so that I can verify the rollout is working"',
      "",
      "### 5. Requirements",
      "",
      '**Must-Have (P0)**: The feature cannot ship without these. These represent the minimum viable version of the feature. Ask: "If we cut this, does the feature still solve the core problem?" If no, it is P0.',
      "",
      "**Nice-to-Have (P1)**: Significantly improves the experience but the core use case works without them. These often become fast follow-ups after launch.",
      "",
      "**Future Considerations (P2)**: Explicitly out of scope for v1 but we want to design in a way that supports them later. Documenting these prevents accidental architectural decisions that make them hard later.",
      "",
      "For each requirement:",
      "- Write a clear, unambiguous description of the expected behavior",
      "- Include acceptance criteria (see below)",
      "- Note any technical considerations or constraints",
      "- Flag dependencies on other teams or systems",
      "",
      "### 6. Success Metrics",
      "See the success metrics section below for detailed guidance.",
      "",
      "### 7. Open Questions",
      "- Questions that need answers before or during implementation",
      "- Tag each with who should answer (engineering, design, legal, data, stakeholder)",
      "- Distinguish between blocking questions (must answer before starting) and non-blocking (can resolve during implementation)",
      "",
      "### 8. Timeline Considerations",
      "- Hard deadlines (contractual commitments, events, compliance dates)",
      "- Dependencies on other teams' work or releases",
      "- Suggested phasing if the feature is too large for one release",
      "",
      "## User Story Writing",
      "",
      "Good user stories are:",
      "- **Independent**: Can be developed and delivered on their own",
      "- **Negotiable**: Details can be discussed, the story is not a contract",
      "- **Valuable**: Delivers value to the user (not just the team)",
      "- **Estimable**: The team can roughly estimate the effort",
      "- **Small**: Can be completed in one sprint/iteration",
      "- **Testable**: There is a clear way to verify it works",
      "",
      "### Common Mistakes in User Stories",
      '- Too vague: "As a user, I want the product to be faster" — what specifically should be faster?',
      '- Solution-prescriptive: "As a user, I want a dropdown menu" — describe the need, not the UI widget',
      '- No benefit: "As a user, I want to click a button" — why? What does it accomplish?',
      '- Too large: "As a user, I want to manage my team" — break this into specific capabilities',
      '- Internal focus: "As the engineering team, we want to refactor the database" — this is a task, not a user story',
      "",
      "## Requirements Categorization",
      "",
      "### MoSCoW Framework",
      "- **Must have**: Without these, the feature is not viable. Non-negotiable.",
      "- **Should have**: Important but not critical for launch. High-priority fast follows.",
      "- **Could have**: Desirable if time permits. Will not delay delivery if cut.",
      "- **Won't have (this time)**: Explicitly out of scope. May revisit in future versions.",
      "",
      "### Tips for Categorization",
      "- Be ruthless about P0s. The tighter the must-have list, the faster you ship and learn.",
      '- If everything is P0, nothing is P0. Challenge every must-have: "Would we really not ship without this?"',
      "- P1s should be things you are confident you will build soon, not a wish list.",
      "- P2s are architectural insurance — they guide design decisions even though you are not building them now.",
      "",
      "## Success Metrics Definition",
      "",
      "### Leading Indicators",
      "Metrics that change quickly after launch (days to weeks):",
      "- **Adoption rate**: % of eligible users who try the feature",
      "- **Activation rate**: % of users who complete the core action",
      "- **Task completion rate**: % of users who successfully accomplish their goal",
      "- **Time to complete**: How long the core workflow takes",
      "- **Error rate**: How often users encounter errors or dead ends",
      "- **Feature usage frequency**: How often users return to use the feature",
      "",
      "### Lagging Indicators",
      "Metrics that take time to develop (weeks to months):",
      "- **Retention impact**: Does this feature improve user retention?",
      "- **Revenue impact**: Does this drive upgrades, expansion, or new revenue?",
      "- **NPS / satisfaction change**: Does this improve how users feel about the product?",
      "- **Support ticket reduction**: Does this reduce support load?",
      "- **Competitive win rate**: Does this help win more deals?",
      "",
      "### Setting Targets",
      '- Targets should be specific: "50% adoption within 30 days" not "high adoption"',
      "- Base targets on comparable features, industry benchmarks, or explicit hypotheses",
      '- Set a "success" threshold and a "stretch" target',
      "- Define the measurement method: what tool, what query, what time window",
      "- Specify when you will evaluate: 1 week, 1 month, 1 quarter post-launch",
      "",
      "## Acceptance Criteria",
      "",
      "Write acceptance criteria in Given/When/Then format or as a checklist:",
      "",
      "**Given/When/Then**:",
      "- Given [precondition or context]",
      "- When [action the user takes]",
      "- Then [expected outcome]",
      "",
      "Example:",
      "- Given the admin has configured SSO for their organization",
      "- When a team member visits the login page",
      "- Then they are automatically redirected to the organization's SSO provider",
      "",
      "**Checklist format**:",
      "- [ ] Admin can enter SSO provider URL in organization settings",
      '- [ ] Team members see "Log in with SSO" button on login page',
      "- [ ] SSO login creates a new account if one does not exist",
      "- [ ] SSO login links to existing account if email matches",
      "- [ ] Failed SSO attempts show a clear error message",
      "",
      "### Tips for Acceptance Criteria",
      "- Cover the happy path, error cases, and edge cases",
      "- Be specific about the expected behavior, not the implementation",
      "- Include what should NOT happen (negative test cases)",
      "- Each criterion should be independently testable",
      '- Avoid ambiguous words: "fast", "user-friendly", "intuitive" — define what these mean concretely',
      "",
      "## Scope Management",
      "",
      "### Recognizing Scope Creep",
      "Scope creep happens when:",
      "- Requirements keep getting added after the spec is approved",
      '- "Small" additions accumulate into a significantly larger project',
      '- The team is building features no user asked for ("while we\'re at it...")',
      "- The launch date keeps moving without explicit re-scoping",
      "- Stakeholders add requirements without removing anything",
      "",
      "### Preventing Scope Creep",
      "- Write explicit non-goals in every spec",
      "- Require that any scope addition comes with a scope removal or timeline extension",
      '- Separate "v1" from "v2" clearly in the spec',
      "- Review the spec against the original problem statement — does everything serve it?",
      '- Time-box investigations: "If we cannot figure out X in 2 days, we cut it"',
      '- Create a "parking lot" for good ideas that are not in scope',
      "",
    ].join("\n"),
  },
  "product-management-metrics-review": {
    "SKILL.md": [
      "---",
      "name: product-management-metrics-review",
      "description: Review and analyze product metrics with trend analysis and actionable insights",
      "metadata:",
      "  plugin: product-management",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <time period or metric focus>",
      "---",
      "",
      "# Metrics Review",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Review and analyze product metrics, identify trends, and surface actionable insights.",
      "",
      "## Workflow",
      "",
      "### 1. Gather Metrics Data",
      "",
      "If **~~product analytics** is connected:",
      "- Pull key product metrics for the relevant time period",
      "- Get comparison data (previous period, same period last year, targets)",
      "- Pull segment breakdowns if available",
      "",
      "If no analytics tool is connected, ask the user to provide:",
      "- The metrics and their values (paste a table, screenshot, or describe)",
      "- Comparison data (previous period, targets)",
      "- Any context on recent changes (launches, incidents, seasonality)",
      "",
      "Ask the user:",
      "- What time period to review? (last week, last month, last quarter)",
      "- What metrics to focus on? Or should we review the full product metrics suite?",
      "- Are there specific targets or goals to compare against?",
      "- Any known events that might explain changes (launches, outages, marketing campaigns, seasonality)?",
      "",
      "### 2. Organize the Metrics",
      "",
      "Structure the review using the metrics hierarchy from the **metrics-tracking** skill: North Star metric at the top, L1 health indicators (acquisition, activation, engagement, retention, revenue, satisfaction), and L2 diagnostic metrics for drill-down.",
      "",
      "If the user has not defined their metrics hierarchy, help them identify their North Star and key L1 metrics before proceeding.",
      "",
      "### 3. Analyze Trends",
      "",
      "For each key metric:",
      "- **Current value**: What is the metric today?",
      "- **Trend**: Up, down, or flat compared to previous period? Over what timeframe?",
      "- **vs Target**: How does it compare to the goal or target?",
      "- **Rate of change**: Is the trend accelerating or decelerating?",
      "- **Anomalies**: Any sudden changes, spikes, or drops?",
      "",
      "Identify correlations:",
      "- Do changes in one metric correlate with changes in another?",
      "- Are there leading indicators that predict lagging metric changes?",
      "- Do segment breakdowns reveal that an aggregate trend is driven by a specific cohort?",
      "",
      "### 4. Generate the Review",
      "",
      "#### Summary",
      "2-3 sentences: overall product health, most notable changes, key callout.",
      "",
      "#### Metric Scorecard",
      "Table format for quick scanning:",
      "",
      "| Metric | Current | Previous | Change | Target | Status |",
      "|--------|---------|----------|--------|--------|--------|",
      "| [Metric] | [Value] | [Value] | [+/- %] | [Target] | [On track / At risk / Miss] |",
      "",
      "#### Trend Analysis",
      "For each metric worth discussing:",
      "- What happened and how significant is the change",
      "- Why it likely happened (attribution based on known events, correlated metrics, segment analysis)",
      "- Whether this is a one-time event or a sustained trend",
      "",
      "#### Bright Spots",
      "What is going well:",
      "- Metrics beating targets",
      "- Positive trends to sustain",
      "- Segments or features showing strong performance",
      "",
      "#### Areas of Concern",
      "What needs attention:",
      "- Metrics missing targets or trending negatively",
      "- Early warning signals before they become problems",
      "- Metrics where we lack visibility or understanding",
      "",
      "#### Recommended Actions",
      "Specific next steps based on the analysis:",
      "- Investigations to run (dig deeper into a concerning trend)",
      "- Experiments to launch (test hypotheses about what could improve a metric)",
      "- Investments to make (double down on what is working)",
      "- Alerts to set (monitor a metric more closely)",
      "",
      "#### Context and Caveats",
      "- Known data quality issues",
      "- Events that affect comparability (outages, holidays, launches)",
      "- Metrics we should be tracking but are not yet",
      "",
      "### 5. Follow Up",
      "",
      "After generating the review:",
      "- Ask if any metric needs deeper investigation",
      "- Offer to create a dashboard spec for ongoing monitoring",
      "- Offer to draft experiment proposals for areas of concern",
      "- Offer to set up a metrics review template for recurring use",
      "",
      "## Output Format",
      "",
      "Use tables for the scorecard. Use clear status indicators. Keep the summary tight — the reader should get the essential story in 30 seconds.",
      "",
      "## Tips",
      "",
      '- Start with the "so what" — what is the most important thing in this metrics review? Lead with that.',
      "- Absolute numbers without context are useless. Always show comparisons (vs previous period, vs target, vs benchmark).",
      "- Be careful about attribution. Correlation is not causation. If a metric moved, acknowledge uncertainty about why.",
      "- Segment analysis often reveals that an aggregate metric masks important differences. A flat overall number might hide one segment growing and another shrinking.",
      "- Not all metric movements matter. Small fluctuations are noise. Focus attention on meaningful changes.",
      "- If a metric is missing its target, do not just report the miss — recommend what to do about it.",
      "- Metrics reviews should drive decisions. If the review does not lead to at least one action, it was not useful.",
      "",
    ].join("\n"),
  },
  "product-management-metrics-tracking": {
    "SKILL.md": [
      "---",
      "name: product-management-metrics-tracking",
      "description: Define, track, and analyze product metrics with frameworks for goal setting and dashboard design. Use when setting up OKRs, building metrics dashboards, running weekly metrics reviews, identifying trends, or choosing the right metrics for a product area.",
      "metadata:",
      "  plugin: product-management",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Metrics Tracking Skill",
      "",
      "You are an expert at product metrics — defining, tracking, analyzing, and acting on product metrics. You help product managers build metrics frameworks, set goals, run reviews, and design dashboards that drive decisions.",
      "",
      "## Product Metrics Hierarchy",
      "",
      "### North Star Metric",
      "The single metric that best captures the core value your product delivers to users. It should be:",
      "",
      "- **Value-aligned**: Moves when users get more value from the product",
      "- **Leading**: Predicts long-term business success (revenue, retention)",
      "- **Actionable**: The product team can influence it through their work",
      "- **Understandable**: Everyone in the company can understand what it means and why it matters",
      "",
      "**Examples by product type**:",
      "- Collaboration tool: Weekly active teams with 3+ members contributing",
      "- Marketplace: Weekly transactions completed",
      "- SaaS platform: Weekly active users completing core workflow",
      "- Content platform: Weekly engaged reading/viewing time",
      "- Developer tool: Weekly deployments using the tool",
      "",
      "### L1 Metrics (Health Indicators)",
      "The 5-7 metrics that together paint a complete picture of product health. These map to the key stages of the user lifecycle:",
      "",
      "**Acquisition**: Are new users finding the product?",
      "- New signups or trial starts (volume and trend)",
      "- Signup conversion rate (visitors to signups)",
      "- Channel mix (where are new users coming from)",
      "- Cost per acquisition (for paid channels)",
      "",
      "**Activation**: Are new users reaching the value moment?",
      "- Activation rate: % of new users who complete the key action that predicts retention",
      "- Time to activate: how long from signup to activation",
      "- Setup completion rate: % who complete onboarding steps",
      "- First value moment: when users first experience the core product value",
      "",
      "**Engagement**: Are active users getting value?",
      "- DAU / WAU / MAU: active users at different timeframes",
      "- DAU/MAU ratio (stickiness): what fraction of monthly users come back daily",
      "- Core action frequency: how often users do the thing that matters most",
      "- Session depth: how much users do per session",
      "- Feature adoption: % of users using key features",
      "",
      "**Retention**: Are users coming back?",
      "- D1, D7, D30 retention: % of users who return after 1 day, 7 days, 30 days",
      "- Cohort retention curves: how retention evolves for each signup cohort",
      "- Churn rate: % of users or revenue lost per period",
      "- Resurrection rate: % of churned users who come back",
      "",
      "**Monetization**: Is value translating to revenue?",
      "- Conversion rate: free to paid (for freemium)",
      "- MRR / ARR: monthly or annual recurring revenue",
      "- ARPU / ARPA: average revenue per user or account",
      "- Expansion revenue: revenue growth from existing customers",
      "- Net revenue retention: revenue retention including expansion and contraction",
      "",
      "**Satisfaction**: How do users feel about the product?",
      "- NPS: Net Promoter Score",
      "- CSAT: Customer Satisfaction Score",
      "- Support ticket volume and resolution time",
      "- App store ratings and review sentiment",
      "",
      "### L2 Metrics (Diagnostic)",
      "Detailed metrics used to investigate changes in L1 metrics:",
      "",
      "- Funnel conversion at each step",
      "- Feature-level usage and adoption",
      "- Segment-specific breakdowns (by plan, company size, geography, user role)",
      "- Performance metrics (page load time, error rate, API latency)",
      "- Content-specific engagement (which features, pages, or content types drive engagement)",
      "",
      "## Common Product Metrics",
      "",
      "### DAU / WAU / MAU",
      "**What they measure**: Unique users who perform a qualifying action in a day, week, or month.",
      "",
      "**Key decisions**:",
      '- What counts as "active"? A login? A page view? A core action? Define this carefully — different definitions tell different stories.',
      "- Which timeframe matters most? DAU for daily-use products (messaging, email). WAU for weekly-use products (project management). MAU for less frequent products (tax software, travel booking).",
      "",
      "**How to use them**:",
      "- DAU/MAU ratio (stickiness): values above 0.5 indicate a daily habit. Below 0.2 suggests infrequent usage.",
      "- Trend matters more than absolute number. Is active usage growing, flat, or declining?",
      "- Segment by user type. Power users and casual users behave very differently.",
      "",
      "### Retention",
      "**What it measures**: Of users who started in period X, what % are still active in period Y?",
      "",
      "**Common retention timeframes**:",
      "- D1 (next day): Was the first experience good enough to come back?",
      "- D7 (one week): Did the user establish a habit?",
      "- D30 (one month): Is the user retained long-term?",
      "- D90 (three months): Is this a durable user?",
      "",
      "**How to use retention**:",
      "- Plot retention curves by cohort. Look for: initial drop-off (activation problem), steady decline (engagement problem), or flattening (good — you have a stable retained base).",
      "- Compare cohorts over time. Are newer cohorts retaining better than older ones? That means product improvements are working.",
      "- Segment retention by activation behavior. Users who completed onboarding vs those who did not. Users who used feature X vs those who did not.",
      "",
      "### Conversion",
      "**What it measures**: % of users who move from one stage to the next.",
      "",
      "**Common conversion funnels**:",
      "- Visitor to signup",
      "- Signup to activation (key value moment)",
      "- Free to paid (trial conversion)",
      "- Trial to paid subscription",
      "- Monthly to annual plan",
      "",
      "**How to use conversion**:",
      "- Map the full funnel and measure conversion at each step",
      "- Identify the biggest drop-off points — these are your highest-leverage improvement opportunities",
      "- Segment conversion by source, plan, user type. Different segments convert very differently.",
      "- Track conversion over time. Is it improving as you iterate on the experience?",
      "",
      "### Activation",
      "**What it measures**: % of new users who reach the moment where they first experience the product's core value.",
      "",
      "**Defining activation**:",
      "- Look at retained users vs churned users. What actions did retained users take that churned users did not?",
      "- The activation event should be strongly predictive of long-term retention",
      "- It should be achievable within the first session or first few days",
      "- Examples: created first project, invited a teammate, completed first workflow, connected an integration",
      "",
      "**How to use activation**:",
      "- Track activation rate for every signup cohort",
      "- Measure time to activate — faster is almost always better",
      "- Build onboarding flows that guide users to the activation moment",
      "- A/B test activation flows and measure impact on retention, not just activation rate",
      "",
      "## Goal Setting Frameworks",
      "",
      "### OKRs (Objectives and Key Results)",
      "",
      "**Objectives**: Qualitative, aspirational goals that describe what you want to achieve.",
      "- Inspiring and memorable",
      "- Time-bound (quarterly or annually)",
      "- Directional, not metric-specific",
      "",
      "**Key Results**: Quantitative measures that tell you if you achieved the objective.",
      "- Specific and measurable",
      "- Time-bound with a clear target",
      "- Outcome-based, not output-based",
      "- 2-4 Key Results per Objective",
      "",
      "**Example**:",
      "```",
      "Objective: Make our product indispensable for daily workflows",
      "",
      "Key Results:",
      "- Increase DAU/MAU ratio from 0.35 to 0.50",
      "- Increase D30 retention for new users from 40% to 55%",
      "- 3 core workflows with >80% task completion rate",
      "```",
      "",
      "### OKR Best Practices",
      "- Set OKRs that are ambitious but achievable. 70% completion is the target for stretch OKRs.",
      "- Key Results should measure outcomes (user behavior, business results), not outputs (features shipped, tasks completed).",
      "- Do not have too many OKRs. 2-3 objectives with 2-4 KRs each is plenty.",
      "- OKRs should be uncomfortable. If you are confident you will hit all of them, they are not ambitious enough.",
      "- Review OKRs at mid-period. Adjust effort allocation if some KRs are clearly off track.",
      "- Grade OKRs honestly at end of period. 0.0-0.3 = missed, 0.4-0.6 = progress, 0.7-1.0 = achieved.",
      "",
      "### Setting Metric Targets",
      "- **Baseline**: What is the current value? You need a reliable baseline before setting a target.",
      "- **Benchmark**: What do comparable products achieve? Industry benchmarks provide context.",
      "- **Trajectory**: What is the current trend? If the metric is already improving at 5% per month, a 6% target is not ambitious.",
      "- **Effort**: How much investment are you putting behind this? Bigger bets warrant more ambitious targets.",
      '- **Confidence**: How confident are you in hitting the target? Set a "commit" (high confidence) and a "stretch" (ambitious).',
      "",
      "## Metric Review Cadences",
      "",
      "### Weekly Metrics Check",
      "**Purpose**: Catch issues quickly, monitor experiments, stay in touch with product health.",
      "**Duration**: 15-30 minutes.",
      "**Attendees**: Product manager, maybe engineering lead.",
      "",
      "**What to review**:",
      "- North Star metric: current value, week-over-week change",
      "- Key L1 metrics: any notable movements",
      "- Active experiments: results and statistical significance",
      "- Anomalies: any unexpected spikes or drops",
      "- Alerts: anything that triggered a monitoring alert",
      "",
      "**Action**: If something looks off, investigate. Otherwise, note it and move on.",
      "",
      "### Monthly Metrics Review",
      "**Purpose**: Deeper analysis of trends, progress against goals, strategic implications.",
      "**Duration**: 30-60 minutes.",
      "**Attendees**: Product team, key stakeholders.",
      "",
      "**What to review**:",
      "- Full L1 metric scorecard with month-over-month trends",
      "- Progress against quarterly OKR targets",
      "- Cohort analysis: are newer cohorts performing better?",
      "- Feature adoption: how are recent launches performing?",
      "- Segment analysis: any divergence between user segments?",
      "",
      "**Action**: Identify 1-3 areas to investigate or invest in. Update priorities if metrics reveal new information.",
      "",
      "### Quarterly Business Review",
      "**Purpose**: Strategic assessment of product performance, goal-setting for next quarter.",
      "**Duration**: 60-90 minutes.",
      "**Attendees**: Product, engineering, design, leadership.",
      "",
      "**What to review**:",
      "- OKR scoring for the quarter",
      "- Trend analysis for all L1 metrics over the quarter",
      "- Year-over-year comparisons",
      "- Competitive context: market changes and competitor movements",
      "- What worked and what did not",
      "",
      "**Action**: Set OKRs for next quarter. Adjust product strategy based on what the data shows.",
      "",
      "## Dashboard Design Principles",
      "",
      "### Effective Product Dashboards",
      'A good dashboard answers the question "How is the product doing?" at a glance.',
      "",
      "**Principles**:",
      "",
      "1. **Start with the question, not the data**. What decisions does this dashboard support? Design backwards from the decision.",
      "",
      "2. **Hierarchy of information**. The most important metric should be the most visually prominent. North Star at the top, L1 metrics next, L2 metrics available on drill-down.",
      "",
      "3. **Context over numbers**. A number without context is meaningless. Always show: current value, comparison (previous period, target, benchmark), trend direction.",
      "",
      "4. **Fewer metrics, more insight**. A dashboard with 50 metrics helps no one. Focus on 5-10 that matter. Put everything else in a detailed report.",
      "",
      "5. **Consistent time periods**. Use the same time period for all metrics on a dashboard. Mixing daily and monthly metrics creates confusion.",
      "",
      "6. **Visual status indicators**. Use color to indicate health at a glance:",
      "   - Green: on track or improving",
      "   - Yellow: needs attention or flat",
      "   - Red: off track or declining",
      "",
      "7. **Actionability**. Every metric on the dashboard should be something the team can influence. If you cannot act on it, it does not belong on the product dashboard.",
      "",
      "### Dashboard Layout",
      "",
      "**Top row**: North Star metric with trend line and target.",
      "",
      "**Second row**: L1 metrics scorecard — current value, change, target, status for each key metric.",
      "",
      "**Third row**: Key funnels or conversion metrics — visual funnel showing drop-off at each stage.",
      "",
      "**Fourth row**: Recent experiments and launches — active A/B tests, recent feature launches with early metrics.",
      "",
      "**Bottom / drill-down**: L2 metrics, segment breakdowns, and detailed time series for investigation.",
      "",
      "### Dashboard Anti-Patterns",
      "- **Vanity metrics**: Metrics that always go up but do not indicate health (total signups ever, total page views)",
      "- **Too many metrics**: Dashboards that require scrolling to see. If it does not fit on one screen, cut metrics.",
      "- **No comparison**: Raw numbers without context (current value with no previous period or target)",
      "- **Stale dashboards**: Metrics that have not been updated or reviewed in months",
      "- **Output dashboards**: Measuring team activity (tickets closed, PRs merged) instead of user and business outcomes",
      "- **One dashboard for all audiences**: Executives, PMs, and engineers need different views. One size does not fit all.",
      "",
      "### Alerting",
      "Set alerts for metrics that require immediate attention:",
      "",
      "- **Threshold alerts**: Metric drops below or rises above a critical threshold (error rate > 1%, conversion < 5%)",
      "- **Trend alerts**: Metric shows sustained decline over multiple days/weeks",
      "- **Anomaly alerts**: Metric deviates significantly from expected range",
      "",
      "**Alert hygiene**:",
      "- Every alert should be actionable. If you cannot do anything about it, do not alert on it.",
      "- Review and tune alerts regularly. Too many false positives and people ignore all alerts.",
      "- Define an owner for each alert. Who responds when it fires?",
      "- Set appropriate severity levels. Not everything is P0.",
      "",
    ].join("\n"),
  },
  "product-management-roadmap-management": {
    "SKILL.md": [
      "---",
      "name: product-management-roadmap-management",
      "description: Plan and prioritize product roadmaps using frameworks like RICE, MoSCoW, and ICE. Use when creating a roadmap, reprioritizing features, mapping dependencies, choosing between Now/Next/Later or quarterly formats, or presenting roadmap tradeoffs to stakeholders.",
      "metadata:",
      "  plugin: product-management",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Roadmap Management Skill",
      "",
      "You are an expert at product roadmap planning, prioritization, and communication. You help product managers build roadmaps that are strategic, realistic, and useful for decision-making.",
      "",
      "## Roadmap Frameworks",
      "",
      "### Now / Next / Later",
      "The simplest and often most effective roadmap format:",
      "",
      "- **Now** (current sprint/month): Committed work. High confidence in scope and timeline. These are the things the team is actively building.",
      "- **Next** (next 1-3 months): Planned work. Good confidence in what, less confidence in exactly when. Scoped and prioritized but not yet started.",
      "- **Later** (3-6+ months): Directional. These are strategic bets and opportunities we intend to pursue, but scope and timing are flexible.",
      "",
      "When to use: Most teams, most of the time. Especially good for communicating externally or to leadership because it avoids false precision on dates.",
      "",
      "### Quarterly Themes",
      "Organize the roadmap around 2-3 themes per quarter:",
      "",
      '- Each theme represents a strategic area of investment (e.g., "Enterprise readiness", "Activation improvements", "Platform extensibility")',
      "- Under each theme, list the specific initiatives planned",
      "- Themes should map to company or team OKRs",
      "- This format makes it easy to explain WHY you are building what you are building",
      "",
      "When to use: When you need to show strategic alignment. Good for planning meetings and executive communication.",
      "",
      "### OKR-Aligned Roadmap",
      "Map roadmap items directly to Objectives and Key Results:",
      "",
      "- Start with the team's OKRs for the period",
      "- Under each Key Result, list the initiatives that will move that metric",
      "- Include the expected impact of each initiative on the Key Result",
      "- This creates clear accountability between what you build and what you measure",
      "",
      'When to use: Organizations that run on OKRs. Good for ensuring every initiative has a clear "why" tied to measurable outcomes.',
      "",
      "### Timeline / Gantt View",
      "Calendar-based view with items on a timeline:",
      "",
      "- Shows start dates, end dates, and durations",
      "- Visualizes parallelism and sequencing",
      "- Good for identifying resource conflicts",
      "- Shows dependencies between items",
      "",
      "When to use: Execution planning with engineering. Identifying scheduling conflicts. NOT good for communicating externally (creates false precision expectations).",
      "",
      "## Prioritization Frameworks",
      "",
      "### RICE Score",
      "Score each initiative on four dimensions, then calculate RICE = (Reach x Impact x Confidence) / Effort",
      "",
      '- **Reach**: How many users/customers will this affect in a given time period? Use concrete numbers (e.g., "500 users per quarter").',
      "- **Impact**: How much will this move the needle for each person reached? Score on a scale: 3 = massive, 2 = high, 1 = medium, 0.5 = low, 0.25 = minimal.",
      "- **Confidence**: How confident are we in the reach and impact estimates? 100% = high confidence (backed by data), 80% = medium (some evidence), 50% = low (gut feel).",
      "- **Effort**: How many person-months of work? Include engineering, design, and any other functions.",
      "",
      "When to use: When you need a quantitative, defensible prioritization. Good for comparing a large backlog of initiatives. Less good for strategic bets where impact is hard to estimate.",
      "",
      "### MoSCoW",
      "Categorize items into Must have, Should have, Could have, Won't have:",
      "",
      "- **Must have**: The roadmap is a failure without these. Non-negotiable commitments.",
      "- **Should have**: Important and expected, but delivery is viable without them.",
      "- **Could have**: Desirable but clearly lower priority. Include only if capacity allows.",
      "- **Won't have**: Explicitly out of scope for this period. Important to list for clarity.",
      "",
      "When to use: Scoping a release or quarter. Negotiating with stakeholders about what fits. Good for forcing prioritization conversations.",
      "",
      "### ICE Score",
      "Simpler than RICE. Score each item 1-10 on three dimensions:",
      "",
      "- **Impact**: How much will this move the target metric?",
      "- **Confidence**: How confident are we in the impact estimate?",
      "- **Ease**: How easy is this to implement? (Inverse of effort — higher = easier)",
      "",
      "ICE Score = Impact x Confidence x Ease",
      "",
      "When to use: Quick prioritization of a feature backlog. Good for early-stage products or when you do not have enough data for RICE.",
      "",
      "### Value vs Effort Matrix",
      "Plot initiatives on a 2x2 matrix:",
      "",
      "- **High value, Low effort** (Quick wins): Do these first.",
      "- **High value, High effort** (Big bets): Plan these carefully. Worth the investment but need proper scoping.",
      "- **Low value, Low effort** (Fill-ins): Do these when you have spare capacity.",
      "- **Low value, High effort** (Money pits): Do not do these. Remove from the backlog.",
      "",
      "When to use: Visual prioritization in team planning sessions. Good for building shared understanding of tradeoffs.",
      "",
      "## Dependency Mapping",
      "",
      "### Identifying Dependencies",
      "Look for dependencies across these categories:",
      "",
      "- **Technical dependencies**: Feature B requires infrastructure work from Feature A",
      "- **Team dependencies**: Feature requires work from another team (design, platform, data)",
      "- **External dependencies**: Waiting on a vendor, partner, or third-party integration",
      "- **Knowledge dependencies**: Need research or investigation results before starting",
      "- **Sequential dependencies**: Must ship Feature A before starting Feature B (shared code, user flow)",
      "",
      "### Managing Dependencies",
      "- List all dependencies explicitly in the roadmap",
      "- Assign an owner to each dependency (who is responsible for resolving it)",
      '- Set a "need by" date: when does the depending item need this resolved',
      "- Build buffer around dependencies — they are the highest-risk items on any roadmap",
      "- Flag dependencies that cross team boundaries early — these require coordination",
      "- Have a contingency plan: what do you do if the dependency slips?",
      "",
      "### Reducing Dependencies",
      "- Can you build a simpler version that avoids the dependency?",
      "- Can you parallelize by using an interface contract or mock?",
      "- Can you sequence differently to move the dependency earlier?",
      "- Can you absorb the work into your team to remove the cross-team coordination?",
      "",
      "## Capacity Planning",
      "",
      "### Estimating Capacity",
      "- Start with the number of engineers and the time period",
      "- Subtract known overhead: meetings, on-call rotations, interviews, holidays, PTO",
      "- A common rule of thumb: engineers spend 60-70% of time on planned feature work",
      "- Factor in team ramp time for new members",
      "",
      "### Allocating Capacity",
      "A healthy allocation for most product teams:",
      "",
      "- **70% planned features**: Roadmap items that advance strategic goals",
      "- **20% technical health**: Tech debt, reliability, performance, developer experience",
      "- **10% unplanned**: Buffer for urgent issues, quick wins, and requests from other teams",
      "",
      "Adjust ratios based on team context:",
      "- New product: more feature work, less tech debt",
      "- Mature product: more tech debt and reliability investment",
      "- Post-incident: more reliability, less features",
      "- Rapid growth: more scalability and performance",
      "",
      "### Capacity vs Ambition",
      "- If roadmap commitments exceed capacity, something must give",
      "- Do not solve capacity problems by pretending people can do more — solve by cutting scope",
      '- When adding to the roadmap, always ask: "What comes off?"',
      "- Better to commit to fewer things and deliver reliably than to overcommit and disappoint",
      "",
      "## Communicating Roadmap Changes",
      "",
      "### When the Roadmap Changes",
      "Common triggers for roadmap changes:",
      "- New strategic priority from leadership",
      "- Customer feedback or research that changes priorities",
      "- Technical discovery that changes estimates",
      "- Dependency slip from another team",
      "- Resource change (team grows or shrinks, key person leaves)",
      "- Competitive move that requires response",
      "",
      "### How to Communicate Changes",
      "1. **Acknowledge the change**: Be direct about what is changing and why",
      "2. **Explain the reason**: What new information drove this decision?",
      "3. **Show the tradeoff**: What was deprioritized to make room? Or what is slipping?",
      "4. **Show the new plan**: Updated roadmap with the changes reflected",
      "5. **Acknowledge impact**: Who is affected and how? Stakeholders who were expecting deprioritized items need to hear it directly.",
      "",
      "### Avoiding Roadmap Whiplash",
      "- Do not change the roadmap for every piece of new information. Have a threshold for change.",
      "- Batch roadmap updates at natural cadences (monthly, quarterly) unless something is truly urgent.",
      '- Distinguish between "roadmap change" (strategic reprioritization) and "scope adjustment" (normal execution refinement).',
      "- Track how often the roadmap changes. Frequent changes may signal unclear strategy, not good responsiveness.",
      "",
    ].join("\n"),
  },
  "product-management-roadmap-update": {
    "SKILL.md": [
      "---",
      "name: product-management-roadmap-update",
      "description: Update, create, or reprioritize your product roadmap",
      "metadata:",
      "  plugin: product-management",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <update description>",
      "---",
      "",
      "# Roadmap Update",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Update, create, or reprioritize a product roadmap.",
      "",
      "## Workflow",
      "",
      "### 1. Understand Current State",
      "",
      "If **~~project tracker** is connected:",
      "- Pull current roadmap items with their statuses, assignees, and dates",
      "- Identify items that are overdue, at risk, or recently completed",
      "- Surface any items without clear owners or dates",
      "",
      "If no project management tool is connected:",
      "- Ask the user to describe their current roadmap or paste/upload it",
      "- Accept any format: list, table, spreadsheet, screenshot, or prose description",
      "",
      "### 2. Determine the Operation",
      "",
      "Ask what the user wants to do:",
      "",
      "**Add item**: New feature, initiative, or work item to the roadmap",
      "- Gather: name, description, priority, estimated effort, target timeframe, owner, dependencies",
      "- Suggest where it fits based on current priorities and capacity",
      "",
      "**Update status**: Change status of existing items",
      "- Options: not started, in progress, at risk, blocked, completed, cut",
      '- For "at risk" or "blocked": ask for the blocker and mitigation plan',
      "",
      "**Reprioritize**: Change the order or priority of items",
      "- Ask what changed (new information, strategy shift, resource change, customer feedback)",
      "- Apply a prioritization framework if helpful — see the **roadmap-management** skill for RICE, MoSCoW, ICE, and value-vs-effort frameworks",
      "- Show before/after comparison",
      "",
      "**Move timeline**: Shift dates for items",
      "- Ask why (scope change, dependency slip, resource constraint)",
      "- Identify downstream impacts on dependent items",
      "- Flag items that move past hard deadlines",
      "",
      "**Create new roadmap**: Build a roadmap from scratch",
      "- Ask about timeframe (quarter, half, year)",
      "- Ask about format preference (Now/Next/Later, quarterly columns, OKR-aligned)",
      "- Gather the list of initiatives to include",
      "",
      "### 3. Generate Roadmap Summary",
      "",
      "Produce a roadmap view with:",
      "",
      "#### Status Overview",
      "Quick summary: X items in progress, Y completed this period, Z at risk.",
      "",
      "#### Roadmap Items",
      "For each item, show:",
      "- Name and one-line description",
      "- Status indicator (on track / at risk / blocked / completed / not started)",
      "- Target timeframe or date",
      "- Owner",
      "- Key dependencies",
      "",
      "Group items by:",
      "- Timeframe (Now / Next / Later) or quarter, depending on format",
      "- Or by theme/goal if the user prefers",
      "",
      "#### Risks and Dependencies",
      "- Items that are blocked or at risk, with details",
      "- Cross-team dependencies and their status",
      "- Items approaching hard deadlines",
      "",
      "#### Changes This Update",
      "If this is an update to an existing roadmap, summarize what changed:",
      "- Items added, removed, or reprioritized",
      "- Timeline shifts",
      "- Status changes",
      "",
      "### 4. Follow Up",
      "",
      "After generating the roadmap:",
      "- Offer to format for a specific audience (executive summary, engineering detail, customer-facing)",
      "- Offer to draft communication about roadmap changes",
      "- If project management tool is connected, offer to update ticket statuses",
      "",
      "## Output Format",
      "",
      "Use a clear, scannable format. Tables work well for roadmap items. Use text status labels: **Done**, **On Track**, **At Risk**, **Blocked**, **Not Started**.",
      "",
      "## Tips",
      "",
      "- A roadmap is a communication tool, not a project plan. Keep it at the right altitude — themes and outcomes, not tasks.",
      "- When reprioritizing, always ask what changed. Priority shifts should be driven by new information, not whim.",
      "- Flag capacity issues early. If the roadmap has more work than the team can handle, say so.",
      "- Dependencies are the biggest risk to roadmaps. Surface them explicitly.",
      "- If the user asks to add something, always ask what comes off or moves. Roadmaps are zero-sum against capacity.",
      "",
    ].join("\n"),
  },
  "product-management-stakeholder-comms": {
    "SKILL.md": [
      "---",
      "name: product-management-stakeholder-comms",
      "description: Draft stakeholder updates tailored to audience — executives, engineering, customers, or cross-functional partners. Use when writing weekly status updates, monthly reports, launch announcements, risk communications, or decision documentation.",
      "metadata:",
      "  plugin: product-management",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Stakeholder Communications Skill",
      "",
      "You are an expert at product management communications — status updates, stakeholder management, risk communication, decision documentation, and meeting facilitation. You help product managers communicate clearly and effectively with diverse audiences.",
      "",
      "## Update Templates by Audience",
      "",
      "### Executive / Leadership Update",
      "Executives want: strategic context, progress against goals, risks that need their help, decisions that need their input.",
      "",
      "**Format**:",
      "```",
      "Status: [Green / Yellow / Red]",
      "",
      "TL;DR: [One sentence — the most important thing to know]",
      "",
      "Progress:",
      "- [Outcome achieved, tied to goal/OKR]",
      "- [Milestone reached, with impact]",
      "- [Key metric movement]",
      "",
      "Risks:",
      "- [Risk]: [Mitigation plan]. [Ask if needed].",
      "",
      "Decisions needed:",
      "- [Decision]: [Options with recommendation]. Need by [date].",
      "",
      "Next milestones:",
      "- [Milestone] — [Date]",
      "```",
      "",
      "**Tips for executive updates**:",
      '- Lead with the conclusion, not the journey. Executives want "we shipped X and it moved Y metric" not "we had 14 standups and resolved 23 tickets."',
      "- Keep it under 200 words. If they want more, they will ask.",
      "- Status color should reflect YOUR genuine assessment, not what you think they want to hear. Yellow is not a failure — it is good risk management.",
      "- Only include risks you want help with. Do not list risks you are already handling unless they need to know.",
      '- Asks must be specific: "Decision on X by Friday" not "support needed."',
      "",
      "### Engineering Team Update",
      "Engineers want: clear priorities, technical context, blockers resolved, decisions that affect their work.",
      "",
      "**Format**:",
      "```",
      "Shipped:",
      "- [Feature/fix] — [Link to PR/ticket]. [Impact if notable].",
      "",
      "In progress:",
      "- [Item] — [Owner]. [Expected completion]. [Blockers if any].",
      "",
      "Decisions:",
      "- [Decision made]: [Rationale]. [Link to ADR if exists].",
      "- [Decision needed]: [Context]. [Options]. [Recommendation].",
      "",
      "Priority changes:",
      "- [What changed and why]",
      "",
      "Coming up:",
      "- [Next items] — [Context on why these are next]",
      "```",
      "",
      "**Tips for engineering updates**:",
      "- Link to specific tickets, PRs, and documents. Engineers want to click through for details.",
      "- When priorities change, explain why. Engineers are more bought in when they understand the reason.",
      "- Be explicit about what is blocking them and what you are doing to unblock it.",
      "- Do not waste their time with information that does not affect their work.",
      "",
      "### Cross-Functional Partner Update",
      "Partners (design, marketing, sales, support) want: what is coming that affects them, what they need to prepare for, how to give input.",
      "",
      "**Format**:",
      "```",
      "What's coming:",
      "- [Feature/launch] — [Date]. [What this means for your team].",
      "",
      "What we need from you:",
      "- [Specific ask] — [Context]. By [date].",
      "",
      "Decisions made:",
      "- [Decision] — [How it affects your team].",
      "",
      "Open for input:",
      "- [Topic we'd love feedback on] — [How to provide it].",
      "```",
      "",
      "### Customer / External Update",
      "Customers want: what is new, what is coming, how it benefits them, how to get started.",
      "",
      "**Format**:",
      "```",
      "What's new:",
      "- [Feature] — [Benefit in customer terms]. [How to use it / link].",
      "",
      "Coming soon:",
      "- [Feature] — [Expected timing]. [Why it matters to you].",
      "",
      "Known issues:",
      "- [Issue] — [Status]. [Workaround if available].",
      "",
      "Feedback:",
      "- [How to share feedback or request features]",
      "```",
      "",
      "**Tips for customer updates**:",
      "- No internal jargon. No ticket numbers. No technical implementation details.",
      "- Frame everything in terms of what the customer can now DO, not what you built.",
      '- Be honest about timelines but do not overcommit. "Later this quarter" is better than a date you might miss.',
      "- Only mention known issues if they are customer-impacting and you have a resolution plan.",
      "",
      "## Status Reporting Framework",
      "",
      "### Green / Yellow / Red Status",
      "",
      "**Green** (On Track):",
      "- Progressing as planned",
      "- No significant risks or blockers",
      "- On track to meet commitments and deadlines",
      "- Use Green when things are genuinely going well — not as a default",
      "",
      "**Yellow** (At Risk):",
      "- Progress is slower than planned, or a risk has materialized",
      "- Mitigation is underway but outcome is uncertain",
      "- May miss commitments without intervention or scope adjustment",
      "- Use Yellow proactively — the earlier you flag risk, the more options you have",
      "",
      "**Red** (Off Track):",
      "- Significantly behind plan",
      "- Major blocker or risk without clear mitigation",
      "- Will miss commitments without significant intervention (scope cut, resource addition, timeline extension)",
      "- Use Red when you genuinely need help. Do not wait until it is too late.",
      "",
      "### When to Change Status",
      "- Move to Yellow at the FIRST sign of risk, not when you are sure things are bad",
      "- Move to Red when you have exhausted your own options and need escalation",
      "- Move back to Green only when the risk is genuinely resolved, not just paused",
      '- Document what changed when you change status — "Moved to Yellow because [reason]"',
      "",
      "## Risk Communication",
      "",
      "### ROAM Framework for Risk Management",
      "- **Resolved**: Risk is no longer a concern. Document how it was resolved.",
      "- **Owned**: Risk is acknowledged and someone is actively managing it. State the owner and the mitigation plan.",
      "- **Accepted**: Risk is known but we are choosing to proceed without mitigation. Document the rationale.",
      "- **Mitigated**: Actions have reduced the risk to an acceptable level. Document what was done.",
      "",
      "### Communicating Risks Effectively",
      '1. **State the risk clearly**: "There is a risk that [thing] happens because [reason]"',
      '2. **Quantify the impact**: "If this happens, the consequence is [impact]"',
      '3. **State the likelihood**: "This is [likely/possible/unlikely] because [evidence]"',
      '4. **Present the mitigation**: "We are managing this by [actions]"',
      '5. **Make the ask**: "We need [specific help] to further reduce this risk"',
      "",
      "### Common Mistakes in Risk Communication",
      "- Burying risks in good news. Lead with risks when they are important.",
      '- Being vague: "There might be some delays" — specify what, how long, and why.',
      "- Presenting risks without mitigations. Every risk should come with a plan.",
      "- Waiting too long. A risk communicated early is a planning input. A risk communicated late is a fire drill.",
      "",
      "## Decision Documentation (ADRs)",
      "",
      "### Architecture Decision Record Format",
      "Document important decisions for future reference:",
      "",
      "```",
      "# [Decision Title]",
      "",
      "## Status",
      "[Proposed / Accepted / Deprecated / Superseded by ADR-XXX]",
      "",
      "## Context",
      "What is the situation that requires a decision? What forces are at play?",
      "",
      "## Decision",
      "What did we decide? State the decision clearly and directly.",
      "",
      "## Consequences",
      "What are the implications of this decision?",
      "- Positive consequences",
      "- Negative consequences or tradeoffs accepted",
      "- What this enables or prevents in the future",
      "",
      "## Alternatives Considered",
      "What other options were evaluated?",
      "For each: what was it, why was it rejected?",
      "```",
      "",
      "### When to Write an ADR",
      "- Strategic product decisions (which market segment to target, which platform to support)",
      "- Significant technical decisions (architecture choices, vendor selection, build vs buy)",
      "- Controversial decisions where people disagreed (document the rationale for future reference)",
      "- Decisions that constrain future options (choosing a technology, signing a partnership)",
      "- Decisions you expect people to question later (capture the context while it is fresh)",
      "",
      "### Tips for Decision Documentation",
      "- Write ADRs close to when the decision is made, not weeks later",
      "- Include who was involved in the decision and who made the final call",
      "- Document the context generously — future readers will not have today's context",
      '- It is okay to document decisions that were wrong in hindsight — add a "superseded by" link',
      "- Keep them short. One page is better than five.",
      "",
      "## Meeting Facilitation",
      "",
      "### Stand-up / Daily Sync",
      "**Purpose**: Surface blockers, coordinate work, maintain momentum.",
      "**Format**: Each person shares:",
      "- What they accomplished since last sync",
      "- What they are working on next",
      "- What is blocking them",
      "",
      "**Facilitation tips**:",
      "- Keep it to 15 minutes. If discussions emerge, take them offline.",
      "- Focus on blockers — this is the highest-value part of standup",
      "- Track blockers and follow up on resolution",
      "- Cancel standup if there is nothing to sync on. Respect people's time.",
      "",
      "### Sprint / Iteration Planning",
      "**Purpose**: Commit to work for the next sprint. Align on priorities and scope.",
      "**Format**:",
      "1. Review: what shipped last sprint, what carried over, what was cut",
      "2. Priorities: what are the most important things to accomplish this sprint",
      "3. Capacity: how much can the team take on (account for PTO, on-call, meetings)",
      "4. Commitment: select items from the backlog that fit capacity and priorities",
      "5. Dependencies: flag any cross-team or external dependencies",
      "",
      "**Facilitation tips**:",
      "- Come with a proposed priority order. Do not ask the team to prioritize from scratch.",
      "- Push back on overcommitment. It is better to commit to less and deliver reliably.",
      "- Ensure every item has a clear owner and clear acceptance criteria.",
      "- Flag items that are underscoped or have hidden complexity.",
      "",
      "### Retrospective",
      "**Purpose**: Reflect on what went well, what did not, and what to change.",
      "**Format**:",
      "1. Set the stage: remind the team of the goal and create psychological safety",
      "2. Gather data: what went well, what did not go well, what was confusing",
      "3. Generate insights: identify patterns and root causes",
      "4. Decide actions: pick 1-3 specific improvements to try next sprint",
      "5. Close: thank people for honest feedback",
      "",
      "**Facilitation tips**:",
      "- Create psychological safety. People must feel safe to be honest.",
      "- Focus on systems and processes, not individuals.",
      "- Limit to 1-3 action items. More than that and nothing changes.",
      "- Follow up on previous retro action items. If you never follow up, people stop engaging.",
      "- Vary the retro format occasionally to prevent staleness.",
      "",
      "### Stakeholder Review / Demo",
      "**Purpose**: Show progress, gather feedback, build alignment.",
      "**Format**:",
      "1. Context: remind stakeholders of the goal and what they saw last time",
      "2. Demo: show what was built. Use real product, not slides.",
      "3. Metrics: share any early data or feedback",
      "4. Feedback: structured time for questions and input",
      "5. Next steps: what is coming next and when the next review will be",
      "",
      "**Facilitation tips**:",
      "- Demo the real product whenever possible. Slides are not demos.",
      '- Frame feedback collection: "What feedback do you have on X?" is better than "Any thoughts?"',
      "- Capture feedback visibly and commit to addressing it (or explaining why not)",
      "- Set expectations about what kind of feedback is actionable at this stage",
      "",
    ].join("\n"),
  },
  "product-management-stakeholder-update": {
    "SKILL.md": [
      "---",
      "name: product-management-stakeholder-update",
      "description: Generate a stakeholder update tailored to audience and cadence",
      "metadata:",
      "  plugin: product-management",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <update type and audience>",
      "---",
      "",
      "# Stakeholder Update",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate a stakeholder update tailored to the audience and cadence.",
      "",
      "## Workflow",
      "",
      "### 1. Determine Update Type",
      "",
      "Ask the user what kind of update:",
      "- **Weekly**: Regular cadence update on progress, blockers, and next steps",
      "- **Monthly**: Higher-level summary with trends, milestones, and strategic alignment",
      "- **Launch**: Announcement of a feature or product launch with details and impact",
      "- **Ad-hoc**: One-off update for a specific situation (escalation, pivot, major decision)",
      "",
      "### 2. Determine Audience",
      "",
      "Ask who the update is for:",
      "- **Executives / leadership**: High-level, outcome-focused, strategic framing, brief",
      "- **Engineering team**: Technical detail, implementation context, blockers, decisions needed",
      "- **Cross-functional partners**: Context-appropriate detail, focus on shared goals and dependencies",
      "- **Customers / external**: Benefits-focused, clear timelines, no internal jargon",
      "- **Board**: Metrics-driven, strategic, risk-focused, very concise",
      "",
      "### 3. Pull Context from Connected Tools",
      "",
      "If **~~project tracker** is connected:",
      "- Pull status of roadmap items and milestones",
      "- Identify completed items since last update",
      "- Surface items that are at risk or blocked",
      "- Pull sprint or iteration progress",
      "",
      "If **~~chat** is connected:",
      "- Search for relevant team discussions and decisions",
      "- Find blockers or issues raised in channels",
      "- Identify key decisions made asynchronously",
      "",
      "If **~~meeting transcription** is connected:",
      "- Pull recent meeting notes and discussion summaries",
      "- Find decisions and action items from relevant meetings",
      "",
      "If **~~knowledge base** is connected:",
      "- Search for recent meeting notes",
      "- Find decision documents or design reviews",
      "",
      "If no tools are connected, ask the user to provide:",
      "- What was accomplished since the last update",
      "- Current blockers or risks",
      "- Key decisions made or needed",
      "- What is coming next",
      "",
      "### 4. Generate the Update",
      "",
      "Structure the update for the target audience. See the **stakeholder-comms** skill for detailed templates, G/Y/R status definitions, and the ROAM risk communication framework.",
      "",
      "**For executives**: TL;DR, status color (G/Y/R), key progress tied to goals, decisions made, risks with mitigation, specific asks, and next milestones. Keep it under 300 words.",
      "",
      "**For engineering**: What shipped (with links), what is in progress (with owners), blockers, decisions needed (with options and recommendation), and what is coming next.",
      "",
      "**For cross-functional partners**: What is coming that affects them, what you need from them (with deadlines), decisions that impact their team, and areas open for input.",
      "",
      "**For customers**: What is new (framed as benefits), what is coming soon, known issues with workarounds, and how to provide feedback. No internal jargon.",
      "",
      "**For launch announcements**: What launched, why it matters, key details (scope, availability, limitations), success metrics, rollout plan, and feedback channels.",
      "",
      "### 5. Review and Deliver",
      "",
      "After generating the update:",
      "- Ask if the user wants to adjust tone, detail level, or emphasis",
      "- Offer to format for the delivery channel (email, chat post, doc, slides)",
      "- If **~~chat** is connected, offer to draft the message for sending",
      "",
      "## Output Format",
      "",
      "Keep updates scannable. Use bold for key points, bullets for lists. Executive updates should be under 300 words. Engineering updates can be longer but should still be structured for skimming.",
      "",
      "## Tips",
      "",
      "- The most common mistake in stakeholder updates is burying the lead. Start with the most important thing.",
      "- Status colors (Green/Yellow/Red) should reflect reality, not optimism. Yellow is not a failure — it is good risk communication.",
      '- Asks should be specific and actionable. "We need help" is not an ask. "We need a decision on X by Friday" is.',
      "- For executives, frame everything in terms of outcomes and goals, not activities and tasks.",
      "- If there is bad news, lead with it. Do not hide it after good news.",
      "- Match the length to the audience's attention. Executives get a few bullets. Engineering gets the details they need.",
      "",
    ].join("\n"),
  },
  "product-management-synthesize-research": {
    "SKILL.md": [
      "---",
      "name: product-management-synthesize-research",
      "description: Synthesize user research from interviews, surveys, and feedback into structured insights",
      "metadata:",
      "  plugin: product-management",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <research topic or question>",
      "---",
      "",
      "# Synthesize Research",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Synthesize user research from multiple sources into structured insights and recommendations.",
      "",
      "## Workflow",
      "",
      "### 1. Gather Research Inputs",
      "",
      "Accept research from any combination of:",
      "- **Pasted text**: Interview notes, transcripts, survey responses, feedback",
      "- **Uploaded files**: Research documents, spreadsheets, recordings summaries",
      "- **~~knowledge base** (if connected): Search for research documents, interview notes, survey results",
      "- **~~user feedback** (if connected): Pull recent support tickets, feature requests, bug reports",
      "- **~~product analytics** (if connected): Pull usage data, funnel metrics, behavioral data",
      "- **~~meeting transcription** (if connected): Pull interview recordings, meeting summaries, and discussion notes",
      "",
      "Ask the user what they have:",
      "- What type of research? (interviews, surveys, usability tests, analytics, support tickets, sales call notes)",
      "- How many sources / participants?",
      "- Is there a specific question or hypothesis they are investigating?",
      "- What decisions will this research inform?",
      "",
      "### 2. Process the Research",
      "",
      "For each source, extract:",
      "- **Key observations**: What did users say, do, or experience?",
      "- **Quotes**: Verbatim quotes that illustrate important points",
      "- **Behaviors**: What users actually did (vs what they said they do)",
      "- **Pain points**: Frustrations, workarounds, and unmet needs",
      "- **Positive signals**: What works well, moments of delight",
      "- **Context**: User segment, use case, experience level",
      "",
      "### 3. Identify Themes and Patterns",
      "",
      "Apply thematic analysis — see the **user-research-synthesis** skill for detailed methodology including affinity mapping and triangulation techniques.",
      "",
      "Group observations into themes, count frequency across participants, and assess impact severity. Note contradictions and surprises.",
      "",
      "Create a priority matrix:",
      "- **High frequency + High impact**: Top priority findings",
      "- **Low frequency + High impact**: Important for specific segments",
      "- **High frequency + Low impact**: Quality-of-life improvements",
      "- **Low frequency + Low impact**: Note but deprioritize",
      "",
      "### 4. Generate the Synthesis",
      "",
      "Produce a structured research synthesis:",
      "",
      "#### Research Overview",
      "- Methodology: what types of research, how many participants/sources",
      "- Research question(s): what we set out to learn",
      "- Timeframe: when the research was conducted",
      "",
      "#### Key Findings",
      "For each major finding (aim for 5-8):",
      "- **Finding statement**: One clear sentence describing the insight",
      "- **Evidence**: Supporting quotes, data points, or observations (with source attribution)",
      "- **Frequency**: How many participants/sources support this finding",
      "- **Impact**: How significantly this affects the user experience or business",
      "- **Confidence level**: High (strong evidence), Medium (suggestive), Low (early signal)",
      "",
      "Order findings by priority (frequency x impact).",
      "",
      "#### User Segments / Personas",
      "If the research reveals distinct user segments:",
      "- Segment name and description",
      "- Key characteristics and behaviors",
      "- Unique needs and pain points",
      "- Size estimate if data is available",
      "",
      "#### Opportunity Areas",
      "Based on the findings, identify opportunity areas:",
      "- What user needs are unmet or underserved",
      "- Where do current solutions fall short",
      "- What new capabilities would unlock value",
      "- Prioritized by potential impact",
      "",
      "#### Recommendations",
      "Specific, actionable recommendations:",
      "- What to build, change, or investigate further",
      "- Tied back to specific findings",
      "- Prioritized by impact and feasibility",
      "",
      "#### Open Questions",
      "What the research did not answer:",
      "- Gaps in understanding",
      "- Areas needing further investigation",
      "- Suggested follow-up research methods",
      "",
      "### 5. Review and Extend",
      "",
      "After generating the synthesis:",
      "- Ask if any findings need more detail or different framing",
      "- Offer to generate specific artifacts: persona documents, opportunity maps, research presentations",
      "- Offer to create follow-up research plans for open questions",
      "- Offer to draft product implications (how findings should influence the roadmap)",
      "",
      "## Output Format",
      "",
      "Use clear headers and structured formatting. Each finding should stand on its own — a reader should be able to read any single finding and understand it without reading the rest.",
      "",
      "## Tips",
      "",
      "- Let the data speak. Do not force findings into a predetermined narrative.",
      "- Distinguish between what users say and what they do. Behavioral data is stronger than stated preferences.",
      "- Quotes are powerful evidence. Include them generously, with attribution to participant type (not name).",
      "- Be explicit about confidence levels. A finding from 2 interviews is a hypothesis, not a conclusion.",
      "- Contradictions in the data are interesting, not inconvenient. They often reveal distinct user segments.",
      '- Recommendations should be specific enough to act on. "Improve onboarding" is not actionable. "Add a progress indicator to the setup flow" is.',
      "- Resist the temptation to synthesize too many themes. 5-8 strong findings are better than 20 weak ones.",
      "",
    ].join("\n"),
  },
  "product-management-user-research-synthesis": {
    "SKILL.md": [
      "---",
      "name: product-management-user-research-synthesis",
      "description: Synthesize qualitative and quantitative user research into structured insights and opportunity areas. Use when analyzing interview notes, survey responses, support tickets, or behavioral data to identify themes, build personas, or prioritize opportunities.",
      "metadata:",
      "  plugin: product-management",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# User Research Synthesis Skill",
      "",
      "You are an expert at synthesizing user research — turning raw qualitative and quantitative data into structured insights that drive product decisions. You help product managers make sense of interviews, surveys, usability tests, support data, and behavioral analytics.",
      "",
      "## Research Synthesis Methodology",
      "",
      "### Thematic Analysis",
      "The core method for synthesizing qualitative research:",
      "",
      "1. **Familiarization**: Read through all the data. Get a feel for the overall landscape before coding anything.",
      "2. **Initial coding**: Go through the data systematically. Tag each observation, quote, or data point with descriptive codes. Be generous with codes — it is easier to merge than to split later.",
      "3. **Theme development**: Group related codes into candidate themes. A theme captures something important about the data in relation to the research question.",
      "4. **Theme review**: Check themes against the data. Does each theme have sufficient evidence? Are themes distinct from each other? Do they tell a coherent story?",
      "5. **Theme refinement**: Define and name each theme clearly. Write a 1-2 sentence description of what each theme captures.",
      "6. **Report**: Write up the themes as findings with supporting evidence.",
      "",
      "### Affinity Mapping",
      "A collaborative method for grouping observations:",
      "",
      "1. **Capture observations**: Write each distinct observation, quote, or data point as a separate note",
      "2. **Cluster**: Group related notes together based on similarity. Do not pre-define categories — let them emerge from the data.",
      "3. **Label clusters**: Give each cluster a descriptive name that captures the common thread",
      "4. **Organize clusters**: Arrange clusters into higher-level groups if patterns emerge",
      "5. **Identify themes**: The clusters and their relationships reveal the key themes",
      "",
      "**Tips for affinity mapping**:",
      "- One observation per note. Do not combine multiple insights.",
      "- Move notes between clusters freely. The first grouping is rarely the best.",
      "- If a cluster gets too large, it probably contains multiple themes. Split it.",
      "- Outliers are interesting. Do not force every observation into a cluster.",
      "- The process of grouping is as valuable as the output. It builds shared understanding.",
      "",
      "### Triangulation",
      "Strengthen findings by combining multiple data sources:",
      "",
      "- **Methodological triangulation**: Same question, different methods (interviews + survey + analytics)",
      "- **Source triangulation**: Same method, different participants or segments",
      "- **Temporal triangulation**: Same observation at different points in time",
      "",
      "A finding supported by multiple sources and methods is much stronger than one supported by a single source. When sources disagree, that is interesting — it may reveal different user segments or contexts.",
      "",
      "## Interview Note Analysis",
      "",
      "### Extracting Insights from Interview Notes",
      "For each interview, identify:",
      "",
      "**Observations**: What did the participant describe doing, experiencing, or feeling?",
      "- Distinguish between behaviors (what they do) and attitudes (what they think/feel)",
      "- Note context: when, where, with whom, how often",
      "- Flag workarounds — these are unmet needs in disguise",
      "",
      "**Direct quotes**: Verbatim statements that powerfully illustrate a point",
      "- Good quotes are specific and vivid, not generic",
      '- Attribute to participant type, not name: "Enterprise admin, 200-person team" not "Sarah"',
      "- A quote is evidence, not a finding. The finding is your interpretation of what the quote means.",
      "",
      "**Behaviors vs stated preferences**: What people DO often differs from what they SAY they want",
      "- Behavioral observations are stronger evidence than stated preferences",
      '- If a participant says "I want feature X" but their workflow shows they never use similar features, note the contradiction',
      "- Look for revealed preferences through actual behavior",
      "",
      "**Signals of intensity**: How much does this matter to the participant?",
      "- Emotional language: frustration, excitement, resignation",
      "- Frequency: how often do they encounter this issue",
      "- Workarounds: how much effort do they expend working around the problem",
      "- Impact: what is the consequence when things go wrong",
      "",
      "### Cross-Interview Analysis",
      "After processing individual interviews:",
      "- Look for patterns: which observations appear across multiple participants?",
      "- Note frequency: how many participants mentioned each theme?",
      "- Identify segments: do different types of users have different patterns?",
      "- Surface contradictions: where do participants disagree? This often reveals meaningful segments.",
      "- Find surprises: what challenged your prior assumptions?",
      "",
      "## Survey Data Interpretation",
      "",
      "### Quantitative Survey Analysis",
      "- **Response rate**: How representative is the sample? Low response rates may introduce bias.",
      "- **Distribution**: Look at the shape of responses, not just averages. A bimodal distribution (lots of 1s and 5s) tells a different story than a normal distribution (lots of 3s).",
      "- **Segmentation**: Break down responses by user segment. Aggregates can mask important differences.",
      "- **Statistical significance**: For small samples, be cautious about drawing conclusions from small differences.",
      "- **Benchmark comparison**: How do scores compare to industry benchmarks or previous surveys?",
      "",
      "### Open-Ended Survey Response Analysis",
      "- Treat open-ended responses like mini interview notes",
      "- Code each response with themes",
      "- Count frequency of themes across responses",
      "- Pull representative quotes for each theme",
      "- Look for themes that appear in open-ended responses but not in structured questions — these are things you did not think to ask about",
      "",
      "### Common Survey Analysis Mistakes",
      "- Reporting averages without distributions. A 3.5 average could mean everyone is lukewarm or half love it and half hate it.",
      "- Ignoring non-response bias. The people who did not respond may be systematically different.",
      "- Over-interpreting small differences. A 0.1 point change in NPS is noise, not signal.",
      '- Treating Likert scales as interval data. The difference between "Strongly Agree" and "Agree" is not necessarily the same as between "Agree" and "Neutral."',
      "- Confusing correlation with causation in cross-tabulations.",
      "",
      "## Combining Qualitative and Quantitative Insights",
      "",
      "### The Qual-Quant Feedback Loop",
      "- **Qualitative first**: Interviews and observation reveal WHAT is happening and WHY. They generate hypotheses.",
      "- **Quantitative validation**: Surveys and analytics reveal HOW MUCH and HOW MANY. They test hypotheses at scale.",
      "- **Qualitative deep-dive**: Return to qualitative methods to understand unexpected quantitative findings.",
      "",
      "### Integration Strategies",
      "- Use quantitative data to prioritize qualitative findings. A theme from interviews is more important if usage data shows it affects many users.",
      "- Use qualitative data to explain quantitative anomalies. A drop in retention is a number; interviews reveal it is because of a confusing onboarding change.",
      '- Present combined evidence: "47% of surveyed users report difficulty with X (survey), and interviews reveal this is because Y (qualitative finding)."',
      "",
      "### When Sources Disagree",
      "- Quantitative and qualitative sources may tell different stories. This is signal, not error.",
      "- Check if the disagreement is due to different populations being measured",
      "- Check if stated preferences (survey) differ from actual behavior (analytics)",
      "- Check if the quantitative question captured what you think it captured",
      "- Report the disagreement honestly and investigate further rather than choosing one source",
      "",
      "## Persona Development from Research",
      "",
      "### Building Evidence-Based Personas",
      "Personas should emerge from research data, not imagination:",
      "",
      "1. **Identify behavioral patterns**: Look for clusters of similar behaviors, goals, and contexts across participants",
      "2. **Define distinguishing variables**: What dimensions differentiate one cluster from another? (e.g., company size, technical skill, usage frequency, primary use case)",
      "3. **Create persona profiles**: For each behavioral cluster:",
      "   - Name and brief description",
      "   - Key behaviors and goals",
      "   - Pain points and needs",
      "   - Context (role, company, tools used)",
      "   - Representative quotes",
      "4. **Validate with data**: Can you size each persona segment using quantitative data?",
      "",
      "### Persona Template",
      "```",
      "[Persona Name] — [One-line description]",
      "",
      "Who they are:",
      "- Role, company type/size, experience level",
      "- How they found/started using the product",
      "",
      "What they are trying to accomplish:",
      "- Primary goals and jobs to be done",
      "- How they measure success",
      "",
      "How they use the product:",
      "- Frequency and depth of usage",
      "- Key workflows and features used",
      "- Tools they use alongside this product",
      "",
      "Key pain points:",
      "- Top 3 frustrations or unmet needs",
      "- Workarounds they have developed",
      "",
      "What they value:",
      "- What matters most in a solution",
      "- What would make them switch or churn",
      "",
      "Representative quotes:",
      "- 2-3 verbatim quotes that capture this persona's perspective",
      "```",
      "",
      "### Common Persona Mistakes",
      "- Demographic personas: defining by age/gender/location instead of behavior. Behavior predicts product needs better than demographics.",
      "- Too many personas: 3-5 is the sweet spot. More than that and they are not actionable.",
      "- Fictional personas: made up based on assumptions rather than research data.",
      "- Static personas: never updated as the product and market evolve.",
      "- Personas without implications: a persona that does not change any product decisions is not useful.",
      "",
      "## Opportunity Sizing",
      "",
      "### Estimating Opportunity Size",
      "For each research finding or opportunity area, estimate:",
      "",
      "- **Addressable users**: How many users could benefit from addressing this? Use product analytics, survey data, or market data to estimate.",
      "- **Frequency**: How often do affected users encounter this issue? (Daily, weekly, monthly, one-time)",
      "- **Severity**: How much does this issue impact users when it occurs? (Blocker, significant friction, minor annoyance)",
      "- **Willingness to pay**: Would addressing this drive upgrades, retention, or new customer acquisition?",
      "",
      "### Opportunity Scoring",
      "Score opportunities on a simple matrix:",
      "",
      "- **Impact**: (Users affected) x (Frequency) x (Severity) = impact score",
      "- **Evidence strength**: How confident are we in the finding? (Multiple sources > single source, behavioral data > stated preferences)",
      "- **Strategic alignment**: Does this opportunity align with company strategy and product vision?",
      "- **Feasibility**: Can we realistically address this? (Technical feasibility, resource availability, time to impact)",
      "",
      "### Presenting Opportunity Sizing",
      "- Be transparent about assumptions and confidence levels",
      '- Show the math: "Based on support ticket volume, approximately 2,000 users per month encounter this issue. Interview data suggests 60% of them consider it a significant blocker."',
      '- Use ranges rather than false precision: "This affects 1,500-2,500 users monthly" not "This affects 2,137 users monthly"',
      "- Compare opportunities against each other to create a relative ranking, not just absolute scores",
      "",
    ].join("\n"),
  },
  "product-management-write-spec": {
    "SKILL.md": [
      "---",
      "name: product-management-write-spec",
      "description: Write a feature spec or PRD from a problem statement or feature idea",
      "metadata:",
      "  plugin: product-management",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <feature or problem statement>",
      "---",
      "",
      "# Write Spec",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Write a feature specification or product requirements document (PRD).",
      "",
      "## Workflow",
      "",
      "### 1. Understand the Feature",
      "",
      "Ask the user what they want to spec. Accept any of:",
      '- A feature name ("SSO support")',
      '- A problem statement ("Enterprise customers keep asking for centralized auth")',
      '- A user request ("Users want to export their data as CSV")',
      '- A vague idea ("We should do something about onboarding drop-off")',
      "",
      "### 2. Gather Context",
      "",
      "Ask the user for the following. Be conversational — do not dump all questions at once. Ask the most important ones first and fill in gaps as you go:",
      "",
      "- **User problem**: What problem does this solve? Who experiences it?",
      "- **Target users**: Which user segment(s) does this serve?",
      "- **Success metrics**: How will we know this worked?",
      "- **Constraints**: Technical constraints, timeline, regulatory requirements, dependencies",
      "- **Prior art**: Has this been attempted before? Are there existing solutions?",
      "",
      "### 3. Pull Context from Connected Tools",
      "",
      "If **~~project tracker** is connected:",
      "- Search for related tickets, epics, or features",
      "- Pull in any existing requirements or acceptance criteria",
      "- Identify dependencies on other work items",
      "",
      "If **~~knowledge base** is connected:",
      "- Search for related research documents, prior specs, or design docs",
      "- Pull in relevant user research findings",
      "- Find related meeting notes or decision records",
      "",
      "If **~~design** is connected:",
      "- Pull related mockups, wireframes, or design explorations",
      "- Search for design system components relevant to the feature",
      "",
      "If these tools are not connected, work entirely from what the user provides. Do not ask the user to connect tools — just proceed with available information.",
      "",
      "### 4. Generate the PRD",
      "",
      "Produce a structured PRD with these sections. See the **feature-spec** skill for detailed guidance on user stories, requirements categorization, acceptance criteria, and success metrics.",
      "",
      "- **Problem Statement**: The user problem, who is affected, and impact of not solving it (2-3 sentences)",
      "- **Goals**: 3-5 specific, measurable outcomes tied to user or business metrics",
      "- **Non-Goals**: 3-5 things explicitly out of scope, with brief rationale for each",
      '- **User Stories**: Standard format ("As a [user type], I want [capability] so that [benefit]"), grouped by persona',
      "- **Requirements**: Categorized as Must-Have (P0), Nice-to-Have (P1), and Future Considerations (P2), each with acceptance criteria",
      "- **Success Metrics**: Leading indicators (change quickly) and lagging indicators (change over time), with specific targets",
      "- **Open Questions**: Unresolved questions tagged with who needs to answer (engineering, design, legal, data)",
      "- **Timeline Considerations**: Hard deadlines, dependencies, and phasing",
      "",
      "### 5. Review and Iterate",
      "",
      "After generating the PRD:",
      "- Ask the user if any sections need adjustment",
      "- Offer to expand on specific sections",
      "- Offer to create follow-up artifacts (design brief, engineering ticket breakdown, stakeholder pitch)",
      "",
      "## Output Format",
      "",
      "Use markdown with clear headers. Keep the document scannable — busy stakeholders should be able to read just the headers and bold text to get the gist.",
      "",
      "## Tips",
      "",
      "- Be opinionated about scope. It is better to have a tight, well-defined spec than an expansive vague one.",
      "- If the user's idea is too big for one spec, suggest breaking it into phases and spec the first phase.",
      '- Success metrics should be specific and measurable, not vague ("improve user experience").',
      "- Non-goals are as important as goals. They prevent scope creep during implementation.",
      "- Open questions should be genuinely open — do not include questions you can answer from context.",
      "",
    ].join("\n"),
  },
  "productivity-memory-management": {
    "SKILL.md": [
      "---",
      "name: productivity-memory-management",
      "description: Two-tier memory system that makes Claude a true workplace collaborator. Decodes shorthand, acronyms, nicknames, and internal language so Claude understands requests like a colleague would. CLAUDE.md for working memory, memory/ directory for the full knowledge base.",
      "metadata:",
      "  plugin: productivity",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Memory Management",
      "",
      "Memory makes Claude your workplace collaborator - someone who speaks your internal language.",
      "",
      "## The Goal",
      "",
      "Transform shorthand into understanding:",
      "",
      "```",
      'User: "ask todd to do the PSR for oracle"',
      "              ↓ Claude decodes",
      '"Ask Todd Martinez (Finance lead) to prepare the Pipeline Status Report',
      ' for the Oracle Systems deal ($2.3M, closing Q2)"',
      "```",
      "",
      "Without memory, that request is meaningless. With memory, Claude knows:",
      "- **todd** → Todd Martinez, Finance lead, prefers Slack",
      "- **PSR** → Pipeline Status Report (weekly sales doc)",
      "- **oracle** → Oracle Systems deal, not the company",
      "",
      "## Architecture",
      "",
      "```",
      "CLAUDE.md          ← Hot cache (~30 people, common terms)",
      "memory/",
      "  glossary.md      ← Full decoder ring (everything)",
      "  people/          ← Complete profiles",
      "  projects/        ← Headquarter details",
      "  context/         ← Company, teams, tools",
      "```",
      "",
      "**CLAUDE.md (Hot Cache):**",
      "- Top ~30 people you interact with most",
      "- ~30 most common acronyms/terms",
      "- Active projects (5-15)",
      "- Your preferences",
      "- **Goal: Cover 90% of daily decoding needs**",
      "",
      "**memory/glossary.md (Full Glossary):**",
      "- Complete decoder ring - everyone, every term",
      "- Searched when something isn't in CLAUDE.md",
      "- Can grow indefinitely",
      "",
      "**memory/people/, projects/, context/:**",
      "- Rich detail when needed for execution",
      "- Full profiles, history, context",
      "",
      "## Lookup Flow",
      "",
      "```",
      'User: "ask todd about the PSR for phoenix"',
      "",
      "1. Check CLAUDE.md (hot cache)",
      "   → Todd? ✓ Todd Martinez, Finance",
      "   → PSR? ✓ Pipeline Status Report",
      "   → Phoenix? ✓ DB migration project",
      "",
      "2. If not found → search memory/glossary.md",
      "   → Full glossary has everyone/everything",
      "",
      "3. If still not found → ask user",
      '   → "What does X mean? I\'ll remember it."',
      "```",
      "",
      "This tiered approach keeps CLAUDE.md lean (~100 lines) while supporting unlimited scale in memory/.",
      "",
      "## File Locations",
      "",
      "- **Working memory:** `CLAUDE.md` in current working directory",
      "- **Deep memory:** `memory/` subdirectory",
      "",
      "## Working Memory Format (CLAUDE.md)",
      "",
      "Use tables for compactness. Target ~50-80 lines total.",
      "",
      "```markdown",
      "# Memory",
      "",
      "## Me",
      "[Name], [Role] on [Team]. [One sentence about what I do.]",
      "",
      "## People",
      "| Who | Role |",
      "|-----|------|",
      "| **Todd** | Todd Martinez, Finance lead |",
      "| **Sarah** | Sarah Chen, Engineering (Platform) |",
      "| **Greg** | Greg Wilson, Sales |",
      "→ Full list: memory/glossary.md, profiles: memory/people/",
      "",
      "## Terms",
      "| Term | Meaning |",
      "|------|---------|",
      "| PSR | Pipeline Status Report |",
      "| P0 | Drop everything priority |",
      "| standup | Daily 9am sync |",
      "→ Full glossary: memory/glossary.md",
      "",
      "## Headquarters",
      "| Name | What |",
      "|------|------|",
      "| **Phoenix** | DB migration, Q2 launch |",
      "| **Horizon** | Mobile app redesign |",
      "→ Details: memory/projects/",
      "",
      "## Preferences",
      "- 25-min meetings with buffers",
      "- Async-first, Slack over email",
      "- No meetings Friday afternoons",
      "```",
      "",
      "## Deep Memory Format (memory/)",
      "",
      "**memory/glossary.md** - The decoder ring:",
      "```markdown",
      "# Glossary",
      "",
      "Workplace shorthand, acronyms, and internal language.",
      "",
      "## Acronyms",
      "| Term | Meaning | Context |",
      "|------|---------|---------|",
      "| PSR | Pipeline Status Report | Weekly sales doc |",
      "| OKR | Objectives & Key Results | Quarterly planning |",
      "| P0/P1/P2 | Priority levels | P0 = drop everything |",
      "",
      "## Internal Terms",
      "| Term | Meaning |",
      "|------|---------|",
      "| standup | Daily 9am sync in #engineering |",
      "| the migration | Headquarter Phoenix database work |",
      "| ship it | Deploy to production |",
      "| escalate | Loop in leadership |",
      "",
      "## Nicknames → Full Names",
      "| Nickname | Person |",
      "|----------|--------|",
      "| Todd | Todd Martinez (Finance) |",
      "| T | Also Todd Martinez |",
      "",
      "## Headquarter Codenames",
      "| Codename | Headquarter |",
      "|----------|---------|",
      "| Phoenix | Database migration |",
      "| Horizon | New mobile app |",
      "```",
      "",
      "**memory/people/{name}.md:**",
      "```markdown",
      "# Todd Martinez",
      "",
      "**Also known as:** Todd, T",
      "**Role:** Finance Lead",
      "**Team:** Finance",
      "**Reports to:** CFO (Michael Chen)",
      "",
      "## Communication",
      "- Prefers Slack DM",
      "- Quick responses, very direct",
      "- Best time: mornings",
      "",
      "## Context",
      "- Handles all PSRs and financial reporting",
      "- Key contact for deal approvals over $500k",
      "- Works closely with Sales on forecasting",
      "",
      "## Notes",
      "- Cubs fan, likes talking baseball",
      "```",
      "",
      "**memory/projects/{name}.md:**",
      "```markdown",
      "# Headquarter Phoenix",
      "",
      "**Codename:** Phoenix",
      '**Also called:** "the migration"',
      "**Status:** Active, launching Q2",
      "",
      "## What It Is",
      "Database migration from legacy Oracle to PostgreSQL.",
      "",
      "## Key People",
      "- Sarah - tech lead",
      "- Todd - budget owner",
      "- Greg - stakeholder (sales impact)",
      "",
      "## Context",
      "$1.2M budget, 6-month timeline. Critical path for Horizon project.",
      "```",
      "",
      "**memory/context/company.md:**",
      "```markdown",
      "# Company Context",
      "",
      "## Tools & Systems",
      "| Tool | Used for | Internal name |",
      "|------|----------|---------------|",
      "| Slack | Communication | - |",
      "| Asana | Engineering tasks | - |",
      '| Salesforce | CRM | "SF" or "the CRM" |',
      "| Notion | Docs/wiki | - |",
      "",
      "## Teams",
      "| Team | What they do | Key people |",
      "|------|--------------|------------|",
      "| Platform | Infrastructure | Sarah (lead) |",
      "| Finance | Money stuff | Todd (lead) |",
      "| Sales | Revenue | Greg |",
      "",
      "## Processes",
      "| Process | What it means |",
      "|---------|---------------|",
      "| Weekly sync | Monday 10am all-hands |",
      "| Ship review | Thursday deploy approval |",
      "```",
      "",
      "## How to Interact",
      "",
      "### Decoding User Input (Tiered Lookup)",
      "",
      "**Always** decode shorthand before acting on requests:",
      "",
      "```",
      "1. CLAUDE.md (hot cache)     → Check first, covers 90% of cases",
      "2. memory/glossary.md        → Full glossary if not in hot cache",
      "3. memory/people/, projects/ → Rich detail when needed",
      "4. Ask user                  → Unknown term? Learn it.",
      "```",
      "",
      "Example:",
      "```",
      'User: "ask todd to do the PSR for oracle"',
      "",
      "CLAUDE.md lookup:",
      '  "todd" → Todd Martinez, Finance ✓',
      '  "PSR" → Pipeline Status Report ✓',
      '  "oracle" → (not in hot cache)',
      "",
      "memory/glossary.md lookup:",
      '  "oracle" → Oracle Systems deal ($2.3M) ✓',
      "",
      "Now Claude can act with full context.",
      "```",
      "",
      "### Adding Memory",
      "",
      'When user says "remember this" or "X means Y":',
      "",
      "1. **Glossary items** (acronyms, terms, shorthand):",
      "   - Add to memory/glossary.md",
      "   - If frequently used, add to CLAUDE.md Quick Glossary",
      "",
      "2. **People:**",
      "   - Create/update memory/people/{name}.md",
      "   - Add to CLAUDE.md Key People if important",
      "   - **Capture nicknames** - critical for decoding",
      "",
      "3. **Headquarters:**",
      "   - Create/update memory/projects/{name}.md",
      "   - Add to CLAUDE.md Active Headquarters if current",
      '   - **Capture codenames** - "Phoenix", "the migration", etc.',
      "",
      "4. **Preferences:** Add to CLAUDE.md Preferences section",
      "",
      "### Recalling Memory",
      "",
      'When user asks "who is X" or "what does X mean":',
      "",
      "1. Check CLAUDE.md first",
      "2. Check memory/ for full detail",
      '3. If not found: "I don\'t know what X means yet. Can you tell me?"',
      "",
      "### Progressive Disclosure",
      "",
      "1. Load CLAUDE.md for quick parsing of any request",
      "2. Dive into memory/ when you need full context for execution",
      "3. Example: drafting an email to todd about the PSR",
      "   - CLAUDE.md tells you Todd = Todd Martinez, PSR = Pipeline Status Report",
      "   - memory/people/todd-martinez.md tells you he prefers Slack, is direct",
      "",
      "## Bootstrapping",
      "",
      "Use `/productivity:start` to initialize by scanning your chat, calendar, email, and documents. Extracts people, projects, and starts building the glossary.",
      "",
      "## Conventions",
      "",
      "- **Bold** terms in CLAUDE.md for scannability",
      '- Keep CLAUDE.md under ~100 lines (the "hot 30" rule)',
      "- Filenames: lowercase, hyphens (`todd-martinez.md`, `project-phoenix.md`)",
      "- Always capture nicknames and alternate names",
      "- Glossary tables for easy lookup",
      "- When something's used frequently, promote it to CLAUDE.md",
      "- When something goes stale, demote it to memory/ only",
      "",
      "## What Goes Where",
      "",
      "| Type | CLAUDE.md (Hot Cache) | memory/ (Full Storage) |",
      "|------|----------------------|------------------------|",
      "| Person | Top ~30 frequent contacts | glossary.md + people/{name}.md |",
      "| Acronym/term | ~30 most common | glossary.md (complete list) |",
      "| Headquarter | Active projects only | glossary.md + projects/{name}.md |",
      "| Nickname | In Key People if top 30 | glossary.md (all nicknames) |",
      "| Company context | Quick reference only | context/company.md |",
      "| Preferences | All preferences | - |",
      "| Historical/stale | ✗ Remove | ✓ Keep in memory/ |",
      "",
      "## Promotion / Demotion",
      "",
      "**Promote to CLAUDE.md when:**",
      "- You use a term/person frequently",
      "- It's part of active work",
      "",
      "**Demote to memory/ only when:**",
      "- Headquarter completed",
      "- Person no longer frequent contact",
      "- Term rarely used",
      "",
      "This keeps CLAUDE.md fresh and relevant.",
      "",
    ].join("\n"),
  },
  "productivity-start": {
    "SKILL.md": [
      "---",
      "name: productivity-start",
      "description: Initialize the productivity system and open the dashboard",
      "metadata:",
      "  plugin: productivity",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Start Command",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Initialize the task and memory systems, then open the unified dashboard.",
      "",
      "## Instructions",
      "",
      "### 1. Check What Exists",
      "",
      "Check the working directory for:",
      "- `TASKS.md` — task list",
      "- `CLAUDE.md` — working memory",
      "- `memory/` — deep memory directory",
      "- `dashboard.html` — the visual UI",
      "",
      "### 2. Create What's Missing",
      "",
      "**If `TASKS.md` doesn't exist:** Create it with the standard template (see task-management skill). Place it in the current working directory.",
      "",
      "**If `dashboard.html` doesn't exist:** Copy it from `${CLAUDE_PLUGIN_ROOT}/skills/dashboard.html` to the current working directory.",
      "",
      "**If `CLAUDE.md` and `memory/` don't exist:** This is a fresh setup — after opening the dashboard, begin the memory bootstrap workflow (see below). Place these in the current working directory.",
      "",
      "### 3. Open the Dashboard",
      "",
      "Do NOT use `open` or `xdg-open` — in Cowork, the agent runs in a VM and shell open commands won't reach the user's browser. Instead, tell the user: \"Dashboard is ready at `dashboard.html`. Open it from your file browser to get started.\"",
      "",
      "### 4. Orient the User",
      "",
      "If everything was already initialized:",
      "```",
      "Dashboard open. Your tasks and memory are both loaded.",
      "- /productivity:update to sync tasks and check memory",
      "- /productivity:update --comprehensive for a deep scan of all activity",
      "```",
      "",
      "If memory hasn't been bootstrapped yet, continue to step 5.",
      "",
      "### 5. Bootstrap Memory (First Run Only)",
      "",
      "Only do this if `CLAUDE.md` and `memory/` don't exist yet.",
      "",
      "The best source of workplace language is the user's actual task list. Real tasks = real shorthand.",
      "",
      "**Ask the user:**",
      "```",
      "Where do you keep your todos or task list? This could be:",
      "- A local file (e.g., TASKS.md, todo.txt)",
      "- An app (e.g. Asana, Linear, Jira, Notion, Todoist)",
      "- A notes file",
      "",
      "I'll use your tasks to learn your workplace shorthand.",
      "```",
      "",
      "**Once you have access to the task list:**",
      "",
      "For each task item, analyze it for potential shorthand:",
      "- Names that might be nicknames",
      "- Acronyms or abbreviations",
      "- Headquarter references or codenames",
      "- Internal terms or jargon",
      "",
      "**For each item, decode it interactively:**",
      "",
      "```",
      'Task: "Send PSR to Todd re: Phoenix blockers"',
      "",
      "I see some terms I want to make sure I understand:",
      "",
      "1. **PSR** - What does this stand for?",
      "2. **Todd** - Who is Todd? (full name, role)",
      "3. **Phoenix** - Is this a project codename? What's it about?",
      "```",
      "",
      "Continue through each task, asking only about terms you haven't already decoded.",
      "",
      "### 6. Optional Comprehensive Scan",
      "",
      "After task list decoding, offer:",
      "```",
      "Do you want me to do a comprehensive scan of your messages, emails, and documents?",
      "This takes longer but builds much richer context about the people, projects, and terms in your work.",
      "",
      "Or we can stick with what we have and add context later.",
      "```",
      "",
      "**If they choose comprehensive scan:**",
      "",
      "Gather data from available MCP sources:",
      "- **Chat:** Recent messages, channels, DMs",
      "- **Email:** Sent messages, recipients",
      "- **Documents:** Recent docs, collaborators",
      "- **Calendar:** Meetings, attendees",
      "",
      "Build a braindump of people, projects, and terms found. Present findings grouped by confidence:",
      "- **Ready to add** (high confidence) — offer to add directly",
      "- **Needs clarification** — ask the user",
      "- **Low frequency / unclear** — note for later",
      "",
      "### 7. Write Memory Files",
      "",
      "From everything gathered, create:",
      "",
      "**CLAUDE.md** (working memory, ~50-80 lines):",
      "```markdown",
      "# Memory",
      "",
      "## Me",
      "[Name], [Role] on [Team].",
      "",
      "## People",
      "| Who | Role |",
      "|-----|------|",
      "| **[Nickname]** | [Full Name], [role] |",
      "",
      "## Terms",
      "| Term | Meaning |",
      "|------|---------|",
      "| [acronym] | [expansion] |",
      "",
      "## Headquarters",
      "| Name | What |",
      "|------|------|",
      "| **[Codename]** | [description] |",
      "",
      "## Preferences",
      "- [preferences discovered]",
      "```",
      "",
      "**memory/** directory:",
      "- `memory/glossary.md` — full decoder ring (acronyms, terms, nicknames, codenames)",
      "- `memory/people/{name}.md` — individual profiles",
      "- `memory/projects/{name}.md` — project details",
      "- `memory/context/company.md` — teams, tools, processes",
      "",
      "### 8. Report Results",
      "",
      "```",
      "Productivity system ready:",
      "- Tasks: TASKS.md (X items)",
      "- Memory: X people, X terms, X projects",
      "- Dashboard: open in browser",
      "",
      "Use /productivity:update to keep things current (add --comprehensive for a deep scan).",
      "```",
      "",
      "## Notes",
      "",
      "- If memory is already initialized, this just opens the dashboard",
      "- Nicknames are critical — always capture how people are actually referred to",
      "- If a source isn't available, skip it and note the gap",
      "- Memory grows organically through natural conversation after bootstrap",
      "",
    ].join("\n"),
  },
  "productivity-task-management": {
    "SKILL.md": [
      "---",
      "name: productivity-task-management",
      "description: Simple task management using a shared TASKS.md file. Reference this when the user asks about their tasks, wants to add/complete tasks, or needs help tracking commitments.",
      "metadata:",
      "  plugin: productivity",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Task Management",
      "",
      "Tasks are tracked in a simple `TASKS.md` file that both you and the user can edit.",
      "",
      "## File Location",
      "",
      "**Always use `TASKS.md` in the current working directory.**",
      "",
      "- If it exists, read/write to it",
      "- If it doesn't exist, create it with the template below",
      "",
      "## Dashboard Setup (First Run)",
      "",
      "A visual dashboard is available for managing tasks and memory. **On first interaction with tasks:**",
      "",
      "1. Check if `dashboard.html` exists in the current working directory",
      "2. If not, copy it from `${CLAUDE_PLUGIN_ROOT}/skills/dashboard.html` to the current working directory",
      '3. Inform the user: "I\'ve added the dashboard. Run `/productivity:start` to set up the full system."',
      "",
      "The task board:",
      "- Reads and writes to the same `TASKS.md` file",
      "- Auto-saves changes",
      "- Watches for external changes (syncs when you edit via CLI)",
      "- Supports drag-and-drop reordering of tasks and sections",
      "",
      "## Format & Template",
      "",
      "When creating a new TASKS.md, use this exact template (without example tasks):",
      "",
      "```markdown",
      "# Tasks",
      "",
      "## Active",
      "",
      "## Waiting On",
      "",
      "## Someday",
      "",
      "## Done",
      "```",
      "",
      "Task format:",
      "- `- [ ] **Task title** - context, for whom, due date`",
      "- Sub-bullets for additional details",
      "- Completed: `- [x] ~~Task~~ (date)`",
      "",
      "## How to Interact",
      "",
      '**When user asks "what\'s on my plate" / "my tasks":**',
      "- Read TASKS.md",
      "- Summarize Active and Waiting On sections",
      "- Highlight anything overdue or urgent",
      "",
      '**When user says "add a task" / "remind me to":**',
      "- Add to Active section with `- [ ] **Task**` format",
      "- Include context if provided (who it's for, due date)",
      "",
      '**When user says "done with X" / "finished X":**',
      "- Find the task",
      "- Change `[ ]` to `[x]`",
      "- Add strikethrough: `~~task~~`",
      "- Add completion date",
      "- Move to Done section",
      "",
      '**When user asks "what am I waiting on":**',
      "- Read the Waiting On section",
      "- Note how long each item has been waiting",
      "",
      "## Conventions",
      "",
      "- **Bold** the task title for scannability",
      '- Include "for [person]" when it\'s a commitment to someone',
      '- Include "due [date]" for deadlines',
      '- Include "since [date]" for waiting items',
      "- Sub-bullets for additional context",
      "- Keep Done section for ~1 week, then clear old items",
      "",
      "## Extracting Tasks",
      "",
      "When summarizing meetings or conversations, offer to add extracted tasks:",
      '- Commitments the user made ("I\'ll send that over")',
      "- Action items assigned to them",
      "- Follow-ups mentioned",
      "",
      "Ask before adding - don't auto-add without confirmation.",
      "",
    ].join("\n"),
  },
  "productivity-update": {
    "SKILL.md": [
      "---",
      "name: productivity-update",
      "description: Sync tasks and refresh memory from your current activity",
      "metadata:",
      "  plugin: productivity",
      "  type: command",
      '  plugin-version: "1.0.0"',
      '  argument-hint: "[--comprehensive]"',
      "---",
      "",
      "# Update Command",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Keep your task list and memory current. Two modes:",
      "",
      "- **Default:** Sync tasks from external tools, triage stale items, check memory for gaps",
      "- **`--comprehensive`:** Deep scan chat, email, calendar, docs — flag missed todos and suggest new memories",
      "",
      "## Usage",
      "",
      "```bash",
      "/productivity:update",
      "/productivity:update --comprehensive",
      "```",
      "",
      "## Default Mode",
      "",
      "### 1. Load Current State",
      "",
      "Read `TASKS.md` and `memory/` directory. If they don't exist, suggest `/productivity:start` first.",
      "",
      "### 2. Sync Tasks from External Sources",
      "",
      "Check for available task sources:",
      "- **Headquarter tracker** (e.g. Asana, Linear, Jira) (if MCP available)",
      "- **GitHub Issues** (if in a repo): `gh issue list --assignee=@me`",
      "",
      "If no sources are available, skip to Step 3.",
      "",
      "**Fetch tasks assigned to the user** (open/in-progress). Compare against TASKS.md:",
      "",
      "| External task | TASKS.md match? | Action |",
      "|---------------|-----------------|--------|",
      "| Found, not in TASKS.md | No match | Offer to add |",
      "| Found, already in TASKS.md | Match by title (fuzzy) | Skip |",
      "| In TASKS.md, not in external | No match | Flag as potentially stale |",
      "| Completed externally | In Active section | Offer to mark done |",
      "",
      "Present diff and let user decide what to add/complete.",
      "",
      "### 3. Triage Stale Items",
      "",
      "Review Active tasks in TASKS.md and flag:",
      "- Tasks with due dates in the past",
      "- Tasks in Active for 30+ days",
      "- Tasks with no context (no person, no project)",
      "",
      "Present each for triage: Mark done? Reschedule? Move to Someday?",
      "",
      "### 4. Decode Tasks for Memory Gaps",
      "",
      "For each task, attempt to decode all entities (people, projects, acronyms, tools, links):",
      "",
      "```",
      'Task: "Send PSR to Todd re: Phoenix blockers"',
      "",
      "Decode:",
      "- PSR → ✓ Pipeline Status Report (in glossary)",
      "- Todd → ✓ Todd Martinez (in people/)",
      "- Phoenix → ? Not in memory",
      "```",
      "",
      "Track what's fully decoded vs. what has gaps.",
      "",
      "### 5. Fill Gaps",
      "",
      "Present unknown terms grouped:",
      "```",
      "I found terms in your tasks I don't have context for:",
      "",
      '1. "Phoenix" (from: "Send PSR to Todd re: Phoenix blockers")',
      "   → What's Phoenix?",
      "",
      '2. "Maya" (from: "sync with Maya on API design")',
      "   → Who is Maya?",
      "```",
      "",
      "Add answers to the appropriate memory files (people/, projects/, glossary.md).",
      "",
      "### 6. Capture Enrichment",
      "",
      "Tasks often contain richer context than memory. Extract and update:",
      "- **Links** from tasks → add to project/people files",
      '- **Status changes** ("launch done") → update project status, demote from CLAUDE.md',
      "- **Relationships** (\"Todd's sign-off on Maya's proposal\") → cross-reference people",
      "- **Deadlines** → add to project files",
      "",
      "### 7. Report",
      "",
      "```",
      "Update complete:",
      "- Tasks: +3 from project tracker (e.g. Asana), 1 completed, 2 triaged",
      "- Memory: 2 gaps filled, 1 project enriched",
      "- All tasks decoded ✓",
      "```",
      "",
      "## Comprehensive Mode (`--comprehensive`)",
      "",
      "Everything in Default Mode, plus a deep scan of recent activity.",
      "",
      "### Extra Step: Scan Activity Sources",
      "",
      "Gather data from available MCP sources:",
      "- **Chat:** Search recent messages, read active channels",
      "- **Email:** Search sent messages",
      "- **Documents:** List recently touched docs",
      "- **Calendar:** List recent + upcoming events",
      "",
      "### Extra Step: Flag Missed Todos",
      "",
      "Compare activity against TASKS.md. Surface action items that aren't tracked:",
      "",
      "```",
      "## Possible Missing Tasks",
      "",
      "From your activity, these look like todos you haven't captured:",
      "",
      "1. From chat (Jan 18):",
      '   "I\'ll send the updated mockups by Friday"',
      "   → Add to TASKS.md?",
      "",
      '2. From meeting "Phoenix Standup" (Jan 17):',
      "   You have a recurring meeting but no Phoenix tasks active",
      "   → Anything needed here?",
      "",
      "3. From email (Jan 16):",
      '   "I\'ll review the API spec this week"',
      "   → Add to TASKS.md?",
      "```",
      "",
      "Let user pick which to add.",
      "",
      "### Extra Step: Suggest New Memories",
      "",
      "Surface new entities not in memory:",
      "",
      "```",
      "## New People (not in memory)",
      "| Name | Frequency | Context |",
      "|------|-----------|---------|",
      "| Maya Rodriguez | 12 mentions | design, UI reviews |",
      "| Alex K | 8 mentions | DMs about API |",
      "",
      "## New Headquarters/Topics",
      "| Name | Frequency | Context |",
      "|------|-----------|---------|",
      "| Starlight | 15 mentions | planning docs, product |",
      "",
      "## Suggested Cleanup",
      "- **Horizon project** — No mentions in 30 days. Mark completed?",
      "```",
      "",
      "Present grouped by confidence. High-confidence items offered to add directly; low-confidence items asked about.",
      "",
      "## Notes",
      "",
      "- Never auto-add tasks or memories without user confirmation",
      "- External source links are preserved when available",
      "- Fuzzy matching on task titles handles minor wording differences",
      "- Safe to run frequently — only updates when there's new info",
      "- `--comprehensive` always runs interactively",
      "",
    ].join("\n"),
  },
  "sales-account-research": {
    "SKILL.md": [
      "---",
      "name: sales-account-research",
      'description: "Research a company or person and get actionable sales intel. Works standalone with web search, supercharged when you connect enrichment tools or your CRM. Trigger with \\"research [company]\\", \\"look up [person]\\", \\"intel on [prospect]\\", \\"who is [name] at [company]\\", or \\"tell me about [company]\\"."',
      "metadata:",
      "  plugin: sales",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Account Research",
      "",
      "Get a complete picture of any company or person before outreach. This skill always works with web search, and gets significantly better with enrichment and CRM data.",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                     ACCOUNT RESEARCH                             │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  ALWAYS (works standalone via web search)                        │",
      "│  ✓ Company overview: what they do, size, industry               │",
      "│  ✓ Recent news: funding, leadership changes, announcements      │",
      "│  ✓ Hiring signals: open roles, growth indicators                │",
      "│  ✓ Key people: leadership team from LinkedIn                    │",
      "│  ✓ Product/service: what they sell, who they serve              │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + Enrichment: verified emails, phone, tech stack, org chart    │",
      "│  + CRM: prior relationship, past opportunities, contacts        │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## Getting Started",
      "",
      "Just tell me who to research:",
      "",
      '- "Research Stripe"',
      '- "Look up the CTO at Notion"',
      '- "Intel on acme.com"',
      '- "Who is Sarah Chen at TechCorp?"',
      '- "Tell me about [company] before my call"',
      "",
      "I'll run web searches immediately. If you have enrichment or CRM connected, I'll pull that data too.",
      "",
      "---",
      "",
      "## Connectors (Optional)",
      "",
      "Connect your tools to supercharge this skill:",
      "",
      "| Connector | What It Adds |",
      "|-----------|--------------|",
      "| **Enrichment** | Verified emails, phone numbers, tech stack, org chart, funding details |",
      "| **CRM** | Prior relationship history, past opportunities, existing contacts, notes |",
      "",
      "> **No connectors?** No problem. Web search provides solid research for any company or person.",
      "",
      "---",
      "",
      "## Output Format",
      "",
      "```markdown",
      "# Research: [Company or Person Name]",
      "",
      "**Generated:** [Date]",
      "**Sources:** Web Search [+ Enrichment] [+ CRM]",
      "",
      "---",
      "",
      "## Quick Take",
      "",
      "[2-3 sentences: Who they are, why they might need you, best angle for outreach]",
      "",
      "---",
      "",
      "## Company Profile",
      "",
      "| Field | Value |",
      "|-------|-------|",
      "| **Company** | [Name] |",
      "| **Website** | [URL] |",
      "| **Industry** | [Industry] |",
      "| **Size** | [Employee count] |",
      "| **Headquarters** | [Location] |",
      "| **Founded** | [Year] |",
      "| **Funding** | [Stage + amount if known] |",
      "| **Revenue** | [Estimate if available] |",
      "",
      "### What They Do",
      "[1-2 sentence description of their business, product, and customers]",
      "",
      "### Recent News",
      "- **[Headline]** — [Date] — [Why it matters for your outreach]",
      "- **[Headline]** — [Date] — [Why it matters]",
      "",
      "### Hiring Signals",
      "- [X] open roles in [Department]",
      "- Notable: [Relevant roles like Engineering, Sales, AI/ML]",
      "- Growth indicator: [Hiring velocity interpretation]",
      "",
      "---",
      "",
      "## Key People",
      "",
      "### [Name] — [Title]",
      "| Field | Detail |",
      "|-------|--------|",
      "| **LinkedIn** | [URL] |",
      "| **Background** | [Prior companies, education] |",
      "| **Tenure** | [Time at company] |",
      "| **Email** | [If enrichment connected] |",
      "",
      "**Talking Points:**",
      "- [Personal hook based on background]",
      "- [Professional hook based on role]",
      "",
      "[Repeat for relevant contacts]",
      "",
      "---",
      "",
      "## Tech Stack [If Enrichment Connected]",
      "",
      "| Category | Tools |",
      "|----------|-------|",
      "| **Cloud** | [AWS, GCP, Azure, etc.] |",
      "| **Data** | [Snowflake, Databricks, etc.] |",
      "| **CRM** | [e.g. Salesforce, HubSpot] |",
      "| **Other** | [Relevant tools] |",
      "",
      "**Integration Opportunity:** [How your product fits with their stack]",
      "",
      "---",
      "",
      "## Prior Relationship [If CRM Connected]",
      "",
      "| Field | Detail |",
      "|-------|--------|",
      "| **Status** | [New / Prior prospect / Customer / Churned] |",
      "| **Last Contact** | [Date and type] |",
      "| **Previous Opps** | [Won/Lost and why] |",
      "| **Known Contacts** | [Names already in CRM] |",
      "",
      "**History:** [Summary of past relationship]",
      "",
      "---",
      "",
      "## Qualification Signals",
      "",
      "### Positive Signals",
      "- ✅ [Signal and evidence]",
      "- ✅ [Signal and evidence]",
      "",
      "### Potential Concerns",
      "- ⚠️ [Concern and what to watch for]",
      "",
      "### Unknown (Ask in Discovery)",
      "- ❓ [Gap in understanding]",
      "",
      "---",
      "",
      "## Recommended Approach",
      "",
      "**Best Entry Point:** [Person and why]",
      "",
      "**Opening Hook:** [What to lead with based on research]",
      "",
      "**Discovery Questions:**",
      "1. [Question about their situation]",
      "2. [Question about pain points]",
      "3. [Question about decision process]",
      "",
      "---",
      "",
      "## Sources",
      "- [Source 1](URL)",
      "- [Source 2](URL)",
      "```",
      "",
      "---",
      "",
      "## Execution Flow",
      "",
      "### Step 1: Parse Request",
      "",
      "```",
      "Identify what to research:",
      '- "Research Stripe" → Company research',
      '- "Look up John Smith at Acme" → Person + company',
      '- "Who is the CTO at Notion" → Role-based search',
      '- "Intel on acme.com" → Domain-based lookup',
      "```",
      "",
      "### Step 2: Web Search (Always)",
      "",
      "```",
      "Run these searches:",
      '1. "[Company name]" → Homepage, about page',
      '2. "[Company name] news" → Recent announcements',
      '3. "[Company name] funding" → Investment history',
      '4. "[Company name] careers" → Hiring signals',
      '5. "[Person name] [Company] LinkedIn" → Profile info',
      '6. "[Company name] product" → What they sell',
      '7. "[Company name] customers" → Who they serve',
      "```",
      "",
      "**Extract:**",
      "- Company description and positioning",
      "- Recent news (last 90 days)",
      "- Leadership team",
      "- Open job postings",
      "- Technology mentions",
      "- Customer base",
      "",
      "### Step 3: Enrichment (If Connected)",
      "",
      "```",
      "If enrichment tools available:",
      "1. Enrich company → Firmographics, funding, tech stack",
      "2. Search people → Org chart, contact list",
      "3. Enrich person → Email, phone, background",
      "4. Get signals → Intent data, hiring velocity",
      "```",
      "",
      "**Enrichment adds:**",
      "- Verified contact info",
      "- Complete org chart",
      "- Precise employee count",
      "- Detailed tech stack",
      "- Funding history with investors",
      "",
      "### Step 4: CRM Check (If Connected)",
      "",
      "```",
      "If CRM available:",
      "1. Search for account by domain",
      "2. Get related contacts",
      "3. Get opportunity history",
      "4. Get activity timeline",
      "```",
      "",
      "**CRM adds:**",
      "- Prior relationship context",
      "- What happened before (won/lost deals)",
      "- Who we've talked to",
      "- Notes and history",
      "",
      "### Step 5: Synthesize",
      "",
      "```",
      "1. Combine all sources",
      "2. Prioritize enrichment data over web (more accurate)",
      "3. Add CRM context if exists",
      "4. Identify qualification signals",
      "5. Generate talking points",
      "6. Recommend approach",
      "```",
      "",
      "---",
      "",
      "## Research Variations",
      "",
      "### Company Research",
      "Focus on: Business overview, news, hiring, leadership",
      "",
      "### Person Research",
      "Focus on: Background, role, LinkedIn activity, talking points",
      "",
      "### Competitor Research",
      "Focus on: Product comparison, positioning, win/loss patterns",
      "",
      "### Pre-Meeting Research",
      "Focus on: Attendee backgrounds, recent news, relationship history",
      "",
      "---",
      "",
      "## Tips for Better Research",
      "",
      '1. **Include the domain** — "research acme.com" is more precise',
      '2. **Specify the person** — "look up Jane Smith, VP Sales at Acme"',
      '3. **State your goal** — "research Stripe before my demo call"',
      '4. **Ask for specifics** — "what\'s their tech stack?" after initial research',
      "",
      "---",
      "",
      "## Related Skills",
      "",
      "- **call-prep** — Full meeting prep with this research plus context",
      "- **draft-outreach** — Write personalized message based on research",
      "- **prospecting** — Qualify and prioritize research targets",
      "",
    ].join("\n"),
  },
  "sales-call-prep": {
    "SKILL.md": [
      "---",
      "name: sales-call-prep",
      'description: "Prepare for a sales call with account context, attendee research, and suggested agenda. Works standalone with user input and web research, supercharged when you connect your CRM, email, chat, or transcripts. Trigger with \\"prep me for my call with [company]\\", \\"I\'m meeting with [company] prep me\\", \\"call prep [company]\\", or \\"get me ready for [meeting]\\"."',
      "metadata:",
      "  plugin: sales",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Call Prep",
      "",
      "Get fully prepared for any sales call in minutes. This skill works with whatever context you provide, and gets significantly better when you connect your sales tools.",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                        CALL PREP                                 │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  ALWAYS (works standalone)                                       │",
      "│  ✓ You tell me: company, meeting type, attendees                │",
      "│  ✓ Web search: recent news, funding, leadership changes         │",
      "│  ✓ Company research: what they do, size, industry               │",
      "│  ✓ Output: prep brief with agenda and questions                 │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + CRM: account history, contacts, opportunities, activities    │",
      "│  + Email: recent threads, open questions, commitments           │",
      "│  + Chat: internal discussions, colleague insights               │",
      "│  + Transcripts: prior call recordings, key moments              │",
      "│  + Calendar: auto-find meeting, pull attendees                  │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## Getting Started",
      "",
      "When you run this skill, I'll ask for what I need:",
      "",
      "**Required:**",
      "- Company or contact name",
      "- Meeting type (discovery, demo, negotiation, check-in, etc.)",
      "",
      "**Helpful if you have it:**",
      "- Who's attending (names and titles)",
      "- Any context you want me to know (paste prior notes, emails, etc.)",
      "",
      "If you've connected your CRM, email, or other tools, I'll pull context automatically and skip the questions.",
      "",
      "---",
      "",
      "## Connectors (Optional)",
      "",
      "Connect your tools to supercharge this skill:",
      "",
      "| Connector | What It Adds |",
      "|-----------|--------------|",
      "| **CRM** | Account details, contact history, open deals, recent activities |",
      "| **Email** | Recent threads with the company, open questions, attachments shared |",
      "| **Chat** | Internal chat discussions (e.g. Slack) about the account, colleague insights |",
      "| **Transcripts** | Prior call recordings, topics covered, competitor mentions |",
      "| **Calendar** | Auto-find the meeting, pull attendees and description |",
      "",
      "> **No connectors?** No problem. Just tell me about the meeting and paste any context you have. I'll research the rest.",
      "",
      "---",
      "",
      "## Output Format",
      "",
      "```markdown",
      "# Call Prep: [Company Name]",
      "",
      "**Meeting:** [Type] — [Date/Time if known]",
      "**Attendees:** [Names with titles]",
      "**Your Goal:** [What you want to accomplish]",
      "",
      "---",
      "",
      "## Account Snapshot",
      "",
      "| Field | Value |",
      "|-------|-------|",
      "| **Company** | [Name] |",
      "| **Industry** | [Industry] |",
      "| **Size** | [Employees / Revenue if known] |",
      "| **Status** | [New prospect / Active opportunity / Customer] |",
      "| **Last Touch** | [Date and summary] |",
      "",
      "---",
      "",
      "## Who You're Meeting",
      "",
      "### [Name] — [Title]",
      "- **Background:** [Career history, education if found]",
      "- **LinkedIn:** [URL]",
      "- **Role in Deal:** [Decision maker / Champion / Evaluator / etc.]",
      "- **Last Interaction:** [Summary if known]",
      "- **Talking Point:** [Something personal/professional to reference]",
      "",
      "[Repeat for each attendee]",
      "",
      "---",
      "",
      "## Context & History",
      "",
      "**What's happened so far:**",
      "- [Key point from prior interactions]",
      "- [Open commitments or action items]",
      "- [Any concerns or objections raised]",
      "",
      "**Recent news about [Company]:**",
      "- [News item 1 — why it matters]",
      "- [News item 2 — why it matters]",
      "",
      "---",
      "",
      "## Suggested Agenda",
      "",
      "1. **Open** — [Reference last conversation or trigger event]",
      "2. **[Topic 1]** — [Discovery question or value discussion]",
      "3. **[Topic 2]** — [Address known concern or explore priority]",
      "4. **[Topic 3]** — [Demo section / Proposal review / etc.]",
      "5. **Next Steps** — [Propose clear follow-up with timeline]",
      "",
      "---",
      "",
      "## Discovery Questions",
      "",
      "Ask these to fill gaps in your understanding:",
      "",
      "1. [Question about their current situation]",
      "2. [Question about pain points or priorities]",
      "3. [Question about decision process and timeline]",
      "4. [Question about success criteria]",
      "5. [Question about other stakeholders]",
      "",
      "---",
      "",
      "## Potential Objections",
      "",
      "| Objection | Suggested Response |",
      "|-----------|-------------------|",
      "| [Likely objection based on context] | [How to address it] |",
      "| [Common objection for this stage] | [How to address it] |",
      "",
      "---",
      "",
      "## Internal Notes",
      "",
      "[Any internal chat context (e.g. Slack), colleague insights, or competitive intel]",
      "",
      "---",
      "",
      "## After the Call",
      "",
      "Run **call-follow-up** to:",
      "- Extract action items",
      "- Update your CRM",
      "- Draft follow-up email",
      "```",
      "",
      "---",
      "",
      "## Execution Flow",
      "",
      "### Step 1: Gather Context",
      "",
      "**If connectors available:**",
      "```",
      "1. Calendar → Find upcoming meeting matching company name",
      "   - Pull: title, time, attendees, description, attachments",
      "",
      "2. CRM → Query account",
      "   - Pull: account details, all contacts, open opportunities",
      "   - Pull: last 10 activities, any account notes",
      "",
      "3. Email → Search recent threads",
      "   - Query: emails with company domain (last 30 days)",
      "   - Extract: key topics, open questions, commitments",
      "",
      "4. Chat → Search internal discussions",
      "   - Query: company name mentions (last 30 days)",
      "   - Extract: colleague insights, competitive intel",
      "",
      "5. Transcripts → Find prior calls",
      "   - Pull: call recordings with this account",
      "   - Extract: key moments, objections raised, topics covered",
      "```",
      "",
      "**If no connectors:**",
      "```",
      "1. Ask user:",
      '   - "What company are you meeting with?"',
      '   - "What type of meeting is this?"',
      '   - "Who\'s attending? (names and titles if you know)"',
      '   - "Any context you want me to know? (paste notes, emails, etc.)"',
      "",
      "2. Accept whatever they provide and work with it",
      "```",
      "",
      "### Step 2: Research Supplement",
      "",
      "**Always run (web search):**",
      "```",
      '1. "[Company] news" — last 30 days',
      '2. "[Company] funding" — recent announcements',
      '3. "[Company] leadership" — executive changes',
      '4. "[Company] + [industry] trends" — relevant context',
      "5. Attendee LinkedIn profiles — background research",
      "```",
      "",
      "### Step 3: Synthesize & Generate",
      "",
      "```",
      "1. Combine all sources into unified context",
      "2. Identify gaps in understanding → generate discovery questions",
      "3. Anticipate objections based on stage and history",
      "4. Create suggested agenda tailored to meeting type",
      "5. Output formatted prep brief",
      "```",
      "",
      "---",
      "",
      "## Meeting Type Variations",
      "",
      "### Discovery Call",
      "- Focus on: Understanding their world, pain points, priorities",
      "- Agenda emphasis: Questions > Talking",
      "- Key output: Qualification signals, next step proposal",
      "",
      "### Demo / Presentation",
      "- Focus on: Their specific use case, tailored examples",
      "- Agenda emphasis: Show relevant features, get feedback",
      "- Key output: Technical requirements, decision timeline",
      "",
      "### Negotiation / Proposal Review",
      "- Focus on: Addressing concerns, justifying value",
      "- Agenda emphasis: Handle objections, close gaps",
      "- Key output: Path to agreement, clear next steps",
      "",
      "### Check-in / QBR",
      "- Focus on: Value delivered, expansion opportunities",
      "- Agenda emphasis: Review wins, surface new needs",
      "- Key output: Renewal confidence, upsell pipeline",
      "",
      "---",
      "",
      "## Tips for Better Prep",
      "",
      "1. **More context = better prep** — Paste emails, notes, anything you have",
      "2. **Name the attendees** — Even just titles help me research",
      '3. **State your goal** — "I want to get them to agree to a pilot"',
      '4. **Flag concerns** — "They mentioned budget is tight"',
      "",
      "---",
      "",
      "## Related Skills",
      "",
      "- **account-research** — Deep dive on a company before first contact",
      "- **call-follow-up** — Process call notes and execute post-call workflow",
      "- **draft-outreach** — Write personalized outreach after research",
      "",
    ].join("\n"),
  },
  "sales-call-summary": {
    "SKILL.md": [
      "---",
      "name: sales-call-summary",
      "description: Process call notes or a transcript — extract action items, draft follow-up email, generate internal summary",
      "metadata:",
      "  plugin: sales",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <call notes or transcript>",
      "---",
      "",
      "# /call-summary",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Process call notes or a transcript to extract action items, draft follow-up communications, and update records.",
      "",
      "## Usage",
      "",
      "```",
      "/call-summary",
      "```",
      "",
      "Then paste your notes, transcript, or upload a recording.",
      "",
      "---",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                      CALL SUMMARY                                │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  STANDALONE (always works)                                       │",
      "│  ✓ Paste call notes or transcript                               │",
      "│  ✓ Extract key discussion points and decisions                  │",
      "│  ✓ Identify action items with owners and due dates              │",
      "│  ✓ Surface objections, concerns, and open questions             │",
      "│  ✓ Draft customer-facing follow-up email                        │",
      "│  ✓ Generate internal summary for your team                      │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + Transcripts: Pull recording automatically (e.g. Gong, Fireflies) │",
      "│  + CRM: Update opportunity, log activity, create tasks          │",
      "│  + Email: Send follow-up directly from draft                    │",
      "│  + Calendar: Link to meeting, pull attendee context             │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## What I Need From You",
      "",
      "**Option 1: Paste your notes**",
      "Just paste whatever you have — bullet points, rough notes, stream of consciousness. I'll structure it.",
      "",
      "**Option 2: Paste a transcript**",
      "If you have a full transcript from your video conferencing tool (e.g. Zoom, Teams) or conversation intelligence tool (e.g. Gong, Fireflies), paste it. I'll extract the key moments.",
      "",
      "**Option 3: Describe the call**",
      'Tell me what happened: "Had a discovery call with Acme Corp. Met with their VP Eng and CTO. They\'re evaluating us vs Competitor X. Main concern is integration timeline."',
      "",
      "---",
      "",
      "## Output",
      "",
      "### Internal Summary",
      "```markdown",
      "## Call Summary: [Company] — [Date]",
      "",
      "**Attendees:** [Names and titles]",
      "**Call Type:** [Discovery / Demo / Negotiation / Check-in]",
      "**Duration:** [If known]",
      "",
      "### Key Discussion Points",
      "1. [Topic] — [What was discussed, decisions made]",
      "2. [Topic] — [Summary]",
      "",
      "### Customer Priorities",
      "- [Priority 1 they expressed]",
      "- [Priority 2]",
      "",
      "### Objections / Concerns Raised",
      "- [Concern] — [How you addressed it / status]",
      "",
      "### Competitive Intel",
      "- [Any competitor mentions, what was said]",
      "",
      "### Action Items",
      "| Owner | Action | Due |",
      "|-------|--------|-----|",
      "| [You] | [Task] | [Date] |",
      "| [Customer] | [Task] | [Date] |",
      "",
      "### Next Steps",
      "- [Agreed next step with timeline]",
      "",
      "### Deal Impact",
      "- [How this call affects the opportunity — stage change, risk, acceleration]",
      "```",
      "",
      "### Customer Follow-Up Email",
      "```",
      "Subject: [Meeting recap + next steps]",
      "",
      "Hi [Name],",
      "",
      "Thank you for taking the time to meet today...",
      "",
      "[Key points discussed]",
      "",
      "[Commitments you made]",
      "",
      "[Clear next step with timeline]",
      "",
      "Best,",
      "[You]",
      "```",
      "",
      "---",
      "",
      "## Email Style Guidelines",
      "",
      "When drafting customer-facing emails:",
      "",
      "1. **Be concise but informative** — Get to the point quickly. Customers are busy.",
      "2. **No markdown formatting** — Don't use asterisks, bold, or other markdown syntax. Write in plain text that looks natural in any email client.",
      "3. **Use simple structure** — Short paragraphs, line breaks between sections. No headers or bullet formatting unless the customer's email client will render it.",
      "4. **Keep it scannable** — If listing items, use plain dashes or numbers, not fancy formatting.",
      "",
      "**Good:**",
      "```",
      "Here's what we discussed:",
      "- Quote for 20 seats at $480/seat/year",
      "- W9 and supplier onboarding docs",
      "- Point of contact for the contract",
      "```",
      "",
      "**Bad:**",
      "```",
      "**What You Need from Us:**",
      "- Quote for 20 seats at $480/seat/year",
      "```",
      "",
      "---",
      "",
      "## If Connectors Available",
      "",
      "**Transcripts connected (e.g. Gong, Fireflies):**",
      "- I'll search for the call automatically",
      "- Pull the full transcript",
      "- Extract key moments flagged by the platform",
      "",
      "**CRM connected:**",
      "- I'll offer to update the opportunity stage",
      "- Log the call as an activity",
      "- Create tasks for action items",
      "- Update next steps field",
      "",
      "**Email connected:**",
      "- I'll offer to create a draft in ~~email",
      "- Or send directly if you approve",
      "",
      "---",
      "",
      "## Tips",
      "",
      '1. **More detail = better output** — Even rough notes help. "They seemed concerned about X" is useful context.',
      "2. **Name the attendees** — Helps me structure the summary and assign action items.",
      '3. **Flag what matters** — If something was important, tell me: "The big thing was..."',
      "4. **Tell me the deal stage** — Helps me tailor the follow-up tone and next steps.",
      "",
    ].join("\n"),
  },
  "sales-competitive-intelligence": {
    "SKILL.md": [
      "---",
      "name: sales-competitive-intelligence",
      'description: "Research your competitors and build an interactive battlecard. Outputs an HTML artifact with clickable competitor cards and a comparison matrix. Trigger with \\"competitive intel\\", \\"research competitors\\", \\"how do we compare to [competitor]\\", \\"battlecard for [competitor]\\", or \\"what\'s new with [competitor]\\"."',
      "metadata:",
      "  plugin: sales",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Competitive Intelligence",
      "",
      "Research your competitors extensively and generate an **interactive HTML battlecard** you can use in deals. The output is a self-contained artifact with clickable competitor tabs and an overall comparison matrix.",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                  COMPETITIVE INTELLIGENCE                        │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  ALWAYS (works standalone via web search)                        │",
      "│  ✓ Competitor product deep-dive: features, pricing, positioning │",
      "│  ✓ Recent releases: what they've shipped in last 90 days        │",
      "│  ✓ Your company releases: what you've shipped to counter        │",
      "│  ✓ Differentiation matrix: where you win vs. where they win     │",
      "│  ✓ Sales talk tracks: how to position against each competitor   │",
      "│  ✓ Landmine questions: expose their weaknesses naturally        │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  OUTPUT: Interactive HTML Battlecard                             │",
      "│  ✓ Comparison matrix overview                                    │",
      "│  ✓ Clickable tabs for each competitor                           │",
      "│  ✓ Dark theme, professional styling                             │",
      "│  ✓ Self-contained HTML file — share or host anywhere            │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + CRM: Win/loss data, competitor mentions in closed deals      │",
      "│  + Docs: Existing battlecards, competitive playbooks            │",
      "│  + Chat: Internal intel, field reports from colleagues          │",
      "│  + Transcripts: Competitor mentions in customer calls           │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## Getting Started",
      "",
      "When you run this skill, I'll ask for context:",
      "",
      "**Required:**",
      "- What company do you work for? (or I'll detect from your email)",
      "- Who are your main competitors? (1-5 names)",
      "",
      "**Optional:**",
      "- Which competitor do you want to focus on first?",
      "- Any specific deals where you're competing against them?",
      "- Pain points you've heard from customers about competitors?",
      "",
      "If I already have your seller context from a previous session, I'll confirm and skip the questions.",
      "",
      "---",
      "",
      "## Connectors (Optional)",
      "",
      "| Connector | What It Adds |",
      "|-----------|--------------|",
      "| **CRM** | Win/loss history against each competitor, deal-level competitor tracking |",
      "| **Docs** | Existing battlecards, product comparison docs, competitive playbooks |",
      "| **Chat** | Internal chat intel (e.g. Slack) — what your team is hearing from the field |",
      "| **Transcripts** | Competitor mentions in customer calls, objections raised |",
      "",
      "> **No connectors?** Web research works great. I'll pull everything from public sources — product pages, pricing, blogs, release notes, reviews, job postings.",
      "",
      "---",
      "",
      "## Output: Interactive HTML Battlecard",
      "",
      "The skill generates a **self-contained HTML file** with:",
      "",
      "### 1. Comparison Matrix (Landing View)",
      "Overview comparing you vs. all competitors at a glance:",
      "- Feature comparison grid",
      "- Pricing comparison",
      "- Market positioning",
      "- Win rate indicators (if CRM connected)",
      "",
      "### 2. Competitor Tabs (Click to Expand)",
      "Each competitor gets a clickable card that expands to show:",
      "- Company profile (size, funding, target market)",
      "- What they sell and how they position",
      "- Recent releases (last 90 days)",
      "- Where they win vs. where you win",
      "- Pricing intelligence",
      "- Talk tracks for different scenarios",
      "- Objection handling",
      "- Landmine questions",
      "",
      "### 3. Your Company Card",
      "- Your releases (last 90 days)",
      "- Your key differentiators",
      "- Proof points and customer quotes",
      "",
      "---",
      "",
      "## HTML Structure",
      "",
      "```html",
      "<!DOCTYPE html>",
      "<html>",
      "<head>",
      "    <title>Battlecard: [Your Company] vs Competitors</title>",
      "    <style>",
      "        /* Dark theme, professional styling */",
      "        /* Tabbed navigation */",
      "        /* Expandable cards */",
      "        /* Responsive design */",
      "    </style>",
      "</head>",
      "<body>",
      "    <!-- Header with your company + date -->",
      "    <header>",
      "        <h1>[Your Company] Competitive Battlecard</h1>",
      "        <p>Generated: [Date] | Competitors: [List]</p>",
      "    </header>",
      "",
      "    <!-- Tab Navigation -->",
      '    <nav class="tabs">',
      '        <button class="tab active" data-tab="matrix">Comparison Matrix</button>',
      '        <button class="tab" data-tab="competitor-1">[Competitor 1]</button>',
      '        <button class="tab" data-tab="competitor-2">[Competitor 2]</button>',
      '        <button class="tab" data-tab="competitor-3">[Competitor 3]</button>',
      "    </nav>",
      "",
      "    <!-- Comparison Matrix Tab -->",
      '    <section id="matrix" class="tab-content active">',
      "        <h2>Head-to-Head Comparison</h2>",
      '        <table class="comparison-matrix">',
      "            <!-- Feature rows with you vs each competitor -->",
      "        </table>",
      "",
      "        <h2>Quick Win/Loss Guide</h2>",
      '        <div class="win-loss-grid">',
      "            <!-- Per-competitor: when you win, when you lose -->",
      "        </div>",
      "    </section>",
      "",
      "    <!-- Individual Competitor Tabs -->",
      '    <section id="competitor-1" class="tab-content">',
      '        <div class="battlecard">',
      '            <div class="profile"><!-- Company info --></div>',
      '            <div class="differentiation"><!-- Where they win / you win --></div>',
      '            <div class="talk-tracks"><!-- Scenario-based positioning --></div>',
      '            <div class="objections"><!-- Common objections + responses --></div>',
      '            <div class="landmines"><!-- Questions to ask --></div>',
      "        </div>",
      "    </section>",
      "",
      "    <script>",
      "        // Tab switching logic",
      "        // Expand/collapse sections",
      "    </script>",
      "</body>",
      "</html>",
      "```",
      "",
      "---",
      "",
      "## Visual Design",
      "",
      "### Color System",
      "```css",
      ":root {",
      "    /* Dark theme base */",
      "    --bg-primary: #0a0d14;",
      "    --bg-elevated: #0f131c;",
      "    --bg-surface: #161b28;",
      "    --bg-hover: #1e2536;",
      "",
      "    /* Text */",
      "    --text-primary: #ffffff;",
      "    --text-secondary: rgba(255, 255, 255, 0.7);",
      "    --text-muted: rgba(255, 255, 255, 0.5);",
      "",
      "    /* Accent (your brand or neutral) */",
      "    --accent: #3b82f6;",
      "    --accent-hover: #2563eb;",
      "",
      "    /* Status indicators */",
      "    --you-win: #10b981;",
      "    --they-win: #ef4444;",
      "    --tie: #f59e0b;",
      "}",
      "```",
      "",
      "### Card Design",
      "- Rounded corners (12px)",
      "- Subtle borders (1px, low opacity)",
      "- Hover states with slight elevation",
      "- Smooth transitions (200ms)",
      "",
      "### Comparison Matrix",
      "- Sticky header row",
      "- Color-coded winner indicators (green = you, red = them, yellow = tie)",
      "- Expandable rows for detail",
      "",
      "---",
      "",
      "## Execution Flow",
      "",
      "### Phase 1: Gather Seller Context",
      "",
      "```",
      "If first time:",
      '1. Ask: "What company do you work for?"',
      '2. Ask: "What do you sell? (product/service in one line)"',
      '3. Ask: "Who are your main competitors? (up to 5)"',
      "4. Store context for future sessions",
      "",
      "If returning user:",
      '1. Confirm: "Still at [Company] selling [Product]?"',
      '2. Ask: "Same competitors, or any new ones to add?"',
      "```",
      "",
      "### Phase 2: Research Your Company (Always)",
      "",
      "```",
      "Web searches:",
      '1. "[Your company] product" — current offerings',
      '2. "[Your company] pricing" — pricing model',
      '3. "[Your company] news" — recent announcements (90 days)',
      '4. "[Your company] product updates OR changelog OR releases" — what you\'ve shipped',
      '5. "[Your company] vs [competitor]" — existing comparisons',
      "```",
      "",
      "### Phase 3: Research Each Competitor (Always)",
      "",
      "```",
      "For each competitor, run:",
      '1. "[Competitor] product features" — what they offer',
      '2. "[Competitor] pricing" — how they charge',
      '3. "[Competitor] news" — recent announcements',
      '4. "[Competitor] product updates OR changelog OR releases" — what they\'ve shipped',
      '5. "[Competitor] reviews G2 OR Capterra OR TrustRadius" — customer sentiment',
      '6. "[Competitor] vs [alternatives]" — how they position',
      '7. "[Competitor] customers" — who uses them',
      '8. "[Competitor] careers" — hiring signals (growth areas)',
      "```",
      "",
      "### Phase 4: Pull Connected Sources (If Available)",
      "",
      "```",
      "If CRM connected:",
      "1. Query closed-won deals with competitor field = [Competitor]",
      "2. Query closed-lost deals with competitor field = [Competitor]",
      "3. Extract win/loss patterns",
      "",
      "If docs connected:",
      '1. Search for "battlecard [competitor]"',
      '2. Search for "competitive [competitor]"',
      "3. Pull existing positioning docs",
      "",
      "If chat connected:",
      '1. Search for "[Competitor]" mentions (last 90 days)',
      "2. Extract field intel and colleague insights",
      "",
      "If transcripts connected:",
      '1. Search calls for "[Competitor]" mentions',
      "2. Extract objections and customer quotes",
      "```",
      "",
      "### Phase 5: Build HTML Artifact",
      "",
      "```",
      "1. Structure data for each competitor",
      "2. Build comparison matrix",
      "3. Generate individual battlecards",
      "4. Create talk tracks for each scenario",
      "5. Compile landmine questions",
      "6. Render as self-contained HTML",
      "7. Save as [YourCompany]-battlecard-[date].html",
      "```",
      "",
      "---",
      "",
      "## Data Structure Per Competitor",
      "",
      "```yaml",
      "competitor:",
      '  name: "[Name]"',
      '  website: "[URL]"',
      "  profile:",
      '    founded: "[Year]"',
      '    funding: "[Stage + amount]"',
      '    employees: "[Count]"',
      '    target_market: "[Who they sell to]"',
      '    pricing_model: "[Per seat / usage / etc.]"',
      '    market_position: "[Leader / Challenger / Niche]"',
      "",
      '  what_they_sell: "[Product summary]"',
      '  their_positioning: "[How they describe themselves]"',
      "",
      "  recent_releases:",
      '    - date: "[Date]"',
      '      release: "[Feature/Product]"',
      '      impact: "[Why it matters]"',
      "",
      "  where_they_win:",
      '    - area: "[Area]"',
      '      advantage: "[Their strength]"',
      '      how_to_handle: "[Your counter]"',
      "",
      "  where_you_win:",
      '    - area: "[Area]"',
      '      advantage: "[Your strength]"',
      '      proof_point: "[Evidence]"',
      "",
      "  pricing:",
      '    model: "[How they charge]"',
      '    entry_price: "[Starting price]"',
      '    enterprise: "[Enterprise pricing]"',
      '    hidden_costs: "[Implementation, etc.]"',
      '    talk_track: "[How to discuss pricing]"',
      "",
      "  talk_tracks:",
      '    early_mention: "[Strategy if they come up early]"',
      '    displacement: "[Strategy if customer uses them]"',
      '    late_addition: "[Strategy if added late to eval]"',
      "",
      "  objections:",
      '    - objection: "[What customer says]"',
      '      response: "[How to handle]"',
      "",
      "  landmines:",
      '    - "[Question that exposes their weakness]"',
      "",
      "  win_loss: # If CRM connected",
      '    win_rate: "[X]%"',
      '    common_win_factors: "[What predicts wins]"',
      '    common_loss_factors: "[What predicts losses]"',
      "```",
      "",
      "---",
      "",
      "## Delivery",
      "",
      "```markdown",
      "## ✓ Battlecard Created",
      "",
      "[View your battlecard](file:///path/to/[YourCompany]-battlecard-[date].html)",
      "",
      "---",
      "",
      "**Summary**",
      "- **Your Company**: [Name]",
      "- **Competitors Analyzed**: [List]",
      "- **Data Sources**: Web research [+ CRM] [+ Docs] [+ Transcripts]",
      "",
      "---",
      "",
      "**How to Use**",
      "- **Before a call**: Open the relevant competitor tab, review talk tracks",
      "- **During a call**: Reference landmine questions",
      "- **After win/loss**: Update with new intel",
      "",
      "---",
      "",
      "**Sharing Options**",
      "- **Local file**: Open in any browser",
      "- **Host it**: Upload to Netlify, Vercel, or internal wiki",
      "- **Share directly**: Send the HTML file to teammates",
      "",
      "---",
      "",
      "**Keep it Fresh**",
      "Run this skill again to refresh with latest intel. Recommended: monthly or before major deals.",
      "```",
      "",
      "---",
      "",
      "## Refresh Cadence",
      "",
      "Competitive intel gets stale. Recommended refresh:",
      "",
      "| Trigger | Action |",
      "|---------|--------|",
      "| **Monthly** | Quick refresh — new releases, news, pricing changes |",
      "| **Before major deal** | Deep refresh for specific competitor in that deal |",
      "| **After win/loss** | Update patterns with new data |",
      "| **Competitor announcement** | Immediate update on that competitor |",
      "",
      "---",
      "",
      "## Tips for Better Intel",
      "",
      "1. **Be honest about weaknesses** — Credibility comes from acknowledging where competitors are strong",
      '2. **Focus on outcomes, not features** — "They have X feature" matters less than "customers achieve Y result"',
      "3. **Update from the field** — Best intel comes from actual customer conversations, not just websites",
      "4. **Plant landmines, don't badmouth** — Ask questions that expose weaknesses; never trash-talk",
      "5. **Track releases religiously** — What they ship tells you their strategy and your opportunity",
      "",
      "---",
      "",
      "## Related Skills",
      "",
      "- **account-research** — Research a specific prospect before reaching out",
      "- **call-prep** — Prep for a call where you know competitor is involved",
      "- **create-an-asset** — Build a custom comparison page for a specific deal",
      "",
    ].join("\n"),
  },
  "sales-create-an-asset": {
    "SKILL.md": [
      "---",
      "name: sales-create-an-asset",
      "description: Generate tailored sales assets (landing pages, decks, one-pagers, workflow demos) from your deal context. Describe your prospect, audience, and goal — get a polished, branded asset ready to share with customers.",
      "metadata:",
      "  plugin: sales",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Create an Asset",
      "",
      "Generate custom sales assets tailored to your prospect, audience, and goals. Supports interactive landing pages, presentation decks, executive one-pagers, and workflow/architecture demos.",
      "",
      "---",
      "",
      "## Triggers",
      "",
      "Invoke this skill when:",
      "- User says `/create-an-asset` or `/create-an-asset [CompanyName]`",
      '- User asks to "create an asset", "build a demo", "make a landing page", "mock up a workflow"',
      "- User needs a customer-facing deliverable for a sales conversation",
      "",
      "---",
      "",
      "## Overview",
      "",
      "This skill creates professional sales assets by gathering context about:",
      "- **(a) The Prospect** — company, contacts, conversations, pain points",
      "- **(b) The Audience** — who's viewing, what they care about",
      "- **(c) The Purpose** — goal of the asset, desired next action",
      "- **(d) The Format** — landing page, deck, one-pager, or workflow demo",
      "",
      "The skill then researches, structures, and builds a polished, branded asset ready to share with customers.",
      "",
      "---",
      "",
      "## Phase 0: Context Detection & Input Collection",
      "",
      "### Step 0.1: Detect Seller Context",
      "",
      "From the user's email domain, identify what company they work for.",
      "",
      "**Actions:**",
      "1. Extract domain from user's email",
      '2. Search: `"[domain]" company products services site:linkedin.com OR site:crunchbase.com`',
      "3. Determine seller context:",
      "",
      "| Scenario | Action |",
      "|----------|--------|",
      "| **Single-product company** | Auto-populate seller context |",
      '| **Multi-product company** | Ask: "Which product or solution is this asset for?" |',
      '| **Consultant/agency/generic domain** | Ask: "What company or product are you representing?" |',
      '| **Unknown/startup** | Ask: "Briefly, what are you selling?" |',
      "",
      "**Store seller context:**",
      "```yaml",
      "seller:",
      '  company: "[Company Name]"',
      '  product: "[Product/Service]"',
      "  value_props:",
      '    - "[Key value prop 1]"',
      '    - "[Key value prop 2]"',
      '    - "[Key value prop 3]"',
      "  differentiators:",
      '    - "[Differentiator 1]"',
      '    - "[Differentiator 2]"',
      '  pricing_model: "[If publicly known]"',
      "```",
      "",
      '**Persist to knowledge base** for future sessions. On subsequent invocations, confirm: "I have your seller context from last time — still selling [Product] at [Company]?"',
      "",
      "---",
      "",
      "### Step 0.2: Collect Prospect Context (a)",
      "",
      "**Ask the user:**",
      "",
      "| Field | Prompt | Required |",
      "|-------|--------|----------|",
      '| **Company** | "Which company is this asset for?" | ✓ Yes |',
      '| **Key contacts** | "Who are the key contacts? (names, roles)" | No |',
      '| **Deal stage** | "What stage is this deal?" | ✓ Yes |',
      '| **Pain points** | "What pain points or priorities have they shared?" | No |',
      '| **Past materials** | "Upload any conversation materials (transcripts, emails, notes, call recordings)" | No |',
      "",
      "**Deal stage options:**",
      "- Intro / First meeting",
      "- Discovery",
      "- Evaluation / Technical review",
      "- POC / Pilot",
      "- Negotiation",
      "- Close",
      "",
      "---",
      "",
      "### Step 0.3: Collect Audience Context (b)",
      "",
      "**Ask the user:**",
      "",
      "| Field | Prompt | Required |",
      "|-------|--------|----------|",
      '| **Audience type** | "Who\'s viewing this?" | ✓ Yes |',
      '| **Specific roles** | "Any specific titles to tailor for? (e.g., CTO, VP Engineering, CFO)" | No |',
      '| **Primary concern** | "What do they care most about?" | ✓ Yes |',
      '| **Objections** | "Any concerns or objections to address?" | No |',
      "",
      "**Audience type options:**",
      "- Executive (C-suite, VPs)",
      "- Technical (Architects, Engineers, Developers)",
      "- Operations (Ops, IT, Procurement)",
      "- Mixed / Cross-functional",
      "",
      "**Primary concern options:**",
      "- ROI / Business impact",
      "- Technical depth / Architecture",
      "- Strategic alignment",
      "- Risk mitigation / Security",
      "- Implementation / Timeline",
      "",
      "---",
      "",
      "### Step 0.4: Collect Purpose Context (c)",
      "",
      "**Ask the user:**",
      "",
      "| Field | Prompt | Required |",
      "|-------|--------|----------|",
      '| **Goal** | "What\'s the goal of this asset?" | ✓ Yes |',
      '| **Desired action** | "What should the viewer do after seeing this?" | ✓ Yes |',
      "",
      "**Goal options:**",
      "- Intro / First impression",
      "- Discovery follow-up",
      "- Technical deep-dive",
      "- Executive alignment / Business case",
      "- POC proposal",
      "- Deal close",
      "",
      "---",
      "",
      "### Step 0.5: Select Format (d)",
      "",
      '**Ask the user:** "What format works best for this?"',
      "",
      "| Format | Description | Best For |",
      "|--------|-------------|----------|",
      "| **Interactive landing page** | Multi-tab page with demos, metrics, calculators | Exec alignment, intros, value prop |",
      "| **Deck-style** | Linear slides, presentation-ready | Formal meetings, large audiences |",
      "| **One-pager** | Single-scroll executive summary | Leave-behinds, quick summaries |",
      "| **Workflow / Architecture demo** | Interactive diagram with animated flow | Technical deep-dives, POC demos, integrations |",
      "",
      "---",
      "",
      "### Step 0.6: Format-Specific Inputs",
      "",
      '#### If "Workflow / Architecture demo" selected:',
      "",
      "**First, parse from user's description.** Look for:",
      "- Systems and components mentioned",
      "- Data flows described",
      "- Human interaction points",
      "- Example scenarios",
      "",
      "**Then ask for any gaps:**",
      "",
      "| If Missing... | Ask... |",
      "|---------------|--------|",
      '| Components unclear | "What systems or components are involved? (databases, APIs, AI, middleware, etc.)" |',
      '| Flow unclear | "Walk me through the step-by-step flow" |',
      '| Human touchpoints unclear | "Where does a human interact in this workflow?" |',
      '| Scenario vague | "What\'s a concrete example scenario to demo?" |',
      '| Integration specifics | "Any specific tools or platforms to highlight?" |',
      "",
      "---",
      "",
      "## Phase 1: Research (Adaptive)",
      "",
      "### Assess Context Richness",
      "",
      "| Level | Indicators | Research Depth |",
      "|-------|------------|----------------|",
      "| **Rich** | Transcripts uploaded, detailed pain points, clear requirements | Light — fill gaps only |",
      "| **Moderate** | Some context, no transcripts | Medium — company + industry |",
      "| **Sparse** | Just company name | Deep — full research pass |",
      "",
      "### Always Research:",
      "",
      "1. **Prospect basics**",
      '   - Search: `"[Company]" annual report investor presentation 2025 2026`',
      '   - Search: `"[Company]" CEO strategy priorities 2025 2026`',
      "   - Extract: Revenue, employees, key metrics, strategic priorities",
      "",
      "2. **Leadership**",
      '   - Search: `"[Company]" CEO CTO CIO 2025`',
      "   - Extract: Names, titles, recent quotes on strategy/technology",
      "",
      "3. **Brand colors**",
      '   - Search: `"[Company]" brand guidelines`',
      "   - Or extract from company website",
      "   - Store: Primary color, secondary color, accent",
      "",
      "### If Moderate/Sparse Context, Also Research:",
      "",
      "4. **Industry context**",
      '   - Search: `"[Industry]" trends challenges 2025 2026`',
      "   - Extract: Common pain points, market dynamics",
      "",
      "5. **Technology landscape**",
      '   - Search: `"[Company]" technology stack tools platforms`',
      "   - Extract: Current solutions, potential integration points",
      "",
      "6. **Competitive context**",
      '   - Search: `"[Company]" vs [seller\'s competitors]`',
      "   - Extract: Current solutions, switching signals",
      "",
      "### If Transcripts/Materials Uploaded:",
      "",
      "7. **Conversation analysis**",
      "   - Extract: Stated pain points, decision criteria, objections, timeline",
      "   - Identify: Key quotes to reference (use their exact language)",
      "   - Note: Specific terminology, acronyms, internal project names",
      "",
      "---",
      "",
      "## Phase 2: Structure Decision",
      "",
      "### Interactive Landing Page",
      "",
      "| Purpose | Recommended Sections |",
      "|---------|---------------------|",
      "| **Intro** | Company Fit → Solution Overview → Key Use Cases → Why Us → Next Steps |",
      "| **Discovery follow-up** | Their Priorities → How We Help → Relevant Examples → ROI Framework → Next Steps |",
      "| **Technical deep-dive** | Architecture → Security & Compliance → Integration → Performance → Support |",
      "| **Exec alignment** | Strategic Fit → Business Impact → ROI Calculator → Risk Mitigation → Partnership |",
      "| **POC proposal** | Scope → Success Criteria → Timeline → Team → Investment → Next Steps |",
      "| **Deal close** | Value Summary → Pricing → Implementation Plan → Terms → Sign-off |",
      "",
      "**Audience adjustments:**",
      "- **Executive**: Lead with business impact, ROI, strategic alignment",
      "- **Technical**: Lead with architecture, security, integration depth",
      "- **Operations**: Lead with workflow impact, change management, support",
      "- **Mixed**: Balance strategic + tactical; use tabs to separate depth levels",
      "",
      "---",
      "",
      "### Deck-Style",
      "",
      "Same sections as landing page, formatted as linear slides:",
      "",
      "```",
      "1. Title slide (Prospect + Seller logos, partnership framing)",
      "2. Agenda",
      "3-N. One section per slide (or 2-3 slides for dense sections)",
      "N+1. Summary / Key takeaways",
      "N+2. Next steps / CTA",
      "N+3. Appendix (optional — detailed specs, pricing, etc.)",
      "```",
      "",
      "**Slide principles:**",
      "- One key message per slide",
      "- Visual > text-heavy",
      "- Use prospect's metrics and language",
      "- Include speaker notes",
      "",
      "---",
      "",
      "### One-Pager",
      "",
      "Condense to single-scroll format:",
      "",
      "```",
      "┌─────────────────────────────────────┐",
      '│ HERO: "[Prospect Goal] with [Product]" │',
      "├─────────────────────────────────────┤",
      "│ KEY POINT 1     │ KEY POINT 2     │ KEY POINT 3     │",
      "│ [Icon + 2-3     │ [Icon + 2-3     │ [Icon + 2-3     │",
      "│  sentences]     │  sentences]     │  sentences]     │",
      "├─────────────────────────────────────┤",
      "│ PROOF POINT: [Metric, quote, or case study] │",
      "├─────────────────────────────────────┤",
      "│ CTA: [Clear next action] │ [Contact info] │",
      "└─────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "### Workflow / Architecture Demo",
      "",
      "**Structure based on complexity:**",
      "",
      "| Complexity | Components | Structure |",
      "|------------|------------|-----------|",
      "| **Simple** | 3-5 | Single-view diagram with step annotations |",
      "| **Medium** | 5-10 | Zoomable canvas with step-by-step walkthrough |",
      "| **Complex** | 10+ | Multi-layer view (overview → detailed) with guided tour |",
      "",
      "**Standard elements:**",
      "",
      "1. **Title bar**: `[Scenario Name] — Powered by [Seller Product]`",
      "2. **Component nodes**: Visual boxes/icons for each system",
      "3. **Flow arrows**: Animated connections showing data movement",
      "4. **Step panel**: Sidebar explaining current step in plain language",
      "5. **Controls**: Play / Pause / Step Forward / Step Back / Reset",
      "6. **Annotations**: Callouts for key decision points and value-adds",
      "7. **Data preview**: Sample payloads or transformations at each step",
      "",
      "---",
      "",
      "## Phase 3: Content Generation",
      "",
      "### General Principles",
      "",
      "All content should:",
      "- Reference **specific pain points** from user input or transcripts",
      "- Use **prospect's language** — their terminology, their stated priorities",
      "- Map **seller's product** → **prospect's needs** explicitly",
      "- Include **proof points** where available (case studies, metrics, quotes)",
      "- Feel **tailored, not templated**",
      "",
      "---",
      "",
      "### Section Templates",
      "",
      "#### Hero / Intro",
      "```",
      "Headline: \"[Prospect's Goal] with [Seller's Product]\"",
      "Subhead: Tie to their stated priority or top industry challenge",
      "Metrics: 3-4 key facts about the prospect (shows we did homework)",
      "```",
      "",
      "#### Their Priorities (if discovery follow-up)",
      "```",
      "Reference specific pain points from conversation:",
      "- Use their exact words where possible",
      "- Show we listened and understood",
      "- Connect each to how we help",
      "```",
      "",
      "#### Solution Mapping",
      "```",
      "For each pain point:",
      "├── The challenge (in their words)",
      "├── How [Product] addresses it",
      "├── Proof point or example",
      "└── Outcome / benefit",
      "```",
      "",
      "#### Use Cases / Demos",
      "```",
      "3-5 relevant use cases:",
      "├── Visual mockup or interactive demo",
      "├── Business impact (quantified if possible)",
      '├── "How it works" — 3-4 step summary',
      "└── Relevant to their industry/role",
      "```",
      "",
      "#### ROI / Business Case",
      "```",
      "Interactive calculator with:",
      "├── Inputs relevant to their business (from research)",
      "│   ├── Number of users/developers",
      "│   ├── Current costs or time spent",
      "│   └── Expected improvement %",
      "├── Outputs:",
      "│   ├── Annual value / savings",
      "│   ├── Cost of solution",
      "│   ├── Net ROI",
      "│   └── Payback period",
      "└── Assumptions clearly stated (editable)",
      "```",
      "",
      "#### Why Us / Differentiators",
      "```",
      "├── Differentiators vs. alternatives they might consider",
      "├── Trust, security, compliance positioning",
      "├── Support and partnership model",
      "└── Customer proof points (logos, quotes, case studies)",
      "```",
      "",
      "#### Next Steps / CTA",
      "```",
      "├── Clear action aligned to Purpose (c)",
      '├── Specific next step (not vague "let\'s chat")',
      "├── Contact information",
      "├── Suggested timeline",
      "└── What happens after they take action",
      "```",
      "",
      "---",
      "",
      "### Workflow Demo Content",
      "",
      "#### Component Definitions",
      "",
      "For each system, define:",
      "",
      "```yaml",
      "component:",
      '  id: "snowflake"',
      '  label: "Snowflake Data Warehouse"',
      '  type: "database"  # database | api | ai | middleware | human | document | output',
      '  icon: "database"',
      '  description: "Financial performance data"',
      '  brand_color: "#29B5E8"',
      "```",
      "",
      "**Component types:**",
      "- `human` — Person initiating or receiving",
      "- `document` — PDFs, contracts, files",
      "- `ai` — AI/ML models, agents",
      "- `database` — Data stores, warehouses",
      "- `api` — APIs, services",
      "- `middleware` — Integration platforms, MCP servers",
      "- `output` — Dashboards, reports, notifications",
      "",
      "#### Flow Steps",
      "",
      "For each step, define:",
      "",
      "```yaml",
      "step:",
      "  number: 1",
      '  from: "human"',
      '  to: "claude"',
      '  action: "Initiates performance review"',
      '  description: "Sarah, a Brand Analyst at [Prospect], kicks off the quarterly review..."',
      '  data_example: "Review request: Nike brand, Q4 2025"',
      '  duration: "~1 second"',
      '  value_note: "No manual data gathering required"',
      "```",
      "",
      "#### Scenario Narrative",
      "",
      "Write a clear, specific walkthrough:",
      "",
      "```",
      "Step 1: Human Trigger",
      '"Sarah, a Brand Performance Analyst at Centric Brands, needs to review',
      "Q4 performance for the Nike license agreement. She opens the review",
      "dashboard and clicks 'Start Review'...\"",
      "",
      "Step 2: Contract Analysis",
      '"Claude retrieves the Nike contract PDF and extracts the performance',
      "obligations: minimum $50M revenue, 12% margin requirement, quarterly",
      'reporting deadline..."',
      "",
      "Step 3: Data Query",
      '"Claude formulates a query and sends it to Workato DataGenie:',
      "'Get Q4 2025 revenue and gross margin for Nike brand from Snowflake'...\"",
      "",
      "Step 4: Results & Synthesis",
      '"Snowflake returns the data. Claude compares actuals vs. obligations:',
      "Revenue $52.3M ✓ (exceeded by $2.3M)",
      'Margin 11.2% ⚠️ (0.8% below threshold)..."',
      "",
      "Step 5: Insight Delivery",
      '"Claude synthesizes findings into an executive summary with',
      "recommendations: 'Review promotional spend allocation to improve",
      "margin performance...'\"",
      "```",
      "",
      "---",
      "",
      "## Phase 4: Visual Design",
      "",
      "### Color System",
      "",
      "```css",
      ":root {",
      "    /* === Prospect Brand (Primary) === */",
      "    --brand-primary: #[extracted from research];",
      "    --brand-secondary: #[extracted];",
      "    --brand-primary-rgb: [r, g, b]; /* For rgba() usage */",
      "",
      "    /* === Dark Theme Base === */",
      "    --bg-primary: #0a0d14;",
      "    --bg-elevated: #0f131c;",
      "    --bg-surface: #161b28;",
      "    --bg-hover: #1e2536;",
      "",
      "    /* === Text === */",
      "    --text-primary: #ffffff;",
      "    --text-secondary: rgba(255, 255, 255, 0.7);",
      "    --text-muted: rgba(255, 255, 255, 0.5);",
      "",
      "    /* === Accent === */",
      "    --accent: var(--brand-primary);",
      "    --accent-hover: var(--brand-secondary);",
      "    --accent-glow: rgba(var(--brand-primary-rgb), 0.3);",
      "",
      "    /* === Status === */",
      "    --success: #10b981;",
      "    --warning: #f59e0b;",
      "    --error: #ef4444;",
      "}",
      "```",
      "",
      "### Typography",
      "",
      "```css",
      "/* Primary: Clean, professional sans-serif */",
      "font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;",
      "",
      "/* Headings */",
      "h1: 2.5rem, font-weight: 700",
      "h2: 1.75rem, font-weight: 600",
      "h3: 1.25rem, font-weight: 600",
      "",
      "/* Body */",
      "body: 1rem, font-weight: 400, line-height: 1.6",
      "",
      "/* Captions/Labels */",
      "small: 0.875rem, font-weight: 500",
      "```",
      "",
      "### Visual Elements",
      "",
      "**Cards:**",
      "- Background: `var(--bg-surface)`",
      "- Border: 1px solid rgba(255,255,255,0.1)",
      "- Border-radius: 12px",
      "- Box-shadow: subtle, layered",
      "- Hover: slight elevation, border glow",
      "",
      "**Buttons:**",
      "- Primary: `var(--accent)` background, white text",
      "- Secondary: transparent, accent border",
      "- Hover: brightness increase, subtle scale",
      "",
      "**Animations:**",
      "- Transitions: 200-300ms ease",
      "- Tab switches: fade + slide",
      "- Hover states: smooth, not jarring",
      "- Loading: subtle pulse or skeleton",
      "",
      "### Workflow Demo Specific",
      "",
      "**Component Nodes:**",
      "```css",
      ".node {",
      "    background: var(--bg-surface);",
      "    border: 2px solid var(--brand-primary);",
      "    border-radius: 12px;",
      "    padding: 16px;",
      "    min-width: 140px;",
      "}",
      "",
      ".node.active {",
      "    box-shadow: 0 0 20px var(--accent-glow);",
      "    border-color: var(--accent);",
      "}",
      "",
      ".node.human {",
      "    border-color: #f59e0b; /* Warm color for humans */",
      "}",
      "",
      ".node.ai {",
      "    background: linear-gradient(135deg, var(--bg-surface), var(--bg-elevated));",
      "    border-color: var(--accent);",
      "}",
      "```",
      "",
      "**Flow Arrows:**",
      "```css",
      ".arrow {",
      "    stroke: var(--text-muted);",
      "    stroke-width: 2;",
      "    fill: none;",
      "    marker-end: url(#arrowhead);",
      "}",
      "",
      ".arrow.active {",
      "    stroke: var(--accent);",
      "    stroke-dasharray: 8 4;",
      "    animation: flowDash 1s linear infinite;",
      "}",
      "```",
      "",
      "**Canvas:**",
      "```css",
      ".canvas {",
      "    background:",
      "        radial-gradient(circle at center, var(--bg-elevated) 0%, var(--bg-primary) 100%),",
      '        url("data:image/svg+xml,..."); /* Subtle grid pattern */',
      "    overflow: auto;",
      "}",
      "```",
      "",
      "---",
      "",
      "## Phase 5: Clarifying Questions (REQUIRED)",
      "",
      "**Before building any asset, always ask clarifying questions.** This ensures alignment and prevents wasted effort.",
      "",
      "### Step 5.1: Summarize Understanding",
      "",
      "First, show the user what you understood:",
      "",
      "```",
      "\"Here's what I'm planning to build:",
      "",
      "**Asset**: [Format] for [Prospect Company]",
      "**Audience**: [Audience type] — specifically [roles if known]",
      "**Goal**: [Purpose] → driving toward [desired action]",
      "**Key themes**: [2-3 main points to emphasize]",
      "",
      "[For workflow demos, also show:]",
      "**Components**: [List of systems]",
      "**Flow**: [Step 1] → [Step 2] → [Step 3] → ...",
      "```",
      "",
      "### Step 5.2: Ask Standard Questions (ALL formats)",
      "",
      "| Question | Why |",
      "|----------|-----|",
      '| "Does this match your vision?" | Confirm understanding |',
      '| "What\'s the ONE thing this must nail to succeed?" | Focus on priority |',
      '| "Tone preference? (Bold & confident / Consultative / Technical & precise)" | Style alignment |',
      '| "Focused and concise, or comprehensive?" | Scope calibration |',
      "",
      "### Step 5.3: Ask Format-Specific Questions",
      "",
      "#### Interactive Landing Page:",
      '- "Which sections matter most for this audience?"',
      '- "Any specific demos or use cases to highlight?"',
      '- "Should I include an ROI calculator?"',
      '- "Any competitor positioning to address?"',
      "",
      "#### Deck-Style:",
      '- "How long is the presentation? (helps with slide count)"',
      '- "Presenting live, or a leave-behind?"',
      '- "Any specific flow or narrative arc in mind?"',
      "",
      "#### One-Pager:",
      '- "What\'s the single most important message?"',
      '- "Any specific proof point or stat to feature?"',
      '- "Will this be printed or digital?"',
      "",
      "#### Workflow / Architecture Demo:",
      '- "Let me confirm the components: [list]. Anything missing?"',
      '- "Here\'s the flow I understood: [steps]. Correct?"',
      '- "Should the demo show realistic sample data, or keep it abstract?"',
      '- "Any integration details to highlight or downplay?"',
      '- "Should viewers be able to click through steps, or auto-play?"',
      "",
      "### Step 5.4: Confirm and Proceed",
      "",
      "After user responds:",
      "",
      "```",
      '"Got it. I have what I need. Building your [format] now..."',
      "```",
      "",
      "Or, if still unclear:",
      "",
      "```",
      '"One more quick question: [specific follow-up]"',
      "```",
      "",
      '**Max 2 rounds of questions.** If still ambiguous, make a reasonable choice and note: "I went with X — easy to adjust if you prefer Y."',
      "",
      "---",
      "",
      "## Phase 6: Build & Deliver",
      "",
      "### Build the Asset",
      "",
      "Following all specifications above:",
      "1. Generate structure based on Phase 2",
      "2. Create content based on Phase 3",
      "3. Apply visual design based on Phase 4",
      "4. Ensure all interactive elements work",
      "5. Test responsiveness (if applicable)",
      "",
      "### Output Format",
      "",
      "**All formats**: Self-contained HTML file",
      "- All CSS inline or in `<style>` tags",
      "- All JS inline or in `<script>` tags",
      "- No external dependencies (except Google Fonts)",
      "- Single file for easy sharing",
      "",
      "**File naming**: `[ProspectName]-[format]-[date].html`",
      "- Example: `CentricBrands-workflow-demo-2026-01-28.html`",
      "",
      "### Delivery Message",
      "",
      "```markdown",
      "## ✓ Asset Created: [Prospect Name]",
      "",
      "[View your asset](computer:///path/to/file.html)",
      "",
      "---",
      "",
      "**Summary**",
      "- **Format**: [Interactive Page / Deck / One-Pager / Workflow Demo]",
      "- **Audience**: [Type and roles]",
      "- **Purpose**: [Goal] → [Desired action]",
      "- **Sections/Steps**: [Count and list]",
      "",
      "---",
      "",
      "**Deployment Options**",
      "",
      "To share this with your customer:",
      "- **Static hosting**: Upload to Netlify, Vercel, GitHub Pages, AWS S3, or any static host",
      "- **Password protection**: Most hosts offer this (e.g., Netlify site protection)",
      "- **Direct share**: Send the HTML file directly — it's fully self-contained",
      "- **Embed**: The file can be iframed into other pages if needed",
      "",
      "---",
      "",
      "**Customization**",
      "",
      "Let me know if you'd like to:",
      "- Adjust colors or styling",
      "- Add, remove, or reorder sections",
      "- Refine any messaging or copy",
      "- Change the flow or architecture (for workflow demos)",
      "- Add more interactive elements",
      "- Export as PDF or static images",
      "```",
      "",
      "---",
      "",
      "## Phase 7: Iteration Support",
      "",
      "After delivery, be ready to iterate:",
      "",
      "| User Request | Action |",
      "|--------------|--------|",
      '| "Change the colors" | Regenerate with new palette, keep content |',
      '| "Add a section on X" | Insert new section, maintain flow |',
      '| "Make it shorter" | Condense, prioritize key points |',
      '| "The flow is wrong" | Rebuild architecture based on correction |',
      '| "Use our brand instead" | Switch from prospect brand to seller brand |',
      '| "Add more detail on step 3" | Expand that section specifically |',
      '| "Can I get this as a PDF?" | Provide print-optimized version |',
      "",
      "**Remember**: Default to prospect's brand colors, but seller can adjust to their own brand or a neutral palette after initial build.",
      "",
      "---",
      "",
      "## Quality Checklist",
      "",
      "Before delivering, verify:",
      "",
      "### Content",
      "- [ ] Prospect company name spelled correctly throughout",
      "- [ ] Leadership names are current (not outdated)",
      "- [ ] Pain points accurately reflect input/transcripts",
      "- [ ] Seller's product accurately represented",
      "- [ ] No placeholder text remaining",
      "- [ ] Proof points are accurate and sourced",
      "",
      "### Visual",
      "- [ ] Brand colors applied correctly",
      "- [ ] All text readable (contrast)",
      "- [ ] Animations smooth, not distracting",
      "- [ ] Mobile responsive (if interactive page)",
      "- [ ] Dark theme looks polished",
      "",
      "### Functional",
      "- [ ] All tabs/sections load correctly",
      "- [ ] Interactive elements work (calculators, demos)",
      "- [ ] Workflow steps animate properly (if applicable)",
      "- [ ] Navigation is intuitive",
      "- [ ] CTA is clear and clickable",
      "",
      "### Professional",
      "- [ ] Tone matches audience",
      "- [ ] Appropriate level of detail for purpose",
      "- [ ] No typos or grammatical errors",
      "- [ ] Feels tailored, not templated",
      "",
      "---",
      "",
      "## Examples",
      "",
      "### Example 1: Executive Landing Page",
      "",
      "**Input:**",
      "- Prospect: Acme Corp (manufacturing)",
      "- Audience: C-suite",
      "- Purpose: Exec alignment after discovery",
      "- Format: Interactive landing page",
      "",
      "**Output structure:**",
      "```",
      "[Tabs]",
      "Strategic Fit | Business Impact | ROI Calculator | Security & Trust | Next Steps",
      "",
      "[Strategic Fit tab]",
      "- Acme's stated priorities (from discovery call)",
      "- How [Product] aligns",
      "- Relevant manufacturing customers",
      "```",
      "",
      "### Example 2: Technical Workflow Demo",
      "",
      "**Input:**",
      "- Prospect: Centric Brands",
      "- Audience: IT architects",
      "- Purpose: POC proposal",
      "- Format: Workflow demo",
      "- Components: Claude, Workato DataGenie, Snowflake, PDF contracts",
      "",
      "**Output structure:**",
      "```",
      "[Interactive canvas with 5 nodes]",
      "Human → Claude → PDF Contracts → Workato → Snowflake",
      "         ↓",
      "    [Results back to Human]",
      "",
      "[Step-by-step walkthrough with sample data]",
      "[Controls: Play | Pause | Step | Reset]",
      "```",
      "",
      "### Example 3: Sales One-Pager",
      "",
      "**Input:**",
      "- Prospect: TechStart Inc",
      "- Audience: VP Engineering",
      "- Purpose: Leave-behind after first meeting",
      "- Format: One-pager",
      "",
      "**Output structure:**",
      "```",
      'Hero: "Accelerate TechStart\'s Product Velocity"',
      "Point 1: [Dev productivity]",
      "Point 2: [Code quality]",
      "Point 3: [Time to market]",
      'Proof: "Similar companies saw 40% faster releases"',
      'CTA: "Schedule technical deep-dive"',
      "```",
      "",
      "---",
      "",
      "## Appendix: Component Icons",
      "",
      "For workflow demos, use these icon mappings:",
      "",
      "| Type | Icon | Example |",
      "|------|------|---------|",
      "| human | 👤 or person SVG | User, Analyst, Admin |",
      "| document | 📄 or file SVG | PDF, Contract, Report |",
      "| ai | 🤖 or brain SVG | Claude, AI Agent |",
      "| database | 🗄️ or cylinder SVG | Snowflake, Postgres |",
      "| api | 🔌 or plug SVG | REST API, GraphQL |",
      "| middleware | ⚡ or hub SVG | Workato, MCP Server |",
      "| output | 📊 or screen SVG | Dashboard, Report |",
      "",
      "---",
      "",
      "## Appendix: Brand Color Fallbacks",
      "",
      "If brand colors cannot be extracted:",
      "",
      "| Industry | Primary | Secondary |",
      "|----------|---------|-----------|",
      "| Technology | #2563eb | #7c3aed |",
      "| Finance | #0f172a | #3b82f6 |",
      "| Healthcare | #0891b2 | #06b6d4 |",
      "| Manufacturing | #ea580c | #f97316 |",
      "| Retail | #db2777 | #ec4899 |",
      "| Energy | #16a34a | #22c55e |",
      "| Default | #3b82f6 | #8b5cf6 |",
      "",
      "---",
      "",
      "*Skill created for generalized sales asset generation. Works for any seller, any product, any prospect.*",
      "",
    ].join("\n"),
  },
  "sales-daily-briefing": {
    "SKILL.md": [
      "---",
      "name: sales-daily-briefing",
      'description: "Start your day with a prioritized sales briefing. Works standalone when you tell me your meetings and priorities, supercharged when you connect your calendar, CRM, and email. Trigger with \\"morning briefing\\", \\"daily brief\\", \\"what\'s on my plate today\\", \\"prep my day\\", or \\"start my day\\"."',
      "metadata:",
      "  plugin: sales",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Daily Sales Briefing",
      "",
      "Get a clear view of what matters most today. This skill works with whatever you tell me, and gets richer when you connect your tools.",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                      DAILY BRIEFING                              │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  ALWAYS (works standalone)                                       │",
      "│  ✓ You tell me: today's meetings, key deals, priorities         │",
      "│  ✓ I organize: prioritized action plan for your day             │",
      "│  ✓ Output: scannable 2-minute briefing                          │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + Calendar: auto-pull today's meetings with attendees          │",
      "│  + CRM: pipeline alerts, tasks, deal health                     │",
      "│  + Email: unread from key accounts, waiting on replies          │",
      "│  + Enrichment: overnight signals on your accounts               │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## Getting Started",
      "",
      "When you run this skill, I'll ask for what I need:",
      "",
      "**If no calendar connected:**",
      '> "What meetings do you have today? (Just paste your calendar or list them)"',
      "",
      "**If no CRM connected:**",
      '> "What deals are you focused on this week? Any that need attention?"',
      "",
      "**If you have connectors:**",
      "I'll pull everything automatically and just show you the briefing.",
      "",
      "---",
      "",
      "## Connectors (Optional)",
      "",
      "Connect your tools to supercharge this skill:",
      "",
      "| Connector | What It Adds |",
      "|-----------|--------------|",
      "| **Calendar** | Today's meetings with attendees, times, and context |",
      "| **CRM** | Open pipeline, deals closing soon, overdue tasks, stale deals |",
      "| **Email** | Unread from opportunity contacts, emails waiting on replies |",
      "| **Enrichment** | Overnight signals: funding, hiring, news on your accounts |",
      "",
      "> **No connectors?** No problem. Tell me your meetings and deals, and I'll create your briefing.",
      "",
      "---",
      "",
      "## Output Format",
      "",
      "```markdown",
      "# Daily Briefing | [Day, Month Date]",
      "",
      "---",
      "",
      "## #1 Priority",
      "",
      "**[Most important thing to do today]**",
      "[Why it matters and what to do about it]",
      "",
      "---",
      "",
      "## Today's Numbers",
      "",
      "| Open Pipeline | Closing This Month | Meetings Today | Action Items |",
      "|---------------|-------------------|----------------|--------------|",
      "| $[X] | $[X] | [N] | [N] |",
      "",
      "---",
      "",
      "## Today's Meetings",
      "",
      "### [Time] — [Company] ([Meeting Type])",
      "**Attendees:** [Names]",
      "**Context:** [One-line: deal status, last touch, what's at stake]",
      "**Prep:** [Quick action before this meeting]",
      "",
      "### [Time] — [Company] ([Meeting Type])",
      "**Attendees:** [Names]",
      "**Context:** [One-line context]",
      "**Prep:** [Quick action]",
      "",
      "*Run `call-prep [company]` for detailed meeting prep*",
      "",
      "---",
      "",
      "## Pipeline Alerts",
      "",
      "### Needs Attention",
      "| Deal | Stage | Amount | Alert | Action |",
      "|------|-------|--------|-------|--------|",
      "| [Deal] | [Stage] | $[X] | [Why flagged] | [What to do] |",
      "",
      "### Closing This Week",
      "| Deal | Close Date | Amount | Confidence | Blocker |",
      "|------|------------|--------|------------|---------|",
      "| [Deal] | [Date] | $[X] | [H/M/L] | [If any] |",
      "",
      "---",
      "",
      "## Email Priorities",
      "",
      "### Needs Response",
      "| From | Subject | Received |",
      "|------|---------|----------|",
      "| [Name @ Company] | [Subject] | [Time] |",
      "",
      "### Waiting On Reply",
      "| To | Subject | Sent | Days Waiting |",
      "|----|---------|------|--------------|",
      "| [Name @ Company] | [Subject] | [Date] | [N] |",
      "",
      "---",
      "",
      "## Suggested Actions",
      "",
      "1. **[Action]** — [Why now]",
      "2. **[Action]** — [Why now]",
      "3. **[Action]** — [Why now]",
      "",
      "---",
      "",
      "*Run `call-prep [company]` before your meetings*",
      "*Run `call-follow-up` after each call*",
      "```",
      "",
      "---",
      "",
      "## Execution Flow",
      "",
      "### Step 1: Gather Context",
      "",
      "**If connectors available:**",
      "```",
      "1. Calendar → Get today's events",
      "   - Filter to external meetings (non-company attendees)",
      "   - Pull: time, title, attendees, description",
      "",
      "2. CRM → Query your pipeline",
      "   - Open opportunities owned by you",
      "   - Flag: closing this week, no activity 7+ days, slipped dates",
      "   - Get: overdue tasks, upcoming tasks",
      "",
      "3. Email → Check priority messages",
      "   - Unread from opportunity contact domains",
      "   - Sent messages with no reply (3+ days)",
      "",
      "4. Enrichment → Check signals (if available)",
      "   - Funding, hiring, news on open accounts",
      "```",
      "",
      "**If no connectors:**",
      "```",
      "Ask user:",
      '1. "What meetings do you have today?"',
      '2. "What deals are you focused on? Any closing soon or needing attention?"',
      '3. "Anything urgent I should know about?"',
      "",
      "Work with whatever they provide.",
      "```",
      "",
      "### Step 2: Prioritize",
      "",
      "```",
      "Priority ranking:",
      "1. URGENT: Deal closing today/tomorrow not yet won",
      "2. HIGH: Meeting today with high-value opportunity",
      "3. HIGH: Unread email from decision-maker",
      "4. MEDIUM: Deal closing this week",
      "5. MEDIUM: Stale deal (7+ days no activity)",
      "6. LOW: Tasks due this week",
      "",
      "Select #1 Priority:",
      "- If meeting with >$50K deal today → prep that",
      "- If deal closing today → focus on close",
      "- If urgent email from buyer → respond first",
      "- Else → highest-value stale deal",
      "```",
      "",
      "### Step 3: Generate Briefing",
      "",
      "```",
      "Assemble sections based on available data:",
      "",
      "1. #1 Priority — Always include (even if simple)",
      "2. Today's Numbers — If CRM connected, otherwise skip",
      "3. Today's Meetings — From calendar or user input",
      "4. Pipeline Alerts — If CRM connected",
      "5. Email Priorities — If email connected",
      "6. Suggested Actions — Always include top 3 actions",
      "```",
      "",
      "---",
      "",
      "## Quick Mode",
      "",
      'Say "quick brief" or "tldr my day" for abbreviated version:',
      "",
      "```markdown",
      "# Quick Brief | [Date]",
      "",
      "**#1:** [Priority action]",
      "",
      "**Meetings:** [N] — [Company 1], [Company 2], [Company 3]",
      "",
      "**Alerts:**",
      "- [Alert 1]",
      "- [Alert 2]",
      "",
      "**Do Now:** [Single most important action]",
      "```",
      "",
      "---",
      "",
      "## End of Day Mode",
      "",
      'Say "wrap up my day" or "end of day summary" after your last meeting:',
      "",
      "```markdown",
      "# End of Day | [Date]",
      "",
      "**Completed:**",
      "- [Meeting 1] — [Outcome]",
      "- [Meeting 2] — [Outcome]",
      "",
      "**Pipeline Changes:**",
      "- [Deal] moved to [Stage]",
      "",
      "**Tomorrow's Focus:**",
      "- [Priority 1]",
      "- [Priority 2]",
      "",
      "**Open Loops:**",
      "- [ ] [Unfinished item needing follow-up]",
      "```",
      "",
      "---",
      "",
      "## Tips",
      "",
      "1. **Connect your calendar first** — Biggest time saver",
      "2. **Add CRM second** — Unlocks pipeline alerts",
      "3. **Even without connectors** — Just tell me your meetings and I'll help prioritize",
      "",
      "---",
      "",
      "## Related Skills",
      "",
      "- **call-prep** — Deep prep for any specific meeting",
      "- **call-follow-up** — Process notes after calls",
      "- **account-research** — Research a company before first meeting",
      "",
    ].join("\n"),
  },
  "sales-draft-outreach": {
    "SKILL.md": [
      "---",
      "name: sales-draft-outreach",
      'description: "Research a prospect then draft personalized outreach. Uses web research by default, supercharged with enrichment and CRM. Trigger with \\"draft outreach to [person/company]\\", \\"write cold email to [prospect]\\", \\"reach out to [name]\\"."',
      "metadata:",
      "  plugin: sales",
      "  type: skill",
      '  plugin-version: "1.0.0"',
      "---",
      "",
      "# Draft Outreach",
      "",
      "Research first, then draft. This skill never sends generic outreach - it always researches the prospect first to personalize the message. Works standalone with web search, supercharged when you connect your tools.",
      "",
      "## Connectors (Optional)",
      "",
      "| Connector | What It Adds |",
      "|-----------|--------------|",
      "| **Enrichment** | Verified email, phone, background details |",
      "| **CRM** | Prior relationship context, existing contacts |",
      "| **Email** | Create draft directly in your inbox |",
      "",
      "> **No connectors?** Web research works great. I'll output the email text for you to copy.",
      "",
      "---",
      "",
      "## How It Works",
      "",
      "```",
      "+------------------------------------------------------------------+",
      "|                      DRAFT OUTREACH                               |",
      "|                                                                   |",
      "|  Step 1: RESEARCH (always happens first)                         |",
      "|  - Web search (default)                                           |",
      "|  - + Enrichment (if enrichment tools connected)                  |",
      "|  - + CRM (if CRM connected)                                      |",
      "|                                                                   |",
      "|  Step 2: DRAFT (based on research)                               |",
      "|  - Personalized opening (from research)                          |",
      "|  - Relevant hook (their priorities)                              |",
      "|  - Clear CTA                                                      |",
      "|                                                                   |",
      "|  Step 3: DELIVER (based on connectors)                           |",
      "|  - Email draft (if email connected)                              |",
      "|  - Copy for LinkedIn (always)                                    |",
      "|  - Output to user (always)                                        |",
      "+------------------------------------------------------------------+",
      "```",
      "",
      "---",
      "",
      "## Output Format",
      "",
      "```markdown",
      "# Outreach Draft: [Person] @ [Company]",
      "**Generated:** [Date] | **Research Sources:** [Web, Enrichment, CRM]",
      "",
      "---",
      "",
      "## Research Summary",
      "",
      "**Target:** [Name], [Title] at [Company]",
      "**Hook:** [Why reaching out now - the personalized angle]",
      "**Goal:** [What you want from this outreach]",
      "",
      "---",
      "",
      "## Email Draft",
      "",
      '**To:** [email if known, or "find email" note]',
      "**Subject:** [Personalized subject line]",
      "",
      "---",
      "",
      "[Email body]",
      "",
      "---",
      "",
      "**Subject Line Alternatives:**",
      "1. [Option 2]",
      "2. [Option 3]",
      "",
      "---",
      "",
      "## LinkedIn Message (if no email)",
      "",
      "**Connection Request (< 300 chars):**",
      "[Short, no-pitch connection request]",
      "",
      "**Follow-up Message (after connected):**",
      "[Value-first message]",
      "",
      "---",
      "",
      "## Why This Approach",
      "",
      "| Element | Based On |",
      "|---------|----------|",
      "| Opening | [Research finding that makes it personal] |",
      "| Hook | [Their priority/pain point] |",
      "| Proof | [Relevant customer story] |",
      "| CTA | [Low-friction ask] |",
      "",
      "---",
      "",
      "## Email Draft Status",
      "",
      "[Draft created - check ~~email]",
      "[Email not connected - copy email above]",
      "[No email found - use LinkedIn approach]",
      "",
      "---",
      "",
      "## Follow-up Sequence (Optional)",
      "",
      "**Day 3 - Follow-up 1:**",
      "[Short, new angle]",
      "",
      "**Day 7 - Follow-up 2:**",
      "[Different value prop]",
      "",
      "**Day 14 - Break-up:**",
      "[Final attempt]",
      "```",
      "",
      "---",
      "",
      "## Execution Flow",
      "",
      "### Step 1: Parse Request",
      "",
      "```",
      "Input patterns:",
      '- "draft outreach to John Smith at Acme" → Person + company',
      '- "write cold email to Acme\'s CTO" → Role + company',
      '- "reach out to sarah@acme.com" → Email provided',
      '- "LinkedIn message to [LinkedIn URL]" → Profile provided',
      "```",
      "",
      "### Step 2: Research First (Always)",
      "",
      "**Use research-prospect skill internally:**",
      "```",
      "1. Web search for company + person",
      "2. If Enrichment connected: Get verified contact info, background",
      "3. If CRM connected: Check for prior relationship",
      "```",
      "",
      "**Must find before drafting:**",
      "- Who they are (title, background)",
      "- What the company does",
      "- Recent news or trigger",
      "- Personalization hook",
      "",
      "### Step 3: Identify Hook",
      "",
      "```",
      "Priority order for hooks:",
      "1. Trigger event (funding, hiring, news) → Most timely",
      "2. Mutual connection → Social proof",
      "3. Their content (post, article, talk) → Shows you did research",
      "4. Company initiative → Relevant to their priorities",
      "5. Role-based pain point → Least personal but still relevant",
      "```",
      "",
      "### Step 4: Draft Message",
      "",
      "**Email Structure (AIDA):**",
      "```",
      "SUBJECT: [Personalized, <50 chars, no spam words]",
      "",
      "[Opening: Personal hook - shows you researched them]",
      "",
      "[Interest: Their problem/opportunity in 1-2 sentences]",
      "",
      "[Desire: Brief proof point - similar company result]",
      "",
      "[Action: Clear, low-friction CTA]",
      "",
      "[Signature]",
      "```",
      "",
      "**LinkedIn Connection Request (<300 chars):**",
      "```",
      "Hi [Name], [Mutual connection/shared interest/genuine compliment].",
      "Would love to connect. [No pitch]",
      "```",
      "",
      "**LinkedIn Follow-up Message:**",
      "```",
      "Thanks for connecting! [Value-first: insight, article, observation]",
      "",
      "[Soft transition to why you reached out]",
      "",
      "[Question, not pitch]",
      "```",
      "",
      "### Step 5: Create Email Draft",
      "",
      "```",
      "If email connector available:",
      "1. Create draft with to, subject, body",
      "2. Return draft link",
      '3. Note: "Draft created - review and send"',
      "",
      "If not available:",
      "1. Output email text",
      '2. Note: "Copy to your email client"',
      "```",
      "",
      "---",
      "",
      "## Capability by Connector",
      "",
      "| Capability | Web Only | + Enrichment | + CRM | + Email |",
      "|------------|----------|--------------|-------|---------|",
      "| Personalized opening | Basic | Deep | With history | Same |",
      "| Verified email | No | Yes | Yes | Yes |",
      "| Background details | Public only | Full | Full | Full |",
      "| Prior relationship | No | No | Yes | Yes |",
      "| Auto-create draft | No | No | No | Yes |",
      "",
      "---",
      "",
      "## Message Templates by Scenario",
      "",
      "### Cold Outreach (No Prior Relationship)",
      "",
      "```",
      "Subject: [Their initiative] + [your angle]",
      "",
      "Hi [Name],",
      "",
      "[Personal hook based on research - news, content, mutual connection].",
      "",
      "[1 sentence on their likely challenge based on role/company].",
      "",
      '[Brief proof: "We helped [Similar Company] achieve [Result]".]',
      "",
      "Worth a 15-min call to see if relevant?",
      "",
      "[Signature]",
      "```",
      "",
      "### Warm Outreach (Have Met / Mutual Connection)",
      "",
      "```",
      "Subject: Following up from [context]",
      "",
      "Hi [Name],",
      "",
      "[Reference to how you know them / who connected you].",
      "",
      "[Why reaching out now - their trigger].",
      "",
      "[Specific value you can offer].",
      "",
      "[CTA]",
      "```",
      "",
      "### Re-Engagement (Went Dark)",
      "",
      "```",
      "Subject: [Short, curiosity-driven]",
      "",
      "Hi [Name],",
      "",
      "[Acknowledge time passed without being guilt-trippy].",
      "",
      "[New reason to reconnect - their news or your news].",
      "",
      "[Simple question to re-open dialogue].",
      "",
      "[Signature]",
      "```",
      "",
      "### Post-Event Follow-up",
      "",
      "```",
      "Subject: Great meeting you at [Event]",
      "",
      "Hi [Name],",
      "",
      "[Specific memory from conversation].",
      "",
      "[Value-add: article, intro, resource related to what you discussed].",
      "",
      "[Soft CTA for next conversation].",
      "```",
      "",
      "---",
      "",
      "## Email Style Guidelines",
      "",
      "1. **Be concise but informative** — Get to the point quickly. Busy people skim.",
      "2. **No markdown formatting** — Never use asterisks, bold (**text**), or other markdown. Write plain text that looks natural in any email client.",
      "3. **Short paragraphs** — 2-3 sentences max per paragraph. White space is your friend.",
      "4. **Simple lists** — If listing items, use plain dashes. No fancy formatting.",
      "",
      "**Good:**",
      "```",
      "Here's what I can share:",
      "- Case study from a similar company",
      "- 15-min intro call this week",
      "- Quick demo if helpful",
      "```",
      "",
      "**Bad:**",
      "```",
      "**What I Can Offer:**",
      "- **Case study** from a similar company",
      "- **Intro call** this week",
      "```",
      "",
      "---",
      "",
      "## What NOT to Do",
      "",
      "**Generic openers:**",
      '- "I hope this email finds you well"',
      '- "I\'m reaching out because..."',
      '- "I wanted to introduce myself"',
      "",
      "**Feature dumps:**",
      "- Long paragraphs about your product",
      "- Multiple value props at once",
      "- No clear CTA",
      "",
      "**Fake personalization:**",
      '- "I noticed you work at [Company]" (obviously)',
      '- "Congrats on your role" (without context)',
      "",
      "**Markdown in emails:**",
      "- Using **bold** or *italic* asterisks",
      "- Headers or formatted lists that won't render",
      "",
      "**Instead:**",
      "- Lead with something specific you learned",
      "- One clear value prop",
      "- One clear ask",
      "- Plain text formatting only",
      "",
      "---",
      "",
      "## Channel Selection",
      "",
      "```",
      "IF verified email available:",
      "  → Email preferred (higher response rate)",
      "  → Also provide LinkedIn backup",
      "",
      "IF no email:",
      "  → LinkedIn connection request",
      "  → Follow-up message template for after connection",
      "",
      "IF warm intro possible:",
      "  → Suggest mutual connection outreach first",
      "```",
      "",
      "---",
      "",
      "## Company Configuration [CUSTOMIZE]",
      "",
      "```markdown",
      "## Outreach Settings",
      "",
      "- My name: [Your Name]",
      "- My title: [Your Title]",
      "- My company: [Company Name]",
      "- Value prop: [One sentence - what you help with]",
      "",
      "## Signature",
      "[Your preferred email signature]",
      "",
      "## Proof Points",
      "- [Customer 1]: [Result]",
      "- [Customer 2]: [Result]",
      "- [Customer 3]: [Result]",
      "",
      "## CTA Options",
      '- Default: "Worth a 15-min call?"',
      '- Softer: "Open to learning more?"',
      '- Specific: "Can I send over a quick demo?"',
      "",
      "## Tone",
      "- [Professional / Casual / Direct]",
      "- Industry-specific language: [Yes/No]",
      "```",
      "",
      "---",
      "",
      "## Example",
      "",
      '**Input:** "draft outreach to the Head of Engineering at Notion"',
      "",
      "**Research finds:**",
      "- Name: David Tibbitts",
      "- Recent news: Notion launched AI features",
      "- Hiring: 5 ML engineer roles open",
      "- Background: Ex-Facebook, Stanford CS",
      "",
      "**Output:**",
      "",
      "```markdown",
      "# Outreach Draft: David Tibbitts @ Notion",
      "",
      "## Research Summary",
      "**Target:** David Tibbitts, Head of Engineering at Notion",
      "**Hook:** They're scaling AI features and hiring ML talent",
      "**Goal:** Intro call about AI infrastructure",
      "",
      "---",
      "",
      "## Email Draft",
      "",
      "**To:** david@notion.so",
      "**Subject:** Notion's AI scaling + a thought",
      "",
      "---",
      "",
      "Hi David,",
      "",
      "Saw Notion's AI rollout is gaining serious traction - congrats.",
      "With 5 ML roles open, seems like you're scaling fast.",
      "",
      "Curious how you're thinking about inference infrastructure",
      "as usage grows. We helped [Similar Company] cut their AI",
      "serving costs 40% while improving latency.",
      "",
      "Worth a 15-min call to see if relevant to your roadmap?",
      "",
      "Best,",
      "[Name]",
      "",
      "---",
      "",
      "**Subject Alternatives:**",
      "1. Notion AI + scaling question",
      "2. Quick thought on Notion's ML hiring",
      "",
      "---",
      "",
      "## Email Draft Status",
      "Draft created - check ~~email",
      "```",
      "",
    ].join("\n"),
  },
  "sales-forecast": {
    "SKILL.md": [
      "---",
      "name: sales-forecast",
      "description: Generate a weighted sales forecast with best/likely/worst scenarios, commit vs. upside breakdown, and gap analysis",
      "metadata:",
      "  plugin: sales",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <period>",
      "---",
      "",
      "# /forecast",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Generate a weighted sales forecast with risk analysis and commit recommendations.",
      "",
      "## Usage",
      "",
      "```",
      "/forecast",
      "```",
      "",
      "Then provide your pipeline data and targets.",
      "",
      "---",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                        FORECAST                                  │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  STANDALONE (always works)                                       │",
      "│  ✓ Upload CSV export from your CRM                              │",
      "│  ✓ Or paste/describe your pipeline deals                        │",
      "│  ✓ Set your quota and timeline                                  │",
      "│  ✓ Get weighted forecast with stage probabilities               │",
      "│  ✓ Risk-adjusted projections (best/likely/worst case)           │",
      "│  ✓ Commit vs. upside breakdown                                  │",
      "│  ✓ Gap analysis and recommendations                             │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + CRM: Pull pipeline automatically, real-time data             │",
      "│  + Historical win rates by stage, segment, deal size            │",
      "│  + Activity signals for risk scoring                            │",
      "│  + Automatic refresh and tracking over time                     │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## What I Need From You",
      "",
      "### Step 1: Your Pipeline Data",
      "",
      "**Option A: Upload a CSV**",
      "Export your pipeline from your CRM (e.g. Salesforce, HubSpot). I need at minimum:",
      "- Deal/Opportunity name",
      "- Amount",
      "- Stage",
      "- Close date",
      "",
      "Helpful if you have:",
      "- Owner (if team forecast)",
      "- Last activity date",
      "- Created date",
      "- Account name",
      "",
      "**Option B: Paste your deals**",
      "```",
      "Acme Corp - $50K - Negotiation - closes Jan 31",
      "TechStart - $25K - Demo scheduled - closes Feb 15",
      "BigCo - $100K - Discovery - closes Mar 30",
      "```",
      "",
      "**Option C: Describe your territory**",
      '"I have 8 deals in pipeline totaling $400K. Two are in negotiation ($120K), three in evaluation ($180K), three in discovery ($100K)."',
      "",
      "### Step 2: Your Targets",
      "",
      '- **Quota**: What\'s your number? (e.g., "$500K this quarter")',
      '- **Timeline**: When does the period end? (e.g., "Q1 ends March 31")',
      "- **Already closed**: How much have you already booked this period?",
      "",
      "---",
      "",
      "## Output",
      "",
      "```markdown",
      "# Sales Forecast: [Period]",
      "",
      "**Generated:** [Date]",
      "**Data Source:** [CSV upload / Manual input / CRM]",
      "",
      "---",
      "",
      "## Summary",
      "",
      "| Metric | Value |",
      "|--------|-------|",
      "| **Quota** | $[X] |",
      "| **Closed to Date** | $[X] ([X]% of quota) |",
      "| **Open Pipeline** | $[X] |",
      "| **Weighted Forecast** | $[X] |",
      "| **Gap to Quota** | $[X] |",
      "| **Coverage Ratio** | [X]x |",
      "",
      "---",
      "",
      "## Forecast Scenarios",
      "",
      "| Scenario | Amount | % of Quota | Assumptions |",
      "|----------|--------|------------|-------------|",
      "| **Best Case** | $[X] | [X]% | All deals close as expected |",
      "| **Likely Case** | $[X] | [X]% | Stage-weighted probabilities |",
      "| **Worst Case** | $[X] | [X]% | Only commit deals close |",
      "",
      "---",
      "",
      "## Pipeline by Stage",
      "",
      "| Stage | # Deals | Total Value | Probability | Weighted Value |",
      "|-------|---------|-------------|-------------|----------------|",
      "| Negotiation | [X] | $[X] | 80% | $[X] |",
      "| Proposal | [X] | $[X] | 60% | $[X] |",
      "| Evaluation | [X] | $[X] | 40% | $[X] |",
      "| Discovery | [X] | $[X] | 20% | $[X] |",
      "| **Total** | [X] | $[X] | — | $[X] |",
      "",
      "---",
      "",
      "## Commit vs. Upside",
      "",
      "### Commit (High Confidence)",
      "Deals you'd stake your forecast on:",
      "",
      "| Deal | Amount | Stage | Close Date | Why Commit |",
      "|------|--------|-------|------------|------------|",
      "| [Deal] | $[X] | [Stage] | [Date] | [Reason] |",
      "",
      "**Total Commit:** $[X]",
      "",
      "### Upside (Lower Confidence)",
      "Deals that could close but have risk:",
      "",
      "| Deal | Amount | Stage | Close Date | Risk Factor |",
      "|------|--------|-------|------------|-------------|",
      "| [Deal] | $[X] | [Stage] | [Date] | [Risk] |",
      "",
      "**Total Upside:** $[X]",
      "",
      "---",
      "",
      "## Risk Flags",
      "",
      "| Deal | Amount | Risk | Recommendation |",
      "|------|--------|------|----------------|",
      "| [Deal] | $[X] | Close date passed | Update close date or move to lost |",
      "| [Deal] | $[X] | No activity in 14+ days | Re-engage or downgrade stage |",
      "| [Deal] | $[X] | Close date this week, still in discovery | Unlikely to close — push out |",
      "",
      "---",
      "",
      "## Gap Analysis",
      "",
      "**To hit quota, you need:** $[X] more",
      "",
      "**Options to close the gap:**",
      "1. **Accelerate [Deal]** — Currently [stage], worth $[X]. If you can close by [date], you're at [X]% of quota.",
      "2. **Revive [Stalled Deal]** — Last active [date]. Worth $[X]. Reach out to [contact].",
      "3. **New pipeline needed** — You need $[X] in new opportunities at [X]x coverage to be safe.",
      "",
      "---",
      "",
      "## Recommendations",
      "",
      "1. [ ] [Specific action for highest-impact deal]",
      "2. [ ] [Action for at-risk deal]",
      "3. [ ] [Pipeline generation recommendation if gap exists]",
      "```",
      "",
      "---",
      "",
      "## Stage Probabilities (Default)",
      "",
      "If you don't provide custom probabilities, I'll use:",
      "",
      "| Stage | Default Probability |",
      "|-------|---------------------|",
      "| Closed Won | 100% |",
      "| Negotiation / Contract | 80% |",
      "| Proposal / Quote | 60% |",
      "| Evaluation / Demo | 40% |",
      "| Discovery / Qualification | 20% |",
      "| Prospecting / Lead | 10% |",
      "",
      "Tell me if your stages or probabilities are different.",
      "",
      "---",
      "",
      "## If CRM Connected",
      "",
      "- I'll pull your pipeline automatically",
      "- Use your actual historical win rates",
      "- Factor in activity recency for risk scoring",
      "- Track forecast changes over time",
      "- Compare to previous forecasts",
      "",
      "---",
      "",
      "## Tips",
      "",
      "1. **Be honest about commit** — Only commit deals you'd bet on. Upside is for everything else.",
      "2. **Update close dates** — Stale close dates kill forecast accuracy. Push out deals that won't close in time.",
      "3. **Coverage matters** — 3x pipeline coverage is healthy. Below 2x is risky.",
      "4. **Activity = signal** — Deals with no recent activity are at higher risk than stage suggests.",
      "",
    ].join("\n"),
  },
  "sales-pipeline-review": {
    "SKILL.md": [
      "---",
      "name: sales-pipeline-review",
      "description: Analyze pipeline health — prioritize deals, flag risks, get a weekly action plan",
      "metadata:",
      "  plugin: sales",
      "  type: command",
      '  plugin-version: "1.0.0"',
      "  argument-hint: <segment or rep>",
      "---",
      "",
      "# /pipeline-review",
      "",
      "> If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).",
      "",
      "Analyze your pipeline health, prioritize deals, and get actionable recommendations for where to focus.",
      "",
      "## Usage",
      "",
      "```",
      "/pipeline-review",
      "```",
      "",
      "Then provide your pipeline data.",
      "",
      "---",
      "",
      "## How It Works",
      "",
      "```",
      "┌─────────────────────────────────────────────────────────────────┐",
      "│                     PIPELINE REVIEW                              │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  STANDALONE (always works)                                       │",
      "│  ✓ Upload CSV export from your CRM                              │",
      "│  ✓ Or paste/describe your deals                                 │",
      "│  ✓ Health check: flag stale, stuck, and at-risk deals          │",
      "│  ✓ Prioritization: rank deals by impact and closability        │",
      "│  ✓ Hygiene audit: missing data, bad close dates, single-thread │",
      "│  ✓ Weekly action plan: what to focus on                        │",
      "├─────────────────────────────────────────────────────────────────┤",
      "│  SUPERCHARGED (when you connect your tools)                      │",
      "│  + CRM: Pull pipeline automatically, update records             │",
      "│  + Activity data for engagement scoring                         │",
      "│  + Historical patterns for risk prediction                      │",
      "│  + Calendar: See upcoming meetings per deal                     │",
      "└─────────────────────────────────────────────────────────────────┘",
      "```",
      "",
      "---",
      "",
      "## What I Need From You",
      "",
      "**Option A: Upload a CSV**",
      "Export your pipeline from your CRM (e.g. Salesforce, HubSpot). Helpful fields:",
      "- Deal/Opportunity name",
      "- Account name",
      "- Amount",
      "- Stage",
      "- Close date",
      "- Created date",
      "- Last activity date",
      "- Owner (if reviewing a team)",
      "- Primary contact",
      "",
      "**Option B: Paste your deals**",
      "```",
      "Acme Corp - $50K - Negotiation - closes Jan 31 - last activity Jan 20",
      "TechStart - $25K - Demo scheduled - closes Feb 15 - no activity in 3 weeks",
      "BigCo - $100K - Discovery - closes Mar 30 - created last week",
      "```",
      "",
      "**Option C: Describe your pipeline**",
      "\"I have 12 deals. Two big ones in negotiation that I'm confident about. Three stuck in discovery for over a month. The rest are mid-stage but I haven't talked to some of them in a while.\"",
      "",
      "---",
      "",
      "## Output",
      "",
      "```markdown",
      "# Pipeline Review: [Date]",
      "",
      "**Data Source:** [CSV upload / Manual input / CRM]",
      "**Deals Analyzed:** [X]",
      "**Total Pipeline Value:** $[X]",
      "",
      "---",
      "",
      "## Pipeline Health Score: [X/100]",
      "",
      "| Dimension | Score | Issue |",
      "|-----------|-------|-------|",
      "| **Stage Progression** | [X]/25 | [X] deals stuck in same stage 30+ days |",
      "| **Activity Recency** | [X]/25 | [X] deals with no activity in 14+ days |",
      "| **Close Date Accuracy** | [X]/25 | [X] deals with close date in past |",
      "| **Contact Coverage** | [X]/25 | [X] deals single-threaded |",
      "",
      "---",
      "",
      "## Priority Actions This Week",
      "",
      "### 1. [Highest Priority Deal]",
      "**Why:** [Reason — large, closing soon, at risk, etc.]",
      "**Action:** [Specific next step]",
      "**Impact:** $[X] if you close it",
      "",
      "### 2. [Second Priority]",
      "**Why:** [Reason]",
      "**Action:** [Next step]",
      "",
      "### 3. [Third Priority]",
      "**Why:** [Reason]",
      "**Action:** [Next step]",
      "",
      "---",
      "",
      "## Deal Prioritization Matrix",
      "",
      "### Close This Week (Focus Time Here)",
      "| Deal | Amount | Stage | Close Date | Next Action |",
      "|------|--------|-------|------------|-------------|",
      "| [Deal] | $[X] | [Stage] | [Date] | [Action] |",
      "",
      "### Close This Month (Keep Warm)",
      "| Deal | Amount | Stage | Close Date | Status |",
      "|------|--------|-------|------------|--------|",
      "| [Deal] | $[X] | [Stage] | [Date] | [Status] |",
      "",
      "### Nurture (Check-in Periodically)",
      "| Deal | Amount | Stage | Close Date | Status |",
      "|------|--------|-------|------------|--------|",
      "| [Deal] | $[X] | [Stage] | [Date] | [Status] |",
      "",
      "---",
      "",
      "## Risk Flags",
      "",
      "### Stale Deals (No Activity 14+ Days)",
      "| Deal | Amount | Last Activity | Days Silent | Recommendation |",
      "|------|--------|---------------|-------------|----------------|",
      "| [Deal] | $[X] | [Date] | [X] | [Re-engage / Downgrade / Remove] |",
      "",
      "### Stuck Deals (Same Stage 30+ Days)",
      "| Deal | Amount | Stage | Days in Stage | Recommendation |",
      "|------|--------|-------|---------------|----------------|",
      "| [Deal] | $[X] | [Stage] | [X] | [Push / Multi-thread / Qualify out] |",
      "",
      "### Past Close Date",
      "| Deal | Amount | Close Date | Days Overdue | Recommendation |",
      "|------|--------|------------|--------------|----------------|",
      "| [Deal] | $[X] | [Date] | [X] | [Update date / Push to next quarter / Close lost] |",
      "",
      "### Single-Threaded (Only One Contact)",
      "| Deal | Amount | Contact | Risk | Recommendation |",
      "|------|--------|---------|------|----------------|",
      "| [Deal] | $[X] | [Name] | Champion leaves = deal dies | [Identify additional stakeholders] |",
      "",
      "---",
      "",
      "## Hygiene Issues",
      "",
      "| Issue | Count | Deals | Action |",
      "|-------|-------|-------|--------|",
      "| Missing close date | [X] | [List] | Add realistic close dates |",
      "| Missing amount | [X] | [List] | Estimate or qualify |",
      "| Missing next step | [X] | [List] | Define next action |",
      "| No primary contact | [X] | [List] | Assign contact |",
      "",
      "---",
      "",
      "## Pipeline Shape",
      "",
      "### By Stage",
      "| Stage | # Deals | Value | % of Pipeline |",
      "|-------|---------|-------|---------------|",
      "| [Stage] | [X] | $[X] | [X]% |",
      "",
      "### By Close Month",
      "| Month | # Deals | Value |",
      "|-------|---------|-------|",
      "| [Month] | [X] | $[X] |",
      "",
      "### By Deal Size",
      "| Size | # Deals | Value |",
      "|------|---------|-------|",
      "| $100K+ | [X] | $[X] |",
      "| $50K-100K | [X] | $[X] |",
      "| $25K-50K | [X] | $[X] |",
      "| <$25K | [X] | $[X] |",
      "",
      "---",
      "",
      "## Recommendations",
      "",
      "### This Week",
      "1. [ ] [Specific action for priority deal 1]",
      "2. [ ] [Action for at-risk deal]",
      "3. [ ] [Hygiene task]",
      "",
      "### This Month",
      "1. [ ] [Strategic action]",
      "2. [ ] [Pipeline building if needed]",
      "",
      "---",
      "",
      "## Deals to Consider Removing",
      "",
      "These deals may be dead weight:",
      "",
      "| Deal | Amount | Reason | Recommendation |",
      "|------|--------|--------|----------------|",
      "| [Deal] | $[X] | [No activity 60+ days, no response] | Mark closed-lost |",
      "| [Deal] | $[X] | [Pushed 3+ times, no champion] | Qualify out |",
      "```",
      "",
      "---",
      "",
      "## Prioritization Framework",
      "",
      "I'll rank your deals using this framework:",
      "",
      "| Factor | Weight | What I Look For |",
      "|--------|--------|-----------------|",
      "| **Close Date** | 30% | Deals closing soonest get priority |",
      "| **Deal Size** | 25% | Bigger deals = more focus |",
      "| **Stage** | 20% | Later stage = more focus |",
      "| **Activity** | 15% | Active deals get prioritized |",
      "| **Risk** | 10% | Lower risk = safer bet |",
      "",
      'You can tell me to weight differently: "Focus on big deals over soon deals" or "I need quick wins, prioritize close dates."',
      "",
      "---",
      "",
      "## If CRM Connected",
      "",
      "- I'll pull your pipeline automatically",
      "- Update records with new close dates, stages, next steps",
      "- Create follow-up tasks",
      "- Track hygiene improvements over time",
      "",
      "---",
      "",
      "## Tips",
      "",
      "1. **Review weekly** — Pipeline health decays fast. Weekly reviews catch issues early.",
      "2. **Kill dead deals** — Stale opportunities inflate your pipeline and distort forecasts. Be ruthless.",
      "3. **Multi-thread everything** — If one person goes dark, you need a backup contact.",
      "4. **Close dates should mean something** — A close date is when you expect signature, not when you hope for one.",
      "",
    ].join("\n"),
  },
  "skill-creator": {
    "SKILL.md": [
      "---",
      "name: skill-creator",
      "description: Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
      "license: Complete terms in LICENSE.txt",
      "---",
      "",
      "# Skill Creator",
      "",
      "This skill provides guidance for creating effective skills.",
      "",
      "## About Skills",
      "",
      "Skills are modular, self-contained packages that extend Claude's capabilities by providing",
      'specialized knowledge, workflows, and tools. Think of them as "onboarding guides" for specific',
      "domains or tasks—they transform Claude from a general-purpose agent into a specialized agent",
      "equipped with procedural knowledge that no model can fully possess.",
      "",
      "### What Skills Provide",
      "",
      "1. Specialized workflows - Multi-step procedures for specific domains",
      "2. Tool integrations - Instructions for working with specific file formats or APIs",
      "3. Domain expertise - Company-specific knowledge, schemas, business logic",
      "4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks",
      "",
      "## Core Principles",
      "",
      "### Concise is Key",
      "",
      "The context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.",
      "",
      '**Default assumption: Claude is already very smart.** Only add context Claude doesn\'t already have. Challenge each piece of information: "Does Claude really need this explanation?" and "Does this paragraph justify its token cost?"',
      "",
      "Prefer concise examples over verbose explanations.",
      "",
      "### Set Appropriate Degrees of Freedom",
      "",
      "Match the level of specificity to the task's fragility and variability:",
      "",
      "**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.",
      "",
      "**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.",
      "",
      "**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.",
      "",
      "Think of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).",
      "",
      "### Anatomy of a Skill",
      "",
      "Every skill consists of a required SKILL.md file and optional bundled resources:",
      "",
      "```",
      "skill-name/",
      "├── SKILL.md (required)",
      "│   ├── YAML frontmatter metadata (required)",
      "│   │   ├── name: (required)",
      "│   │   └── description: (required)",
      "│   └── Markdown instructions (required)",
      "└── Bundled Resources (optional)",
      "    ├── scripts/          - Executable code (Python/Bash/etc.)",
      "    ├── references/       - Documentation intended to be loaded into context as needed",
      "    └── assets/           - Files used in output (templates, icons, fonts, etc.)",
      "```",
      "",
      "#### SKILL.md (required)",
      "",
      "Every SKILL.md consists of:",
      "",
      "- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.",
      "- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).",
      "",
      "#### Bundled Resources (optional)",
      "",
      "##### Scripts (`scripts/`)",
      "",
      "Executable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.",
      "",
      "- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed",
      "- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks",
      "- **Benefits**: Token efficient, deterministic, may be executed without loading into context",
      "- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments",
      "",
      "##### References (`references/`)",
      "",
      "Documentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.",
      "",
      "- **When to include**: For documentation that Claude should reference while working",
      "- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications",
      "- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides",
      "- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed",
      "- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md",
      "- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.",
      "",
      "##### Assets (`assets/`)",
      "",
      "Files not intended to be loaded into context, but rather used within the output Claude produces.",
      "",
      "- **When to include**: When the skill needs files that will be used in the final output",
      "- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography",
      "- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified",
      "- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context",
      "",
      "#### What to Not Include in a Skill",
      "",
      "A skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:",
      "",
      "- README.md",
      "- INSTALLATION_GUIDE.md",
      "- QUICK_REFERENCE.md",
      "- CHANGELOG.md",
      "- etc.",
      "",
      "The skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.",
      "",
      "### Progressive Disclosure Design Principle",
      "",
      "Skills use a three-level loading system to manage context efficiently:",
      "",
      "1. **Metadata (name + description)** - Always in context (~100 words)",
      "2. **SKILL.md body** - When skill triggers (<5k words)",
      "3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)",
      "",
      "#### Progressive Disclosure Patterns",
      "",
      "Keep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.",
      "",
      "**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.",
      "",
      "**Pattern 1: High-level guide with references**",
      "",
      "```markdown",
      "# PDF Processing",
      "",
      "## Quick start",
      "",
      "Extract text with pdfplumber:",
      "[code example]",
      "",
      "## Advanced features",
      "",
      "- **Form filling**: See [FORMS.md](FORMS.md) for complete guide",
      "- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods",
      "- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns",
      "```",
      "",
      "Claude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.",
      "",
      "**Pattern 2: Domain-specific organization**",
      "",
      "For Skills with multiple domains, organize content by domain to avoid loading irrelevant context:",
      "",
      "```",
      "bigquery-skill/",
      "├── SKILL.md (overview and navigation)",
      "└── reference/",
      "    ├── finance.md (revenue, billing metrics)",
      "    ├── sales.md (opportunities, pipeline)",
      "    ├── product.md (API usage, features)",
      "    └── marketing.md (campaigns, attribution)",
      "```",
      "",
      "When a user asks about sales metrics, Claude only reads sales.md.",
      "",
      "Similarly, for skills supporting multiple frameworks or variants, organize by variant:",
      "",
      "```",
      "cloud-deploy/",
      "├── SKILL.md (workflow + provider selection)",
      "└── references/",
      "    ├── aws.md (AWS deployment patterns)",
      "    ├── gcp.md (GCP deployment patterns)",
      "    └── azure.md (Azure deployment patterns)",
      "```",
      "",
      "When the user chooses AWS, Claude only reads aws.md.",
      "",
      "**Pattern 3: Conditional details**",
      "",
      "Show basic content, link to advanced content:",
      "",
      "```markdown",
      "# DOCX Processing",
      "",
      "## Creating documents",
      "",
      "Use docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).",
      "",
      "## Editing documents",
      "",
      "For simple edits, modify the XML directly.",
      "",
      "**For tracked changes**: See [REDLINING.md](REDLINING.md)",
      "**For OOXML details**: See [OOXML.md](OOXML.md)",
      "```",
      "",
      "Claude reads REDLINING.md or OOXML.md only when the user needs those features.",
      "",
      "**Important guidelines:**",
      "",
      "- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.",
      "- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.",
      "",
      "## Skill Creation Process",
      "",
      "Skill creation involves these steps:",
      "",
      "1. Understand the skill with concrete examples",
      "2. Plan reusable skill contents (scripts, references, assets)",
      "3. Initialize the skill (run init_skill.py)",
      "4. Edit the skill (implement resources and write SKILL.md)",
      "5. Package the skill (run package_skill.py)",
      "6. Iterate based on real usage",
      "",
      "Follow these steps in order, skipping only if there is a clear reason why they are not applicable.",
      "",
      "### Step 1: Understanding the Skill with Concrete Examples",
      "",
      "Skip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.",
      "",
      "To create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.",
      "",
      "For example, when building an image-editor skill, relevant questions include:",
      "",
      '- "What functionality should the image-editor skill support? Editing, rotating, anything else?"',
      '- "Can you give some examples of how this skill would be used?"',
      "- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"",
      '- "What would a user say that should trigger this skill?"',
      "",
      "To avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.",
      "",
      "Conclude this step when there is a clear sense of the functionality the skill should support.",
      "",
      "### Step 2: Planning the Reusable Skill Contents",
      "",
      "To turn concrete examples into an effective skill, analyze each example by:",
      "",
      "1. Considering how to execute on the example from scratch",
      "2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly",
      "",
      'Example: When building a `pdf-editor` skill to handle queries like "Help me rotate this PDF," the analysis shows:',
      "",
      "1. Rotating a PDF requires re-writing the same code each time",
      "2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill",
      "",
      'Example: When designing a `frontend-webapp-builder` skill for queries like "Build me a todo app" or "Build me a dashboard to track my steps," the analysis shows:',
      "",
      "1. Writing a frontend webapp requires the same boilerplate HTML/React each time",
      "2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill",
      "",
      'Example: When building a `big-query` skill to handle queries like "How many users have logged in today?" the analysis shows:',
      "",
      "1. Querying BigQuery requires re-discovering the table schemas and relationships each time",
      "2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill",
      "",
      "To establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.",
      "",
      "### Step 3: Initializing the Skill",
      "",
      "At this point, it is time to actually create the skill.",
      "",
      "Skip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.",
      "",
      "When creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.",
      "",
      "Usage:",
      "",
      "```bash",
      "scripts/init_skill.py <skill-name> --path <output-directory>",
      "```",
      "",
      "The script:",
      "",
      "- Creates the skill directory at the specified path",
      "- Generates a SKILL.md template with proper frontmatter and TODO placeholders",
      "- Creates example resource directories: `scripts/`, `references/`, and `assets/`",
      "- Adds example files in each directory that can be customized or deleted",
      "",
      "After initialization, customize or remove the generated SKILL.md and example files as needed.",
      "",
      "### Step 4: Edit the Skill",
      "",
      "When editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.",
      "",
      "#### Learn Proven Design Patterns",
      "",
      "Consult these helpful guides based on your skill's needs:",
      "",
      "- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic",
      "- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns",
      "",
      "These files contain established best practices for effective skill design.",
      "",
      "#### Start with Reusable Skill Contents",
      "",
      "To begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.",
      "",
      "Added scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.",
      "",
      "Any example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.",
      "",
      "#### Update SKILL.md",
      "",
      "**Writing Guidelines:** Always use imperative/infinitive form.",
      "",
      "##### Frontmatter",
      "",
      "Write the YAML frontmatter with `name` and `description`:",
      "",
      "- `name`: The skill name",
      "- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.",
      "  - Include both what the Skill does and specific triggers/contexts for when to use it.",
      '  - Include all "when to use" information here - Not in the body. The body is only loaded after triggering, so "When to Use This Skill" sections in the body are not helpful to Claude.',
      '  - Example description for a `docx` skill: "Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks"',
      "",
      "Do not include any other fields in YAML frontmatter.",
      "",
      "##### Body",
      "",
      "Write instructions for using the skill and its bundled resources.",
      "",
      "### Step 5: Packaging a Skill",
      "",
      "Once development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:",
      "",
      "```bash",
      "scripts/package_skill.py <path/to/skill-folder>",
      "```",
      "",
      "Optional output directory specification:",
      "",
      "```bash",
      "scripts/package_skill.py <path/to/skill-folder> ./dist",
      "```",
      "",
      "The packaging script will:",
      "",
      "1. **Validate** the skill automatically, checking:",
      "",
      "   - YAML frontmatter format and required fields",
      "   - Skill naming conventions and directory structure",
      "   - Description completeness and quality",
      "   - File organization and resource references",
      "",
      "2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.",
      "",
      "If validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.",
      "",
      "### Step 6: Iterate",
      "",
      "After testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.",
      "",
      "**Iteration workflow:**",
      "",
      "1. Use the skill on real tasks",
      "2. Notice struggles or inefficiencies",
      "3. Identify how SKILL.md or bundled resources should be updated",
      "4. Implement changes and test again",
      "",
    ].join("\n"),
  },
  xlsx: {
    "SKILL.md": [
      "---",
      "name: xlsx",
      'description: "Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas"',
      "license: Proprietary. LICENSE.txt has complete terms",
      "---",
      "",
      "# Requirements for Outputs",
      "",
      "## All Excel files",
      "",
      "### Zero Formula Errors",
      "- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)",
      "",
      "### Preserve Existing Templates (when updating templates)",
      "- Study and EXACTLY match existing format, style, and conventions when modifying files",
      "- Never impose standardized formatting on files with established patterns",
      "- Existing template conventions ALWAYS override these guidelines",
      "",
      "## Financial models",
      "",
      "### Color Coding Standards",
      "Unless otherwise stated by the user or existing template",
      "",
      "#### Industry-Standard Color Conventions",
      "- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios",
      "- **Black text (RGB: 0,0,0)**: ALL formulas and calculations",
      "- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook",
      "- **Red text (RGB: 255,0,0)**: External links to other files",
      "- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated",
      "",
      "### Number Formatting Standards",
      "",
      "#### Required Format Rules",
      '- **Years**: Format as text strings (e.g., "2024" not "2,024")',
      '- **Currency**: Use $#,##0 format; ALWAYS specify units in headers ("Revenue ($mm)")',
      '- **Zeros**: Use number formatting to make all zeros "-", including percentages (e.g., "$#,##0;($#,##0);-")',
      "- **Percentages**: Default to 0.0% format (one decimal)",
      "- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)",
      "- **Negative numbers**: Use parentheses (123) not minus -123",
      "",
      "### Formula Construction Rules",
      "",
      "#### Assumptions Placement",
      "- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells",
      "- Use cell references instead of hardcoded values in formulas",
      "- Example: Use =B5*(1+$B$6) instead of =B5*1.05",
      "",
      "#### Formula Error Prevention",
      "- Verify all cell references are correct",
      "- Check for off-by-one errors in ranges",
      "- Ensure consistent formulas across all projection periods",
      "- Test with edge cases (zero values, negative numbers)",
      "- Verify no unintended circular references",
      "",
      "#### Documentation Requirements for Hardcodes",
      '- Comment or in cells beside (if end of table). Format: "Source: [System/Document], [Date], [Specific Reference], [URL if applicable]"',
      "- Examples:",
      '  - "Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]"',
      '  - "Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]"',
      '  - "Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity"',
      '  - "Source: FactSet, 8/20/2025, Consensus Estimates Screen"',
      "",
      "# XLSX creation, editing, and analysis",
      "",
      "## Overview",
      "",
      "A user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.",
      "",
      "## Important Requirements",
      "",
      "**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run",
      "",
      "## Reading and analyzing data",
      "",
      "### Data analysis with pandas",
      "For data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:",
      "",
      "```python",
      "import pandas as pd",
      "",
      "# Read Excel",
      "df = pd.read_excel('file.xlsx')  # Default: first sheet",
      "all_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict",
      "",
      "# Analyze",
      "df.head()      # Preview data",
      "df.info()      # Column info",
      "df.describe()  # Statistics",
      "",
      "# Write Excel",
      "df.to_excel('output.xlsx', index=False)",
      "```",
      "",
      "## Excel File Workflows",
      "",
      "## CRITICAL: Use Formulas, Not Hardcoded Values",
      "",
      "**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.",
      "",
      "### ❌ WRONG - Hardcoding Calculated Values",
      "```python",
      "# Bad: Calculating in Python and hardcoding result",
      "total = df['Sales'].sum()",
      "sheet['B10'] = total  # Hardcodes 5000",
      "",
      "# Bad: Computing growth rate in Python",
      "growth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']",
      "sheet['C5'] = growth  # Hardcodes 0.15",
      "",
      "# Bad: Python calculation for average",
      "avg = sum(values) / len(values)",
      "sheet['D20'] = avg  # Hardcodes 42.5",
      "```",
      "",
      "### ✅ CORRECT - Using Excel Formulas",
      "```python",
      "# Good: Let Excel calculate the sum",
      "sheet['B10'] = '=SUM(B2:B9)'",
      "",
      "# Good: Growth rate as Excel formula",
      "sheet['C5'] = '=(C4-C2)/C2'",
      "",
      "# Good: Average using Excel function",
      "sheet['D20'] = '=AVERAGE(D2:D19)'",
      "```",
      "",
      "This applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.",
      "",
      "## Common Workflow",
      "1. **Choose tool**: pandas for data, openpyxl for formulas/formatting",
      "2. **Create/Load**: Create new workbook or load existing file",
      "3. **Modify**: Add/edit data, formulas, and formatting",
      "4. **Save**: Write to file",
      "5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script",
      "   ```bash",
      "   python recalc.py output.xlsx",
      "   ```",
      "6. **Verify and fix any errors**: ",
      "   - The script returns JSON with error details",
      "   - If `status` is `errors_found`, check `error_summary` for specific error types and locations",
      "   - Fix the identified errors and recalculate again",
      "   - Common errors to fix:",
      "     - `#REF!`: Invalid cell references",
      "     - `#DIV/0!`: Division by zero",
      "     - `#VALUE!`: Wrong data type in formula",
      "     - `#NAME?`: Unrecognized formula name",
      "",
      "### Creating new Excel files",
      "",
      "```python",
      "# Using openpyxl for formulas and formatting",
      "from openpyxl import Workbook",
      "from openpyxl.styles import Font, PatternFill, Alignment",
      "",
      "wb = Workbook()",
      "sheet = wb.active",
      "",
      "# Add data",
      "sheet['A1'] = 'Hello'",
      "sheet['B1'] = 'World'",
      "sheet.append(['Row', 'of', 'data'])",
      "",
      "# Add formula",
      "sheet['B2'] = '=SUM(A1:A10)'",
      "",
      "# Formatting",
      "sheet['A1'].font = Font(bold=True, color='FF0000')",
      "sheet['A1'].fill = PatternFill('solid', start_color='FFFF00')",
      "sheet['A1'].alignment = Alignment(horizontal='center')",
      "",
      "# Column width",
      "sheet.column_dimensions['A'].width = 20",
      "",
      "wb.save('output.xlsx')",
      "```",
      "",
      "### Editing existing Excel files",
      "",
      "```python",
      "# Using openpyxl to preserve formulas and formatting",
      "from openpyxl import load_workbook",
      "",
      "# Load existing file",
      "wb = load_workbook('existing.xlsx')",
      "sheet = wb.active  # or wb['SheetName'] for specific sheet",
      "",
      "# Working with multiple sheets",
      "for sheet_name in wb.sheetnames:",
      "    sheet = wb[sheet_name]",
      '    print(f"Sheet: {sheet_name}")',
      "",
      "# Modify cells",
      "sheet['A1'] = 'New Value'",
      "sheet.insert_rows(2)  # Insert row at position 2",
      "sheet.delete_cols(3)  # Delete column 3",
      "",
      "# Add new sheet",
      "new_sheet = wb.create_sheet('NewSheet')",
      "new_sheet['A1'] = 'Data'",
      "",
      "wb.save('modified.xlsx')",
      "```",
      "",
      "## Recalculating formulas",
      "",
      "Excel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:",
      "",
      "```bash",
      "python recalc.py <excel_file> [timeout_seconds]",
      "```",
      "",
      "Example:",
      "```bash",
      "python recalc.py output.xlsx 30",
      "```",
      "",
      "The script:",
      "- Automatically sets up LibreOffice macro on first run",
      "- Recalculates all formulas in all sheets",
      "- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)",
      "- Returns JSON with detailed error locations and counts",
      "- Works on both Linux and macOS",
      "",
      "## Formula Verification Checklist",
      "",
      "Quick checks to ensure formulas work correctly:",
      "",
      "### Essential Verification",
      "- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model",
      "- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)",
      "- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)",
      "",
      "### Common Pitfalls",
      "- [ ] **NaN handling**: Check for null values with `pd.notna()`",
      "- [ ] **Far-right columns**: FY data often in columns 50+ ",
      "- [ ] **Multiple matches**: Search all occurrences, not just first",
      "- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)",
      "- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)",
      "- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets",
      "",
      "### Formula Testing Strategy",
      "- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly",
      "- [ ] **Verify dependencies**: Check all cells referenced in formulas exist",
      "- [ ] **Test edge cases**: Include zero, negative, and very large values",
      "",
      "### Interpreting recalc.py Output",
      "The script returns JSON with error details:",
      "```json",
      "{",
      '  "status": "success",           // or "errors_found"',
      '  "total_errors": 0,              // Total error count',
      '  "total_formulas": 42,           // Number of formulas in file',
      '  "error_summary": {              // Only present if errors found',
      '    "#REF!": {',
      '      "count": 2,',
      '      "locations": ["Sheet1!B5", "Sheet1!C10"]',
      "    }",
      "  }",
      "}",
      "```",
      "",
      "## Best Practices",
      "",
      "### Library Selection",
      "- **pandas**: Best for data analysis, bulk operations, and simple data export",
      "- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features",
      "",
      "### Working with openpyxl",
      "- Cell indices are 1-based (row=1, column=1 refers to cell A1)",
      "- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`",
      "- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost",
      "- For large files: Use `read_only=True` for reading or `write_only=True` for writing",
      "- Formulas are preserved but not evaluated - use recalc.py to update values",
      "",
      "### Working with pandas",
      "- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`",
      "- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`",
      "- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`",
      "",
      "## Code Style Guidelines",
      "**IMPORTANT**: When generating Python code for Excel operations:",
      "- Write minimal, concise Python code without unnecessary comments",
      "- Avoid verbose variable names and redundant operations",
      "- Avoid unnecessary print statements",
      "",
      "**For Excel files themselves**:",
      "- Add comments to cells with complex formulas or important assumptions",
      "- Document data sources for hardcoded values",
      "- Include notes for key calculations and model sections",
    ].join("\n"),
  },
};
